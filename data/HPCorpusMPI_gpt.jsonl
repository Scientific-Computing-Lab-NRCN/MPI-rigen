{"program": "utluiz_1", "code": "int main(int argc, char *argv[]) {\n\tint\t\t\tmy_rank;\t\t\n\n\tint\t\t\tnum_procs;\t\t\n\n\tint\t\t\tsource;\t\t\t\n\n\tint\t\t\tdest = 0;\t\t\n\n\tint\t\t\ttag = 0;\t\t\n\n\tchar\t\tmessage[100];\t\n\n\tMPI_Status\tstatus ;\t\t\n\n\n\t\n\n\t\n\n\t\n\n\t\n\n\n\tif ( argc != 4 ) {\n\t\tputs(\"Arguments are incorrect!\");\n\t\texit(0);\n\t}\n\n\t\n\n\tint verbose = strtol(argv[2], NULL, 10);\n\n\tif (verbose) puts(\"---BEGIN---\");\n\n\t\n\n\tif (verbose)\n\t\tprintf(\"Input file: '%s', process number: %i, # of processes: %i\\n\\n\",\n\t\t\t\targv[1], my_rank, num_procs);\n\n\t\n\n\tif (verbose && 0) {\n\t\tif (my_rank != 0) {\n\t\t\t\n\n\t\t\tsnprintf(message,26, \"Greetings from process %d!\", my_rank);\n\t\t\t\n\n\t\t} else {\n\t\t\tprintf(\"Num processes: %d\\n\",num_procs);\n\t\t\tfor (source = 1; source < num_procs; source++) {\n\t\t\t\tprintf(\"Process 0 received \\\"%s\\\"\\n\",message);\n\t\t\t}\n\n\t\t\t\n\n\t\t\tsnprintf(message, 26, \"Hi, how are you?           \");\n\t\t}\n\n\t\tif (my_rank != 0) {\n\t\t\tprintf(\"Process %d received \\\"%s\\\"\\n\", my_rank, message);\n\t\t}\n\n\t}\n\n\t\n\n\tmatrix *m = matrix_load(argv[1]);\n\n\t\n\n\tif (verbose && my_rank == 0)\n\t\tmatrix_print(m);\n\n\t\n\n\ttimer* t;\n\tif (my_rank == 0)\n\t\tt = start_timer();\n\n\tjacobi_result* result = jacobi_mpi(m, verbose, my_rank, num_procs);\n\n\t\n\n\tif (my_rank == 0)\n\t\tstop_timer(t, verbose);\n\n\t\n\n\tif (verbose && my_rank == 0 && result != NULL) {\n\t\tint i;\n\t\tprintf(\"\\nResults: \");\n\t\tfor (i = 0; i < m->size; i++) {\n\t\t\tprintf(\"%f, \", result->x[i]);\n\t\t}\n\t\tprintf(\"\\nIterations: %i \", result->k);\n\t}\n\n\t\n\n\tif (my_rank == 0)\n\t\twrite_results(t, argv[1], num_procs, 'M', m->size, argv[3]);\n\n\t\n\n\tmatrix_destroy(m);\n\n\t\n\n\tif (my_rank == 0 && result != NULL) {\n\t\tfree(t);\n\t\t\n\n\t\tfree(result);\n\t}\n\n\tif (verbose) puts(\"\\n\\n---END---\");\n\n\t\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tint\t\t\tmy_rank;\t\t\n\n\tint\t\t\tnum_procs;\t\t\n\n\tint\t\t\tsource;\t\t\t\n\n\tint\t\t\tdest = 0;\t\t\n\n\tint\t\t\ttag = 0;\t\t\n\n\tchar\t\tmessage[100];\t\n\n\tMPI_Status\tstatus ;\t\t\n\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\t\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n\tif ( argc != 4 ) {\n\t\tputs(\"Arguments are incorrect!\");\n\t\tMPI_Finalize();\n\t\texit(0);\n\t}\n\n\t\n\n\tint verbose = strtol(argv[2], NULL, 10);\n\n\tif (verbose) puts(\"---BEGIN---\");\n\n\t\n\n\tif (verbose)\n\t\tprintf(\"Input file: '%s', process number: %i, # of processes: %i\\n\\n\",\n\t\t\t\targv[1], my_rank, num_procs);\n\n\t\n\n\tif (verbose && 0) {\n\t\tif (my_rank != 0) {\n\t\t\t\n\n\t\t\tsnprintf(message,26, \"Greetings from process %d!\", my_rank);\n\t\t\t\n\n\t\t\tMPI_Send(message, strlen(message)+1, MPI_CHAR,\n\t\t\t\t\tdest, tag, MPI_COMM_WORLD);\n\t\t} else {\n\t\t\tprintf(\"Num processes: %d\\n\",num_procs);\n\t\t\tfor (source = 1; source < num_procs; source++) {\n\t\t\t\tMPI_Recv(message, 100, MPI_CHAR, source, tag,\n\t\t\t\t\t\tMPI_COMM_WORLD, &status);\n\t\t\t\tprintf(\"Process 0 received \\\"%s\\\"\\n\",message);\n\t\t\t}\n\n\t\t\t\n\n\t\t\tsnprintf(message, 26, \"Hi, how are you?           \");\n\t\t}\n\t\tMPI_Bcast(message, strlen(message)+1, MPI_CHAR, dest, MPI_COMM_WORLD);\n\n\t\tif (my_rank != 0) {\n\t\t\tprintf(\"Process %d received \\\"%s\\\"\\n\", my_rank, message);\n\t\t}\n\n\t}\n\n\t\n\n\tmatrix *m = matrix_load(argv[1]);\n\n\t\n\n\tif (verbose && my_rank == 0)\n\t\tmatrix_print(m);\n\n\t\n\n\ttimer* t;\n\tif (my_rank == 0)\n\t\tt = start_timer();\n\n\tjacobi_result* result = jacobi_mpi(m, verbose, my_rank, num_procs);\n\n\t\n\n\tif (my_rank == 0)\n\t\tstop_timer(t, verbose);\n\n\t\n\n\tif (verbose && my_rank == 0 && result != NULL) {\n\t\tint i;\n\t\tprintf(\"\\nResults: \");\n\t\tfor (i = 0; i < m->size; i++) {\n\t\t\tprintf(\"%f, \", result->x[i]);\n\t\t}\n\t\tprintf(\"\\nIterations: %i \", result->k);\n\t}\n\n\t\n\n\tif (my_rank == 0)\n\t\twrite_results(t, argv[1], num_procs, 'M', m->size, argv[3]);\n\n\t\n\n\tmatrix_destroy(m);\n\n\t\n\n\tif (my_rank == 0 && result != NULL) {\n\t\tfree(t);\n\t\t\n\n\t\tfree(result);\n\t}\n\n\tif (verbose) puts(\"\\n\\n---END---\");\n\n\t\n\n\tMPI_Finalize(); \n\n\treturn 0;\n}"}
{"program": "mpip_6", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  double *planned_in, *executed_in;\n  pfft_complex *planned_out, *executed_out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT| PFFT_PADDED_R2C,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  planned_in  = pfft_alloc_real (2 * alloc_local);\n  planned_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, planned_in, planned_out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_PADDED_R2C);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, planned_out, planned_in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_PADDED_C2R);\n\n  \n\n  pfft_free(planned_in); pfft_free(planned_out);\n\n  \n\n  executed_in  = pfft_alloc_real(2 * alloc_local);\n  executed_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      executed_in);\n  \n  \n\n  pfft_execute_dft_r2c(plan_forw, executed_in, executed_out);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      executed_in);\n\n  \n\n  pfft_execute_dft_c2r(plan_back, executed_out, executed_in);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    executed_in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, executed_in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(executed_in); pfft_free(executed_out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  double *planned_in, *executed_in;\n  pfft_complex *planned_out, *executed_out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT| PFFT_PADDED_R2C,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  planned_in  = pfft_alloc_real (2 * alloc_local);\n  planned_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, planned_in, planned_out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_PADDED_R2C);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, planned_out, planned_in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_PADDED_C2R);\n\n  \n\n  pfft_free(planned_in); pfft_free(planned_out);\n\n  \n\n  executed_in  = pfft_alloc_real(2 * alloc_local);\n  executed_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      executed_in);\n  \n  \n\n  pfft_execute_dft_r2c(plan_forw, executed_in, executed_out);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      executed_in);\n\n  \n\n  pfft_execute_dft_c2r(plan_back, executed_out, executed_in);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    executed_in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, executed_in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(executed_in); pfft_free(executed_out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "annavicente_7", "code": "int main(int argc, char **argv)\r\n{\r\n    double a, b, dx;\r\n    int N, i, rank, size;\r\n\r\n\r\n\r\n    printf(\"Rank %d de um total de %d\\n\", rank, size);\r\n\r\n    if (argc<4)\r\n    {\r\n        printf(\"Numero de argumentos insuficiente...\\n\");\r\n        exit(-1);\r\n    }\r\n\r\n    if(rank == 0)\r\n    {\r\n      double args[size][3];\r\n\r\n      a = (double) atof(argv[1]);\r\n      b = (double) atof(argv[2]);\r\n      N = atoi(argv[3]);\r\n\r\n      dx = (a+b)/size;\r\n\r\n      args[0][0] = a;  \n\r\n      args[0][1] = dx;  \n\r\n      args[0][2] = N/size; \n\r\n\r\n      for(i=1; i < size; i++){\r\n        args[i][0] = args[i-1][1];\r\n        args[i][1] = args[i][0] + dx;\r\n        args[i][2] = N/size;\r\n      }\r\n\r\n      printf(\"Ola, eu sou o processo 0\\n\");\r\n\r\n  \t\tdouble vet[1];\r\n  \t\tfor(int rank_i=1; rank_i <= size-1; rank_i++)\r\n  \t\t{\r\n  \t\t}\r\n\r\n  \t\tfor(int rank_i=1; rank_i < size; rank_i++)\r\n  \t\t{\r\n  \t\t\tintegral = integral + vet[0]; \n\r\n  \t\t}\r\n\r\n      CalculaArea(args[0]);\r\n      printf(\"Area= %.15lf\\n\", integral);\r\n    }\r\n    else\r\n    {\r\n      double vet_recv[size/2][3];\r\n  \t  double sum[1];\r\n      CalculaArea(vet_recv[0]);\r\n\r\n  \t\tsum[0] = integral;\r\n\r\n\r\n    }\r\n\r\n    return (0);\r\n}", "label": "int main(int argc, char **argv)\r\n{\r\n    double a, b, dx;\r\n    int N, i, rank, size;\r\n\r\n    MPI_Init(&argc, &argv);\r\n\r\n  \tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n  \tMPI_Comm_size(MPI_COMM_WORLD, &size);\r\n\r\n    printf(\"Rank %d de um total de %d\\n\", rank, size);\r\n\r\n    if (argc<4)\r\n    {\r\n        printf(\"Numero de argumentos insuficiente...\\n\");\r\n        exit(-1);\r\n    }\r\n\r\n    if(rank == 0)\r\n    {\r\n      double args[size][3];\r\n\r\n      a = (double) atof(argv[1]);\r\n      b = (double) atof(argv[2]);\r\n      N = atoi(argv[3]);\r\n\r\n      dx = (a+b)/size;\r\n\r\n      args[0][0] = a;  \n\r\n      args[0][1] = dx;  \n\r\n      args[0][2] = N/size; \n\r\n\r\n      for(i=1; i < size; i++){\r\n        args[i][0] = args[i-1][1];\r\n        args[i][1] = args[i][0] + dx;\r\n        args[i][2] = N/size;\r\n      }\r\n\r\n      printf(\"Ola, eu sou o processo 0\\n\");\r\n\r\n  \t\tdouble vet[1];\r\n  \t\tfor(int rank_i=1; rank_i <= size-1; rank_i++)\r\n  \t\t{\r\n  \t\t\t\tMPI_Send(&args[rank_i], 3, MPI_DOUBLE, rank_i, 123, MPI_COMM_WORLD);\r\n  \t\t}\r\n\r\n  \t\tfor(int rank_i=1; rank_i < size; rank_i++)\r\n  \t\t{\r\n  \t\t\tMPI_Recv(vet,1, MPI_DOUBLE, rank_i, 123, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\r\n  \t\t\tintegral = integral + vet[0]; \n\r\n  \t\t}\r\n\r\n      CalculaArea(args[0]);\r\n      printf(\"Area= %.15lf\\n\", integral);\r\n    }\r\n    else\r\n    {\r\n      double vet_recv[size/2][3];\r\n  \t  double sum[1];\r\n  \t\tMPI_Recv(vet_recv,3, MPI_DOUBLE, 0, 123, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\r\n      CalculaArea(vet_recv[0]);\r\n\r\n  \t\tsum[0] = integral;\r\n\r\n  \t\tMPI_Send(&sum, 1, MPI_DOUBLE, 0, 123, MPI_COMM_WORLD);\r\n\r\n    }\r\n\r\n    MPI_Finalize();\r\n    return (0);\r\n}"}
{"program": "pnlnum_8", "code": "nt main(int argc, char *argv[])\n{\n  int rank, nproc;\n\n  pnl_message_on();\n  if ( nproc != 2 )\n    {\n      if ( rank == 0 )\n        {\n          printf(\"Run the test with -np 2.\\n\");\n        }\n      exit (0);\n    }\n\n  if ( rank == 0 )\n    {\n      printf (\"--> DCMT\\n\"); fflush(stdout);\n      printf (\"--> MERSENNE\\n\");fflush(stdout);\n      printf (\"--> MRGK3\\n\"); fflush(stdout);\n      printf (\"--> MRGK5\\n\"); fflush(stdout);\n      printf (\"--> SHUFL\\n\"); fflush(stdout);\n      printf (\"--> LECUYER\\n\"); fflush(stdout);\n      printf (\"--> TAUSWORTHE\\n\"); fflush(stdout);\n      printf (\"--> SQRT\\n\"); fflush(stdout);\n      printf (\"--> HALTON\\n\"); fflush(stdout);\n      printf (\"--> FAURE\\n\"); fflush(stdout);\n      printf (\"--> NIEDERREITER\\n\"); fflush(stdout);\n      printf (\"--> SOBOL_I4\\n\"); fflush(stdout);\n      printf (\"--> SOBOL_I8\\n\"); fflush(stdout);\n    }\n  else\n    {\n    }\n\n  exit (0);\n}\n", "label": "nt main(int argc, char *argv[])\n{\n  int rank, nproc;\n  MPI_Init (&argc, &argv);\n  MPI_Comm_size (MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\n  pnl_message_on();\n  if ( nproc != 2 )\n    {\n      if ( rank == 0 )\n        {\n          printf(\"Run the test with -np 2.\\n\");\n        }\n      MPI_Finalize ();\n      exit (0);\n    }\n\n  if ( rank == 0 )\n    {\n      printf (\"--> DCMT\\n\"); fflush(stdout);\n      send_dcmt ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> MERSENNE\\n\");fflush(stdout);\n      send_rng (PNL_RNG_MERSENNE); fflush(stdout);  MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> MRGK3\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_MRGK3); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> MRGK5\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_MRGK5); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> SHUFL\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_SHUFL); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> LECUYER\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_LECUYER); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> TAUSWORTHE\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_TAUSWORTHE); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> SQRT\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_SQRT); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> HALTON\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_HALTON); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> FAURE\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_FAURE); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> NIEDERREITER\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_NIEDERREITER); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> SOBOL_I4\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_SOBOL_I4); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      printf (\"--> SOBOL_I8\\n\"); fflush(stdout);\n      send_rng (PNL_RNG_SOBOL_I8); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n    }\n  else\n    {\n      recv_dcmt (); fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n      recv_rng ();  fflush(stdout); MPI_Barrier(MPI_COMM_WORLD);\n    }\n\n  MPI_Finalize ();\n  exit (0);\n}\n"}
{"program": "bmi-forum_13", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tStream*      myStream    = Journal_Register( Info_Type, \"TestStream\" );\n\t\tchar*        string        = \"helloWorldHowDoYouDo\";\n\t\tdouble       doubleValue   = 3142e20;\n\t\tdouble       floatValue    = 2.173425;\n\t\tint          intValue      = 3;\n\t\tunsigned int uintValue     = 3980;\n\t\tint          char_I;\n\t\tchar         charValue     = 'V';\n\t\tdouble       doubleArray[] = { 10.23, 393.1, -89, 1231 };        \n\t\tIndex        uintArray[]   = { 10, 2021, 231, 2, 3, 4, 55 };\n\n\t\tStream_RedirectFile_WithPrependedPath( myStream, \"output\", \"output.dat\" );\n\n\t\tfor ( char_I = -1 ; char_I < 25 ; char_I++ ) {\n\t\t\tJournal_PrintString_WithLength( myStream, string, char_I );\n\t\t\tJournal_Printf( myStream, \"\\n\" );\n\t\t}\n\n\t\t\n\n\t\tJournal_PrintString( myStream, string );\n\t\tJournal_PrintValue( myStream, doubleValue );\n\t\tJournal_PrintValue( myStream, floatValue );\n\t\tJournal_PrintValue( myStream, intValue );\n\t\tJournal_PrintValue( myStream, uintValue );\n\t\tJournal_PrintChar(  myStream, charValue );\n\t\tJournal_PrintArray( myStream, doubleArray, 4 );\n\t\tJournal_PrintArray( myStream, uintArray, 7 );\n\n\t}\n\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tStream*      myStream    = Journal_Register( Info_Type, \"TestStream\" );\n\t\tchar*        string        = \"helloWorldHowDoYouDo\";\n\t\tdouble       doubleValue   = 3142e20;\n\t\tdouble       floatValue    = 2.173425;\n\t\tint          intValue      = 3;\n\t\tunsigned int uintValue     = 3980;\n\t\tint          char_I;\n\t\tchar         charValue     = 'V';\n\t\tdouble       doubleArray[] = { 10.23, 393.1, -89, 1231 };        \n\t\tIndex        uintArray[]   = { 10, 2021, 231, 2, 3, 4, 55 };\n\n\t\tStream_RedirectFile_WithPrependedPath( myStream, \"output\", \"output.dat\" );\n\n\t\tfor ( char_I = -1 ; char_I < 25 ; char_I++ ) {\n\t\t\tJournal_PrintString_WithLength( myStream, string, char_I );\n\t\t\tJournal_Printf( myStream, \"\\n\" );\n\t\t}\n\n\t\t\n\n\t\tJournal_PrintString( myStream, string );\n\t\tJournal_PrintValue( myStream, doubleValue );\n\t\tJournal_PrintValue( myStream, floatValue );\n\t\tJournal_PrintValue( myStream, intValue );\n\t\tJournal_PrintValue( myStream, uintValue );\n\t\tJournal_PrintChar(  myStream, charValue );\n\t\tJournal_PrintArray( myStream, doubleArray, 4 );\n\t\tJournal_PrintArray( myStream, uintArray, 7 );\n\n\t}\n\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "byu-vv-lab_16", "code": "int main(int argc, char * argv[])\n{\n    int rank;\n    int procs;\n    int* values;\n    \n    \n    if (rank == 0 || rank == 2) {\n        values = (int*)malloc(sizeof(int)*procs);\n        for(int i = 0; i < procs; i++){\n            values[i] = i;\n        }\n    }else{\n        values = (int*)malloc(sizeof(int));\n    }\n    \n#if defined TYPE\n    if (rank != 2)\n    else\n#elif defined ROOT\n    if (rank != 2)\n    else\n#else\n    if (rank != 2)\n#endif\n    \n    free(values);\n    return 0; \n}", "label": "int main(int argc, char * argv[])\n{\n    int rank;\n    int procs;\n    int* values;\n    \n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    if (rank == 0 || rank == 2) {\n        values = (int*)malloc(sizeof(int)*procs);\n        for(int i = 0; i < procs; i++){\n            values[i] = i;\n        }\n    }else{\n        values = (int*)malloc(sizeof(int));\n    }\n    \n#if defined TYPE\n    if (rank != 2)\n        MPI_Scatter(values, 1, MPI_INT, values, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Scatter(values, 1, MPI_FLOAT, values, 1, MPI_FLOAT, 0, MPI_COMM_WORLD);\n#elif defined ROOT\n    if (rank != 2)\n        MPI_Scatter(values, 1, MPI_INT, values, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Scatter(values, 1, MPI_INT, values, 1, MPI_INT, 2, MPI_COMM_WORLD);\n#else\n    if (rank != 2)\n        MPI_Scatter(values, 1, MPI_INT, values, 1, MPI_INT, 0, MPI_COMM_WORLD);\n#endif\n    \n    free(values);\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "bmi-forum_17", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tnumProcessors;\n\tint\t\t\tprocToWatch;\n\tDictionary*\t\tdictionary;\n\tExtensionManager_Register*\textensionMgr_Register;\n\tTopology*\t\tnTopology;\n\tElementLayout*\t\teLayout;\n\tNodeLayout*\t\tnLayout;\n\tHexaMD*\t\tdecomp;\n\tMeshLayout*\t\tmeshLayout;\n\tMesh*\t\t\tmesh;\n\t\n\t\n\n\t\n\tprocToWatch = (argc >= 2) ? atoi( argv[1] ) : 0;\n\t\n\tif ( rank == procToWatch ) printf( \"Watching rank %d\\n\", rank );\n\t\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\n\tdictionary = Dictionary_New();\n\t\n\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 33 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 33 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 33 ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"buildElementNodeTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"buildElementNeighbourTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"buildNodeElementTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"buildNodeNeighbourTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( True ) );\n\t\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tdecomp = HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\tmeshLayout = MeshLayout_New( \"MeshLayout\", eLayout, nLayout, (MeshDecomp*)decomp );\n\t\n\textensionMgr_Register = ExtensionManager_Register_New();\n\tmesh = Mesh_New( \"Mesh\", meshLayout, sizeof(Node), sizeof(Element), extensionMgr_Register, dictionary );\n\t\n\tif ( rank == procToWatch ) printf( \"Building mesh:\\n\" );\n\tBuild( mesh, 0, False );\n\tInitialise(mesh, 0, False );\n\tif ( rank == procToWatch ) printf( \"Done.\\n\" );\n\tif ( rank == procToWatch ) {\n\t\tPartition_Index proc_I;\n\n\t\tprintf( \"partitionedAxis: { %s, %s, %s }\\n\", decomp->partitionedAxis[0] ? \"True\" : \"False\", \n\t\t\tdecomp->partitionedAxis[1] ? \"True\" : \"False\", decomp->partitionedAxis[2] ? \"True\" : \"False\" );\n\t\tprintf( \"partitionCounts: { %u, %u, %u }\\n\", decomp->partition3DCounts[0], decomp->partition3DCounts[1], \n\t\t\tdecomp->partition3DCounts[2] );\n\t\t\n\t\tprintf( \"elementGlobalCounts: { %u, %u, %u }\\n\", decomp->elementGlobal3DCounts[0], decomp->elementGlobal3DCounts[1], \n\t\t\tdecomp->elementGlobal3DCounts[2] );\n\t\tfor( proc_I = 0; proc_I < decomp->procsInUse; proc_I++ ) {\n\t\t\tprintf( \"\\telementLocalCounts[%u]: { %u, %u, %u }\\n\", proc_I, decomp->elementLocal3DCounts[proc_I][0], \n\t\t\t\tdecomp->elementLocal3DCounts[proc_I][1], decomp->elementLocal3DCounts[proc_I][2] );\n\t\t}\n\t\t\n\t\tprintf( \"nodeGlobalCounts: { %u, %u, %u }\\n\", decomp->nodeGlobal3DCounts[0], decomp->nodeGlobal3DCounts[1], \n\t\t\tdecomp->nodeGlobal3DCounts[2] );\n\t\tfor( proc_I = 0; proc_I < decomp->procsInUse; proc_I++ ) {\n\t\t\tprintf( \"\\tnodeLocalCounts[%u]: { %u, %u, %u }\\n\", proc_I, decomp->nodeLocal3DCounts[proc_I][0], \n\t\t\t\tdecomp->nodeLocal3DCounts[proc_I][1], decomp->nodeLocal3DCounts[proc_I][2] );\n\t\t}\n\t}\n\t\n\tStg_Class_Delete( mesh );\n\tStg_Class_Delete( extensionMgr_Register );\n\tStg_Class_Delete( meshLayout );\n\tStg_Class_Delete( decomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( eLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( dictionary );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tnumProcessors;\n\tint\t\t\tprocToWatch;\n\tDictionary*\t\tdictionary;\n\tExtensionManager_Register*\textensionMgr_Register;\n\tTopology*\t\tnTopology;\n\tElementLayout*\t\teLayout;\n\tNodeLayout*\t\tnLayout;\n\tHexaMD*\t\tdecomp;\n\tMeshLayout*\t\tmeshLayout;\n\tMesh*\t\t\tmesh;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tprocToWatch = (argc >= 2) ? atoi( argv[1] ) : 0;\n\t\n\tif ( rank == procToWatch ) printf( \"Watching rank %d\\n\", rank );\n\t\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\n\tdictionary = Dictionary_New();\n\t\n\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 33 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 33 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 33 ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"buildElementNodeTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"buildElementNeighbourTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"buildNodeElementTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"buildNodeNeighbourTbl\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( True ) );\n\t\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tdecomp = HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\tmeshLayout = MeshLayout_New( \"MeshLayout\", eLayout, nLayout, (MeshDecomp*)decomp );\n\t\n\textensionMgr_Register = ExtensionManager_Register_New();\n\tmesh = Mesh_New( \"Mesh\", meshLayout, sizeof(Node), sizeof(Element), extensionMgr_Register, dictionary );\n\t\n\tif ( rank == procToWatch ) printf( \"Building mesh:\\n\" );\n\tBuild( mesh, 0, False );\n\tInitialise(mesh, 0, False );\n\tif ( rank == procToWatch ) printf( \"Done.\\n\" );\n\tif ( rank == procToWatch ) {\n\t\tPartition_Index proc_I;\n\n\t\tprintf( \"partitionedAxis: { %s, %s, %s }\\n\", decomp->partitionedAxis[0] ? \"True\" : \"False\", \n\t\t\tdecomp->partitionedAxis[1] ? \"True\" : \"False\", decomp->partitionedAxis[2] ? \"True\" : \"False\" );\n\t\tprintf( \"partitionCounts: { %u, %u, %u }\\n\", decomp->partition3DCounts[0], decomp->partition3DCounts[1], \n\t\t\tdecomp->partition3DCounts[2] );\n\t\t\n\t\tprintf( \"elementGlobalCounts: { %u, %u, %u }\\n\", decomp->elementGlobal3DCounts[0], decomp->elementGlobal3DCounts[1], \n\t\t\tdecomp->elementGlobal3DCounts[2] );\n\t\tfor( proc_I = 0; proc_I < decomp->procsInUse; proc_I++ ) {\n\t\t\tprintf( \"\\telementLocalCounts[%u]: { %u, %u, %u }\\n\", proc_I, decomp->elementLocal3DCounts[proc_I][0], \n\t\t\t\tdecomp->elementLocal3DCounts[proc_I][1], decomp->elementLocal3DCounts[proc_I][2] );\n\t\t}\n\t\t\n\t\tprintf( \"nodeGlobalCounts: { %u, %u, %u }\\n\", decomp->nodeGlobal3DCounts[0], decomp->nodeGlobal3DCounts[1], \n\t\t\tdecomp->nodeGlobal3DCounts[2] );\n\t\tfor( proc_I = 0; proc_I < decomp->procsInUse; proc_I++ ) {\n\t\t\tprintf( \"\\tnodeLocalCounts[%u]: { %u, %u, %u }\\n\", proc_I, decomp->nodeLocal3DCounts[proc_I][0], \n\t\t\t\tdecomp->nodeLocal3DCounts[proc_I][1], decomp->nodeLocal3DCounts[proc_I][2] );\n\t\t}\n\t}\n\t\n\tStg_Class_Delete( mesh );\n\tStg_Class_Delete( extensionMgr_Register );\n\tStg_Class_Delete( meshLayout );\n\tStg_Class_Delete( decomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( eLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( dictionary );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "libsmelt_18", "code": "int main(int argc, char **argv){\n\tint rank, size, root=0;\t\n \t\n\n\n    char outname[128];\n    if (argc == 2) {\n        sprintf(outname, \"ab_mpisync_%d\", size);\n    } else {\n        sprintf(outname, \"ab_mpi_%d\", size);\n    }\n    uint64_t *buf = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    sk_m_init(&mes, NVALUES, outname, buf);\n\n    char a = 0;\n    char global = 0;\n\tfor(int n=0; n<NITERS; n++){\n        \n\n        if (argc == 2) {\n            dissem_bar();\n            dissem_bar();\n        }\n\n        sk_m_restart_tsc(&mes);\n        sk_m_add(&mes);\n\t}\n\n    uint64_t *buf2 = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    if (rank == 0) {\n        sk_m_print(&mes);\n        for (int i = 1; i < size; i++) {\n            for (uint32_t j=0; j< NVALUES; j++) {\n                char name[100];\n                printf(\"sk_m_print(%d,%s) idx= %d tscdiff= %ld\\n\",\n                       i, outname, j, buf2[j]);\n            }   \n        }\n    } else {\n    }\n\n}", "label": "int main(int argc, char **argv){\n\tint rank, size, root=0;\t\n \t\n\tMPI_Init(&argc,&argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\tMPI_Comm_size(MPI_COMM_WORLD,&size);\n\n    char outname[128];\n    if (argc == 2) {\n        sprintf(outname, \"ab_mpisync_%d\", size);\n    } else {\n        sprintf(outname, \"ab_mpi_%d\", size);\n    }\n    uint64_t *buf = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    sk_m_init(&mes, NVALUES, outname, buf);\n\n    char a = 0;\n    char global = 0;\n\tfor(int n=0; n<NITERS; n++){\n        \n\n        if (argc == 2) {\n            dissem_bar();\n            dissem_bar();\n        }\n\n        sk_m_restart_tsc(&mes);\n        MPI_Bcast((void*) &a, 1, MPI_BYTE, 0, MPI_COMM_WORLD);\n        sk_m_add(&mes);\n\t}\n\n    uint64_t *buf2 = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    if (rank == 0) {\n        sk_m_print(&mes);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(buf2, NVALUES, MPI_UINT64_T, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (uint32_t j=0; j< NVALUES; j++) {\n                char name[100];\n                printf(\"sk_m_print(%d,%s) idx= %d tscdiff= %ld\\n\",\n                       i, outname, j, buf2[j]);\n            }   \n        }\n    } else {\n        MPI_Send(buf, NVALUES,MPI_UINT64_T, 0, 0, MPI_COMM_WORLD);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n\tMPI_Finalize();\n}"}
{"program": "likesmusik_19", "code": "int\nmain(int argc, char** argv) {\n    struct options options;\n    struct calculation_arguments arguments;\n    struct calculation_results results;\n\n    \n\n    options.rank = -1;\n    options.size = -1;\n\n    \n\n\n    \n\n    AskParams(&options, argc, argv);\n\n    \n\n    initVariables(&arguments, &results, &options);\n    allocateMatrices(&arguments);\n    initMatrices(&arguments, &options);\n\n    gettimeofday(&start_time, NULL); \n\n    if (options.method == METH_JACOBI) {\n        \n\n    } else {\n        \n\n        calculateGaussSeidel(&arguments, &results, &options); \n    }\n    gettimeofday(&comp_time, NULL); \n\n\n    \n    \n\n    if (options.rank <= 0) {\n        displayStatistics(&arguments, &results, &options);\n    }\n\n    \n\n    if (options.method == METH_JACOBI) {\n    } else {\n        DisplayMatrix(&arguments, &results, &options);\n    }\n\n    \n\n\n    \n\n    freeMatrices(&arguments); \n\n\n    return 0;\n}", "label": "int\nmain(int argc, char** argv) {\n    struct options options;\n    struct calculation_arguments arguments;\n    struct calculation_results results;\n\n    \n\n    options.rank = -1;\n    options.size = -1;\n\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &(options.rank));\n    MPI_Comm_size(MPI_COMM_WORLD, &(options.size));\n\n    \n\n    AskParams(&options, argc, argv);\n\n    \n\n    initVariables(&arguments, &results, &options);\n    allocateMatrices(&arguments);\n    initMatrices(&arguments, &options);\n\n    gettimeofday(&start_time, NULL); \n\n    if (options.method == METH_JACOBI) {\n        \n\n        MPI_calculateJacobi(&arguments, &results, &options); \n    } else {\n        \n\n        calculateGaussSeidel(&arguments, &results, &options); \n    }\n    gettimeofday(&comp_time, NULL); \n\n\n    \n    \n\n    if (options.rank <= 0) {\n        displayStatistics(&arguments, &results, &options);\n    }\n\n    \n\n    if (options.method == METH_JACOBI) {\n        MPI_DisplayMatrix(&arguments, &results, &options, options.rank, options.size, arguments.row_start + 1, arguments.row_end - 1);\n    } else {\n        DisplayMatrix(&arguments, &results, &options);\n    }\n\n    \n\n    MPI_Finalize();\n\n    \n\n    freeMatrices(&arguments); \n\n\n    return 0;\n}"}
{"program": "gnu3ra_20", "code": "int main (int argc, char **argv) {\n    int rank;\n    int run_test_number = 0;\n    int failed;\n    int while_condition;\n    int i;\n\n    test_param_t predefined_tests[PREDEF_TESTS];\n\n    \n    if (argc != 1) {\n\tif (!rank) {\n\t    printf (\"Use only one process\\n\");\n\t    print_usage ();\n\t}\n\treturn 1;\n    }\n    i = 1;\n    while (i < argc) {\n        if (!strcmp (argv[i], \"-A\")) {\n            run_test_number = 0;\n            i++;\n        }\n        else if (!strcmp (argv[i], \"-T\")) {\n            run_test_number = atoi (argv[i+1]);\n\t    if ((run_test_number > PREDEF_TESTS) || (run_test_number < 1)) {\n\t\tif (!rank)\n\t\t    printf (\"Invalid test number, only %d tests\\n\",\n\t\t\t    PREDEF_TESTS);\n\t\treturn 1;\n\t    }\n            i += 2;\n        }\n        else {\n\t    if (!rank) {\n\t\tprintf (\"Invalid Argument: %s\\n\", argv[i]);\n\t\tprint_usage ();\n\t    }\n            i++;\n        }\n    }\n\n    setup_predefined (predefined_tests, PREDEF_TESTS);\n\n    if (!run_test_number) {\n\ti = 0;\n\twhile_condition = PREDEF_TESTS;\n    }\n    else {\n\ti = run_test_number - 1;\n\twhile_condition = run_test_number;\n    }\n    while (i < while_condition) {\n\tprintf (\"***** Test %d *****\\n\", i+1);\n\tfailed = run_test (&predefined_tests[i]);\n\tprintf (\"******************\\n\");\n\ti++;\n    }\n\n\n    return 0;\n}", "label": "int main (int argc, char **argv) {\n    int rank;\n    int run_test_number = 0;\n    int failed;\n    int while_condition;\n    int i;\n\n    test_param_t predefined_tests[PREDEF_TESTS];\n\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n    \n    if (argc != 1) {\n\tif (!rank) {\n\t    printf (\"Use only one process\\n\");\n\t    print_usage ();\n\t}\n\tMPI_Finalize();\n\treturn 1;\n    }\n    i = 1;\n    while (i < argc) {\n        if (!strcmp (argv[i], \"-A\")) {\n            run_test_number = 0;\n            i++;\n        }\n        else if (!strcmp (argv[i], \"-T\")) {\n            run_test_number = atoi (argv[i+1]);\n\t    if ((run_test_number > PREDEF_TESTS) || (run_test_number < 1)) {\n\t\tif (!rank)\n\t\t    printf (\"Invalid test number, only %d tests\\n\",\n\t\t\t    PREDEF_TESTS);\n\t\tMPI_Finalize ();\n\t\treturn 1;\n\t    }\n            i += 2;\n        }\n        else {\n\t    if (!rank) {\n\t\tprintf (\"Invalid Argument: %s\\n\", argv[i]);\n\t\tprint_usage ();\n\t    }\n            i++;\n        }\n    }\n\n    setup_predefined (predefined_tests, PREDEF_TESTS);\n\n    if (!run_test_number) {\n\ti = 0;\n\twhile_condition = PREDEF_TESTS;\n    }\n    else {\n\ti = run_test_number - 1;\n\twhile_condition = run_test_number;\n    }\n    while (i < while_condition) {\n\tprintf (\"***** Test %d *****\\n\", i+1);\n\tfailed = run_test (&predefined_tests[i]);\n\tprintf (\"******************\\n\");\n\ti++;\n    }\n\n    MPI_Finalize ();\n\n    return 0;\n}"}
{"program": "syftalent_22", "code": "int main(int argc, char **argv)\n{\n    int rank, size, err, errclass;\n    char buf[100000];\n\n    if (size < 2) {\n        fprintf( stderr, \"Must run with at least 2 processes\\n\" );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n        if ((err) && (errclass != MPIX_ERR_PROC_FAILED)) {\n            fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        } else {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        }\n#else\n        printf(\" No Errors\\n\");\n        fflush(stdout);\n#endif\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, err, errclass;\n    char buf[100000];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size < 2) {\n        fprintf( stderr, \"Must run with at least 2 processes\\n\" );\n        MPI_Abort( MPI_COMM_WORLD, 1 );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n        err = MPI_Send(buf, 100000, MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n        MPI_Error_class(err, &errclass);\n        if ((err) && (errclass != MPIX_ERR_PROC_FAILED)) {\n            fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        } else {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        }\n#else\n        printf(\" No Errors\\n\");\n        fflush(stdout);\n#endif\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "cleversonledur_23", "code": "int main(int argc, char* argv []){\n\n    vector_row = atoi(argv[1]);\n    vector_col = atoi(argv[2]);\n    intervalo = atoi(argv[3]);\n\n    if(rank==0) \n\n        master();\n    else\n        slave();\n\n    return 0;\n}", "label": "int main(int argc, char* argv []){\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    vector_row = atoi(argv[1]);\n    vector_col = atoi(argv[2]);\n    intervalo = atoi(argv[3]);\n\n    if(rank==0) \n\n        master();\n    else\n        slave();\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ilyak_24", "code": "int\nmain(int argc, char **argv)\n{\n\tsize_t i, oa, va, ob, vb;\n\ttime_t wall;\n\tint rank = 0;\n\n#ifdef LIBPT_USE_MPI\n#endif\n\tif (argc < 4)\n\t\terrx(1, \"usage: benchmark rpt|upt|rft|uft|rptmp|uptmp o v\");\n\toa = ob = strtol(argv[2], NULL, 10);\n\tva = vb = strtol(argv[3], NULL, 10);\n\tif (rank == 0)\n\t\tprintf(\"%s, oa = ob = %zu, va = vb = %zu\\n\", argv[1], oa, va);\n\twall = time(NULL);\n\tfor (i = 0; i < nbenchmarks; i++)\n\t\tif (strcmp(benchmarks[i].type, argv[1]) == 0) {\n\t\t\tbenchmarks[i].fn(oa, va, ob, vb);\n\t\t\tbreak;\n\t\t}\n\tif (i == nbenchmarks)\n\t\terrx(1, \"bad benchmark type\");\n\tif (rank == 0)\n\t\tprintf(\"done in %d sec\\n\", (int)(time(NULL)-wall));\n#ifdef LIBPT_USE_MPI\n#endif\n\treturn 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n\tsize_t i, oa, va, ob, vb;\n\ttime_t wall;\n\tint rank = 0;\n\n#ifdef LIBPT_USE_MPI\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n#endif\n\tif (argc < 4)\n\t\terrx(1, \"usage: benchmark rpt|upt|rft|uft|rptmp|uptmp o v\");\n\toa = ob = strtol(argv[2], NULL, 10);\n\tva = vb = strtol(argv[3], NULL, 10);\n\tif (rank == 0)\n\t\tprintf(\"%s, oa = ob = %zu, va = vb = %zu\\n\", argv[1], oa, va);\n\twall = time(NULL);\n\tfor (i = 0; i < nbenchmarks; i++)\n\t\tif (strcmp(benchmarks[i].type, argv[1]) == 0) {\n\t\t\tbenchmarks[i].fn(oa, va, ob, vb);\n\t\t\tbreak;\n\t\t}\n\tif (i == nbenchmarks)\n\t\terrx(1, \"bad benchmark type\");\n\tif (rank == 0)\n\t\tprintf(\"done in %d sec\\n\", (int)(time(NULL)-wall));\n#ifdef LIBPT_USE_MPI\n\tMPI_Finalize();\n#endif\n\treturn 0;\n}"}
{"program": "bmi-forum_25", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tStream* infoTest1;\n\t\tStream* infoTest2;\n\t\tStream* debugTest1;\n\t\tStream* debugTest2;\n\t\tStream* dumpTest1;\n\t\tStream* dumpTest2;\n\t\tStream* newTest1;\n\t\tStream* newTest2;\n\t\t\n\t\tStream* fileTest1;\n\t\tStream* fileTest2;\n\t\t\n\t\tStream* propTest1;\n\t\tStream* propTest2;\n\t\t\n\t\tDictionary* dictionary = Dictionary_New();\n\t\tXML_IO_Handler* io_handler = XML_IO_Handler_New();\n\n\t\tinfoTest1 = Journal_Register( Info_Type, \"test1\" );\n\t\tinfoTest2 = Journal_Register( Info_Type, \"test2\" );\n\t\tdebugTest1 = Journal_Register( Debug_Type, \"test1\" );\n\t\tdebugTest2 = Journal_Register( Debug_Type, \"test2\" );\n\t\tdumpTest1 = Journal_Register( Dump_Type, \"test1\" );\n\t\tdumpTest2 = Journal_Register( Dump_Type, \"test2\" );\n\t\t\n\t\tIO_Handler_ReadAllFromFile( io_handler, \"data/journal.xml\", dictionary ); \n\n\t\tJournal_ReadFromDictionary( dictionary );\n\n\t\tnewTest1 = Journal_Register( Info_Type, \"test1.new1\" );\n\t\tnewTest2 = Journal_Register( Info_Type, \"test1.new2\" );\n\t\t\n\t\tJournal_Printf( infoTest1, \"infoTest1\\n\" );\n\t\tJournal_Printf( infoTest2, \"infoTest2\\n\" );\n\t\tJournal_Printf( debugTest1, \"debugTest1\\n\" );\n\t\tJournal_Printf( debugTest2, \"debugTest2\\n\" );\n\t\tJournal_Printf( dumpTest1, \"dumpTest1\\n\" );\n\t\tJournal_Printf( dumpTest2, \"dumpTest2\\n\" );\t\t\n\t\tJournal_PrintfL( newTest1, 3, \"newTest1\\n\" );\n\t\tJournal_PrintfL( newTest1, 4, \"newTest1\\n\" );\n\t\tJournal_Printf( newTest2, \"newTest2\\n\" );\t\t\n\t\t\n\t\tfileTest1 = Journal_Register( \"newtype\", \"hello\" );\n\t\tfileTest2 = Journal_Register( \"newtype\", \"other\" );\n\t\t\n\t\tJournal_Printf( fileTest1, \"yay!\" );\n\t\tJournal_Printf( fileTest2, \"double yay!\" );\n\n\t\tpropTest1 = Journal_Register( Info_Type, \"propertiestest1\" );\n\t\tpropTest2 = Journal_Register( Info_Type, \"propertiestest2\" );\n\n\t\tPrint( propTest1, infoTest1 );\n\t\tPrint( propTest2, infoTest1 );\n\t\t\n\t\tStg_Class_Delete( io_handler );\n\t\tStg_Class_Delete( dictionary );\n\t}\n\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tStream* infoTest1;\n\t\tStream* infoTest2;\n\t\tStream* debugTest1;\n\t\tStream* debugTest2;\n\t\tStream* dumpTest1;\n\t\tStream* dumpTest2;\n\t\tStream* newTest1;\n\t\tStream* newTest2;\n\t\t\n\t\tStream* fileTest1;\n\t\tStream* fileTest2;\n\t\t\n\t\tStream* propTest1;\n\t\tStream* propTest2;\n\t\t\n\t\tDictionary* dictionary = Dictionary_New();\n\t\tXML_IO_Handler* io_handler = XML_IO_Handler_New();\n\n\t\tinfoTest1 = Journal_Register( Info_Type, \"test1\" );\n\t\tinfoTest2 = Journal_Register( Info_Type, \"test2\" );\n\t\tdebugTest1 = Journal_Register( Debug_Type, \"test1\" );\n\t\tdebugTest2 = Journal_Register( Debug_Type, \"test2\" );\n\t\tdumpTest1 = Journal_Register( Dump_Type, \"test1\" );\n\t\tdumpTest2 = Journal_Register( Dump_Type, \"test2\" );\n\t\t\n\t\tIO_Handler_ReadAllFromFile( io_handler, \"data/journal.xml\", dictionary ); \n\n\t\tJournal_ReadFromDictionary( dictionary );\n\n\t\tnewTest1 = Journal_Register( Info_Type, \"test1.new1\" );\n\t\tnewTest2 = Journal_Register( Info_Type, \"test1.new2\" );\n\t\t\n\t\tJournal_Printf( infoTest1, \"infoTest1\\n\" );\n\t\tJournal_Printf( infoTest2, \"infoTest2\\n\" );\n\t\tJournal_Printf( debugTest1, \"debugTest1\\n\" );\n\t\tJournal_Printf( debugTest2, \"debugTest2\\n\" );\n\t\tJournal_Printf( dumpTest1, \"dumpTest1\\n\" );\n\t\tJournal_Printf( dumpTest2, \"dumpTest2\\n\" );\t\t\n\t\tJournal_PrintfL( newTest1, 3, \"newTest1\\n\" );\n\t\tJournal_PrintfL( newTest1, 4, \"newTest1\\n\" );\n\t\tJournal_Printf( newTest2, \"newTest2\\n\" );\t\t\n\t\t\n\t\tfileTest1 = Journal_Register( \"newtype\", \"hello\" );\n\t\tfileTest2 = Journal_Register( \"newtype\", \"other\" );\n\t\t\n\t\tJournal_Printf( fileTest1, \"yay!\" );\n\t\tJournal_Printf( fileTest2, \"double yay!\" );\n\n\t\tpropTest1 = Journal_Register( Info_Type, \"propertiestest1\" );\n\t\tpropTest2 = Journal_Register( Info_Type, \"propertiestest2\" );\n\n\t\tPrint( propTest1, infoTest1 );\n\t\tPrint( propTest2, infoTest1 );\n\t\t\n\t\tStg_Class_Delete( io_handler );\n\t\tStg_Class_Delete( dictionary );\n\t}\n\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "calvinwylie_26", "code": "int main(int argc, char** argv)\n{\n    int rank, num_p;\n\n    int i;\n\n    int n = 200;\n    int N = 500;\n    \n\n    extern char* optarg;\n    const char* optstring = \"n:N:\";\n    int c;\n    while ((c = getopt(argc, argv, optstring)) != -1) {\n        switch (c) {\n        case 'n': n = atoi(optarg); break;\n        case 'N': N = atoi(optarg); break;\n        }\n    }\n\n    glp_term_out(GLP_OFF);\n    \n    double* R;\n    int* active_constr;\n    if (rank == 0) {\n        time_t curr_time;\n        time(&curr_time);\n        char* seed = ctime(&curr_time);\n        R = malloc(N*num_p*(n-1)*sizeof(double));\n        generate_random_constraints(seed, N*num_p, n-1, R);\n        active_constr = (int*) calloc(N*num_p, sizeof(int));\n    }\n\n    double t0;\n    if (rank == 0) {\n        t0 = omp_get_wtime();\n    }\n\n    double* R_local = malloc(N*(n-1)*sizeof(double));\n    int* active_constr_local = (int*) calloc(N, sizeof(int));\n\n\n    int ridx[N];\n    for (i = 0; i < N; i++) {\n        ridx[i] = i;\n    }\n    solve_lp(N, n, R_local, ridx, NULL, active_constr_local);\n\n\n    \n\n    double soln[n+1];\n    if (rank == 0) {\n\n        int num_active_constr = 0;\n        for (i = 0; i < N*num_p; i++) {\n            num_active_constr += active_constr[i];\n        }\n\n        int ridx[num_active_constr];\n        int count = 0;\n        int i;\n        for (i = 0; i < N*num_p; i++) {\n            if (active_constr[i] == 1) {\n                ridx[count] = i;\n                count += 1;\n            }\n        }\n        \n        solve_lp(num_active_constr, n, R, ridx, soln, active_constr);\n        \n\n    }\n\n    if (rank == 0) {\n        double t1 = omp_get_wtime();\n        printf(\"%d %g %g\", num_p, t1-t0, soln[n]);\n        for (i = 0; i < n; i++) {\n            printf(\" %g\", soln[i]);\n        }\n        printf(\"\\n\");\n    }\n\n    if (rank == 0) {\n        free(R);\n        free(active_constr);\n    }\n\n    free(R_local);\n    free(active_constr_local);\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n    int rank, num_p;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_p);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int i;\n\n    int n = 200;\n    int N = 500;\n    \n\n    extern char* optarg;\n    const char* optstring = \"n:N:\";\n    int c;\n    while ((c = getopt(argc, argv, optstring)) != -1) {\n        switch (c) {\n        case 'n': n = atoi(optarg); break;\n        case 'N': N = atoi(optarg); break;\n        }\n    }\n\n    glp_term_out(GLP_OFF);\n    \n    double* R;\n    int* active_constr;\n    if (rank == 0) {\n        time_t curr_time;\n        time(&curr_time);\n        char* seed = ctime(&curr_time);\n        R = malloc(N*num_p*(n-1)*sizeof(double));\n        generate_random_constraints(seed, N*num_p, n-1, R);\n        active_constr = (int*) calloc(N*num_p, sizeof(int));\n    }\n\n    double t0;\n    if (rank == 0) {\n        t0 = omp_get_wtime();\n    }\n\n    double* R_local = malloc(N*(n-1)*sizeof(double));\n    int* active_constr_local = (int*) calloc(N, sizeof(int));\n\n    MPI_Scatter(R, N*(n-1), MPI_DOUBLE, R_local, N*(n-1), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int ridx[N];\n    for (i = 0; i < N; i++) {\n        ridx[i] = i;\n    }\n    solve_lp(N, n, R_local, ridx, NULL, active_constr_local);\n\n    MPI_Gather(active_constr_local, N, MPI_INT, active_constr, N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    \n\n    double soln[n+1];\n    if (rank == 0) {\n\n        int num_active_constr = 0;\n        for (i = 0; i < N*num_p; i++) {\n            num_active_constr += active_constr[i];\n        }\n\n        int ridx[num_active_constr];\n        int count = 0;\n        int i;\n        for (i = 0; i < N*num_p; i++) {\n            if (active_constr[i] == 1) {\n                ridx[count] = i;\n                count += 1;\n            }\n        }\n        \n        solve_lp(num_active_constr, n, R, ridx, soln, active_constr);\n        \n\n    }\n\n    if (rank == 0) {\n        double t1 = omp_get_wtime();\n        printf(\"%d %g %g\", num_p, t1-t0, soln[n]);\n        for (i = 0; i < n; i++) {\n            printf(\" %g\", soln[i]);\n        }\n        printf(\"\\n\");\n    }\n\n    if (rank == 0) {\n        free(R);\n        free(active_constr);\n    }\n\n    free(R_local);\n    free(active_constr_local);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "sushant94_27", "code": "int main (int argc, char *argv[])\n{\n\tint rank, size;\n\tint token;\n\tMPI_Status status;\n\n\n\n\tdouble start =\n\n\t\n\n\t\n\n\tif (rank == 0) {\n\t\ttoken = 65;\n\t} else {\n\t\t\n\n\t\tif (rank + 1 < size) {\n\t\t\t\n\n\t\t\t\n\n\t\t\ttoken = token + 1;\n\t\t}\n\t}\n\n\n\tif (rank == 0) {\n\t}\n\n\treturn 0;\n}", "label": "int main (int argc, char *argv[])\n{\n\tint rank, size;\n\tint token;\n\tMPI_Status status;\n\n\tMPI_Init(&argc, &argv);\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tdouble start = MPI_Wtime();\n\n\t\n\n\t\n\n\tif (rank == 0) {\n\t\ttoken = 65;\n\t\tMPI_Send (&token, 1, MPI_INT, 1, MSG_DATA, MPI_COMM_WORLD);\n\t} else {\n\t\tMPI_Recv (&token, 1, MPI_INT, rank - 1, MSG_DATA, MPI_COMM_WORLD, &status);\n\t\t\n\n\t\tif (rank + 1 < size) {\n\t\t\t\n\n\t\t\t\n\n\t\t\ttoken = token + 1;\n\t\t\tMPI_Send (&token, 1, MPI_INT, rank + 1, MSG_DATA, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tprintf(\"%lf\", MPI_Wtime() - start);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "Matt3164_28", "code": "int main( int argc, char *argv[] )\n{\n    int done = 0, n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime=0.0, endwtime;\n    int  namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n    fprintf(stderr,\"Process %d on %s\\n\",\n\t    myid, processor_name);\n\n    n = 0;\n    while (!done)\n    {\n        if (myid == 0)\n        {\n\n\n\t    if (n==0) n=100; else n=0;\n\n\t    startwtime =\n        }\n        if (n == 0)\n            done = 1;\n        else\n        {\n            h   = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs)\n            {\n                x = h * ((double)i - 0.5);\n                sum += f(x);\n            }\n            mypi = h * sum;\n\n\n            if (myid == 0)\n\t    {\n                printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n                       pi, fabs(pi - PI25DT));\n\t\tendwtime =\n\t\tprintf(\"wall clock time = %f\\n\",\n\t\t       endwtime-startwtime);\t       \n\t    }\n        }\n    }\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int done = 0, n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime=0.0, endwtime;\n    int  namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    fprintf(stderr,\"Process %d on %s\\n\",\n\t    myid, processor_name);\n\n    n = 0;\n    while (!done)\n    {\n        if (myid == 0)\n        {\n\n\n\t    if (n==0) n=100; else n=0;\n\n\t    startwtime = MPI_Wtime();\n        }\n        MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        if (n == 0)\n            done = 1;\n        else\n        {\n            h   = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs)\n            {\n                x = h * ((double)i - 0.5);\n                sum += f(x);\n            }\n            mypi = h * sum;\n\n            MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n            if (myid == 0)\n\t    {\n                printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n                       pi, fabs(pi - PI25DT));\n\t\tendwtime = MPI_Wtime();\n\t\tprintf(\"wall clock time = %f\\n\",\n\t\t       endwtime-startwtime);\t       \n\t    }\n        }\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "callmetaste_29", "code": "int main(int argc, char* argv[])\n{\n  int numprocs,myid,fail=-1;\n  int m,p,q,sum,nd_tp;\n  double startwtime, endwtime;\n\n\n  srand(time(NULL));\n  adainit(); \n\n  dimension_broadcast(myid,&m,&p,&q,&nd_tp); \n  input_planes_broadcast(myid,m,p,q);\n  interpolation_points_broadcast(myid,m,p,q);\n\n  if(myid==0)\n  {\n    startwtime =\n    sum = server_distribute(m,p,q,numprocs,nd_tp);\n    endwtime =\n    printf(\"\\nTotal wall time = %lf seconds on %d processors\\n\",\n           endwtime-startwtime, numprocs);\n  }\n  else \n    client_compute(m,p,q);  \n\n  adafinal();      \n\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  int numprocs,myid,fail=-1;\n  int m,p,q,sum,nd_tp;\n  double startwtime, endwtime;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n  srand(time(NULL));\n  adainit(); \n\n  dimension_broadcast(myid,&m,&p,&q,&nd_tp); \n  input_planes_broadcast(myid,m,p,q);\n  interpolation_points_broadcast(myid,m,p,q);\n\n  if(myid==0)\n  {\n    startwtime = MPI_Wtime();\n    sum = server_distribute(m,p,q,numprocs,nd_tp);\n    endwtime = MPI_Wtime();\n    printf(\"\\nTotal wall time = %lf seconds on %d processors\\n\",\n           endwtime-startwtime, numprocs);\n  }\n  else \n    client_compute(m,p,q);  \n\n  adafinal();      \n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "mpip_32", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex(4, ni, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_complex(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex(4, ni, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_complex(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu3ra_33", "code": "int main( int argc, char *argv[] )\n{\n    double usecPerCall = 100;\n    double t, t1, tsum;\n    int i, nLoop = 100;\n    int rank;\n\n\n    \n\n    for (i=1; i<argc; i++) {\n\tif (strcmp( argv[i], \"-delaycount\" ) == 0) {\n\t    i++;\n\t    lCount = atoi( argv[i] );\n\t}\n\telse if (strcmp( argv[i], \"-v\" ) == 0) {\n\t    verbose = 1;\n\t}\n\telse {\n\t    fprintf( stderr, \"Unrecognized argument %s\\n\", argv[i] );\n\t    exit(1);\n\t}\n    }\n\n    if (lCount == 0) {\n\tSetupDelay( usecPerCall );\n    }\n    \n\n    t =\n    for (i=0; i<nLoop; i++) {\n\tDelay( lCount );\n    }\n    t = MPI_Wtime() - t;\n    if (rank == 0) {\n\tprintf( \"For delay count %d, time is %e\\n\", lCount, t );\n    }\n    \n\n    \n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    double usecPerCall = 100;\n    double t, t1, tsum;\n    int i, nLoop = 100;\n    int rank;\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\n    \n\n    for (i=1; i<argc; i++) {\n\tif (strcmp( argv[i], \"-delaycount\" ) == 0) {\n\t    i++;\n\t    lCount = atoi( argv[i] );\n\t}\n\telse if (strcmp( argv[i], \"-v\" ) == 0) {\n\t    verbose = 1;\n\t}\n\telse {\n\t    fprintf( stderr, \"Unrecognized argument %s\\n\", argv[i] );\n\t    exit(1);\n\t}\n    }\n\n    if (lCount == 0) {\n\tSetupDelay( usecPerCall );\n    }\n    \n    MPI_Barrier( MPI_COMM_WORLD );\n\n    t = MPI_Wtime();\n    for (i=0; i<nLoop; i++) {\n\tMPI_Allreduce( &t1, &tsum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );\n\tDelay( lCount );\n    }\n    t = MPI_Wtime() - t;\n    MPI_Barrier( MPI_COMM_WORLD );\n    if (rank == 0) {\n\tprintf( \"For delay count %d, time is %e\\n\", lCount, t );\n    }\n    \n    MPI_Barrier( MPI_COMM_WORLD );\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "sunwell1994_34", "code": "int main(int argc, char **argv)\n{\n\n\tint pid, total_processes;\n\n\tFILE *input, *output;\n\tint i;\n\tchar model_file[1024];\n\n\t\n\n\tfor(i=1;i<argc;i++)\n\t{\n\t\tif(argv[i][0] != '-') break;\n\t\t++i;\n\t\tswitch(argv[i-1][1])\n\t\t{\n\t\t\tcase 'b':\n\t\t\t\tflag_predict_probability = atoi(argv[i]);\n\t\t\t\tbreak;\n\t\t\tcase 'q':\n\t\t\t\tinfo = &print_null;\n\t\t\t\ti--;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tfprintf(stderr,\"unknown option: -%c\\n\", argv[i-1][1]);\n\t\t\t\texit_with_help();\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif(i>=argc)\n\t\texit_with_help();\n\n\tinput = fopen(argv[i],\"r\");\n\tif(input == NULL)\n\t{\n\t\tfprintf(stderr,\"can't open input file %s\\n\",argv[i]);\n\t\texit(1);\n\t}\n\n\toutput = fopen(argv[i+2],\"w\");\n\tif(output == NULL)\n\t{\n\t\tfprintf(stderr,\"can't open output file %s\\n\",argv[i+2]);\n\t\texit(1);\n\t}\n\n\tfor (int left = 0; left < BLOCK; left++) {\n\t\tsprintf(model_file,\"L%d%s%d%s\",left + 1, \"_R\", pid + 1,\".model\");\n\t\tprintf(\"%s\\n\", model_file);\n\t\tif ((model_[left] = load_model(model_file)) == 0) {\n\t\t\tfprintf(stderr,\"can't open model file %s\\n\",argv[i+1]);\n\t\t\texit(1);\n\t\t}\n\t\n\n\t}\n\n\tx = (struct feature_node *) malloc(max_nr_attr*sizeof(struct feature_node));\n\tdouble start_time, end_time;\n\tstart_time =\n\tdo_predict(input, output, pid);\n\tend_time =\n\tprintf(\"parallel exec time is %lf\\n\", end_time - start_time);\n\tfor ( int left = 0; left < BLOCK; left++) {\n\t\tfree_and_destroy_model(&model_[left]);\n\t}\n\tfree(line);\n\tfree(x);\n\tfclose(input);\n\tfclose(output);\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(&argc, &argv);\n\n\tint pid, total_processes;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &pid);\n\tMPI_Comm_size(MPI_COMM_WORLD, &total_processes);\n\n\tFILE *input, *output;\n\tint i;\n\tchar model_file[1024];\n\n\t\n\n\tfor(i=1;i<argc;i++)\n\t{\n\t\tif(argv[i][0] != '-') break;\n\t\t++i;\n\t\tswitch(argv[i-1][1])\n\t\t{\n\t\t\tcase 'b':\n\t\t\t\tflag_predict_probability = atoi(argv[i]);\n\t\t\t\tbreak;\n\t\t\tcase 'q':\n\t\t\t\tinfo = &print_null;\n\t\t\t\ti--;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tfprintf(stderr,\"unknown option: -%c\\n\", argv[i-1][1]);\n\t\t\t\texit_with_help();\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tif(i>=argc)\n\t\texit_with_help();\n\n\tinput = fopen(argv[i],\"r\");\n\tif(input == NULL)\n\t{\n\t\tfprintf(stderr,\"can't open input file %s\\n\",argv[i]);\n\t\texit(1);\n\t}\n\n\toutput = fopen(argv[i+2],\"w\");\n\tif(output == NULL)\n\t{\n\t\tfprintf(stderr,\"can't open output file %s\\n\",argv[i+2]);\n\t\texit(1);\n\t}\n\n\tfor (int left = 0; left < BLOCK; left++) {\n\t\tsprintf(model_file,\"L%d%s%d%s\",left + 1, \"_R\", pid + 1,\".model\");\n\t\tprintf(\"%s\\n\", model_file);\n\t\tif ((model_[left] = load_model(model_file)) == 0) {\n\t\t\tfprintf(stderr,\"can't open model file %s\\n\",argv[i+1]);\n\t\t\texit(1);\n\t\t}\n\t\n\n\t}\n\n\tx = (struct feature_node *) malloc(max_nr_attr*sizeof(struct feature_node));\n\tdouble start_time, end_time;\n\tstart_time = MPI_Wtime();\n\tdo_predict(input, output, pid);\n\tend_time = MPI_Wtime();\n\tprintf(\"parallel exec time is %lf\\n\", end_time - start_time);\n\tfor ( int left = 0; left < BLOCK; left++) {\n\t\tfree_and_destroy_model(&model_[left]);\n\t}\n\tfree(line);\n\tfree(x);\n\tfclose(input);\n\tfclose(output);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "leftarm_36", "code": "int main(int argc, char **argv)\r\n{\r\n   if(argc != 2)\r\n   {\r\n      printf(\"Usage: test {N}\\n\");\r\n      exit(-1);\r\n   }\r\n\r\n\r\n   n = atoi(argv[1]);\r\n   root = commSize - 1;\t\r\n\r\n   if(rank == root) master(argc, argv);\r\n   else node(argc, argv);\r\n\r\n   return 0;\r\n}", "label": "int main(int argc, char **argv)\r\n{\r\n   if(argc != 2)\r\n   {\r\n      printf(\"Usage: test {N}\\n\");\r\n      exit(-1);\r\n   }\r\n\r\n   MPI_Init(&argc, &argv);\r\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n   MPI_Comm_size(MPI_COMM_WORLD, &commSize);\r\n\r\n   n = atoi(argv[1]);\r\n   root = commSize - 1;\t\r\n\r\n   if(rank == root) master(argc, argv);\r\n   else node(argc, argv);\r\n\r\n   MPI_Finalize();\r\n   return 0;\r\n}"}
{"program": "ghisvail_37", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c(4, n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c(\n      4, n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r(\n      4, n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c(4, n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c(\n      4, n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r(\n      4, n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bjpop_39", "code": "int main(int argc, char* argv[])\n{\n    int align_size, rank, nprocs; \n    int pairs;\n\n\n    align_size = getpagesize();\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    memset(s_buf, 0, MAX_MSG_SIZE);\n    memset(r_buf, 0, MAX_MSG_SIZE);\n\n\n    pairs = nprocs/2;\n\n    if(rank == 0) {\n        fprintf(stdout, \"# %s v%s\\n\", BENCHMARK, PACKAGE_VERSION);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH, \"Latency (us)\");\n        fflush(stdout);\n    }\n\n\n    multi_latency(rank, pairs);\n    \n\n\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char* argv[])\n{\n    int align_size, rank, nprocs; \n    int pairs;\n\n    MPI_Init(&argc, &argv);\n\n    align_size = getpagesize();\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    memset(s_buf, 0, MAX_MSG_SIZE);\n    memset(r_buf, 0, MAX_MSG_SIZE);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    pairs = nprocs/2;\n\n    if(rank == 0) {\n        fprintf(stdout, \"# %s v%s\\n\", BENCHMARK, PACKAGE_VERSION);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH, \"Latency (us)\");\n        fflush(stdout);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    multi_latency(rank, pairs);\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "iomartin_40", "code": "int main(void) {\n    int comm_sz;\n    int my_rank;\n    int m;  \n\n    int n;  \n\n    double array[MAX_SIZE];\n    double my_sum = 0, received_sum;\n    int i;\n\n\n    \n\n    if (my_rank == 0) {\n        scanf(\"%d\", &n);\n        for (i = 0; i < n; i++) {\n            scanf(\"%lf\", &array[i]);\n        }\n        if (comm_sz > n) {\n            \n\n            m = 1;\n            for (i = 0; i < n; i++) {\n            }\n            \n\n            m = 0;\n            for ( ; i < comm_sz; i++) {\n            }\n        }\n        else {\n            m = n/comm_sz; \n\n            for (i = 0; i < comm_sz; i++) {\n                \n\n            }\n        }\n    }\n\n    \n\n    if (m != 0) {\n\n        for (i = 0; i < m; i++) {\n            my_sum += array[i];\n        }\n        \n        \n\n        int num_steps = (int)(log(n)/log(2));\n        int step;\n        for (step = 0; step < num_steps; step++) {\n            if (should_send(my_rank, step)) {\n            }\n            else if (should_receive(my_rank, step)) {\n                my_sum += received_sum;\n            }\n        }\n        if (my_rank == 0) {\n            printf(\"%.2lf\\n\", my_sum);\n        }\n       \n        \n\n    }\n    \n\n\n\n    return 0;\n}\n", "label": "int main(void) {\n    int comm_sz;\n    int my_rank;\n    int m;  \n\n    int n;  \n\n    double array[MAX_SIZE];\n    double my_sum = 0, received_sum;\n    int i;\n\n    MPI_Init(NULL, NULL);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    \n\n    if (my_rank == 0) {\n        scanf(\"%d\", &n);\n        for (i = 0; i < n; i++) {\n            scanf(\"%lf\", &array[i]);\n        }\n        if (comm_sz > n) {\n            \n\n            m = 1;\n            for (i = 0; i < n; i++) {\n                MPI_Send(&m, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                MPI_Send(&n, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                MPI_Send(&array[i*m], m, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            }\n            \n\n            m = 0;\n            for ( ; i < comm_sz; i++) {\n                MPI_Send(&m, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n            }\n        }\n        else {\n            m = n/comm_sz; \n\n            for (i = 0; i < comm_sz; i++) {\n                \n\n                MPI_Send(&m, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                MPI_Send(&comm_sz, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                MPI_Send(&array[i*m], m, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n\n    \n\n    MPI_Recv(&m, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    if (m != 0) {\n        MPI_Recv(&n, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(array, m, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        for (i = 0; i < m; i++) {\n            my_sum += array[i];\n        }\n        \n        \n\n        int num_steps = (int)(log(n)/log(2));\n        int step;\n        for (step = 0; step < num_steps; step++) {\n            if (should_send(my_rank, step)) {\n                MPI_Send(&my_sum, 1, MPI_DOUBLE, send_to(my_rank, step) , 0, MPI_COMM_WORLD);\n            }\n            else if (should_receive(my_rank, step)) {\n                MPI_Recv(&received_sum, 1, MPI_DOUBLE, receive_from(my_rank, step), 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                my_sum += received_sum;\n            }\n        }\n        if (my_rank == 0) {\n            printf(\"%.2lf\\n\", my_sum);\n        }\n       \n        \n\n    }\n    \n\n\n\n    MPI_Finalize();\n    return 0;\n}\n"}
{"program": "Heathckliff_41", "code": "int main(int argc, char *argv[])\n{\n  idx_t i, mype, npes, nelms;\n  idx_t *part, *eptr;\n  mesh_t mesh;\n  MPI_Comm comm;\n  idx_t wgtflag, numflag, edgecut, nparts, options[10];\n  idx_t mgcnum = -1, mgcnums[5] = {-1, 2, 3, 4, 2}, esizes[5] = {-1, 3, 4, 8, 4};\n  real_t *tpwgts, ubvec[MAXNCON];\n\n  gk_malloc_init();\n\n  gkMPI_Comm_size(comm, &npes);\n  gkMPI_Comm_rank(comm, &mype);\n\n  if (argc < 2) {\n    if (mype == 0)\n      printf(\"Usage: %s <mesh-file> [NCommonNodes]\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  ParallelReadMesh(&mesh, argv[1], comm); \n  mgcnum = mgcnums[mesh.etype];\n  mesh.ncon = 1;\n\n  if (argc > 2)\n    mgcnum = atoi(argv[2]);\n\n  if (mype == 0) printf(\"MGCNUM: %\"PRIDX\"\\n\", mgcnum);\n\n  nparts = npes;\n  tpwgts = rmalloc(nparts*mesh.ncon, \"tpwgts\");\n  for (i=0; i<nparts*mesh.ncon; i++)\n    tpwgts[i] = 1.0/(real_t)(nparts);\n\n  for (i=0; i<mesh.ncon; i++)\n    ubvec[i] = UNBALANCE_FRACTION;\n\n  part = imalloc(mesh.nelms, \"part\");\n\n  numflag = wgtflag = 0;\n  options[0] = 1;\n  options[PMV3_OPTION_DBGLVL] = 7;\n  options[PMV3_OPTION_SEED] = 0;\n\n  nelms = mesh.elmdist[mype+1]-mesh.elmdist[mype];\n  eptr = ismalloc(nelms+1, esizes[mesh.etype], \"main; eptr\");\n  MAKECSR(i, nelms, eptr);\n  eptr[nelms]--; \n\n  ParMETIS_V3_PartMeshKway(mesh.elmdist, eptr, mesh.elements, NULL, &wgtflag, \n              &numflag, &(mesh.ncon), &mgcnum, &nparts, tpwgts, ubvec, options, \n\t      &edgecut, part, &comm);\n \n\n \n\n  gk_free((void **)&part, &tpwgts, &eptr, LTERM);\n\n  gk_malloc_cleanup(0);\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n  idx_t i, mype, npes, nelms;\n  idx_t *part, *eptr;\n  mesh_t mesh;\n  MPI_Comm comm;\n  idx_t wgtflag, numflag, edgecut, nparts, options[10];\n  idx_t mgcnum = -1, mgcnums[5] = {-1, 2, 3, 4, 2}, esizes[5] = {-1, 3, 4, 8, 4};\n  real_t *tpwgts, ubvec[MAXNCON];\n\n  gk_malloc_init();\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n  gkMPI_Comm_size(comm, &npes);\n  gkMPI_Comm_rank(comm, &mype);\n\n  if (argc < 2) {\n    if (mype == 0)\n      printf(\"Usage: %s <mesh-file> [NCommonNodes]\\n\", argv[0]);\n\n    MPI_Finalize();\n    exit(0);\n  }\n\n  ParallelReadMesh(&mesh, argv[1], comm); \n  mgcnum = mgcnums[mesh.etype];\n  mesh.ncon = 1;\n\n  if (argc > 2)\n    mgcnum = atoi(argv[2]);\n\n  if (mype == 0) printf(\"MGCNUM: %\"PRIDX\"\\n\", mgcnum);\n\n  nparts = npes;\n  tpwgts = rmalloc(nparts*mesh.ncon, \"tpwgts\");\n  for (i=0; i<nparts*mesh.ncon; i++)\n    tpwgts[i] = 1.0/(real_t)(nparts);\n\n  for (i=0; i<mesh.ncon; i++)\n    ubvec[i] = UNBALANCE_FRACTION;\n\n  part = imalloc(mesh.nelms, \"part\");\n\n  numflag = wgtflag = 0;\n  options[0] = 1;\n  options[PMV3_OPTION_DBGLVL] = 7;\n  options[PMV3_OPTION_SEED] = 0;\n\n  nelms = mesh.elmdist[mype+1]-mesh.elmdist[mype];\n  eptr = ismalloc(nelms+1, esizes[mesh.etype], \"main; eptr\");\n  MAKECSR(i, nelms, eptr);\n  eptr[nelms]--; \n\n  ParMETIS_V3_PartMeshKway(mesh.elmdist, eptr, mesh.elements, NULL, &wgtflag, \n              &numflag, &(mesh.ncon), &mgcnum, &nparts, tpwgts, ubvec, options, \n\t      &edgecut, part, &comm);\n \n\n \n\n  gk_free((void **)&part, &tpwgts, &eptr, LTERM);\n  MPI_Comm_free(&comm);\n  MPI_Finalize();\n\n  gk_malloc_cleanup(0);\n  return 0;\n}"}
{"program": "joeladams_43", "code": "int main(int argc, char** argv) {\n    int myRank = -1;\n    int srcArr[ARRAY_SIZE] = {0};\n    int destArr[ARRAY_SIZE] = {0};\n\n\n    if (myRank == 0) {\n        printf(\"\\nBefore reduction: \");\n        printArray(myRank, \"destArr\", destArr, ARRAY_SIZE);\n    }\n\n    for (unsigned i = 0; i < ARRAY_SIZE; i++) {\n        srcArr[i] = myRank * i;\n    }\n\n    printSeparator(\"\", myRank);\n    printArray(myRank, \"srcArr\", srcArr, ARRAY_SIZE);\n    printSeparator(\"----\", myRank);\n\n\n    if (myRank == 0) {\n        printf(\"\\nAfter reduction:  \");\n        printArray(myRank, \"destArr\", destArr, ARRAY_SIZE);\n        printf(\"\\n\");\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int myRank = -1;\n    int srcArr[ARRAY_SIZE] = {0};\n    int destArr[ARRAY_SIZE] = {0};\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    if (myRank == 0) {\n        printf(\"\\nBefore reduction: \");\n        printArray(myRank, \"destArr\", destArr, ARRAY_SIZE);\n    }\n\n    for (unsigned i = 0; i < ARRAY_SIZE; i++) {\n        srcArr[i] = myRank * i;\n    }\n\n    printSeparator(\"\", myRank);\n    printArray(myRank, \"srcArr\", srcArr, ARRAY_SIZE);\n    printSeparator(\"----\", myRank);\n\n    MPI_Reduce(srcArr, destArr, ARRAY_SIZE, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (myRank == 0) {\n        printf(\"\\nAfter reduction:  \");\n        printArray(myRank, \"destArr\", destArr, ARRAY_SIZE);\n        printf(\"\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "PawseySupercomputing_48", "code": "int main(int argc, char **argv) {\n  long i;\n  long Ncirc = 0;\n  double pi, xy[2];\n  double r = 1.0; \n\n  double r2 = r*r;\n\n  int rank, size, manager = 0;\n  MPI_Status status;\n  long my_trials, temp;\n  int j;\n\n\n  VSLStreamStatePtr stream;\n\n  my_trials = num_trials/size;\n  if (num_trials%(long)size > (long)rank) my_trials++;\n\n  vslNewStream(&stream, VSL_BRNG_MT2203+rank, 1);\n\n  for (i = 0; i < my_trials; i++) {\n    vdRngUniform(VSL_RNG_METHOD_UNIFORMBITS_STD, stream, 2, xy, 0.0, 1.0);\n    if ((xy[0]*xy[0] + xy[1]*xy[1]) <= r2)\n      Ncirc++;\n  }\n\n  if (rank == manager) {\n    for (j = 1; j < size; j++) {\n      Ncirc += temp;\n    }\n    pi = 4.0 * ((double)Ncirc)/((double)num_trials);\n    printf(\"\\n \\t Computing pi using MPI and MKL for random number generator: \\n\");\n    printf(\"\\t For %ld trials, pi = %f\\n\", num_trials, pi);\n    printf(\"\\n\");\n  } else {\n  }\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  long i;\n  long Ncirc = 0;\n  double pi, xy[2];\n  double r = 1.0; \n\n  double r2 = r*r;\n\n  int rank, size, manager = 0;\n  MPI_Status status;\n  long my_trials, temp;\n  int j;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  VSLStreamStatePtr stream;\n\n  my_trials = num_trials/size;\n  if (num_trials%(long)size > (long)rank) my_trials++;\n\n  vslNewStream(&stream, VSL_BRNG_MT2203+rank, 1);\n\n  for (i = 0; i < my_trials; i++) {\n    vdRngUniform(VSL_RNG_METHOD_UNIFORMBITS_STD, stream, 2, xy, 0.0, 1.0);\n    if ((xy[0]*xy[0] + xy[1]*xy[1]) <= r2)\n      Ncirc++;\n  }\n\n  if (rank == manager) {\n    for (j = 1; j < size; j++) {\n      MPI_Recv(&temp, 1, MPI_LONG, j, j, MPI_COMM_WORLD, &status);\n      Ncirc += temp;\n    }\n    pi = 4.0 * ((double)Ncirc)/((double)num_trials);\n    printf(\"\\n \\t Computing pi using MPI and MKL for random number generator: \\n\");\n    printf(\"\\t For %ld trials, pi = %f\\n\", num_trials, pi);\n    printf(\"\\n\");\n  } else {\n    MPI_Send(&Ncirc, 1, MPI_LONG, manager, rank, MPI_COMM_WORLD);\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "ClaudioNahmad_50", "code": "int main(int argc, char* argv[])\n{\n    int rank, size;\n    MPI_Info info, srch;\n    char port[MPI_MAX_PORT_NAME];\n    bool local=false;\n\n    if (1 < argc) {\n        if (0 == strcmp(\"local\", argv[1])) {\n            local = true;\n        }\n    }\n\n\n\n    printf(\"Hello, World, I am %d of %d\\n\", rank, size);\n\n    if (local) {\n    } else {\n    }\n\n    if (0 == rank) {\n        printf(\"Rank %d published port %s\\n\", rank, port);\n    }\n\n\n    if (rank != 0) {\n        printf(\"Rank %d got port %s\\n\", rank, port);\n    }\n\n\n    if (0 == rank) {\n    }\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    int rank, size;\n    MPI_Info info, srch;\n    char port[MPI_MAX_PORT_NAME];\n    bool local=false;\n\n    if (1 < argc) {\n        if (0 == strcmp(\"local\", argv[1])) {\n            local = true;\n        }\n    }\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    printf(\"Hello, World, I am %d of %d\\n\", rank, size);\n\n    MPI_Info_create(&info);\n    if (local) {\n        MPI_Info_set(info, \"ompi_global_scope\", \"false\");\n    } else {\n        MPI_Info_set(info, \"ompi_global_scope\", \"true\");\n    }\n\n    if (0 == rank) {\n        MPI_Open_port(MPI_INFO_NULL, port);\n        MPI_Publish_name(\"pubsub-test\", info, port);\n        printf(\"Rank %d published port %s\\n\", rank, port);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Info_create(&srch);\n    MPI_Info_set(srch, \"ompi_lookup_order\", \"local,global\");\n    if (rank != 0) {\n        MPI_Lookup_name(\"pubsub-test\", srch, port);\n        printf(\"Rank %d got port %s\\n\", rank, port);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (0 == rank) {\n        MPI_Unpublish_name(\"pubsub-test\", info, port);\n    }\n    MPI_Info_free(&info);\n    MPI_Info_free(&srch);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "qingu_51", "code": "int main(int argc, char **argv) {\n    int       i, j, rank, nproc;\n    int       errors = 0, all_errors = 0;\n    int      *val_ptr;\n    MPI_Win   win;\n\n\n\n    val_ptr = malloc(sizeof(int));\n\n    *val_ptr = 0;\n\n\n    \n\n\n    for (i = 0; i < ITER; i++) {\n        int next = i + 1, result = -1;\n        if (result != i) {\n            SQUELCH( printf(\"%d->%d -- Error: next=%d compare=%d result=%d val=%d\\n\", rank,\n                           rank, next, i, result, *val_ptr); );\n            errors++;\n        }\n    }\n\n    *val_ptr = 0;\n\n\n    \n\n\n    for (i = 0; i < ITER; i++) {\n        int next = i + 1, result = -1;\n        if (result != i) {\n            SQUELCH( printf(\"%d->%d -- Error: next=%d compare=%d result=%d val=%d\\n\", rank,\n                           (rank+1)%nproc, next, i, result, *val_ptr); );\n            errors++;\n        }\n    }\n\n    fflush(NULL);\n\n    *val_ptr = 0;\n\n\n    \n\n\n    if (rank != 0) {\n        for (i = 0; i < ITER; i++) {\n            int next = i + 1, result = -1;\n        }\n    }\n\n\n    if (rank == 0 && nproc > 1) {\n        if (*val_ptr != ITER) {\n            SQUELCH( printf(\"%d - Error: expected=%d val=%d\\n\", rank, ITER, *val_ptr); );\n            errors++;\n        }\n    }\n\n\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    free(val_ptr);\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int       i, j, rank, nproc;\n    int       errors = 0, all_errors = 0;\n    int      *val_ptr;\n    MPI_Win   win;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    val_ptr = malloc(sizeof(int));\n\n    *val_ptr = 0;\n\n    MPI_Win_create(val_ptr, sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n    \n\n\n    for (i = 0; i < ITER; i++) {\n        int next = i + 1, result = -1;\n        MPI_Win_lock(MPI_LOCK_EXCLUSIVE, rank, 0, win);\n        MPI_Compare_and_swap(&next, &i, &result, MPI_INT, rank, 0, win);\n        MPI_Win_unlock(rank, win);\n        if (result != i) {\n            SQUELCH( printf(\"%d->%d -- Error: next=%d compare=%d result=%d val=%d\\n\", rank,\n                           rank, next, i, result, *val_ptr); );\n            errors++;\n        }\n    }\n\n    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, rank, 0, win);\n    *val_ptr = 0;\n    MPI_Win_unlock(rank, win);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n\n    for (i = 0; i < ITER; i++) {\n        int next = i + 1, result = -1;\n        MPI_Win_lock(MPI_LOCK_EXCLUSIVE, (rank+1)%nproc, 0, win);\n        MPI_Compare_and_swap(&next, &i, &result, MPI_INT, (rank+1)%nproc, 0, win);\n        MPI_Win_unlock((rank+1)%nproc, win);\n        if (result != i) {\n            SQUELCH( printf(\"%d->%d -- Error: next=%d compare=%d result=%d val=%d\\n\", rank,\n                           (rank+1)%nproc, next, i, result, *val_ptr); );\n            errors++;\n        }\n    }\n\n    fflush(NULL);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, rank, 0, win);\n    *val_ptr = 0;\n    MPI_Win_unlock(rank, win);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n\n    \n\n\n    if (rank != 0) {\n        for (i = 0; i < ITER; i++) {\n            int next = i + 1, result = -1;\n            MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);\n            MPI_Compare_and_swap(&next, &i, &result, MPI_INT, 0, 0, win);\n            MPI_Win_unlock(0, win);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0 && nproc > 1) {\n        if (*val_ptr != ITER) {\n            SQUELCH( printf(\"%d - Error: expected=%d val=%d\\n\", rank, ITER, *val_ptr); );\n            errors++;\n        }\n    }\n\n    MPI_Win_free(&win);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    free(val_ptr);\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_52", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tXML_IO_Handler*\t\tio_handler = XML_IO_Handler_New();\n\t\tDictionary*\t\tdictionary = Dictionary_New();\n\t\tDictionary_Index\tindex;\n\t\tStream*\t\t\tstream = Journal_Register( InfoStream_Type, XML_IO_Handler_Type );\n\t\t\n\t\t\n\n\t\tprintf( \"\\ntest of reading normal xml file:\\n\");\n\t\tIO_Handler_ReadAllFromFile( io_handler, \"data/normal.xml\", dictionary ); \n\n\t\tprintf( \"\\ndictionary now contains:\\n\" );\n\t\tprintf( \"Dictionary:\\n\" );\n\t\tprintf( \"\\tsize: %u\\n\", dictionary->size );\n\t\tprintf( \"\\tdelta: %u\\n\", dictionary->delta );\n\t\tprintf( \"\\tcount: %u\\n\", dictionary->count );\n\t\tprintf( \"\\tentryPtr[0-%u]: {\\n\", dictionary->count );\n\t\tfor( index = 0; index < dictionary->count; index++ ) {\n\t\t\tprintf( \"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary->entryPtr[index], stream ); \n\t\t\tprintf( \"\\n\" );\n\t\t}\n\t\tprintf( \"\\t}\\n\" );\n\n\t\tprintf( \"\\ntest of writing normal xml file:\\n\");\n\t\tIO_Handler_WriteAllToFile( io_handler, \"data/newnormal.xml\", dictionary );\n\t\tXML_IO_Handler_SetWriteExplicitTypes( io_handler, True );\n\t\tIO_Handler_WriteAllToFile( io_handler, \"data/newnormalexplicittypes.xml\", dictionary );\n\n\t\tprintf( \"\\ntest of writing single entry:\\n\");\n\t\tXML_IO_Handler_WriteEntryToFile( io_handler, \"data/newgeom.xml\", \"geom\", Dictionary_Get( dictionary, \"geom\" ), Dictionary_GetSource( dictionary, \"geom\" ) );\n\n\t\tStg_Class_Delete( io_handler );\n\t\tStg_Class_Delete( dictionary );\n\t}\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tXML_IO_Handler*\t\tio_handler = XML_IO_Handler_New();\n\t\tDictionary*\t\tdictionary = Dictionary_New();\n\t\tDictionary_Index\tindex;\n\t\tStream*\t\t\tstream = Journal_Register( InfoStream_Type, XML_IO_Handler_Type );\n\t\t\n\t\t\n\n\t\tprintf( \"\\ntest of reading normal xml file:\\n\");\n\t\tIO_Handler_ReadAllFromFile( io_handler, \"data/normal.xml\", dictionary ); \n\n\t\tprintf( \"\\ndictionary now contains:\\n\" );\n\t\tprintf( \"Dictionary:\\n\" );\n\t\tprintf( \"\\tsize: %u\\n\", dictionary->size );\n\t\tprintf( \"\\tdelta: %u\\n\", dictionary->delta );\n\t\tprintf( \"\\tcount: %u\\n\", dictionary->count );\n\t\tprintf( \"\\tentryPtr[0-%u]: {\\n\", dictionary->count );\n\t\tfor( index = 0; index < dictionary->count; index++ ) {\n\t\t\tprintf( \"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary->entryPtr[index], stream ); \n\t\t\tprintf( \"\\n\" );\n\t\t}\n\t\tprintf( \"\\t}\\n\" );\n\n\t\tprintf( \"\\ntest of writing normal xml file:\\n\");\n\t\tIO_Handler_WriteAllToFile( io_handler, \"data/newnormal.xml\", dictionary );\n\t\tXML_IO_Handler_SetWriteExplicitTypes( io_handler, True );\n\t\tIO_Handler_WriteAllToFile( io_handler, \"data/newnormalexplicittypes.xml\", dictionary );\n\n\t\tprintf( \"\\ntest of writing single entry:\\n\");\n\t\tXML_IO_Handler_WriteEntryToFile( io_handler, \"data/newgeom.xml\", \"geom\", Dictionary_Get( dictionary, \"geom\" ), Dictionary_GetSource( dictionary, \"geom\" ) );\n\n\t\tStg_Class_Delete( io_handler );\n\t\tStg_Class_Delete( dictionary );\n\t}\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "gnu3ra_53", "code": "int main( int argc, char **argv )\n{\n    int              rank, size, i;\n    int              data;\n    int              errors=0;\n    int              result = -100;\n    int              correct_result;\n\n\n    data = rank;\n\n    correct_result = 0;\n    for(i=0;i<size;i++) \n      correct_result += i;\n    if (result != correct_result) errors++;\n\n    if (result != 0) errors++;\n\n    if (result != (size-1)) errors++;\n\n    if (errors)\n      printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}", "label": "int main( int argc, char **argv )\n{\n    int              rank, size, i;\n    int              data;\n    int              errors=0;\n    int              result = -100;\n    int              correct_result;\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n\n    data = rank;\n\n    MPI_Reduce ( &data, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD );\n    MPI_Bcast  ( &result, 1, MPI_INT, 0, MPI_COMM_WORLD );\n    correct_result = 0;\n    for(i=0;i<size;i++) \n      correct_result += i;\n    if (result != correct_result) errors++;\n\n    MPI_Reduce ( &data, &result, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD );\n    MPI_Bcast  ( &result, 1, MPI_INT, 0, MPI_COMM_WORLD );\n    if (result != 0) errors++;\n\n    MPI_Reduce ( &data, &result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD );\n    MPI_Bcast  ( &result, 1, MPI_INT, 0, MPI_COMM_WORLD );\n    if (result != (size-1)) errors++;\n\n    MPI_Finalize();\n    if (errors)\n      printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}"}
{"program": "etmc_58", "code": "int main(int argc, char **argv)\n{\n  int rank;\n  MPI_File fp;\n\n  LemonWriter *w;\n  LemonReader *r;\n  LemonRecordHeader *h;\n\n  int ME_flag=1, MB_flag=1, status=0;\n  char message[512];\n  char message_read[512];\n  char const *type;\n  MPI_Offset bytes;\n  MPI_Offset bytes_read;\n  struct timeval t1;\n\n\n  \n\n\n  w = lemonCreateWriter(&fp, MPI_COMM_WORLD);\n\n  gettimeofday(&t1, NULL);\n  sprintf(message,\"\\n plaquette = %e\\n trajectory nr = %d\\n beta = %f, kappa = %f, mu = %f, c2_rec = %f\\n time = %ld\\n hmcversion = %s\\n mubar = %f\\n epsilonbar = %f\\n date = %s\",\n                  0.0, 1, 6.0, 0.177, 0.5, 0.0, t1.tv_sec, \"5.0.1\", 0.0, 0.0, ctime(&t1.tv_sec));\n\n  bytes = strlen(message);\n  h = lemonCreateHeader(MB_flag, ME_flag, \"xlf-info\", bytes);\n  status = lemonWriteRecordHeader(h, w);\n\n  lemonDestroyHeader( h );\n  lemonWriteRecordData(message, &bytes, w);\n  lemonWriterCloseRecord(w);\n  lemonDestroyWriter(w);\n\n\n  \n\n\n  r = lemonCreateReader(&fp, MPI_COMM_WORLD);\n  if (lemonReaderNextRecord(r))\n    fprintf(stderr, \"Node %d reports: next record failed.\\n\", rank);\n\n  type = lemonReaderType(r);\n  if (strncmp(type, \"xlf-info\", 8))\n    fprintf(stderr, \"Node %d reports: wrong type read.\\n\", rank);\n\n  bytes_read = bytes;\n  lemonReaderReadData(message_read, &bytes_read, r);\n\n  if ((bytes_read != bytes) || strncmp(message_read, message, bytes))\n    fprintf(stderr, \"Node %d reports: wrong message read.\\n\", rank);\n  else\n    fprintf(stderr, \"Node %d reports data okay.\\n\", rank);\n  lemonReaderCloseRecord(r);\n  lemonDestroyReader(r);\n\n\n\n  return(0);\n}", "label": "int main(int argc, char **argv)\n{\n  int rank;\n  MPI_File fp;\n\n  LemonWriter *w;\n  LemonReader *r;\n  LemonRecordHeader *h;\n\n  int ME_flag=1, MB_flag=1, status=0;\n  char message[512];\n  char message_read[512];\n  char const *type;\n  MPI_Offset bytes;\n  MPI_Offset bytes_read;\n  struct timeval t1;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  MPI_File_open(MPI_COMM_WORLD, \"xlf.test\", MPI_MODE_WRONLY | MPI_MODE_CREATE, MPI_INFO_NULL, &fp);\n\n  w = lemonCreateWriter(&fp, MPI_COMM_WORLD);\n\n  gettimeofday(&t1, NULL);\n  sprintf(message,\"\\n plaquette = %e\\n trajectory nr = %d\\n beta = %f, kappa = %f, mu = %f, c2_rec = %f\\n time = %ld\\n hmcversion = %s\\n mubar = %f\\n epsilonbar = %f\\n date = %s\",\n                  0.0, 1, 6.0, 0.177, 0.5, 0.0, t1.tv_sec, \"5.0.1\", 0.0, 0.0, ctime(&t1.tv_sec));\n\n  bytes = strlen(message);\n  h = lemonCreateHeader(MB_flag, ME_flag, \"xlf-info\", bytes);\n  status = lemonWriteRecordHeader(h, w);\n\n  lemonDestroyHeader( h );\n  lemonWriteRecordData(message, &bytes, w);\n  lemonWriterCloseRecord(w);\n  lemonDestroyWriter(w);\n\n  MPI_File_close(&fp);\n\n  \n\n  MPI_File_open(MPI_COMM_WORLD, \"xlf.test\", MPI_MODE_RDONLY, MPI_INFO_NULL, &fp);\n\n  r = lemonCreateReader(&fp, MPI_COMM_WORLD);\n  if (lemonReaderNextRecord(r))\n    fprintf(stderr, \"Node %d reports: next record failed.\\n\", rank);\n\n  type = lemonReaderType(r);\n  if (strncmp(type, \"xlf-info\", 8))\n    fprintf(stderr, \"Node %d reports: wrong type read.\\n\", rank);\n\n  bytes_read = bytes;\n  lemonReaderReadData(message_read, &bytes_read, r);\n\n  if ((bytes_read != bytes) || strncmp(message_read, message, bytes))\n    fprintf(stderr, \"Node %d reports: wrong message read.\\n\", rank);\n  else\n    fprintf(stderr, \"Node %d reports data okay.\\n\", rank);\n  lemonReaderCloseRecord(r);\n  lemonDestroyReader(r);\n\n  MPI_File_close(&fp);\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "LasseRegin_61", "code": "int main(int argc, char* argv[])\n{\n\n  int rank, size;\n\n  \n\n  \n\n  \n\n  GameInfo game;\n\n  int rows = 0, cols = 0;\n  bool* init = NULL;\n\n  \n\n  if (rank == 0)  {\n    rows = 8;\n    cols = 8;\n    init = (bool[64]) {\n      0, 0, 0, 0, 0, 0, 0, 0,\n      1, 1, 1, 0, 0, 1, 1, 1,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      1, 1, 1, 0, 0, 1, 1, 1,\n      0, 0, 0, 0, 0, 0, 0, 0\n    };\n  }\n\n  \n\n  if (initialize_game(&game, size, rank, rows, cols, init)) {\n    return 1;\n  }\n\n  \n\n  \n\n\n  \n\n  for (int iter = 0; iter < 5; iter++) {\n    synchronize_game(&game);\n\n    update_game(&game);\n\n    \n\n    print_global_game(&game, rank);\n  }\n\n  \n\n  destroy_game(&game);\n\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  MPI_Init(&argc,&argv);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  \n\n  \n\n  \n\n  GameInfo game;\n\n  int rows = 0, cols = 0;\n  bool* init = NULL;\n\n  \n\n  if (rank == 0)  {\n    rows = 8;\n    cols = 8;\n    init = (bool[64]) {\n      0, 0, 0, 0, 0, 0, 0, 0,\n      1, 1, 1, 0, 0, 1, 1, 1,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      0, 0, 0, 0, 0, 0, 0, 0,\n      1, 1, 1, 0, 0, 1, 1, 1,\n      0, 0, 0, 0, 0, 0, 0, 0\n    };\n  }\n\n  \n\n  if (initialize_game(&game, size, rank, rows, cols, init)) {\n    return 1;\n  }\n\n  \n\n  \n\n\n  \n\n  for (int iter = 0; iter < 5; iter++) {\n    synchronize_game(&game);\n\n    update_game(&game);\n\n    \n\n    print_global_game(&game, rank);\n  }\n\n  \n\n  destroy_game(&game);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "nschloe_62", "code": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag; \n  MPI_Status status;\n\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  \n  \n\n\n  return 0;\n}", "label": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag; \n  MPI_Status status;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  MPI_Send(message1, strlen(message1)+1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);\n  MPI_Recv(message2, 50, MPI_CHAR, source, tag, MPI_COMM_NULL, &status);\n  \n  \n\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "kareemcarr_64", "code": "int main(int argc, char** argv) {\n    \n\n\n    \n\n    int world_size;\n\n    \n\n    int world_rank;\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n\n    \n\n    printf(\"Hello world from processor %s, rank %d\"\n           \" out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    \n\n}", "label": "int main(int argc, char** argv) {\n    \n\n    MPI_Init(NULL, NULL);\n\n    \n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    \n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    \n\n    printf(\"Hello world from processor %s, rank %d\"\n           \" out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    \n\n    MPI_Finalize();\n}"}
{"program": "bh107_65", "code": "int main (int argc, char **argv)\n{\n    int myrank, worldsize;\n\n    bp_util_type bp = bp_util_create(argc, argv, 3);\n    if (bp.args.has_error) {\n        return 1;\n    }\n    int height      = bp.args.sizes[0];\n    const int width = bp.args.sizes[1];\n    const int iter  = bp.args.sizes[2];\n\n    \n\n    height = (width-2) / worldsize;\n    if(myrank == worldsize-1)\n        height += (width-2) % worldsize;\n    height += 2;\n\n\n    double *grid = malloc(height*width*sizeof(double));\n    for(int j=0; j<width;j++)\n    {\n        if(myrank == 0)\n           grid[j] = 40.0;\n        if(myrank == worldsize-1)\n            grid[j+(height-1)*width] = -273.15;\n    }\n    for(int j=1; j<height-1;j++)\n    {\n        grid[j*width] = -273.15;\n        grid[j*width+width-1]= -273.15;\n    }\n\n    bp.timer_start();\n    openmp(height,width,grid,iter);\n\n    bp.timer_stop();\n\n    if (myrank == 0) {\n        bp.print(\"heat_equation(c99_omp_mpi)\");\n    }\n    free(grid);\n    return 0;\n}", "label": "int main (int argc, char **argv)\n{\n    MPI_Init(&argc,&argv);\n    int myrank, worldsize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &worldsize);\n\n    bp_util_type bp = bp_util_create(argc, argv, 3);\n    if (bp.args.has_error) {\n        return 1;\n    }\n    int height      = bp.args.sizes[0];\n    const int width = bp.args.sizes[1];\n    const int iter  = bp.args.sizes[2];\n\n    \n\n    height = (width-2) / worldsize;\n    if(myrank == worldsize-1)\n        height += (width-2) % worldsize;\n    height += 2;\n\n\n    double *grid = malloc(height*width*sizeof(double));\n    for(int j=0; j<width;j++)\n    {\n        if(myrank == 0)\n           grid[j] = 40.0;\n        if(myrank == worldsize-1)\n            grid[j+(height-1)*width] = -273.15;\n    }\n    for(int j=1; j<height-1;j++)\n    {\n        grid[j*width] = -273.15;\n        grid[j*width+width-1]= -273.15;\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    bp.timer_start();\n    openmp(height,width,grid,iter);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    bp.timer_stop();\n\n    if (myrank == 0) {\n        bp.print(\"heat_equation(c99_omp_mpi)\");\n    }\n    free(grid);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ESiWACE_69", "code": "int main(int argc, char **argv) {\n  const size_t n = atoi(argv[1]);\n  const size_t p = atoi(argv[2]);\n  const procs_t procs = {n, p};\n  const size_t t = atoi(argv[3]);\n  const size_t x = atoi(argv[4]);\n  const size_t y = atoi(argv[5]);\n  const size_t z = atoi(argv[6]);\n  const size_t ndims = 4;\n  const size_t dgeom[4] = {t, x, y, z};\n  const size_t bgeom[4] = {1, x / n, y / p, z};\n  const size_t cgeom[4] = {1, x / n, y / p, z};\n  const char *testfn = \"./test.nc\";\n  const io_mode_t io_mode = NC_INDEPENDENT;\n  const int par_access = NC_CHUNKED;\n  const bool is_unlimited = 0;\n\n  benchmark_t bm;\n  benchmark_init(&bm);\n  benchmark_setup(&bm, procs, ndims, dgeom, bgeom, cgeom, testfn, io_mode, par_access, is_unlimited);\n  benchmark_run(&bm);\n  benchmark_destroy(&bm);\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  MPI_Init(&argc, &argv);\n  const size_t n = atoi(argv[1]);\n  const size_t p = atoi(argv[2]);\n  const procs_t procs = {n, p};\n  const size_t t = atoi(argv[3]);\n  const size_t x = atoi(argv[4]);\n  const size_t y = atoi(argv[5]);\n  const size_t z = atoi(argv[6]);\n  const size_t ndims = 4;\n  const size_t dgeom[4] = {t, x, y, z};\n  const size_t bgeom[4] = {1, x / n, y / p, z};\n  const size_t cgeom[4] = {1, x / n, y / p, z};\n  const char *testfn = \"./test.nc\";\n  const io_mode_t io_mode = NC_INDEPENDENT;\n  const int par_access = NC_CHUNKED;\n  const bool is_unlimited = 0;\n\n  benchmark_t bm;\n  benchmark_init(&bm);\n  benchmark_setup(&bm, procs, ndims, dgeom, bgeom, cgeom, testfn, io_mode, par_access, is_unlimited);\n  benchmark_run(&bm);\n  benchmark_destroy(&bm);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "deskinner_70", "code": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double t1, t2;\n  \n  int req, prov;\n  req = MPI_THREAD_SINGLE;\n\n\n  PMPI_Type_size(DATATYPE, &dsize);\n  \n  buf=(char*)malloc(SIZE*dsize);\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds with IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      PMPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds without IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n\n  fopen(\"/dev/null\", \"r\");  \n}", "label": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double t1, t2;\n  \n  int req, prov;\n  req = MPI_THREAD_SINGLE;\n\n  MPI_Init( &argc, &argv);\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  PMPI_Type_size(DATATYPE, &dsize);\n  \n  buf=(char*)malloc(SIZE*dsize);\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds with IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      PMPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds without IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  MPI_Finalize();\n\n  fopen(\"/dev/null\", \"r\");  \n}"}
{"program": "dagss_71", "code": "int main(int argc, const char **argv)\n  {\n#ifdef USE_MPI\n#else\n  mytask=0; ntasks=1;\n#endif\n\n  UTIL_ASSERT(argc>=2,\"need at least one command line argument\");\n\n  if (strcmp(argv[1],\"acctest\")==0)\n    sharp_acctest();\n  else if (strcmp(argv[1],\"test\")==0)\n    sharp_test(argc,argv);\n  else if (strcmp(argv[1],\"bench\")==0)\n    sharp_bench(argc,argv);\n  else\n    UTIL_FAIL(\"unknown command\");\n\n#ifdef USE_MPI\n#endif\n  return 0;\n  }", "label": "int main(int argc, const char **argv)\n  {\n#ifdef USE_MPI\n  MPI_Init(NULL,NULL);\n  MPI_Comm_size(MPI_COMM_WORLD,&ntasks);\n  MPI_Comm_rank(MPI_COMM_WORLD,&mytask);\n#else\n  mytask=0; ntasks=1;\n#endif\n\n  UTIL_ASSERT(argc>=2,\"need at least one command line argument\");\n\n  if (strcmp(argv[1],\"acctest\")==0)\n    sharp_acctest();\n  else if (strcmp(argv[1],\"test\")==0)\n    sharp_test(argc,argv);\n  else if (strcmp(argv[1],\"bench\")==0)\n    sharp_bench(argc,argv);\n  else\n    UTIL_FAIL(\"unknown command\");\n\n#ifdef USE_MPI\n  MPI_Finalize();\n#endif\n  return 0;\n  }"}
{"program": "TUM-I5_72", "code": "int main(int argc, char** argv)\n{\n  \n  asagi_grid* grid = asagi_grid_create(ASAGI_FLOAT);\n  asagi_grid_set_comm(grid, MPI_COMM_WORLD);\n  \n\n  asagi_grid_set_threads(grid, 1);\n  \n  if (asagi_grid_open(grid, \"/path/to/netcdf/file.nc\", 0) != ASAGI_SUCCESS) {\n    printf(\"Could not load file\\n\");\n    return 1;\n  }\n  \n  double pos[] = {0, 0};\n  printf(\"Value at (0,0): %f\\n\", asagi_grid_get_float(grid, pos, 0));\n  \n  asagi_grid_close(grid);\n  \n  \n  return 0;\n}", "label": "int main(int argc, char** argv)\n{\n  MPI_Init(&argc, &argv);\n  \n  asagi_grid* grid = asagi_grid_create(ASAGI_FLOAT);\n  asagi_grid_set_comm(grid, MPI_COMM_WORLD);\n  \n\n  asagi_grid_set_threads(grid, 1);\n  \n  if (asagi_grid_open(grid, \"/path/to/netcdf/file.nc\", 0) != ASAGI_SUCCESS) {\n    printf(\"Could not load file\\n\");\n    return 1;\n  }\n  \n  double pos[] = {0, 0};\n  printf(\"Value at (0,0): %f\\n\", asagi_grid_get_float(grid, pos, 0));\n  \n  asagi_grid_close(grid);\n  \n  MPI_Finalize();\n  \n  return 0;\n}"}
{"program": "eaubanel_73", "code": "int main(int argc, char **argv){\n\tint n; \n\n\tint ngen; \n\n\tchar **grid; \n\n\tchar **newGrid; \n\n\tint id; \n\n\tint p; \n\n\tFILE *f; \n\n\tdouble time;\n\n\tMPI_Status status;\n  MPI_Request req_send_up, req_send_down;\n\n\tif(argc <2){\n\t\tif(!id) fprintf(stderr,\"usage: %s n [seed]\\n\", argv[0]);\n\t\treturn 1;\n\t}\n\tn = strtol(argv[1], NULL, 10);\n\tif(n%p){\n\t\tif(!id) fprintf(stderr,\"n must be divisible by number of processes\\n\");\n\t\treturn 1;\n\t}\n\tint m = n/p; \n\n\t\n\tint seed;\n\tif(!id){\n\t\tif(3 == argc)\n\t\t\tseed = strtol(argv[2], NULL, 10);\n\t\telse\n\t\t\tseed = -1;\n\t\tgrid = initialize(seed, n);\n\t} else{\n\t\t\tgrid = allocate2D(m+4, n);\n\t}\n\tif(grid == NULL){\n\t\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for grid\\n\");\n\t}\n\n\n\tif(!id)\n\t\tnewGrid = allocate2D(n+2, n);\n\telse\n\t\tnewGrid = allocate2D(m+4, n);\n\tif(newGrid == NULL){\n\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for newGrid\\n\");\n\t\treturn 1;\n\t}\n\tif(!id){\n\t\tf = fopen(\"gameOfLifeMPI.txt\",\"w\");\n\t\tdisplay(f, grid, n);\n\t\tprintf(\"enter number of generations: \");\n\t\tfflush(stdout);\n\t\tscanf(\"%d\", &ngen);\n\t}\n\n\tint nbDown = (id+1)%p; \n\n\tint nbUp = (id-1+p)%p; \n\n\ttime =\n\tfor(int k=0; k<ngen; k++){\n\t\tint offset = k%2;\n\t\tif(!offset){\n\t\t\t\n\n\t\t}\n\t\tupdateGrid(grid, newGrid, offset, m, n);\n\t\tchar ** temp = grid;\n\t\tgrid = newGrid;\n\t\tnewGrid = temp;\n\n\t\t#ifdef DISPLAY\n\t\tif(!id && k % DISP_FREQ == 0){\n\t\t\n\t\t\tdisplay(f, grid, n);\n\t\t}\n\t\t#endif\n\t}\n\ttime +=\n\tdouble ptime;\n\tif(!id)\n\t\tprintf(\"time in seconds: %f\\n\\n\", ptime);\n\tif(!id)\n\t\tdisplay(f, grid, n);\n\treturn 0;\n}", "label": "int main(int argc, char **argv){\n\tint n; \n\n\tint ngen; \n\n\tchar **grid; \n\n\tchar **newGrid; \n\n\tint id; \n\n\tint p; \n\n\tFILE *f; \n\n\tdouble time;\n\n\tMPI_Status status;\n  MPI_Request req_send_up, req_send_down;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &id);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\n\tif(argc <2){\n\t\tif(!id) fprintf(stderr,\"usage: %s n [seed]\\n\", argv[0]);\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\tn = strtol(argv[1], NULL, 10);\n\tif(n%p){\n\t\tif(!id) fprintf(stderr,\"n must be divisible by number of processes\\n\");\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\tint m = n/p; \n\n\t\n\tint seed;\n\tif(!id){\n\t\tif(3 == argc)\n\t\t\tseed = strtol(argv[2], NULL, 10);\n\t\telse\n\t\t\tseed = -1;\n\t\tgrid = initialize(seed, n);\n\t} else{\n\t\t\tgrid = allocate2D(m+4, n);\n\t}\n\tif(grid == NULL){\n\t\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for grid\\n\");\n\t\t\tMPI_Finalize();\n\t}\n\n\tMPI_Scatter(&grid[2][0], m*n, MPI_CHAR, &grid[2][0], m*n, MPI_CHAR,\n\t\t\t\t\t\t\t0, MPI_COMM_WORLD);\n\n\tif(!id)\n\t\tnewGrid = allocate2D(n+2, n);\n\telse\n\t\tnewGrid = allocate2D(m+4, n);\n\tif(newGrid == NULL){\n\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for newGrid\\n\");\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\tif(!id){\n\t\tf = fopen(\"gameOfLifeMPI.txt\",\"w\");\n\t\tdisplay(f, grid, n);\n\t\tprintf(\"enter number of generations: \");\n\t\tfflush(stdout);\n\t\tscanf(\"%d\", &ngen);\n\t}\n\tMPI_Bcast(&ngen, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tint nbDown = (id+1)%p; \n\n\tint nbUp = (id-1+p)%p; \n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\ttime = -MPI_Wtime();\n\tfor(int k=0; k<ngen; k++){\n\t\tint offset = k%2;\n\t\tif(!offset){\n\t\t\t\n\n\t\t\tMPI_Isend(&grid[m][0], 2*n, MPI_CHAR, nbDown, 1, \n\t\t\t\t\t\t\tMPI_COMM_WORLD, &req_send_down);\n\t\t\tMPI_Isend(&grid[2][0], 2*n, MPI_CHAR, nbUp, 2, \n\t\t\t\t\t\t\tMPI_COMM_WORLD, &req_send_up);\n\t\t\tMPI_Recv(&grid[m+2][0], 2*n, MPI_CHAR, nbDown, 2, \n\t\t\t\t\t\t\tMPI_COMM_WORLD, &status);\n\t\t\tMPI_Recv(&grid[0][0], 2*n, MPI_CHAR, nbUp, 1, \n\t\t\t\t\t\t\tMPI_COMM_WORLD, &status);\n\t\t}\n\t\tupdateGrid(grid, newGrid, offset, m, n);\n\t\tchar ** temp = grid;\n\t\tgrid = newGrid;\n\t\tnewGrid = temp;\n\n\t\t#ifdef DISPLAY\n\t\tif(!id && k % DISP_FREQ == 0){\n\t\t\tMPI_Gather(&grid[2][0], m*n, MPI_CHAR, &grid[2][0], m*n, MPI_CHAR,\n\t\t\t\t\t\t\t\t0, MPI_COMM_WORLD);\n\t\t\n\t\t\tdisplay(f, grid, n);\n\t\t}\n\t\t#endif\n\t}\n\ttime += MPI_Wtime();\n\tdouble ptime;\n\tMPI_Reduce(&time, &ptime, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n\tif(!id)\n\t\tprintf(\"time in seconds: %f\\n\\n\", ptime);\n\tMPI_Gather(&grid[2][0], m*n, MPI_CHAR, &grid[2][0], m*n, MPI_CHAR,\n\t\t\t\t\t\t0, MPI_COMM_WORLD);\n\tif(!id)\n\t\tdisplay(f, grid, n);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "joao-lima_75", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\t\n\n\n\tfloat *block = NULL;\n\tstarpu_data_handle_t block_handle;\n\n\tif (rank == 0)\n\t{\n\t\tblock = calloc(BIGSIZE*BIGSIZE*BIGSIZE, sizeof(float));\n\t\tassert(block);\n\n\t\t\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tblock[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] = 1.0f;\n\t\t}\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, BIGSIZE, BIGSIZE*BIGSIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\telse if (rank == 1)\n\t{\n\t\tblock = calloc(SIZE*SIZE*SIZE, sizeof(float));\n\t\tassert(block);\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, SIZE, SIZE*SIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\n\tif (rank == 0)\n\t{\n\t\tret = starpu_mpi_send(block_handle, 1, 0x42, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\n\t\tMPI_Status status;\n\t\tret = starpu_mpi_recv(block_handle, 1, 0x1337, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tret = starpu_data_acquire(block_handle, STARPU_R);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_data_acquire\");\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] == 33.0f);\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t}\n\telse if (rank == 1)\n\t{\n\t\tMPI_Status status;\n\t\tret = starpu_mpi_recv(block_handle, 0, 0x42, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tret = starpu_data_acquire(block_handle, STARPU_RW);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_data_acquire\");\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*SIZE + k*SIZE*SIZE] == 1.0f);\n\t\t\tblock[i + j*SIZE + k*SIZE*SIZE] = 33.0f;\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t\tret = starpu_mpi_send(block_handle, 0, 0x1337, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\t}\n\n\tFPRINTF(stdout, \"Rank %d is done\\n\", rank);\n\tfflush(stdout);\n\n\tif (rank == 0 || rank == 1)\n\t{\n\t\tstarpu_data_unregister(block_handle);\n\t\tfree(block);\n\t}\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\t\n\n\n\tfloat *block = NULL;\n\tstarpu_data_handle_t block_handle;\n\n\tif (rank == 0)\n\t{\n\t\tblock = calloc(BIGSIZE*BIGSIZE*BIGSIZE, sizeof(float));\n\t\tassert(block);\n\n\t\t\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tblock[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] = 1.0f;\n\t\t}\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, BIGSIZE, BIGSIZE*BIGSIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\telse if (rank == 1)\n\t{\n\t\tblock = calloc(SIZE*SIZE*SIZE, sizeof(float));\n\t\tassert(block);\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, SIZE, SIZE*SIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\n\tif (rank == 0)\n\t{\n\t\tret = starpu_mpi_send(block_handle, 1, 0x42, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\n\t\tMPI_Status status;\n\t\tret = starpu_mpi_recv(block_handle, 1, 0x1337, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tret = starpu_data_acquire(block_handle, STARPU_R);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_data_acquire\");\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] == 33.0f);\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t}\n\telse if (rank == 1)\n\t{\n\t\tMPI_Status status;\n\t\tret = starpu_mpi_recv(block_handle, 0, 0x42, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tret = starpu_data_acquire(block_handle, STARPU_RW);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_data_acquire\");\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*SIZE + k*SIZE*SIZE] == 1.0f);\n\t\t\tblock[i + j*SIZE + k*SIZE*SIZE] = 33.0f;\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t\tret = starpu_mpi_send(block_handle, 0, 0x1337, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\t}\n\n\tFPRINTF(stdout, \"Rank %d is done\\n\", rank);\n\tfflush(stdout);\n\n\tif (rank == 0 || rank == 1)\n\t{\n\t\tstarpu_data_unregister(block_handle);\n\t\tfree(block);\n\t}\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "byu-vv-lab_76", "code": "int main(int argc, char **argv){\n    int npes, mype, ierr;\n    double sum, val; int calc, knt=1;\n    ierr =\n    ierr =\n    ierr =\n    \n    val  = (double)mype;\n    \n#ifdef TYPE\n    if(mype%2)\n        ierr =\n    else\n        ierr =\n#elif defined OPERATOR\n    if(mype%2)\n        ierr =\n    else\n        ierr =\n#else\n    ierr =\n#endif\n    \n    calc = ((npes - 1) * npes) / 2;\n    printf(\" PE: %d sum=%5.0f calc=%d\\n\", mype, sum, calc);\n#ifdef _CIVL\n#ifndef OPERATOR\n    $assert(sum == calc);\n#endif\n#endif\n    ierr =\n}", "label": "int main(int argc, char **argv){\n    int npes, mype, ierr;\n    double sum, val; int calc, knt=1;\n    ierr = MPI_Init(&argc, &argv);\n    ierr = MPI_Comm_size(WCOMM, &npes);\n    ierr = MPI_Comm_rank(WCOMM, &mype);\n    \n    val  = (double)mype;\n    \n#ifdef TYPE\n    if(mype%2)\n        ierr = MPI_Allreduce(&val, &sum, knt, MPI_DOUBLE, MPI_SUM, WCOMM);\n    else\n        ierr = MPI_Allreduce(&val, &sum, knt, MPI_INT, MPI_SUM, WCOMM);\n#elif defined OPERATOR\n    if(mype%2)\n        ierr = MPI_Allreduce(&val, &sum, knt, MPI_DOUBLE, MPI_SUM, WCOMM);\n    else\n        ierr = MPI_Allreduce(&val, &sum, knt, MPI_DOUBLE, MPI_MAX, WCOMM);\n#else\n    ierr = MPI_Allreduce(&val, &sum, knt, MPI_DOUBLE, MPI_SUM, WCOMM);\n#endif\n    \n    calc = ((npes - 1) * npes) / 2;\n    printf(\" PE: %d sum=%5.0f calc=%d\\n\", mype, sum, calc);\n#ifdef _CIVL\n#ifndef OPERATOR\n    $assert(sum == calc);\n#endif\n#endif\n    ierr = MPI_Finalize();\n}"}
{"program": "JohnPJenkins_77", "code": "int\nmain()\n{\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n  int types = 0;\n  int am_server;\n  MPI_Comm adlb_comm;\n  ADLB_Init(2, 1, &types, &am_server, MPI_COMM_WORLD, &adlb_comm);\n\n  ADLB_Finalize();\n  return 0;\n}", "label": "int\nmain()\n{\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n  MPI_Init(&mpi_argc, &mpi_argv);\n  int types = 0;\n  int am_server;\n  MPI_Comm adlb_comm;\n  ADLB_Init(2, 1, &types, &am_server, MPI_COMM_WORLD, &adlb_comm);\n\n  ADLB_Finalize();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "annavicente_78", "code": "int main(int argc, char *argv[])\r\n{\r\n\r\n  int i, rank, size, total_works=10, *result;\r\n\r\n\r\n  if(argc > 1)\r\n    total_works = atoi(argv[1]);\r\n\r\n\r\n  if(rank == 0)\r\n  {\r\n      result = (int*) calloc(total_works, sizeof(int)); \n\r\n\r\n      master(rank, size, total_works, result);\r\n\r\n      for(i=0; i < total_works; i++)\r\n        printf(\"res[%d] = %d\\t\",i, result[i]);\r\n      printf(\"\\n\");\r\n\r\n  }\r\n  else\r\n  {\r\n      slave(rank);\r\n  }\r\n\r\n  return 0;\r\n\r\n}", "label": "int main(int argc, char *argv[])\r\n{\r\n\r\n  int i, rank, size, total_works=10, *result;\r\n\r\n  MPI_Init(&argc, &argv);\r\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\r\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n\r\n  if(argc > 1)\r\n    total_works = atoi(argv[1]);\r\n\r\n\r\n  if(rank == 0)\r\n  {\r\n      result = (int*) calloc(total_works, sizeof(int)); \n\r\n\r\n      master(rank, size, total_works, result);\r\n\r\n      for(i=0; i < total_works; i++)\r\n        printf(\"res[%d] = %d\\t\",i, result[i]);\r\n      printf(\"\\n\");\r\n\r\n  }\r\n  else\r\n  {\r\n      slave(rank);\r\n  }\r\n\r\n  MPI_Finalize();\r\n  return 0;\r\n\r\n}"}
{"program": "HopeFOAM_79", "code": "int main(int argc, char* argv[])\n{\n  Grid grid = (Grid) { .NumberOfPoints = 0, .Points = 0, .NumberOfCells = 0, .Cells = 0};\n  unsigned int numPoints[3] = {70, 60, 44};\n  double spacing[3] = {1, 1.1, 1.3};\n  InitializeGrid(&grid, numPoints, spacing);\n  Attributes attributes;\n  InitializeAttributes(&attributes, &grid);\n\n#ifdef USE_CATALYST\n  int fileNameLength = 0;\n  if(argc < 2)\n    {\n    printf(\"Must pass in a Catalyst script\\n\");\n    return 1;\n    }\n  fileNameLength = strlen(argv[1]);\n  coprocessorinitializewithpython(argv[1], &fileNameLength);\n  int i;\n  for(i=2;i<argc;i++)\n    {\n    \n\n    fileNameLength = strlen(argv[i]);\n    coprocessoraddpythonscript(argv[i], &fileNameLength);\n    }\n#endif\n  unsigned int numberOfTimeSteps = 100;\n  unsigned int timeStep;\n  for(timeStep=0;timeStep<numberOfTimeSteps;timeStep++)\n    {\n    \n\n    double time = timeStep * 0.1;\n    UpdateFields(&attributes, time);\n#ifdef USE_CATALYST\n    int lastTimeStep = 0;\n    if(timeStep == numberOfTimeSteps-1)\n      {\n      lastTimeStep = 1;\n      }\n    CatalystCoProcess(grid.NumberOfPoints, grid.Points, grid.NumberOfCells, grid.Cells,\n                      attributes.Velocity, attributes.Pressure, time, timeStep, lastTimeStep);\n#endif\n    }\n\n#ifdef USE_CATALYST\n  CatalystFinalize();\n  coprocessorfinalize();\n#endif\n\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  MPI_Init(&argc, &argv);\n  Grid grid = (Grid) { .NumberOfPoints = 0, .Points = 0, .NumberOfCells = 0, .Cells = 0};\n  unsigned int numPoints[3] = {70, 60, 44};\n  double spacing[3] = {1, 1.1, 1.3};\n  InitializeGrid(&grid, numPoints, spacing);\n  Attributes attributes;\n  InitializeAttributes(&attributes, &grid);\n\n#ifdef USE_CATALYST\n  int fileNameLength = 0;\n  if(argc < 2)\n    {\n    printf(\"Must pass in a Catalyst script\\n\");\n    MPI_Finalize();\n    return 1;\n    }\n  fileNameLength = strlen(argv[1]);\n  coprocessorinitializewithpython(argv[1], &fileNameLength);\n  int i;\n  for(i=2;i<argc;i++)\n    {\n    \n\n    fileNameLength = strlen(argv[i]);\n    coprocessoraddpythonscript(argv[i], &fileNameLength);\n    }\n#endif\n  unsigned int numberOfTimeSteps = 100;\n  unsigned int timeStep;\n  for(timeStep=0;timeStep<numberOfTimeSteps;timeStep++)\n    {\n    \n\n    double time = timeStep * 0.1;\n    UpdateFields(&attributes, time);\n#ifdef USE_CATALYST\n    int lastTimeStep = 0;\n    if(timeStep == numberOfTimeSteps-1)\n      {\n      lastTimeStep = 1;\n      }\n    CatalystCoProcess(grid.NumberOfPoints, grid.Points, grid.NumberOfCells, grid.Cells,\n                      attributes.Velocity, attributes.Pressure, time, timeStep, lastTimeStep);\n#endif\n    }\n\n#ifdef USE_CATALYST\n  CatalystFinalize();\n  coprocessorfinalize();\n#endif\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "tcsiwula_81", "code": "int main(int argc, char* argv[]) {\n   double start, finish;\n   long i, n;\n   double elapsed;\n   double x = 0;\n   int pm = 1;\n\n\n   if (argc != 2) {\n      fprintf(stderr, \"usage:  %s <n>\\n\", argv[0]);\n      exit(0);\n   }\n   n = strtol(argv[1], NULL, 10);\n\n   start =\n   for (i = 0; i < n; i++, pm = -pm)\n      x += pm*exp(sin((double) i));\n   finish =\n\n   elapsed = finish - start;\n   printf(\"Elapsed time = %.14e seconds\\n\", elapsed);\n   printf(\"Resolution of MPI_Wtime = %.14e seconds\\n\",\n\n\n   return 0;\n}", "label": "int main(int argc, char* argv[]) {\n   double start, finish;\n   long i, n;\n   double elapsed;\n   double x = 0;\n   int pm = 1;\n\n   MPI_Init(&argc, &argv);\n\n   if (argc != 2) {\n      fprintf(stderr, \"usage:  %s <n>\\n\", argv[0]);\n      exit(0);\n   }\n   n = strtol(argv[1], NULL, 10);\n\n   start = MPI_Wtime();\n   for (i = 0; i < n; i++, pm = -pm)\n      x += pm*exp(sin((double) i));\n   finish = MPI_Wtime();\n\n   elapsed = finish - start;\n   printf(\"Elapsed time = %.14e seconds\\n\", elapsed);\n   printf(\"Resolution of MPI_Wtime = %.14e seconds\\n\", MPI_Wtick());\n\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "UnProgrammatore_82", "code": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tmpz_t the_number;\n\n\tmpz_init(the_number);\n\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t}\n\telse\n\t\tmpz_set_str(the_number, argv[1], 10); \n\n\n\tif(my_rank == 0) {\n\t\tdouble t1 = get_time();\t\n\n\t\tmaster_procedure(comm_size);\n\n\t\tdouble t2 = get_time();\n\n\t\tstruct result r;\n\t\tmpz_init(r.num);\n\t\tmpz_set(r.num, the_number);\n\t\tr.w_time = t2 - t1;\n\t\t\n\t\tprint_results(r);\n\t}\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tmpz_t the_number;\n\n\tmpz_init(the_number);\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t\tMPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\telse\n\t\tmpz_set_str(the_number, argv[1], 10); \n\n\n\tif(my_rank == 0) {\n\t\tdouble t1 = get_time();\t\n\n\t\tmaster_procedure(comm_size);\n\n\t\tdouble t2 = get_time();\n\n\t\tstruct result r;\n\t\tmpz_init(r.num);\n\t\tmpz_set(r.num, the_number);\n\t\tr.w_time = t2 - t1;\n\t\t\n\t\tprint_results(r);\n\t}\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "RafaelDexter_83", "code": "int main(int argc, char *argv[]) \n{\n  int i,j;\n  int numprocs0,myid0,ID0;\n  int numprocs1,myid1;\n  int num;\n  int Num_Comm_World1;\n  int myworld0,myworld1;\n  int *NPROCS_ID1,*NPROCS_WD1;\n  int *Comm_World1;\n  int *Comm_World_StartID1;\n  MPI_Comm *MPI_CommWD1;\n\n  myworld0 = 0;\n\n  Num_Comm_World1 = 1;\n\n  \n\n\n  NPROCS_ID1 = (int*)malloc(sizeof(int)*numprocs0); \n  Comm_World1 = (int*)malloc(sizeof(int)*numprocs0); \n  NPROCS_WD1 = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  Comm_World_StartID1 = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  MPI_CommWD1 = (MPI_Comm*)malloc(sizeof(MPI_Comm)*Num_Comm_World1); \n\n  \n\n\n  Make_Comm_Worlds(MPI_COMM_WORLD, myid0, numprocs0, Num_Comm_World1, &myworld1, MPI_CommWD1, \n                   NPROCS_ID1, Comm_World1, NPROCS_WD1, Comm_World_StartID1);\n\n  \n\n\n\n  printf(\"numprocs0=%2d myid0=%2d myworld1=%2d numprocs1=%2d myid1=%2d\\n\",\n         numprocs0,myid0,myworld1,numprocs1,myid1);\n\n  {\n\n  int numprocs2,myid2,myworld2;\n  int *NPROCS_ID2,*NPROCS_WD2;\n  int *Comm_World2;\n  int *Comm_World_StartID2;\n  MPI_Comm *MPI_CommWD2;\n  int Num_Comm_World2;\n\n  Num_Comm_World2 = 2;\n\n  \n\n\n  NPROCS_ID2 = (int*)malloc(sizeof(int)*numprocs1); \n  Comm_World2 = (int*)malloc(sizeof(int)*numprocs1); \n  NPROCS_WD2 = (int*)malloc(sizeof(int)*Num_Comm_World2); \n  Comm_World_StartID2 = (int*)malloc(sizeof(int)*Num_Comm_World2); \n  MPI_CommWD2 = (MPI_Comm*)malloc(sizeof(MPI_Comm)*Num_Comm_World2); \n\n  \n\n\n  Make_Comm_Worlds(MPI_CommWD1[myworld1], myid1, numprocs1, Num_Comm_World2, &myworld2,\n                   MPI_CommWD2, NPROCS_ID2, Comm_World2, NPROCS_WD2, Comm_World_StartID2);\n\n  \n\n\n\n  printf(\"numprocs0=%2d myid0=%2d myworld1=%2d numprocs1=%2d myid1=%2d myworld2=%2d numprocs2=%2d myid2=%2d \\n\",\n         numprocs0,myid0,myworld1,numprocs1,myid1,myworld2,numprocs2,myid2);\n\n  \n\n\n  free(NPROCS_ID2);\n  free(Comm_World2);\n  free(NPROCS_WD2);\n  free(Comm_World_StartID2);\n  free(MPI_CommWD2);\n\n\n  }\n\n\n  \n\n\n  free(NPROCS_ID1);\n  free(Comm_World1);\n  free(NPROCS_WD1);\n  free(Comm_World_StartID1);\n  free(MPI_CommWD1);\n\n  \n\n\n\n}", "label": "int main(int argc, char *argv[]) \n{\n  int i,j;\n  int numprocs0,myid0,ID0;\n  int numprocs1,myid1;\n  int num;\n  int Num_Comm_World1;\n  int myworld0,myworld1;\n  int *NPROCS_ID1,*NPROCS_WD1;\n  int *Comm_World1;\n  int *Comm_World_StartID1;\n  MPI_Comm *MPI_CommWD1;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs0);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid0);\n  myworld0 = 0;\n\n  Num_Comm_World1 = 1;\n\n  \n\n\n  NPROCS_ID1 = (int*)malloc(sizeof(int)*numprocs0); \n  Comm_World1 = (int*)malloc(sizeof(int)*numprocs0); \n  NPROCS_WD1 = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  Comm_World_StartID1 = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  MPI_CommWD1 = (MPI_Comm*)malloc(sizeof(MPI_Comm)*Num_Comm_World1); \n\n  \n\n\n  Make_Comm_Worlds(MPI_COMM_WORLD, myid0, numprocs0, Num_Comm_World1, &myworld1, MPI_CommWD1, \n                   NPROCS_ID1, Comm_World1, NPROCS_WD1, Comm_World_StartID1);\n\n  \n\n\n  MPI_Comm_size(MPI_CommWD1[myworld1],&numprocs1);\n  MPI_Comm_rank(MPI_CommWD1[myworld1],&myid1);\n\n  printf(\"numprocs0=%2d myid0=%2d myworld1=%2d numprocs1=%2d myid1=%2d\\n\",\n         numprocs0,myid0,myworld1,numprocs1,myid1);\n\n  {\n\n  int numprocs2,myid2,myworld2;\n  int *NPROCS_ID2,*NPROCS_WD2;\n  int *Comm_World2;\n  int *Comm_World_StartID2;\n  MPI_Comm *MPI_CommWD2;\n  int Num_Comm_World2;\n\n  Num_Comm_World2 = 2;\n\n  \n\n\n  NPROCS_ID2 = (int*)malloc(sizeof(int)*numprocs1); \n  Comm_World2 = (int*)malloc(sizeof(int)*numprocs1); \n  NPROCS_WD2 = (int*)malloc(sizeof(int)*Num_Comm_World2); \n  Comm_World_StartID2 = (int*)malloc(sizeof(int)*Num_Comm_World2); \n  MPI_CommWD2 = (MPI_Comm*)malloc(sizeof(MPI_Comm)*Num_Comm_World2); \n\n  \n\n\n  Make_Comm_Worlds(MPI_CommWD1[myworld1], myid1, numprocs1, Num_Comm_World2, &myworld2,\n                   MPI_CommWD2, NPROCS_ID2, Comm_World2, NPROCS_WD2, Comm_World_StartID2);\n\n  \n\n\n  MPI_Comm_size(MPI_CommWD2[myworld2],&numprocs2);\n  MPI_Comm_rank(MPI_CommWD2[myworld2],&myid2);\n\n  printf(\"numprocs0=%2d myid0=%2d myworld1=%2d numprocs1=%2d myid1=%2d myworld2=%2d numprocs2=%2d myid2=%2d \\n\",\n         numprocs0,myid0,myworld1,numprocs1,myid1,myworld2,numprocs2,myid2);\n\n  \n\n\n  free(NPROCS_ID2);\n  free(Comm_World2);\n  free(NPROCS_WD2);\n  free(Comm_World_StartID2);\n  free(MPI_CommWD2);\n\n\n  }\n\n\n  \n\n\n  free(NPROCS_ID1);\n  free(Comm_World1);\n  free(NPROCS_WD1);\n  free(Comm_World_StartID1);\n  free(MPI_CommWD1);\n\n  \n\n\n  MPI_Finalize();\n\n}"}
{"program": "90jrong_84", "code": "int main(int argc, char** argv)\n{\n    extern int optind;\n    char filename[128];\n    int i, j, rank, nprocs, verbose=1, err;\n    int ncid, cmode, omode, varid, dimid[2], buf[NY][NX];\n    char str_att[128];\n    float float_att[100];\n    size_t global_ny, global_nx, start[2], count[2];\n\n\n    \n\n    while ((i = getopt(argc, argv, \"hq\")) != EOF)\n        switch(i) {\n            case 'q': verbose = 0;\n                      break;\n            case 'h':\n            default:  if (rank==0) usage(argv[0]);\n                      return 0;\n        }\n    argc -= optind;\n    argv += optind;\n    if (argc == 1) strcpy(filename, argv[0]); \n\n    else strcpy(filename, \"testfile.nc\");\n\n\n    \n\n    cmode = NC_CLOBBER;\n    err = nc_create_par(filename, cmode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid); FATAL_ERR\n\n    \n\n    global_ny = NY;\n    global_nx = NX * nprocs;\n\n    for (i=0; i<NY; i++)\n        for (j=0; j<NX; j++)\n             buf[i][j] = rank;\n\n    \n\n    time_t ltime = time(NULL); \n\n    asctime_r(localtime(&ltime), str_att);\n\n    \n\n\n    err = nc_put_att_text(ncid, NC_GLOBAL, \"history\", strlen(str_att),\n                          &str_att[0]); ERR\n\n    \n\n    err = nc_def_dim(ncid, \"Y\", global_ny, &dimid[0]); ERR\n    err = nc_def_dim(ncid, \"X\", global_nx, &dimid[1]); ERR\n\n    \n\n    err = nc_def_var(ncid, \"var\", NC_INT, 2, dimid, &varid); ERR\n\n    \n\n    strcpy(str_att, \"example attribute of type text.\");\n    err = nc_put_att_text(ncid, varid, \"str_att_name\", strlen(str_att),\n                          &str_att[0]); ERR\n\n    for (i=0; i<8; i++) float_att[i] = i;\n    err = nc_put_att_float(ncid, varid, \"float_att_name\", NC_FLOAT, 8,\n                           &float_att[0]); ERR\n\n    \n\n    err = nc_enddef(ncid); ERR\n\n    \n\n    err = nc_var_par_access(ncid, NC_GLOBAL, NC_COLLECTIVE); ERR\n\n    \n\n    start[0] = 0;\n    start[1] = NX * rank;\n    count[0] = NY;\n    count[1] = NX;\n\n    err = nc_put_vara_int(ncid, varid, start, count, &buf[0][0]); ERR\n\n    err = nc_close(ncid); ERR\n\n    omode = NC_NOWRITE;\n    err = nc_open_par(filename, omode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid); FATAL_ERR\n\n    \n\n    err = nc_inq_dimid(ncid, \"Y\", &dimid[0]); ERR\n    err = nc_inq_dimid(ncid, \"X\", &dimid[1]); ERR\n\n    err = nc_inq_dimlen(ncid, dimid[0], &global_ny); ERR\n    err = nc_inq_dimlen(ncid, dimid[1], &global_nx); ERR\n\n    \n\n    err = nc_inq_varid(ncid, \"var\", &varid); ERR\n\n    \n\n    err = nc_var_par_access(ncid, NC_GLOBAL, NC_COLLECTIVE); ERR\n\n    \n\n    err = nc_get_vara_int(ncid, varid, start, count, &buf[0][0]); ERR\n\n    \n\n    err = nc_close(ncid); ERR\n\nfn_exit:\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n    extern int optind;\n    char filename[128];\n    int i, j, rank, nprocs, verbose=1, err;\n    int ncid, cmode, omode, varid, dimid[2], buf[NY][NX];\n    char str_att[128];\n    float float_att[100];\n    size_t global_ny, global_nx, start[2], count[2];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    \n\n    while ((i = getopt(argc, argv, \"hq\")) != EOF)\n        switch(i) {\n            case 'q': verbose = 0;\n                      break;\n            case 'h':\n            default:  if (rank==0) usage(argv[0]);\n                      MPI_Finalize();\n                      return 0;\n        }\n    argc -= optind;\n    argv += optind;\n    if (argc == 1) strcpy(filename, argv[0]); \n\n    else strcpy(filename, \"testfile.nc\");\n\n    MPI_Bcast(filename, 128, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n    \n\n    cmode = NC_CLOBBER;\n    err = nc_create_par(filename, cmode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid); FATAL_ERR\n\n    \n\n    global_ny = NY;\n    global_nx = NX * nprocs;\n\n    for (i=0; i<NY; i++)\n        for (j=0; j<NX; j++)\n             buf[i][j] = rank;\n\n    \n\n    time_t ltime = time(NULL); \n\n    asctime_r(localtime(&ltime), str_att);\n\n    \n\n    MPI_Bcast(str_att, strlen(str_att), MPI_CHAR, 0, MPI_COMM_WORLD);\n\n    err = nc_put_att_text(ncid, NC_GLOBAL, \"history\", strlen(str_att),\n                          &str_att[0]); ERR\n\n    \n\n    err = nc_def_dim(ncid, \"Y\", global_ny, &dimid[0]); ERR\n    err = nc_def_dim(ncid, \"X\", global_nx, &dimid[1]); ERR\n\n    \n\n    err = nc_def_var(ncid, \"var\", NC_INT, 2, dimid, &varid); ERR\n\n    \n\n    strcpy(str_att, \"example attribute of type text.\");\n    err = nc_put_att_text(ncid, varid, \"str_att_name\", strlen(str_att),\n                          &str_att[0]); ERR\n\n    for (i=0; i<8; i++) float_att[i] = i;\n    err = nc_put_att_float(ncid, varid, \"float_att_name\", NC_FLOAT, 8,\n                           &float_att[0]); ERR\n\n    \n\n    err = nc_enddef(ncid); ERR\n\n    \n\n    err = nc_var_par_access(ncid, NC_GLOBAL, NC_COLLECTIVE); ERR\n\n    \n\n    start[0] = 0;\n    start[1] = NX * rank;\n    count[0] = NY;\n    count[1] = NX;\n\n    err = nc_put_vara_int(ncid, varid, start, count, &buf[0][0]); ERR\n\n    err = nc_close(ncid); ERR\n\n    omode = NC_NOWRITE;\n    err = nc_open_par(filename, omode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid); FATAL_ERR\n\n    \n\n    err = nc_inq_dimid(ncid, \"Y\", &dimid[0]); ERR\n    err = nc_inq_dimid(ncid, \"X\", &dimid[1]); ERR\n\n    err = nc_inq_dimlen(ncid, dimid[0], &global_ny); ERR\n    err = nc_inq_dimlen(ncid, dimid[1], &global_nx); ERR\n\n    \n\n    err = nc_inq_varid(ncid, \"var\", &varid); ERR\n\n    \n\n    err = nc_var_par_access(ncid, NC_GLOBAL, NC_COLLECTIVE); ERR\n\n    \n\n    err = nc_get_vara_int(ncid, varid, start, count, &buf[0][0]); ERR\n\n    \n\n    err = nc_close(ncid); ERR\n\nfn_exit:\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "bmi-forum_85", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\tLinkedList *numList = NULL;\n\tLinkedListIterator *iterator = NULL;\n\tLinkedListIterator *iterator1 = NULL;\n\tint i = 0;\n\tint *array[NUM_DATA] = {0};\n\tvoid *result = NULL, *result1 = NULL;\n\tdouble sum;\n\tfloat randomNum, randomMax;\n\tStream *myStream = NULL;\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\t\n\t\tmyStream = Journal_Register( InfoStream_Type, \"LinkedListStream\" );\n\t\tnumList = LinkedList_New(\n\t\t\t\t\tcompareFunction,\n\t\t\t\t\tdataCopyFunction,\n\t\t\t\t\tdataPrintFunction,\n\t\t\t\t\tNULL,\n\t\t\t\t\tLINKEDLIST_UNSORTED);\n\t\t\n\t\titerator = LinkedListIterator_New( numList );\n\t\titerator1 = LinkedListIterator_New( numList );\n\t\t\n\t\tJournal_Printf( myStream, \"Inserting data into the List\\n\");\n\t\t\n\t\tfor(i=0; i<NUM_DATA; i++){\n\t\t\tarray[i] = Memory_Alloc(int, \"testLinkedListIterator_ArrayEntry\");\n\t\t\trandomNum = rand();\n\t\t\trandomMax = RAND_MAX;\n\t\t\t*array[i] = NUM_DATA - i;\n\t\t\tLinkedList_InsertNode(numList, (void*)array[i], sizeof(int));\n\t\t}\n\t\n\t\tJournal_Printf(myStream, \"\\nPrinting the list\\n\");\n\t\tPrint( (void*)numList, myStream );\n\t\t\n\t\tJournal_Printf( myStream, \"Calculating sum.. \\n\");\n\t\tsum = 0;\n\t\tfor( result = LinkedListIterator_First( iterator ); result; result = LinkedListIterator_Next( iterator ) ){\n\t\t\tfor( result1 = LinkedListIterator_First( iterator1 ); result1; result1 = LinkedListIterator_Next( iterator1 ) ){\n\t\t\t\tsum += (double)(*(int*)result) * (*(int*)result1);\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"\\tsum = %lf\\n\", sum );\n\t}\n\n\tJournal_Printf( myStream, \"\\nDeleting the List\\n\" );\n\tStg_Class_Delete( (void*)numList );\n\t\n\n        for(i=0; i<NUM_DATA; i++)\n          Memory_Free(array[i]);\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\tLinkedList *numList = NULL;\n\tLinkedListIterator *iterator = NULL;\n\tLinkedListIterator *iterator1 = NULL;\n\tint i = 0;\n\tint *array[NUM_DATA] = {0};\n\tvoid *result = NULL, *result1 = NULL;\n\tdouble sum;\n\tfloat randomNum, randomMax;\n\tStream *myStream = NULL;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\t\n\t\tmyStream = Journal_Register( InfoStream_Type, \"LinkedListStream\" );\n\t\tnumList = LinkedList_New(\n\t\t\t\t\tcompareFunction,\n\t\t\t\t\tdataCopyFunction,\n\t\t\t\t\tdataPrintFunction,\n\t\t\t\t\tNULL,\n\t\t\t\t\tLINKEDLIST_UNSORTED);\n\t\t\n\t\titerator = LinkedListIterator_New( numList );\n\t\titerator1 = LinkedListIterator_New( numList );\n\t\t\n\t\tJournal_Printf( myStream, \"Inserting data into the List\\n\");\n\t\t\n\t\tfor(i=0; i<NUM_DATA; i++){\n\t\t\tarray[i] = Memory_Alloc(int, \"testLinkedListIterator_ArrayEntry\");\n\t\t\trandomNum = rand();\n\t\t\trandomMax = RAND_MAX;\n\t\t\t*array[i] = NUM_DATA - i;\n\t\t\tLinkedList_InsertNode(numList, (void*)array[i], sizeof(int));\n\t\t}\n\t\n\t\tJournal_Printf(myStream, \"\\nPrinting the list\\n\");\n\t\tPrint( (void*)numList, myStream );\n\t\t\n\t\tJournal_Printf( myStream, \"Calculating sum.. \\n\");\n\t\tsum = 0;\n\t\tfor( result = LinkedListIterator_First( iterator ); result; result = LinkedListIterator_Next( iterator ) ){\n\t\t\tfor( result1 = LinkedListIterator_First( iterator1 ); result1; result1 = LinkedListIterator_Next( iterator1 ) ){\n\t\t\t\tsum += (double)(*(int*)result) * (*(int*)result1);\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"\\tsum = %lf\\n\", sum );\n\t}\n\n\tJournal_Printf( myStream, \"\\nDeleting the List\\n\" );\n\tStg_Class_Delete( (void*)numList );\n\t\n\n        for(i=0; i<NUM_DATA; i++)\n          Memory_Free(array[i]);\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "PSUmodeling_87", "code": "int main(int argc, char *argv[])\n{\n  realtype dx, reltol, abstol, t, tout, umax;\n  N_Vector u;\n  UserData data;\n  void *cvode_mem;\n  int iout, retval, my_pe, npes;\n  long int nst;\n  HYPRE_Int local_N, nperpe, nrem, my_base;\n  HYPRE_ParVector Upar; \n\n  HYPRE_IJVector  Uij;  \n\n  SUNNonlinearSolver NLS;\n\n  MPI_Comm comm;\n\n  u = NULL;\n  data = NULL;\n  cvode_mem = NULL;\n\n  \n\n  comm = MPI_COMM_WORLD;\n\n  \n\n  nperpe = NEQ/npes;\n  nrem = NEQ - npes*nperpe;\n  local_N = (my_pe < nrem) ? nperpe+1 : nperpe;\n  my_base = (my_pe < nrem) ? my_pe*local_N : my_pe*nperpe + nrem;\n  \n  \n\n  HYPRE_IJVectorCreate(comm, my_base, my_base + local_N - 1, &Uij);\n  HYPRE_IJVectorSetObjectType(Uij, HYPRE_PARCSR);\n  HYPRE_IJVectorInitialize(Uij);\n\n  \n\n  data = (UserData) malloc(sizeof *data);  \n\n\n  data->comm = comm;\n  data->npes = npes;\n  data->my_pe = my_pe;\n\n  reltol = ZERO;  \n\n  abstol = ATOL;\n\n  dx = data->dx = XMAX/((realtype)(MX+1));  \n\n  data->hdcoef = RCONST(1.0)/(dx*dx);\n  data->hacoef = RCONST(0.5)/(RCONST(2.0)*dx);\n\n  \n\n  SetIC(Uij, dx, local_N, my_base);\n  HYPRE_IJVectorAssemble(Uij);\n  HYPRE_IJVectorGetObject(Uij, (void**) &Upar);\n\n  u = N_VMake_ParHyp(Upar);  \n\n  \n  \n\n  cvode_mem = CVodeCreate(CV_ADAMS);\n\n  retval = CVodeSetUserData(cvode_mem, data);\n\n  \n\n  retval = CVodeInit(cvode_mem, f, T0, u);\n  if(check_retval(&retval, \"CVodeInit\", 1, my_pe)) return(1);\n\n  \n\n  retval = CVodeSStolerances(cvode_mem, reltol, abstol);\n  if (check_retval(&retval, \"CVodeSStolerances\", 1, my_pe)) return(1);\n\n  \n\n  NLS = SUNNonlinSol_FixedPoint(u, 0);\n  if(check_retval((void *)NLS, \"SUNNonlinSol_FixedPoint\", 0, my_pe)) return(1);\n\n  \n\n  retval = CVodeSetNonlinearSolver(cvode_mem, NLS);\n  if(check_retval(&retval, \"CVodeSetNonlinearSolver\", 1, my_pe)) return(1);\n\n  if (my_pe == 0) PrintIntro(npes);\n\n  umax = N_VMaxNorm(u);\n\n  if (my_pe == 0) {\n    t = T0;\n    PrintData(t, umax, 0);\n  }\n\n  \n\n\n  for (iout=1, tout=T1; iout <= NOUT; iout++, tout += DTOUT) {\n    retval = CVode(cvode_mem, tout, u, &t, CV_NORMAL);\n    if(check_retval(&retval, \"CVode\", 1, my_pe)) break;\n    umax = N_VMaxNorm(u);\n    retval = CVodeGetNumSteps(cvode_mem, &nst);\n    check_retval(&retval, \"CVodeGetNumSteps\", 1, my_pe);\n    if (my_pe == 0) PrintData(t, umax, nst);\n  }\n\n  if (my_pe == 0) \n    PrintFinalStats(cvode_mem);  \n\n\n  N_VDestroy(u);              \n\n  HYPRE_IJVectorDestroy(Uij); \n\n  CVodeFree(&cvode_mem);      \n\n  SUNNonlinSolFree(NLS);      \n\n  free(data);                 \n\n\n\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  realtype dx, reltol, abstol, t, tout, umax;\n  N_Vector u;\n  UserData data;\n  void *cvode_mem;\n  int iout, retval, my_pe, npes;\n  long int nst;\n  HYPRE_Int local_N, nperpe, nrem, my_base;\n  HYPRE_ParVector Upar; \n\n  HYPRE_IJVector  Uij;  \n\n  SUNNonlinearSolver NLS;\n\n  MPI_Comm comm;\n\n  u = NULL;\n  data = NULL;\n  cvode_mem = NULL;\n\n  \n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &my_pe);\n\n  \n\n  nperpe = NEQ/npes;\n  nrem = NEQ - npes*nperpe;\n  local_N = (my_pe < nrem) ? nperpe+1 : nperpe;\n  my_base = (my_pe < nrem) ? my_pe*local_N : my_pe*nperpe + nrem;\n  \n  \n\n  HYPRE_IJVectorCreate(comm, my_base, my_base + local_N - 1, &Uij);\n  HYPRE_IJVectorSetObjectType(Uij, HYPRE_PARCSR);\n  HYPRE_IJVectorInitialize(Uij);\n\n  \n\n  data = (UserData) malloc(sizeof *data);  \n\n  if(check_retval((void *)data, \"malloc\", 2, my_pe)) MPI_Abort(comm, 1);\n\n  data->comm = comm;\n  data->npes = npes;\n  data->my_pe = my_pe;\n\n  reltol = ZERO;  \n\n  abstol = ATOL;\n\n  dx = data->dx = XMAX/((realtype)(MX+1));  \n\n  data->hdcoef = RCONST(1.0)/(dx*dx);\n  data->hacoef = RCONST(0.5)/(RCONST(2.0)*dx);\n\n  \n\n  SetIC(Uij, dx, local_N, my_base);\n  HYPRE_IJVectorAssemble(Uij);\n  HYPRE_IJVectorGetObject(Uij, (void**) &Upar);\n\n  u = N_VMake_ParHyp(Upar);  \n\n  if(check_retval((void *)u, \"N_VNew\", 0, my_pe)) MPI_Abort(comm, 1);\n  \n  \n\n  cvode_mem = CVodeCreate(CV_ADAMS);\n  if(check_retval((void *)cvode_mem, \"CVodeCreate\", 0, my_pe)) MPI_Abort(comm, 1);\n\n  retval = CVodeSetUserData(cvode_mem, data);\n  if(check_retval(&retval, \"CVodeSetUserData\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  retval = CVodeInit(cvode_mem, f, T0, u);\n  if(check_retval(&retval, \"CVodeInit\", 1, my_pe)) return(1);\n\n  \n\n  retval = CVodeSStolerances(cvode_mem, reltol, abstol);\n  if (check_retval(&retval, \"CVodeSStolerances\", 1, my_pe)) return(1);\n\n  \n\n  NLS = SUNNonlinSol_FixedPoint(u, 0);\n  if(check_retval((void *)NLS, \"SUNNonlinSol_FixedPoint\", 0, my_pe)) return(1);\n\n  \n\n  retval = CVodeSetNonlinearSolver(cvode_mem, NLS);\n  if(check_retval(&retval, \"CVodeSetNonlinearSolver\", 1, my_pe)) return(1);\n\n  if (my_pe == 0) PrintIntro(npes);\n\n  umax = N_VMaxNorm(u);\n\n  if (my_pe == 0) {\n    t = T0;\n    PrintData(t, umax, 0);\n  }\n\n  \n\n\n  for (iout=1, tout=T1; iout <= NOUT; iout++, tout += DTOUT) {\n    retval = CVode(cvode_mem, tout, u, &t, CV_NORMAL);\n    if(check_retval(&retval, \"CVode\", 1, my_pe)) break;\n    umax = N_VMaxNorm(u);\n    retval = CVodeGetNumSteps(cvode_mem, &nst);\n    check_retval(&retval, \"CVodeGetNumSteps\", 1, my_pe);\n    if (my_pe == 0) PrintData(t, umax, nst);\n  }\n\n  if (my_pe == 0) \n    PrintFinalStats(cvode_mem);  \n\n\n  N_VDestroy(u);              \n\n  HYPRE_IJVectorDestroy(Uij); \n\n  CVodeFree(&cvode_mem);      \n\n  SUNNonlinSolFree(NLS);      \n\n  free(data);                 \n\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "ghisvail_89", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2r(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "callmetaste_90", "code": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,choice;\n   MPI_Status status; \n\n   adainit();\n\n   if(myid == 0) choice = prompt_for_precision();\n   \n\n\n   if(choice == 0)\n   {\n      standard_dimension_broadcast(myid,&n);\n      standard_monomials_broadcast(myid,n);\n      standard_print_broadcast(myid); \n   }\n   else if(choice == 1)\n   {\n      dobldobl_dimension_broadcast(myid,&n);\n      dobldobl_monomials_broadcast(myid,n);\n      dobldobl_print_broadcast(myid); \n   }\n   else if(choice == 2)\n   {\n      quaddobl_dimension_broadcast(myid,&n);\n      quaddobl_monomials_broadcast(myid,n);\n      quaddobl_print_broadcast(myid); \n   }\n   else\n      if(myid == 0) printf(\"Invalid selection.\\n\");\n\n\n   adafinal();\n\n   return 0;\n}", "label": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,choice;\n   MPI_Status status; \n\n   adainit();\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n   if(myid == 0) choice = prompt_for_precision();\n   \n\n   MPI_Bcast(&choice,1,MPI_INT,0,MPI_COMM_WORLD);  \n\n   if(choice == 0)\n   {\n      standard_dimension_broadcast(myid,&n);\n      standard_monomials_broadcast(myid,n);\n      standard_print_broadcast(myid); \n   }\n   else if(choice == 1)\n   {\n      dobldobl_dimension_broadcast(myid,&n);\n      dobldobl_monomials_broadcast(myid,n);\n      dobldobl_print_broadcast(myid); \n   }\n   else if(choice == 2)\n   {\n      quaddobl_dimension_broadcast(myid,&n);\n      quaddobl_monomials_broadcast(myid,n);\n      quaddobl_print_broadcast(myid); \n   }\n   else\n      if(myid == 0) printf(\"Invalid selection.\\n\");\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   MPI_Finalize();\n   adafinal();\n\n   return 0;\n}"}
{"program": "xuyiqi_91", "code": "int\nmain(int     argc,\n     char ** argv)\n{\n    int           i;\n    IOR_queue_t * tests;\n\n    \n\n    for (i = 1; i < argc; i++) {\n        if (strcmp(argv[i], \"-h\") == 0) {\n            DisplayUsage(argv);\n            return(0);\n        }\n    }\n\n    \n\n    \n\n    \n\n    \n    \n\n    tests = SetupTests(argc, argv);\n    verbose = tests->testParameters.verbose;\n    tests->testParameters.testComm = MPI_COMM_WORLD;\n    \n    \n\n    if (rank == 0 && tests->testParameters.showHelp == TRUE) {\n      DisplayUsage(argv);\n    }\n\tprintf(\"RING:%d\\n\", initialTestParams.ringNumber);\n    sprintf(log_file_name,\"log_file_rank_%d_%d\",initialTestParams.ringNumber,rank); \n    pFile_log_time_stamp = fopen (log_file_name,\"w+\");\n    setbuf(pFile_log_time_stamp, (char *) NULL);\n    \n\n    \n\n\n    \n    \n\n    while (tests != NULL) {\n      verbose = tests->testParameters.verbose;\n      if (rank == 0 && verbose >= VERBOSE_0) {\n          ShowInfo(argc, argv, &tests->testParameters);\n      }\n      if (rank == 0 && verbose >= VERBOSE_3) {\n        ShowTest(&tests->testParameters);\n      }\n      TestIoSys(&tests->testParameters);\n      tests = tests->nextTest;\n    }\n\n    fclose(pFile_log_time_stamp);\n\n    \n\n    if (rank == 0 && verbose >= VERBOSE_0) {\n      fprintf(stdout, \"Run finished: %s\", CurrentTimeString());\n    }\n\n\n    return(totalErrorCount);\n\n}", "label": "int\nmain(int     argc,\n     char ** argv)\n{\n    int           i;\n    IOR_queue_t * tests;\n\n    \n\n    for (i = 1; i < argc; i++) {\n        if (strcmp(argv[i], \"-h\") == 0) {\n            DisplayUsage(argv);\n            return(0);\n        }\n    }\n\n    \n\n    MPI_CHECK(MPI_Init(&argc, &argv), \"cannot initialize MPI\");\n    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &numTasksWorld),\n              \"cannot get number of tasks\");\n    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank), \"cannot get rank\");\n    \n\n    \n\n    \n    \n\n    tests = SetupTests(argc, argv);\n    verbose = tests->testParameters.verbose;\n    tests->testParameters.testComm = MPI_COMM_WORLD;\n    \n    \n\n    if (rank == 0 && tests->testParameters.showHelp == TRUE) {\n      DisplayUsage(argv);\n    }\n\tprintf(\"RING:%d\\n\", initialTestParams.ringNumber);\n    sprintf(log_file_name,\"log_file_rank_%d_%d\",initialTestParams.ringNumber,rank); \n    pFile_log_time_stamp = fopen (log_file_name,\"w+\");\n    setbuf(pFile_log_time_stamp, (char *) NULL);\n    \n\n    \n\n\n    \n    \n\n    while (tests != NULL) {\n      verbose = tests->testParameters.verbose;\n      if (rank == 0 && verbose >= VERBOSE_0) {\n          ShowInfo(argc, argv, &tests->testParameters);\n      }\n      if (rank == 0 && verbose >= VERBOSE_3) {\n        ShowTest(&tests->testParameters);\n      }\n      TestIoSys(&tests->testParameters);\n      tests = tests->nextTest;\n    }\n\n    fclose(pFile_log_time_stamp);\n\n    \n\n    if (rank == 0 && verbose >= VERBOSE_0) {\n      fprintf(stdout, \"Run finished: %s\", CurrentTimeString());\n    }\n\n    MPI_CHECK(MPI_Finalize(), \"cannot finalize MPI\");\n\n    return(totalErrorCount);\n\n}"}
{"program": "rpereira-dev_92", "code": "nt main(int argc, char **argv)\n{\n    int n = atoi(argv[1]);\n    int rank, iter, nproc;\n    char *buf;\n    double tend, tbeg;\n    bcast_info_t binfo;\n\n\n\n    buf = (char*)malloc(n*sizeof(char));\n\n    if (rank == \n0)\n    {\n        printf(\"%d bytes\\n\", n);\n        fflush(stdout);\n        memset(buf, 0, n);\n    }\n\n    init_bcast_info(nproc, rank, &binfo);\n\n    tbeg =\n\n    for(iter = 0 ; iter < NITER ; iter++)\n    {\n        btopo_bcast(buf, n, &binfo);\n    }\n\n    tend =\n\n    if (rank == 0)\n    {\n        printf(\"Telaps for %d bcast = %.3e s\\n\", NITER, tend - tbeg);\n    }\n\n    free(buf);\n\n    return 0;\n}\n\n", "label": "nt main(int argc, char **argv)\n{\n    int n = atoi(argv[1]);\n    int rank, iter, nproc;\n    char *buf;\n    double tend, tbeg;\n    bcast_info_t binfo;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    buf = (char*)malloc(n*sizeof(char));\n\n    if (rank == \n0)\n    {\n        printf(\"%d bytes\\n\", n);\n        fflush(stdout);\n        memset(buf, 0, n);\n    }\n\n    init_bcast_info(nproc, rank, &binfo);\n\n    tbeg = MPI_Wtime();\n\n    for(iter = 0 ; iter < NITER ; iter++)\n    {\n        btopo_bcast(buf, n, &binfo);\n    }\n\n    tend = MPI_Wtime();\n\n    if (rank == 0)\n    {\n        printf(\"Telaps for %d bcast = %.3e s\\n\", NITER, tend - tbeg);\n    }\n\n    free(buf);\n\n    MPI_Finalize();\n    return 0;\n}\n\n"}
{"program": "benkirk_93", "code": "int main (int argc, char *argv[])\n{\n    int myid, numprocs;\n    struct timespec tp_before, tp_after;\n    long duration = 0, min, max, avg;\n\n    clock_gettime(CLOCK_REALTIME, &tp_before);\n    clock_gettime(CLOCK_REALTIME, &tp_after);\n\n    duration = (tp_after.tv_sec - tp_before.tv_sec) * 1e3;\n    duration += (tp_after.tv_nsec - tp_before.tv_nsec) / 1e6;\n\n\n    avg = avg/numprocs;\n\n    if (myid == 0) {\n        fprintf(stdout, HEADER);\n        fprintf(stdout, \"nprocs: %d, min: %ld ms, max: %ld ms, avg: %ld ms\\n\",\n                numprocs, min, max, avg);\n        fflush(stdout);\n    }\n\n\n    return EXIT_SUCCESS;\n}", "label": "int main (int argc, char *argv[])\n{\n    int myid, numprocs;\n    struct timespec tp_before, tp_after;\n    long duration = 0, min, max, avg;\n\n    clock_gettime(CLOCK_REALTIME, &tp_before);\n    MPI_Init(&argc, &argv);\n    clock_gettime(CLOCK_REALTIME, &tp_after);\n\n    duration = (tp_after.tv_sec - tp_before.tv_sec) * 1e3;\n    duration += (tp_after.tv_nsec - tp_before.tv_nsec) / 1e6;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\n    MPI_Reduce(&duration, &min, 1, MPI_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&duration, &max, 1, MPI_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&duration, &avg, 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    avg = avg/numprocs;\n\n    if (myid == 0) {\n        fprintf(stdout, HEADER);\n        fprintf(stdout, \"nprocs: %d, min: %ld ms, max: %ld ms, avg: %ld ms\\n\",\n                numprocs, min, max, avg);\n        fflush(stdout);\n    }\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "gnu3ra_96", "code": "int main(int argc, char ** argv) {\n\tint nprocs, mynod, errcode;\n\toptions my_options = {NULL, 0, 0};\n\tMPI_File fh;\n\tMPI_Status status;\n\tMPI_Info  info;\n\n\n\tparse_args(argc, argv, mynod, &my_options);\n\n\tif (my_options.do_aggregation) {\n\t} else {\n\t\tinfo = MPI_INFO_NULL;\n\t}\n\n\t\n\n\terrcode =\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_open\");\n\t}\n\n\terrcode =\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_close\");\n\t}\n\n\t\n\n\terrcode =\n\tif (errcode == MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_open: expected an error: got\");\n\t}\n\n\t\n\n\n\t\n\n\terrcode =\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_open\");\n\t}\n\n\terrcode =\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_close\");\n\t}\n\n\tif (mynod == 0) {\n\t\tprintf(\" No Errors\\n\");\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char ** argv) {\n\tint nprocs, mynod, errcode;\n\toptions my_options = {NULL, 0, 0};\n\tMPI_File fh;\n\tMPI_Status status;\n\tMPI_Info  info;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n\n\tparse_args(argc, argv, mynod, &my_options);\n\n\tif (my_options.do_aggregation) {\n\t\tMPI_Info_create(&info);\n\t\tMPI_Info_set(info, \"romio_no_indep_rw\", \"true\");\n\t\tMPI_Info_set(info, \"cb_config_list\", \"leela.mcs.anl.gov:1\");\n\t} else {\n\t\tinfo = MPI_INFO_NULL;\n\t}\n\n\t\n\n\terrcode = MPI_File_open(MPI_COMM_WORLD, my_options.fname,\n\t\t\tMPI_MODE_CREATE|MPI_MODE_RDWR, info, &fh);\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_open\");\n\t}\n\n\terrcode = MPI_File_close(&fh);\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_close\");\n\t}\n\n\t\n\n\terrcode = MPI_File_open(MPI_COMM_WORLD, my_options.fname,\n\t\t\tMPI_MODE_CREATE|MPI_MODE_EXCL|MPI_MODE_RDWR, info, &fh);\n\tif (errcode == MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_open: expected an error: got\");\n\t}\n\n\t\n\n\tMPI_File_delete(my_options.fname, info);\n\n\t\n\n\terrcode = MPI_File_open(MPI_COMM_WORLD, my_options.fname,\n\t\t\tMPI_MODE_CREATE|MPI_MODE_EXCL|MPI_MODE_RDWR, info, &fh);\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_open\");\n\t}\n\n\terrcode = MPI_File_close(&fh);\n\tif (errcode != MPI_SUCCESS) {\n\t\thandle_error(errcode, \"MPI_File_close\");\n\t}\n\n\tif (mynod == 0) {\n\t\tprintf(\" No Errors\\n\");\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "qingu_97", "code": "int main(int argc, char *argv[])\n{\n    int mpi_errno;\n    MPI_Datatype type, duptype;\n    int rank;\n\n    \n\n    foo_initialize();\n\n    mpi_errno =\n\n    mpi_errno =\n\n    mpi_errno =\n\n    my_func = \"Free of type\";\n    mpi_errno =\n\n    my_func = \"free of duptype\";\n    mpi_errno =\n\n    foo_finalize();\n\n    if (rank == 0) {\n      int errs = 0;\n      if (copy_called != 1) {\n\tprintf( \"Copy called %d times; expected once\\n\", copy_called );\n\terrs++;\n      }\n      if (delete_called != 2) {\n\tprintf( \"Delete called %d times; expected twice\\n\", delete_called );\n\terrs++;\n      }\n      if (errs == 0) {\n\tprintf( \" No Errors\\n\" );\n      }\n      else {\n\tprintf( \" Found %d errors\\n\", errs );\n      }\n      fflush(stdout);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int mpi_errno;\n    MPI_Datatype type, duptype;\n    int rank;\n\n    MPI_Init(&argc, &argv);\n    \n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\n    foo_initialize();\n\n    mpi_errno = MPI_Type_contiguous(2, MPI_INT, &type);\n\n    mpi_errno = MPI_Type_set_attr(type, foo_keyval, NULL);\n\n    mpi_errno = MPI_Type_dup(type, &duptype);\n\n    my_func = \"Free of type\";\n    mpi_errno = MPI_Type_free(&type);\n\n    my_func = \"free of duptype\";\n    mpi_errno = MPI_Type_free(&duptype);\n\n    foo_finalize();\n\n    if (rank == 0) {\n      int errs = 0;\n      if (copy_called != 1) {\n\tprintf( \"Copy called %d times; expected once\\n\", copy_called );\n\terrs++;\n      }\n      if (delete_called != 2) {\n\tprintf( \"Delete called %d times; expected twice\\n\", delete_called );\n\terrs++;\n      }\n      if (errs == 0) {\n\tprintf( \" No Errors\\n\" );\n      }\n      else {\n\tprintf( \" Found %d errors\\n\", errs );\n      }\n      fflush(stdout);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "erpicci_98", "code": "int main(int argc, char *argv[]) {\n    st_matrix_t M = st_matrix_load(stdin);\n    const unsigned int size = st_matrix_size(M);\n    double *eigenvalues;\n    unsigned int i;\n    stopwatch_t stopwatch = stopwatch_create(\"Sample task\");\n\n    (void) argc;\n    (void) argv;\n\n    \n\n    SAFE_MALLOC(eigenvalues, double *, size * sizeof(double));\n\n\n    \n\n    stopwatch_start(stopwatch, 0, \"Compute eigenvalues\");\n    compute_eigenvalues(M, eigenvalues);\n    stopwatch_stop(stopwatch, 0);\n\n\n    \n\n    printf(\"Eigenvalues:\\n[\");\n    for (i = 0; i < size - 1; ++i) {\n        printf(\"%g, \", eigenvalues[i]);\n    }\n    printf(\"%g]\\n\", eigenvalues[i]);\n    \n\n    \n\n    st_matrix_delete(&M);\n    free(eigenvalues);\n    stopwatch_delete(&stopwatch);\n    \n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    st_matrix_t M = st_matrix_load(stdin);\n    const unsigned int size = st_matrix_size(M);\n    double *eigenvalues;\n    unsigned int i;\n    stopwatch_t stopwatch = stopwatch_create(\"Sample task\");\n\n    (void) argc;\n    (void) argv;\n\n    \n\n    MPI_Init(&argc, &argv);\n    SAFE_MALLOC(eigenvalues, double *, size * sizeof(double));\n\n\n    \n\n    stopwatch_start(stopwatch, 0, \"Compute eigenvalues\");\n    compute_eigenvalues(M, eigenvalues);\n    stopwatch_stop(stopwatch, 0);\n\n\n    \n\n    printf(\"Eigenvalues:\\n[\");\n    for (i = 0; i < size - 1; ++i) {\n        printf(\"%g, \", eigenvalues[i]);\n    }\n    printf(\"%g]\\n\", eigenvalues[i]);\n    \n\n    \n\n    MPI_Finalize();\n    st_matrix_delete(&M);\n    free(eigenvalues);\n    stopwatch_delete(&stopwatch);\n    \n    return 0;\n}"}
{"program": "matthb2_99", "code": "int main(int argc, char** argv)\n{\n    int rank = dan_mpi_rank();\n    int size = dan_mpi_size();\n    int peer = (rank+1)%size;\n    int message = 42;\n    dan_pmsg m;\n    dan_pmsg_init(&m);\n    dan_pmsg_start(&m,dan_pmsg_global);\n    DAN_PMSG_PACK(&m,peer,message,int);\n    dan_pmsg_send(&m);\n    while (dan_pmsg_receive(&m))\n    {\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n        int* unpacked = dan_pmsg_unpack(&m,sizeof(int));\n        fprintf(stderr,\"%d received %d from %d\\n\",\n                rank,*unpacked,dan_pmsg_received_from(&m));\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n    }\n    message = 128;\n    fprintf(stderr,\"%d stage 2\\n\",rank);\n    dan_pmsg_start(&m,dan_pmsg_global);\n    DAN_PMSG_PACK(&m,peer,message,int);\n    dan_pmsg_send(&m);\n    while (dan_pmsg_receive(&m))\n    {\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n        int* unpacked = dan_pmsg_unpack(&m,sizeof(int));\n        fprintf(stderr,\"%d received %d from %d\\n\",\n                rank,*unpacked,dan_pmsg_received_from(&m));\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n    }\n    int peer2 = (rank+size-1)%size;\n    fprintf(stderr,\"%d stage 3\\n\",rank);\n    message = 3;\n    dan_pmsg_start(&m,dan_pmsg_local);\n    DAN_PMSG_PACK(&m,peer,message,int);\n    DAN_PMSG_PACK(&m,peer2,message,int);\n    dan_pmsg_send(&m);\n    while (dan_pmsg_receive(&m))\n    {\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n        int* unpacked = dan_pmsg_unpack(&m,sizeof(int));\n        fprintf(stderr,\"%d received %d from %d\\n\",\n                rank,*unpacked,dan_pmsg_received_from(&m));\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n    }\n    dan_pmsg_free(&m);\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n    MPI_Init(&argc,&argv);\n    int rank = dan_mpi_rank();\n    int size = dan_mpi_size();\n    int peer = (rank+1)%size;\n    int message = 42;\n    dan_pmsg m;\n    dan_pmsg_init(&m);\n    dan_pmsg_start(&m,dan_pmsg_global);\n    DAN_PMSG_PACK(&m,peer,message,int);\n    dan_pmsg_send(&m);\n    while (dan_pmsg_receive(&m))\n    {\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n        int* unpacked = dan_pmsg_unpack(&m,sizeof(int));\n        fprintf(stderr,\"%d received %d from %d\\n\",\n                rank,*unpacked,dan_pmsg_received_from(&m));\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n    }\n    message = 128;\n    fprintf(stderr,\"%d stage 2\\n\",rank);\n    dan_pmsg_start(&m,dan_pmsg_global);\n    DAN_PMSG_PACK(&m,peer,message,int);\n    dan_pmsg_send(&m);\n    while (dan_pmsg_receive(&m))\n    {\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n        int* unpacked = dan_pmsg_unpack(&m,sizeof(int));\n        fprintf(stderr,\"%d received %d from %d\\n\",\n                rank,*unpacked,dan_pmsg_received_from(&m));\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n    }\n    int peer2 = (rank+size-1)%size;\n    fprintf(stderr,\"%d stage 3\\n\",rank);\n    message = 3;\n    dan_pmsg_start(&m,dan_pmsg_local);\n    DAN_PMSG_PACK(&m,peer,message,int);\n    DAN_PMSG_PACK(&m,peer2,message,int);\n    dan_pmsg_send(&m);\n    while (dan_pmsg_receive(&m))\n    {\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n        int* unpacked = dan_pmsg_unpack(&m,sizeof(int));\n        fprintf(stderr,\"%d received %d from %d\\n\",\n                rank,*unpacked,dan_pmsg_received_from(&m));\n        assert(dan_pmsg_received_size(&m) == sizeof(message));\n    }\n    dan_pmsg_free(&m);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "airqinc_102", "code": "int main(int argc, char *argv[])  {\r\n\tint numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;\r\n\tMPI_Request reqs[4];   \n\n\tMPI_Status stats[4];   \n\n\t\n\n\r\n\r\n\t\n\n\tprev = rank-1;\r\n\tnext = rank+1;\r\n\tif (rank == 0)  prev = numtasks - 1;\r\n\tif (rank == (numtasks - 1))  next = 0;\r\n\r\n\t\n\n\r\n\t\n\n\t\n\n\t  \n\n\r\n\t\n\n\tprintf(\"Proc. %d received %d from prev and %d from next.\\n\",rank, buf[0], buf[1]);\r\n\r\n\t  \n\n\r\n\treturn 0;\r\n}", "label": "int main(int argc, char *argv[])  {\r\n\tint numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;\r\n\tMPI_Request reqs[4];   \n\n\tMPI_Status stats[4];   \n\n\t\n\n\r\n\tMPI_Init(&argc,&argv);\r\n\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\r\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n\r\n\t\n\n\tprev = rank-1;\r\n\tnext = rank+1;\r\n\tif (rank == 0)  prev = numtasks - 1;\r\n\tif (rank == (numtasks - 1))  next = 0;\r\n\r\n\t\n\n\tMPI_Irecv(&buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &reqs[0]);\r\n\tMPI_Irecv(&buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &reqs[1]);\r\n\r\n\tMPI_Isend(&rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &reqs[2]);\r\n\tMPI_Isend(&rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &reqs[3]);\r\n\t\n\n\t\n\n\t  \n\n\r\n\t\n\n\tMPI_Waitall(4, reqs, stats);\r\n\tprintf(\"Proc. %d received %d from prev and %d from next.\\n\",rank, buf[0], buf[1]);\r\n\r\n\t  \n\n\r\n\tMPI_Finalize();\r\n\treturn 0;\r\n}"}
{"program": "Janelia-Farm-Xfam_103", "code": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS    *go      = p7_CreateDefaultApp(options, 1, argc, argv, banner, usage);\n  char           *hmmfile = esl_opt_GetArg(go, 1);\n  ESL_ALPHABET   *abc     = esl_alphabet_Create(eslAMINO);\n  P7_BG          *bg      = p7_bg_Create(abc);\n  int             my_rank;\n  int             nproc;\n  char           *buf    = NULL;\n  int             nbuf   = 0;\n  int             subtotalM = 0;\n  int             allM   = 0;\n  int             stalling = esl_opt_GetBoolean(go, \"--stall\");\n\n\n  while (stalling); \n\n  \n\n  if (my_rank == 0) \n    {\n      ESL_STOPWATCH *w        = esl_stopwatch_Create();\n      P7_HMMFILE    *hfp      = NULL;\n      P7_PROFILE     *gm      = NULL;\n      P7_HMM         *hmm     = NULL;\n\n\n      \n\n      if (p7_hmmfile_OpenE(hmmfile, NULL, &hfp, NULL) != eslOK) p7_Fail(\"Failed to open HMM file %s\", hmmfile);\n\n      esl_stopwatch_Start(w);\n      while (p7_hmmfile_Read(hfp, &abc, &hmm)     == eslOK) \n\t{\n\t  gm = p7_profile_Create(hmm->M, abc);\t  \n\t  p7_ProfileConfig(hmm, bg, gm, 400, p7_LOCAL);\n\t  if (!esl_opt_GetBoolean(go, \"-b\"))\n\t    p7_profile_MPISend(gm, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n\n\t  p7_hmm_Destroy(hmm);\n\t  p7_profile_Destroy(gm);\n\t}\n      p7_profile_MPISend(NULL, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n\n      printf(\"total: %d\\n\", allM);\n      esl_stopwatch_Stop(w);\n      esl_stopwatch_Display(stdout, w, \"CPU Time: \");\n      esl_stopwatch_Destroy(w);\n    }\n  \n\n  else \n    {\n      P7_PROFILE     *gm_recd = NULL;      \n\n      while (p7_profile_MPIRecv(0, 0, MPI_COMM_WORLD, abc, bg, &buf, &nbuf, &gm_recd) == eslOK) \n\t{\n\t  subtotalM += gm_recd->M;\n\t  p7_profile_Destroy(gm_recd);  \n\t}\n    }\n\n  free(buf);\n  p7_bg_Destroy(bg);\n  esl_alphabet_Destroy(abc);\n  esl_getopts_Destroy(go);\n  exit(0);\n}", "label": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS    *go      = p7_CreateDefaultApp(options, 1, argc, argv, banner, usage);\n  char           *hmmfile = esl_opt_GetArg(go, 1);\n  ESL_ALPHABET   *abc     = esl_alphabet_Create(eslAMINO);\n  P7_BG          *bg      = p7_bg_Create(abc);\n  int             my_rank;\n  int             nproc;\n  char           *buf    = NULL;\n  int             nbuf   = 0;\n  int             subtotalM = 0;\n  int             allM   = 0;\n  int             stalling = esl_opt_GetBoolean(go, \"--stall\");\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  while (stalling); \n\n  \n\n  if (my_rank == 0) \n    {\n      ESL_STOPWATCH *w        = esl_stopwatch_Create();\n      P7_HMMFILE    *hfp      = NULL;\n      P7_PROFILE     *gm      = NULL;\n      P7_HMM         *hmm     = NULL;\n\n\n      \n\n      if (p7_hmmfile_OpenE(hmmfile, NULL, &hfp, NULL) != eslOK) p7_Fail(\"Failed to open HMM file %s\", hmmfile);\n\n      esl_stopwatch_Start(w);\n      while (p7_hmmfile_Read(hfp, &abc, &hmm)     == eslOK) \n\t{\n\t  gm = p7_profile_Create(hmm->M, abc);\t  \n\t  p7_ProfileConfig(hmm, bg, gm, 400, p7_LOCAL);\n\t  if (!esl_opt_GetBoolean(go, \"-b\"))\n\t    p7_profile_MPISend(gm, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n\n\t  p7_hmm_Destroy(hmm);\n\t  p7_profile_Destroy(gm);\n\t}\n      p7_profile_MPISend(NULL, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n      MPI_Reduce(&subtotalM, &allM, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n      printf(\"total: %d\\n\", allM);\n      esl_stopwatch_Stop(w);\n      esl_stopwatch_Display(stdout, w, \"CPU Time: \");\n      esl_stopwatch_Destroy(w);\n    }\n  \n\n  else \n    {\n      P7_PROFILE     *gm_recd = NULL;      \n\n      while (p7_profile_MPIRecv(0, 0, MPI_COMM_WORLD, abc, bg, &buf, &nbuf, &gm_recd) == eslOK) \n\t{\n\t  subtotalM += gm_recd->M;\n\t  p7_profile_Destroy(gm_recd);  \n\t}\n      MPI_Reduce(&subtotalM, &allM, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n  free(buf);\n  p7_bg_Destroy(bg);\n  esl_alphabet_Destroy(abc);\n  esl_getopts_Destroy(go);\n  MPI_Finalize();\n  exit(0);\n}"}
{"program": "dash-project_105", "code": "int main( int argc, char* argv[] )\n{\n  int myrank, nprocs;\n\n  \n\n\n  if( nprocs%2 ) {\n    fprintf(stderr, \"Use even number of procs!\\n\");\n    exit(1);\n  }\n  \n  fibsender(myrank, 5);\n  fibreceiver(myrank, 5);\n  \n}", "label": "int main( int argc, char* argv[] )\n{\n  int myrank, nprocs;\n\n  \n  MPI_Init( &argc, &argv);\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n\n  if( nprocs%2 ) {\n    fprintf(stderr, \"Use even number of procs!\\n\");\n    exit(1);\n  }\n  \n  fibsender(myrank, 5);\n  fibreceiver(myrank, 5);\n  \n  MPI_Finalize();\n}"}
{"program": "jeremiahyan_108", "code": "int main(int narg, char **arg)\n{\n  int n,me,nprocs;\n  int nprocs_lammps, lammps;\n  char line[1024];\n  MPI_Comm comm_lammps;\n  FILE *fp = NULL;\n  liblammpsplugin_t *plugin = NULL;\n  void *lmp = NULL;\n  double *x = NULL;\n  double *v = NULL;\n  char *strtwo;\n  char *cmds[2];\n  int *type = NULL;\n\n  \n\n\n\n  if (narg != 4) {\n    printf(\"Syntax: simpleC P in.lammps /path/to/liblammps.so\\n\");\n    exit(1);\n  }\n\n\n  nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n  }\n\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n\n  \n\n\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n    }\n  }\n\n  \n\n\n  if (lammps == 1) {\n    plugin = liblammpsplugin_load(arg[3]);\n    if (plugin == NULL) {\n      if (me == 0) printf(\"ERROR: Could not load shared LAMMPS library\\n\");\n    }\n  }\n  if (lammps == 1) plugin->open(0,NULL,comm_lammps,&lmp);\n\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    if (n == 0) break;\n    if (lammps == 1) plugin->command(lmp,line);\n  }\n\n  \n\n\n  if (lammps == 1) {\n    plugin->command(lmp,\"run 10\");\n\n    int natoms = plugin->get_natoms(lmp);\n    x = (double *) malloc(3*natoms*sizeof(double));\n    plugin->gather_atoms(lmp,\"x\",1,3,x);\n    v = (double *) malloc(3*natoms*sizeof(double));\n    plugin->gather_atoms(lmp,\"v\",1,3,v);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    plugin->scatter_atoms(lmp,\"x\",1,3,x);\n\n    plugin->command(lmp,\"run 1\");\n  }\n\n  \n\n\n  if (lammps == 1) {\n    double **f = (double **) plugin->extract_atom(lmp,\"f\");\n    printf(\"Force on 1 atom via extract_atom: %g\\n\",f[0][0]);\n\n    double *fx = (double *) plugin->extract_variable(lmp,\"fx\",\"all\");\n    printf(\"Force on 1 atom via extract_variable: %g\\n\",fx[0]);\n  }\n\n  \n\n\n  strtwo = (char *)\"run 10\\nrun 20\";\n  if (lammps == 1) plugin->commands_string(lmp,strtwo);\n\n  cmds[0] = (char *)\"run 10\";\n  cmds[1] = (char *)\"run 20\";\n  if (lammps == 1) plugin->commands_list(lmp,2,cmds);\n\n  \n\n\n  if (lammps == 1) {\n    int natoms = plugin->get_natoms(lmp);\n    type = (int *) malloc(natoms*sizeof(double));\n    int i;\n    for (i = 0; i < natoms; i++) type[i] = 1;\n\n    plugin->command(lmp,\"delete_atoms group all\");\n    plugin->create_atoms(lmp,natoms,NULL,type,x,v,NULL,0);\n    plugin->command(lmp,\"run 10\");\n  }\n\n  if (x) free(x);\n  if (v) free(v);\n  if (type) free(type);\n\n  \n\n\n  if (lammps == 1) {\n    plugin->close(lmp);\n    liblammpsplugin_release(plugin);\n  }\n\n  \n\n\n  if (lammps == 1)\n}", "label": "int main(int narg, char **arg)\n{\n  int n,me,nprocs;\n  int nprocs_lammps, lammps;\n  char line[1024];\n  MPI_Comm comm_lammps;\n  FILE *fp = NULL;\n  liblammpsplugin_t *plugin = NULL;\n  void *lmp = NULL;\n  double *x = NULL;\n  double *v = NULL;\n  char *strtwo;\n  char *cmds[2];\n  int *type = NULL;\n\n  \n\n\n  MPI_Init(&narg,&arg);\n\n  if (narg != 4) {\n    printf(\"Syntax: simpleC P in.lammps /path/to/liblammps.so\\n\");\n    exit(1);\n  }\n\n  MPI_Comm_rank(MPI_COMM_WORLD,&me);\n  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n\n  nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm_split(MPI_COMM_WORLD,lammps,0,&comm_lammps);\n\n  \n\n\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n      MPI_Abort(MPI_COMM_WORLD,1);\n    }\n  }\n\n  \n\n\n  if (lammps == 1) {\n    plugin = liblammpsplugin_load(arg[3]);\n    if (plugin == NULL) {\n      if (me == 0) printf(\"ERROR: Could not load shared LAMMPS library\\n\");\n      MPI_Abort(MPI_COMM_WORLD,1);\n    }\n  }\n  if (lammps == 1) plugin->open(0,NULL,comm_lammps,&lmp);\n\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);\n    if (n == 0) break;\n    MPI_Bcast(line,n,MPI_CHAR,0,MPI_COMM_WORLD);\n    if (lammps == 1) plugin->command(lmp,line);\n  }\n\n  \n\n\n  if (lammps == 1) {\n    plugin->command(lmp,\"run 10\");\n\n    int natoms = plugin->get_natoms(lmp);\n    x = (double *) malloc(3*natoms*sizeof(double));\n    plugin->gather_atoms(lmp,\"x\",1,3,x);\n    v = (double *) malloc(3*natoms*sizeof(double));\n    plugin->gather_atoms(lmp,\"v\",1,3,v);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    plugin->scatter_atoms(lmp,\"x\",1,3,x);\n\n    plugin->command(lmp,\"run 1\");\n  }\n\n  \n\n\n  if (lammps == 1) {\n    double **f = (double **) plugin->extract_atom(lmp,\"f\");\n    printf(\"Force on 1 atom via extract_atom: %g\\n\",f[0][0]);\n\n    double *fx = (double *) plugin->extract_variable(lmp,\"fx\",\"all\");\n    printf(\"Force on 1 atom via extract_variable: %g\\n\",fx[0]);\n  }\n\n  \n\n\n  strtwo = (char *)\"run 10\\nrun 20\";\n  if (lammps == 1) plugin->commands_string(lmp,strtwo);\n\n  cmds[0] = (char *)\"run 10\";\n  cmds[1] = (char *)\"run 20\";\n  if (lammps == 1) plugin->commands_list(lmp,2,cmds);\n\n  \n\n\n  if (lammps == 1) {\n    int natoms = plugin->get_natoms(lmp);\n    type = (int *) malloc(natoms*sizeof(double));\n    int i;\n    for (i = 0; i < natoms; i++) type[i] = 1;\n\n    plugin->command(lmp,\"delete_atoms group all\");\n    plugin->create_atoms(lmp,natoms,NULL,type,x,v,NULL,0);\n    plugin->command(lmp,\"run 10\");\n  }\n\n  if (x) free(x);\n  if (v) free(v);\n  if (type) free(type);\n\n  \n\n\n  if (lammps == 1) {\n    plugin->close(lmp);\n    liblammpsplugin_release(plugin);\n  }\n\n  \n\n\n  if (lammps == 1) MPI_Comm_free(&comm_lammps);\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}"}
{"program": "8l_110", "code": "int main(int argc, char *argv[], char *envp[] )\n{ \n    int i;\n    printf(\"hello %s argc=%d\\n\", argv[0], argc );\n\n    for ( i = 1; i < argc; i++ ) {\n        printf(\"arg %d \\\"%s\\\"\\n\",i,argv[i]); \n    } \n    char* env = getenv(\"HOSTNAME\");\n    if ( env )\n    printf(\"%s\\n\", env );\n\n    mkfifo( \"/tmp/fifo\",  0777);\n    _exit(1);\n\n    setenv( \"OPAL_PKGDATADIR\", \"/etc/opal\", 1 );\n    setenv( \"OPAL_SYSCONFDIR\", \"/etc/opal\", 1 );\n    setenv( \"OMPI_MCA_orte_debug\", \"0\", 1 );\n    setenv( \"OMPI_MCA_btl_base_debug\", \"0\", 1 );\n\n    char* buf[100];\n    char* sbuf = \"hello world\";\n    MPI_Request request;\n    MPI_Status status;\n    int ret;\n\n    memset(buf,0,100);\n\n    \nprintf(\"calling MPI_Irecv\\n\");\n    if ( ret != MPI_SUCCESS ) {\n        printf(\"MPI_Irecv failed\\n\");\n    }\t\n\nprintf(\"calling MPI_Secv\\n\");\n    ret =\n    if ( ret != MPI_SUCCESS ) {\n        printf(\"MPI_Send failed\\n\");\n    }\t\n\nprintf(\"calling MPI_Wait\\n\");\n    ret =\n    if ( ret != MPI_SUCCESS ) {\n        printf(\"MPI_Wait failed\\n\");\n    }\t\n\nprintf(\"got `%s`\\n\",buf);\n\n\n    _exit(0);\n#if 0\n    struct NidPid {\n        uint nid;\n        uint pid;\n    };\n    \n    uint* nRanks = (uint*) aspace_find_region_start(\"NidPidMap\");\n    struct NidPid* map = (struct NidPid*)(nRanks+1); \n\n    printf(\"nRanks=%p nRanks=%d\\n\",nRanks,*nRanks);\n\n    for ( i = 0; i < *nRanks; i++ ) {\n        printf(\"rank=%d nid=%d pid=%d\\n\",i,map[i].nid,map[i].pid);\n    }\n\n\n#endif\n    return -1;\n}", "label": "int main(int argc, char *argv[], char *envp[] )\n{ \n    int i;\n    printf(\"hello %s argc=%d\\n\", argv[0], argc );\n\n    for ( i = 1; i < argc; i++ ) {\n        printf(\"arg %d \\\"%s\\\"\\n\",i,argv[i]); \n    } \n    char* env = getenv(\"HOSTNAME\");\n    if ( env )\n    printf(\"%s\\n\", env );\n\n    mkfifo( \"/tmp/fifo\",  0777);\n    _exit(1);\n\n    setenv( \"OPAL_PKGDATADIR\", \"/etc/opal\", 1 );\n    setenv( \"OPAL_SYSCONFDIR\", \"/etc/opal\", 1 );\n    setenv( \"OMPI_MCA_orte_debug\", \"0\", 1 );\n    setenv( \"OMPI_MCA_btl_base_debug\", \"0\", 1 );\n\n    MPI_Init( &argc, & argv);\n    char* buf[100];\n    char* sbuf = \"hello world\";\n    MPI_Request request;\n    MPI_Status status;\n    int ret;\n\n    memset(buf,0,100);\n\n    \nprintf(\"calling MPI_Irecv\\n\");\n    ret=MPI_Irecv( buf, strlen(sbuf), MPI_CHARACTER, 0, 0xbeef, MPI_COMM_WORLD, &request);\n    if ( ret != MPI_SUCCESS ) {\n        printf(\"MPI_Irecv failed\\n\");\n    }\t\n\nprintf(\"calling MPI_Secv\\n\");\n    ret = MPI_Send( sbuf, strlen(sbuf), MPI_CHARACTER, 0, 0xbeef, MPI_COMM_WORLD );\n    if ( ret != MPI_SUCCESS ) {\n        printf(\"MPI_Send failed\\n\");\n    }\t\n\nprintf(\"calling MPI_Wait\\n\");\n    ret = MPI_Wait(&request,&status);\n    if ( ret != MPI_SUCCESS ) {\n        printf(\"MPI_Wait failed\\n\");\n    }\t\n\nprintf(\"got `%s`\\n\",buf);\n\n    MPI_Finalize();\n\n    _exit(0);\n#if 0\n    struct NidPid {\n        uint nid;\n        uint pid;\n    };\n    \n    uint* nRanks = (uint*) aspace_find_region_start(\"NidPidMap\");\n    struct NidPid* map = (struct NidPid*)(nRanks+1); \n\n    printf(\"nRanks=%p nRanks=%d\\n\",nRanks,*nRanks);\n\n    for ( i = 0; i < *nRanks; i++ ) {\n        printf(\"rank=%d nid=%d pid=%d\\n\",i,map[i].nid,map[i].pid);\n    }\n\n\n#endif\n    return -1;\n}"}
{"program": "aarestad_113", "code": "int main(int argc, char* argv[]) {\n    int num_parts;\n    int myid, numprocs;\n    int i;\n    double starttime, endtime;\n    double localsum;\n    double globalsum;\n    \n    point** workToDo;\n    int* workSize;\n    point* points;\n    \n    MPI_Datatype pointType;\n    \n    \n    \n\n\n    if (argc < 2) {\n        fprintf (stderr, \"Usage: recursive_bisection <#of partitions>\\n\");\n        return 1;\n    }\n\n    num_parts = atoi(argv[1]);\n\n    points   = (point*) calloc(NUM_POINTS, sizeof(point));\n    bzero(points, NUM_POINTS*sizeof(point));\n    workToDo = (point**) calloc(num_parts, sizeof(point*));\n    bzero(workToDo, num_parts*sizeof(point*));\n    workSize = (int*) calloc(num_parts, sizeof(int));\n    bzero(workSize, num_parts*sizeof(int));\n\n    if (myid == 0) {\n        printf(\"Extracting %d partitions with %d processors\\n\",\n               num_parts, numprocs);\n\n        srandom(1000);\n\n        for (i = 0; i < NUM_POINTS; i++) {\n            points[i].x = (unsigned int) random();\n            points[i].y = (unsigned int) random();\n        }\n\tstarttime =\n    }\n\n\n    printf(\"Entering recursivePart()\\n\");\n    \n\n    recursivePart(points, NUM_POINTS, 1, workToDo, workSize, 0, num_parts,\n                  myid, numprocs, 0);\n    printf(\"Leaving recursivePart()\\n\");\n    globalsum = 0;\n    localsum = 0;\n    \n\n    for (i = 0; i < num_parts; ++i) {\n        double answer;\n\tint minx, maxx, miny, maxy;\n\tint j;\n\tif (workToDo[i] == NULL) continue;\n\tif (workSize[i] == 0) {\n\t\tprintf(\"Proc %d: Cluster %d is empty!\\n\", myid, \n\t\t       i + (num_parts/numprocs)*myid);\n\t\tcontinue;\n\t}\n\n\tqsort(workToDo[i], workSize[i], sizeof(point), compare_x);\n        printf(\"Cluster %d:\", i + (num_parts/numprocs)*myid);\n        for (j = 0; j < workSize[i]; ++j) {\n\t\tprintf(\"[(%d,%d)]\", workToDo[i][j].x, workToDo[i][j].y);\n\t}\n\tprintf(\"\\n\");\n        printf(\"Number of points: %d\\n\", workSize[i]);\n\tminx = workToDo[i][0].x;\n\tmaxx = workToDo[i][workSize[i]-1].x;\n\tprintf(\"Proc %d: Cluster %d coords: xmin=%d, xmax=%d\\n\", \n               myid, i + (num_parts/numprocs)*myid, minx, maxx);\n\n\tqsort(workToDo[i], workSize[i], sizeof(point), compare_y);\n\tprintf(\"Proc %d: Cluster %d coords: ymin=%d, ymax=%d\\n\", \n               myid, i + (num_parts/numprocs)*myid, workToDo[i][0].y,\n               workToDo[i][workSize[i]-1].y);\n        answer = scoreCluster(workToDo[i], workSize[i]);\n        printf(\"Proc %d: Cost for cluster %d=%g\\n\", myid,\n                i + (num_parts/numprocs) * myid, answer);\n        localsum += answer;\n    }\n    if (myid == 0) {\n\tendtime =\n\tprintf(\"Proc 0: Total cost=%g\\n\", globalsum);\n\tprintf(\"Total running time: %g\\n\", endtime-starttime);\n    }\n \n    return 0;\n}", "label": "int main(int argc, char* argv[]) {\n    int num_parts;\n    int myid, numprocs;\n    int i;\n    double starttime, endtime;\n    double localsum;\n    double globalsum;\n    \n    point** workToDo;\n    int* workSize;\n    point* points;\n    \n    MPI_Datatype pointType;\n    \n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n    \n    \n\n    MPI_Type_contiguous(2, MPI_INT, &pointType);\n    MPI_Type_commit(&pointType);\n\n    if (argc < 2) {\n        fprintf (stderr, \"Usage: recursive_bisection <#of partitions>\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    num_parts = atoi(argv[1]);\n\n    points   = (point*) calloc(NUM_POINTS, sizeof(point));\n    bzero(points, NUM_POINTS*sizeof(point));\n    workToDo = (point**) calloc(num_parts, sizeof(point*));\n    bzero(workToDo, num_parts*sizeof(point*));\n    workSize = (int*) calloc(num_parts, sizeof(int));\n    bzero(workSize, num_parts*sizeof(int));\n\n    if (myid == 0) {\n        printf(\"Extracting %d partitions with %d processors\\n\",\n               num_parts, numprocs);\n\n        srandom(1000);\n\n        for (i = 0; i < NUM_POINTS; i++) {\n            points[i].x = (unsigned int) random();\n            points[i].y = (unsigned int) random();\n        }\n\tstarttime = MPI_Wtime();\n    }\n\n    MPI_Bcast(points, NUM_POINTS, pointType, 0, MPI_COMM_WORLD);\n\n    printf(\"Entering recursivePart()\\n\");\n    \n\n    recursivePart(points, NUM_POINTS, 1, workToDo, workSize, 0, num_parts,\n                  myid, numprocs, 0);\n    printf(\"Leaving recursivePart()\\n\");\n    globalsum = 0;\n    localsum = 0;\n    \n\n    for (i = 0; i < num_parts; ++i) {\n        double answer;\n\tint minx, maxx, miny, maxy;\n\tint j;\n\tif (workToDo[i] == NULL) continue;\n\tif (workSize[i] == 0) {\n\t\tprintf(\"Proc %d: Cluster %d is empty!\\n\", myid, \n\t\t       i + (num_parts/numprocs)*myid);\n\t\tcontinue;\n\t}\n\n\tqsort(workToDo[i], workSize[i], sizeof(point), compare_x);\n        printf(\"Cluster %d:\", i + (num_parts/numprocs)*myid);\n        for (j = 0; j < workSize[i]; ++j) {\n\t\tprintf(\"[(%d,%d)]\", workToDo[i][j].x, workToDo[i][j].y);\n\t}\n\tprintf(\"\\n\");\n        printf(\"Number of points: %d\\n\", workSize[i]);\n\tminx = workToDo[i][0].x;\n\tmaxx = workToDo[i][workSize[i]-1].x;\n\tprintf(\"Proc %d: Cluster %d coords: xmin=%d, xmax=%d\\n\", \n               myid, i + (num_parts/numprocs)*myid, minx, maxx);\n\n\tqsort(workToDo[i], workSize[i], sizeof(point), compare_y);\n\tprintf(\"Proc %d: Cluster %d coords: ymin=%d, ymax=%d\\n\", \n               myid, i + (num_parts/numprocs)*myid, workToDo[i][0].y,\n               workToDo[i][workSize[i]-1].y);\n        answer = scoreCluster(workToDo[i], workSize[i]);\n        printf(\"Proc %d: Cost for cluster %d=%g\\n\", myid,\n                i + (num_parts/numprocs) * myid, answer);\n        localsum += answer;\n    }\n    MPI_Reduce(&localsum, &globalsum, 1, MPI_DOUBLE, MPI_SUM, 0, \n               MPI_COMM_WORLD);\n    if (myid == 0) {\n\tendtime = MPI_Wtime();\n\tprintf(\"Proc 0: Total cost=%g\\n\", globalsum);\n\tprintf(\"Total running time: %g\\n\", endtime-starttime);\n    }\n \n    MPI_Finalize();\n    return 0;\n}"}
{"program": "Janelia-Farm-Xfam_114", "code": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS *go = p7_CreateDefaultApp(options, 0, argc, argv, banner, usage);\n  int          stalling = FALSE;\n  int          my_rank;\n  int          nproc;\n\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) stalling = TRUE;\n  while (stalling);\n\n\n  utest_HMMSendRecv(my_rank, nproc);\n  utest_ProfileSendRecv(my_rank, nproc);\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS *go = p7_CreateDefaultApp(options, 0, argc, argv, banner, usage);\n  int          stalling = FALSE;\n  int          my_rank;\n  int          nproc;\n\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) stalling = TRUE;\n  while (stalling);\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  utest_HMMSendRecv(my_rank, nproc);\n  utest_ProfileSendRecv(my_rank, nproc);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "ghisvail_115", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "acbauer_118", "code": "int main( int argc , char **argv )\n{\n  int nthread[ TEST_THREAD_MAX ];\n  int i ;\n\n\n  for ( i = 0 ; i < TEST_THREAD_MAX ; ++i ) { nthread[i] = 0 ; }\n\n  if ( 0 == comm_rank( MPI_COMM_WORLD ) ) {\n    if ( 1 < argc && argc < TEST_THREAD_MAX ) {\n      nthread[0] = 1 ;\n      nthread[1] = argc - 1 ;\n      for ( i = 1 ; i < argc ; ++i ) { nthread[i+1] = atoi( argv[i] ); }\n    }\n    else {\n      nthread[0] = 0 ;\n      nthread[1] = 1 ;\n      nthread[2] = 1 ;\n    }\n  }\n\n\n  if ( nthread[0] ) {\n    test_accuracy(    MPI_COMM_WORLD , nthread[1] , nthread + 2 , test_count );\n    test_performance( MPI_COMM_WORLD , nthread[1] , nthread + 2 );\n  }\n  else {\n    test_accuracy(    MPI_COMM_WORLD , nthread[1] , nthread + 2 , 3 );\n  }\n\n\n  return 0 ;\n}", "label": "int main( int argc , char **argv )\n{\n  int nthread[ TEST_THREAD_MAX ];\n  int i ;\n\n  MPI_Init( & argc , & argv );\n\n  for ( i = 0 ; i < TEST_THREAD_MAX ; ++i ) { nthread[i] = 0 ; }\n\n  if ( 0 == comm_rank( MPI_COMM_WORLD ) ) {\n    if ( 1 < argc && argc < TEST_THREAD_MAX ) {\n      nthread[0] = 1 ;\n      nthread[1] = argc - 1 ;\n      for ( i = 1 ; i < argc ; ++i ) { nthread[i+1] = atoi( argv[i] ); }\n    }\n    else {\n      nthread[0] = 0 ;\n      nthread[1] = 1 ;\n      nthread[2] = 1 ;\n    }\n  }\n\n  MPI_Bcast( nthread , TEST_THREAD_MAX , MPI_INT , 0 , MPI_COMM_WORLD );\n\n  if ( nthread[0] ) {\n    test_accuracy(    MPI_COMM_WORLD , nthread[1] , nthread + 2 , test_count );\n    test_performance( MPI_COMM_WORLD , nthread[1] , nthread + 2 );\n  }\n  else {\n    test_accuracy(    MPI_COMM_WORLD , nthread[1] , nthread + 2 , 3 );\n  }\n\n  MPI_Finalize();\n\n  return 0 ;\n}"}
{"program": "eddelbuettel_121", "code": "int main(int argc, char **argv) {\n     PGAContext *ctx;\n     int         len, maxiter;\n\n\n     len     = GetIntegerParameter(\"String length?\\n\");\n     maxiter = GetIntegerParameter(\"How many iterations?\\n\");\n\n     ctx = PGACreate(&argc, argv, PGA_DATATYPE_INTEGER, len, PGA_MAXIMIZE);\n\n     PGASetRandomSeed(ctx, 1);\n     PGASetUserFunction(ctx, PGA_USERFUNCTION_MUTATION, (void *)myMutation);\n     PGASetIntegerInitPermute(ctx, 1, len);\n\n     PGASetMaxGAIterValue(ctx, maxiter);\n     PGASetNumReplaceValue(ctx, 90);\n     PGASetMutationAndCrossoverFlag(ctx, PGA_TRUE);\n     PGASetPrintOptions(ctx, PGA_REPORT_AVERAGE);\n\n     PGASetUp(ctx);\n\n     PGARun(ctx, evaluate);\n     PGADestroy(ctx);\n\n\n     return(0);\n}", "label": "int main(int argc, char **argv) {\n     PGAContext *ctx;\n     int         len, maxiter;\n\n     MPI_Init(&argc, &argv);\n\n     len     = GetIntegerParameter(\"String length?\\n\");\n     maxiter = GetIntegerParameter(\"How many iterations?\\n\");\n\n     ctx = PGACreate(&argc, argv, PGA_DATATYPE_INTEGER, len, PGA_MAXIMIZE);\n\n     PGASetRandomSeed(ctx, 1);\n     PGASetUserFunction(ctx, PGA_USERFUNCTION_MUTATION, (void *)myMutation);\n     PGASetIntegerInitPermute(ctx, 1, len);\n\n     PGASetMaxGAIterValue(ctx, maxiter);\n     PGASetNumReplaceValue(ctx, 90);\n     PGASetMutationAndCrossoverFlag(ctx, PGA_TRUE);\n     PGASetPrintOptions(ctx, PGA_REPORT_AVERAGE);\n\n     PGASetUp(ctx);\n\n     PGARun(ctx, evaluate);\n     PGADestroy(ctx);\n\n     MPI_Finalize();\n\n     return(0);\n}"}
{"program": "cbries_122", "code": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble *A = NULL;\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tB[i] = (rand()%100) / 100.0f + myrank;\n\t}\n\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"On processor of rank %d B=%.2f\\n\", myrank, B[i]);\n\t\tfflush(stdout);\n\t}\n\t\n\tif(myrank==0) {\n\t\tA = (double*) malloc(DIMENSION * nprocs * sizeof(double));\n\t\tif(A==NULL) {\n\t\t\tperror(\"malloc for A failed\");\n\t\t}\n\n\t\tfor(i=0; i<DIMENSION; i++) {\n\t\t\tA[i] = B[i];\n\t\t}\n\t\t\n\t\tfor(i=1; i<nprocs; i++) {\n\t\t}\n\t\t\n\t\tfor(i=0; i<DIMENSION; i++) {\n\t\t\tprintf(\"column(%d): \", i);\n\t\t\tfor(j=0; j<nprocs; j++) {\n\t\t\t\tprintf(\"%.2f   \", A[j*DIMENSION+i]);\n\t\t\t}\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\t\n\t\tfree(A); A = NULL;\n\t} else {\n\t}\n\n\t\n\n\n\treturn 0;\n}\n", "label": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble *A = NULL;\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tB[i] = (rand()%100) / 100.0f + myrank;\n\t}\n\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"On processor of rank %d B=%.2f\\n\", myrank, B[i]);\n\t\tfflush(stdout);\n\t}\n\t\n\tif(myrank==0) {\n\t\tA = (double*) malloc(DIMENSION * nprocs * sizeof(double));\n\t\tif(A==NULL) {\n\t\t\tperror(\"malloc for A failed\");\n\t\t}\n\n\t\tfor(i=0; i<DIMENSION; i++) {\n\t\t\tA[i] = B[i];\n\t\t}\n\t\t\n\t\tfor(i=1; i<nprocs; i++) {\n\t\t\tMPI_Recv(A+DIMENSION+((i-1)*DIMENSION), DIMENSION, MPI_DOUBLE, i, TAG, MPI_COMM_WORLD, &status);\n\t\t}\n\t\t\n\t\tfor(i=0; i<DIMENSION; i++) {\n\t\t\tprintf(\"column(%d): \", i);\n\t\t\tfor(j=0; j<nprocs; j++) {\n\t\t\t\tprintf(\"%.2f   \", A[j*DIMENSION+i]);\n\t\t\t}\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\t\n\t\tfree(A); A = NULL;\n\t} else {\n\t\tMPI_Send(B, DIMENSION, MPI_DOUBLE, 0, TAG, MPI_COMM_WORLD);\n\t}\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "ghisvail_123", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3], N[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  pfft_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n\n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r_3d(\n      n, in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n \n  \n\n  plan_back = pfft_plan_r2r_3d(\n      n, out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2r_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_r2r_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3], N[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  pfft_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n\n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r_3d(\n      n, in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n \n  \n\n  plan_back = pfft_plan_r2r_3d(\n      n, out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2r_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_r2r_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "markfasheh_124", "code": "int main(int argc, char *argv[])\n{\n\tint ret;\n\terrcode_t error;\n\tstruct o2dlm_ctxt *dlm = NULL;\n\n\tinitialize_o2dl_error_table();\n\n\terror =\n\tif (error != MPI_SUCCESS)\n\t\trprintf(-1, \"MPI_Init failed: %d\\n\", error);\n\n\tprog = strrchr(argv[0], '/');\n\tif (prog == NULL)\n\t\tprog = argv[0];\n\telse\n\t\tprog++;\n\n\tif (parse_opts(argc, argv))\n\t\tusage(prog);\n\n\terror =\n        if (error != MPI_SUCCESS)\n                rprintf(-1, \"MPI_Comm_rank failed: %d\\n\", error);\n\n\terror =\n\tif (error != MPI_SUCCESS)\n\t\trprintf(rank, \"MPI_Comm_size failed: %d\\n\", error);\n\n        if (gethostname(hostname, HOSTNAME_SIZE) < 0)\n                rprintf(rank, \"gethostname failed: %s\\n\", strerror(errno));\n\n        printf(\"%s: rank: %d, nodes: %d, dlm: %s, dom: %s, lock: %s, iter: %llu\\n\", hostname, rank, num_procs, dlmfs_path, domain, lockid, (unsigned long long) max_iter);\n\n\t\n\tif (access(dlmfs_path, W_OK) < 0) {\n\t\tsleep(2);\n\t\trprintf(rank, \"%s has no write permission.\\n\", dlmfs_path);\n\t\treturn EACCES;\n\t\t}\n\n\terror = setup_signals();\n\tif (error)\n\t\trprintf(rank, \"setup_signals failed\\n\");\n\n\tif (hb_dev) {\n\t\tret = start_heartbeat(HB_CTL_PATH, hb_dev);\n\t\tif (ret)\n\t\t\trprintf(rank, \"start_heartbeat failed\\n\");\n\t}\n\n\terror = o2dlm_initialize(dlmfs_path, domain, &dlm);\n\tif (error)\n\t\trprintf(rank, \"o2dlm_initialize failed: %d\\n\", error);\n\n\tif (rank == 0)\n\t\tclear_lock(dlm, lockid);\n\n\tret =\n\tif (ret != MPI_SUCCESS)\n\t\trprintf(rank, \"prep MPI_Barrier failed: %d\\n\", ret);\n\n\trun_test(dlm, lockid);\n\n\terror = o2dlm_destroy(dlm);\n\tif (error)\n\t\trprintf(rank, \"o2dlm_destroy failed: %d\\n\", error);\n\n\tif (hb_dev) {\n\t\tret = stop_heartbeat(HB_CTL_PATH, hb_dev);\n\t\tif (ret)\n\t\t\trprintf(rank, \"stop_heartbeat failed\\n\");\n\t}\n\n        return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint ret;\n\terrcode_t error;\n\tstruct o2dlm_ctxt *dlm = NULL;\n\n\tinitialize_o2dl_error_table();\n\n\terror = MPI_Init(&argc, &argv);\n\tif (error != MPI_SUCCESS)\n\t\trprintf(-1, \"MPI_Init failed: %d\\n\", error);\n\n\tprog = strrchr(argv[0], '/');\n\tif (prog == NULL)\n\t\tprog = argv[0];\n\telse\n\t\tprog++;\n\n\tif (parse_opts(argc, argv))\n\t\tusage(prog);\n\n\terror = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        if (error != MPI_SUCCESS)\n                rprintf(-1, \"MPI_Comm_rank failed: %d\\n\", error);\n\n\terror = MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\tif (error != MPI_SUCCESS)\n\t\trprintf(rank, \"MPI_Comm_size failed: %d\\n\", error);\n\n        if (gethostname(hostname, HOSTNAME_SIZE) < 0)\n                rprintf(rank, \"gethostname failed: %s\\n\", strerror(errno));\n\n        printf(\"%s: rank: %d, nodes: %d, dlm: %s, dom: %s, lock: %s, iter: %llu\\n\", hostname, rank, num_procs, dlmfs_path, domain, lockid, (unsigned long long) max_iter);\n\n\t\n\tif (access(dlmfs_path, W_OK) < 0) {\n\t\tsleep(2);\n\t\trprintf(rank, \"%s has no write permission.\\n\", dlmfs_path);\n\t\treturn EACCES;\n\t\t}\n\n\terror = setup_signals();\n\tif (error)\n\t\trprintf(rank, \"setup_signals failed\\n\");\n\n\tif (hb_dev) {\n\t\tret = start_heartbeat(HB_CTL_PATH, hb_dev);\n\t\tif (ret)\n\t\t\trprintf(rank, \"start_heartbeat failed\\n\");\n\t}\n\n\terror = o2dlm_initialize(dlmfs_path, domain, &dlm);\n\tif (error)\n\t\trprintf(rank, \"o2dlm_initialize failed: %d\\n\", error);\n\n\tif (rank == 0)\n\t\tclear_lock(dlm, lockid);\n\n\tret = MPI_Barrier(MPI_COMM_WORLD);\n\tif (ret != MPI_SUCCESS)\n\t\trprintf(rank, \"prep MPI_Barrier failed: %d\\n\", ret);\n\n\trun_test(dlm, lockid);\n\n\terror = o2dlm_destroy(dlm);\n\tif (error)\n\t\trprintf(rank, \"o2dlm_destroy failed: %d\\n\", error);\n\n\tif (hb_dev) {\n\t\tret = stop_heartbeat(HB_CTL_PATH, hb_dev);\n\t\tif (ret)\n\t\t\trprintf(rank, \"stop_heartbeat failed\\n\");\n\t}\n\n        MPI_Finalize();\n        return 0;\n}"}
{"program": "d-meiser_125", "code": "int main(int argn, char **argv)\n{\n#ifdef BL_WITH_MPI\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  TestSuite *suite = create_test_suite();\n  add_test_with_context(suite, AtomFieldInteraction, canBeCreated);\n  add_test_with_context(suite, AtomFieldInteraction, isNormConserving);\n  add_test_with_context(suite, AtomFieldInteraction, producesRabiOscillations);\n  int result = run_test_suite(suite, create_text_reporter());\n  destroy_test_suite(suite);\n#ifdef BL_WITH_MPI\n#endif\n  return result;\n}", "label": "int main(int argn, char **argv)\n{\n#ifdef BL_WITH_MPI\n  MPI_Init(&argn, &argv);\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  TestSuite *suite = create_test_suite();\n  add_test_with_context(suite, AtomFieldInteraction, canBeCreated);\n  add_test_with_context(suite, AtomFieldInteraction, isNormConserving);\n  add_test_with_context(suite, AtomFieldInteraction, producesRabiOscillations);\n  int result = run_test_suite(suite, create_text_reporter());\n  destroy_test_suite(suite);\n#ifdef BL_WITH_MPI\n  MPI_Finalize();\n#endif\n  return result;\n}"}
{"program": "UnProgrammatore_126", "code": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tmpz_t the_number;\n\t\n\tmpz_init(the_number);\n\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t}\n\telse\n\t\tmpz_set_str(the_number, argv[1], 10); \n\n\n\tif(my_rank == 0)\n\t\tmaster_procedure(comm_size);\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tmpz_t the_number;\n\t\n\tmpz_init(the_number);\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t\tMPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\telse\n\t\tmpz_set_str(the_number, argv[1], 10); \n\n\n\tif(my_rank == 0)\n\t\tmaster_procedure(comm_size);\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "NonWhite_129", "code": "int main( int argc , char * argv[] ){\n\tint myrank , npes ;\n\tdouble start , end ;\n\tint nRowsPerProcess = TAM / npes ;\n\tint a[ TMAT ] , b[ TMAT ] , sum[ TMAT ] , sender , target ;\n\tint myA[ nRowsPerProcess * TAM ] , i , k ;\n\tif( myrank == ROOT ){\n\t\tfor( i = 0 ; i < TAM ; i++){\n\t\t\tfor( j = 0 ; j < TAM ; j++){\n\t\t\t\ta[ i * TAM + j ] = ( i + j ) ;\n\t\t\t\tb[ i * TAM + j ] = ( i + j ) * MUL ;\n\t\t\t\tsum[ i * TAM + j ] = 0 ;\n\t\t\t}\n\t\t}\n\t\t\n\n\t\tfor( target = 0 ; target < npes ; target++ )\n\t}\n\tif( myrank == ROOT ) start =\n\tMPI_Status status ;\n\tint frow = myrank * nRowsPerProcess ;\n\tfor( k = 0 ; k < nRowsPerProcess ; k++)\n\t\tfor( i = 0 ; i < TAM ; i++)\n\t\t\tsum[ ( k + frow ) * TAM + i ] = myA[ ( k + frow ) * TAM + i ] + b[ ( k + frow ) * TAM + i ] ;\n\tif( myrank == ROOT ){\n\t\t\n\n\t\tfor( sender = 0 ; sender < npes ; sender++ )\n\t\tprintf(\"MATRIX PRODUCT\\n\" ) ;\n\t\tfor( i = 0 ; i < TAM ; i++){ for( j = 0 ; j < TAM ; j++) printf(\"%d \" , sum[ i * TAM + j ] ) ; printf(\"\\n\" ) ; } printf(\"\\n\" ) ;\n\t\tend =\n\t\tprintf(\"TIME = %.6lf\\n\" , end - start ) ;\n\t}\n\treturn 0 ;\n}", "label": "int main( int argc , char * argv[] ){\n\tint myrank , npes ;\n\tdouble start , end ;\n\tMPI_Init( &argc , &argv ) ;\n\tMPI_Comm_size( MPI_COMM_WORLD , &npes ) ;\n\tMPI_Comm_rank( MPI_COMM_WORLD , &myrank ) ;\n\tint nRowsPerProcess = TAM / npes ;\n\tint a[ TMAT ] , b[ TMAT ] , sum[ TMAT ] , sender , target ;\n\tint myA[ nRowsPerProcess * TAM ] , i , k ;\n\tif( myrank == ROOT ){\n\t\tfor( i = 0 ; i < TAM ; i++){\n\t\t\tfor( j = 0 ; j < TAM ; j++){\n\t\t\t\ta[ i * TAM + j ] = ( i + j ) ;\n\t\t\t\tb[ i * TAM + j ] = ( i + j ) * MUL ;\n\t\t\t\tsum[ i * TAM + j ] = 0 ;\n\t\t\t}\n\t\t}\n\t\t\n\n\t\tfor( target = 0 ; target < npes ; target++ )\n\t\t\tMPI_Send( a + target * TAM * nRowsPerProcess , TAM * nRowsPerProcess , MPI_INT , target , target , MPI_COMM_WORLD ) ;\n\t}\n\tif( myrank == ROOT ) start = MPI_Wtime() ;\n\tMPI_Bcast( b , TMAT , MPI_INT , 0 , MPI_COMM_WORLD ) ;\n\tMPI_Status status ;\n\tMPI_Recv( myA , TAM * nRowsPerProcess , MPI_INT , ROOT , myrank , MPI_COMM_WORLD , &status ) ;\n\tint frow = myrank * nRowsPerProcess ;\n\tfor( k = 0 ; k < nRowsPerProcess ; k++)\n\t\tfor( i = 0 ; i < TAM ; i++)\n\t\t\tsum[ ( k + frow ) * TAM + i ] = myA[ ( k + frow ) * TAM + i ] + b[ ( k + frow ) * TAM + i ] ;\n\tMPI_Send( sum + TAM * frow , TAM * nRowsPerProcess , MPI_INT , ROOT , myrank , MPI_COMM_WORLD ) ;\n\tif( myrank == ROOT ){\n\t\t\n\n\t\tfor( sender = 0 ; sender < npes ; sender++ )\n\t\t\tMPI_Recv( sum + TAM * nRowsPerProcess * sender , TAM * nRowsPerProcess , MPI_INT , sender , sender , MPI_COMM_WORLD , &status ) ;\n\t\tprintf(\"MATRIX PRODUCT\\n\" ) ;\n\t\tfor( i = 0 ; i < TAM ; i++){ for( j = 0 ; j < TAM ; j++) printf(\"%d \" , sum[ i * TAM + j ] ) ; printf(\"\\n\" ) ; } printf(\"\\n\" ) ;\n\t\tend = MPI_Wtime() ;\n\t\tprintf(\"TIME = %.6lf\\n\" , end - start ) ;\n\t}\n\tMPI_Finalize() ;\n\treturn 0 ;\n}"}
{"program": "erpicci_131", "code": "int main(int argc, char *argv[]) {\n    int mpi_rank;\n\n\n    if (ROOT == mpi_rank) {\n        st_matrix_t M = st_matrix_load(stdin);\n        const unsigned int size = st_matrix_size(M);\n        double *eigenvalues;\n        unsigned int i;\n        stopwatch_t stopwatch = stopwatch_create(\"Householder_solver\");\n\n        (void) argc;\n        (void) argv;\n\n        \n\n        SAFE_MALLOC(eigenvalues, double *, size * sizeof(double));\n\n\n        \n\n        stopwatch_start(stopwatch, 0, \"Compute eigenvalues\");\n        qr_householder_r(M, eigenvalues, NULL);\n        stopwatch_stop(stopwatch, 0);\n\n\n        \n\n        printf(\"Eigenvalues:\\n[\");\n        for (i = 0; i < size - 1; ++i) {\n            printf(\"%g, \", eigenvalues[i]);\n        }\n        printf(\"%g]\\n\", eigenvalues[i]);\n\n\n        \n\n        st_matrix_delete(&M);\n        free(eigenvalues);\n        stopwatch_delete(&stopwatch);\n    }\n\n    \n\n    else {\n        qr_householder_r(NULL, NULL, NULL);\n    }\n\n\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char *argv[]) {\n    int mpi_rank;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    if (ROOT == mpi_rank) {\n        st_matrix_t M = st_matrix_load(stdin);\n        const unsigned int size = st_matrix_size(M);\n        double *eigenvalues;\n        unsigned int i;\n        stopwatch_t stopwatch = stopwatch_create(\"Householder_solver\");\n\n        (void) argc;\n        (void) argv;\n\n        \n\n        SAFE_MALLOC(eigenvalues, double *, size * sizeof(double));\n\n\n        \n\n        stopwatch_start(stopwatch, 0, \"Compute eigenvalues\");\n        qr_householder_r(M, eigenvalues, NULL);\n        stopwatch_stop(stopwatch, 0);\n\n\n        \n\n        printf(\"Eigenvalues:\\n[\");\n        for (i = 0; i < size - 1; ++i) {\n            printf(\"%g, \", eigenvalues[i]);\n        }\n        printf(\"%g]\\n\", eigenvalues[i]);\n\n\n        \n\n        st_matrix_delete(&M);\n        free(eigenvalues);\n        stopwatch_delete(&stopwatch);\n    }\n\n    \n\n    else {\n        qr_householder_r(NULL, NULL, NULL);\n    }\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "qingu_132", "code": "int main (int argc, char ** argv)\n{\n\tint rank, size;\n    SOCKET_FD_TYPE fd = INVALID_SOCKET_FD;\n\tMPI_Comm intercomm, intracomm;\n\n\tif (size != 1) {\n\t    fprintf( stderr, \"This test requires that only one process be in each comm_world\\n\" );\n\t}\n\tif(parse_args(argc, argv) == -1){\n        fprintf(stderr, \"Unable to parse the command line arguments\\n\");\n    }\n\n\tif (is_server)  {\n\t\tfd = server_routine(opt_port);\n\n\t} else if (is_client) {\n\t\tfd = client_routine(opt_port);\n\t}\n\n\tif (fd == INVALID_SOCKET_FD) {\n\t\treturn -1;\n\t}\n\n#ifdef SINGLETON_KICK\n\n\n\n\n\t{\n\t\tint *usize, aflag;\n\t}\n#endif\n\n\n\tif (is_server) {\n\t}\n\telse {\n\t    printf( \"Completed receive on intercomm\\n\" ); fflush(stdout);\n\t}\n\n\n\tprintf(\"[%d/%d] after Intercomm_merge\\n\", rank, size);\n\n\treturn 0;\n}", "label": "int main (int argc, char ** argv)\n{\n\tint rank, size;\n    SOCKET_FD_TYPE fd = INVALID_SOCKET_FD;\n\tMPI_Comm intercomm, intracomm;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size( MPI_COMM_WORLD, &size );\n\tif (size != 1) {\n\t    fprintf( stderr, \"This test requires that only one process be in each comm_world\\n\" );\n\t    MPI_Abort( MPI_COMM_WORLD, 1 );\n\t}\n\tif(parse_args(argc, argv) == -1){\n        fprintf(stderr, \"Unable to parse the command line arguments\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n\tif (is_server)  {\n\t\tfd = server_routine(opt_port);\n\n\t} else if (is_client) {\n\t\tfd = client_routine(opt_port);\n\t}\n\n\tif (fd == INVALID_SOCKET_FD) {\n\t\treturn -1;\n\t}\n\n#ifdef SINGLETON_KICK\n\n\n\n\n\t{\n\t\tint *usize, aflag;\n\t\tMPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, \n\t\t\t\t&usize, &aflag);\n\t}\n#endif\n\n\tMPI_Comm_join(fd, &intercomm);\n\n\tif (is_server) {\n\t    MPI_Send( MPI_BOTTOM, 0, MPI_INT, 0, 0, intercomm );\n\t}\n\telse {\n\t    MPI_Recv( MPI_BOTTOM, 0, MPI_INT, 0, 0, intercomm, \n\t\t    MPI_STATUS_IGNORE );\n\t    printf( \"Completed receive on intercomm\\n\" ); fflush(stdout);\n\t}\n\n\tMPI_Intercomm_merge(intercomm, 0, &intracomm);\n\tMPI_Comm_rank(intracomm, &rank);\n\tMPI_Comm_size(intracomm, &size);\n\n\tprintf(\"[%d/%d] after Intercomm_merge\\n\", rank, size);\n\n\tMPI_Comm_free(&intracomm);\n\tMPI_Comm_disconnect(&intercomm);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "gyaikhom_134", "code": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    FILE *f;\n    char fname[128];\n\n    bc_init(BC_ERR);\n\n    sprintf(fname, \"bc_mc_lockstep_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i,\n                lock_step_exchange_bc_mc(11, 1L << i));\n    }\n    fclose(f);\n\n    sleep(1);\n\n    sprintf(fname, \"bc_nmc_lockstep_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i,\n                lock_step_exchange_bc_nmc(11, 1L << i));\n    }\n    fclose(f);\n\n    sleep(1);\n\n    sprintf(fname, \"mpi_lockstep_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i,\n                lock_step_exchange_mpi(11, 1L << i));\n    }\n    fclose(f);\n\n    bc_final();\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    FILE *f;\n    char fname[128];\n\n    MPI_Init(&argc, &argv);\n    bc_init(BC_ERR);\n\n    sprintf(fname, \"bc_mc_lockstep_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i,\n                lock_step_exchange_bc_mc(11, 1L << i));\n    }\n    fclose(f);\n\n    sleep(1);\n\n    sprintf(fname, \"bc_nmc_lockstep_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i,\n                lock_step_exchange_bc_nmc(11, 1L << i));\n    }\n    fclose(f);\n\n    sleep(1);\n\n    sprintf(fname, \"mpi_lockstep_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i,\n                lock_step_exchange_mpi(11, 1L << i));\n    }\n    fclose(f);\n\n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "dgergel_135", "code": "int\nmain(int    argc,\n     char **argv)\n{\n    int          status;\n    timer_struct global_timers[N_TIMERS];\n    char         state_filename[MAXSTRING];\n\n    \n\n    timer_start(&(global_timers[TIMER_VIC_ALL]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_INIT]));\n\n    \n\n    status =\n    if (status != MPI_SUCCESS) {\n        exit(EXIT_FAILURE);\n    }\n\n    \n\n    initialize_log();\n\n    \n\n    initialize_mpi();\n\n    \n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        cmd_proc(argc, argv, filenames.global);\n    }\n\n    \n\n    vic_image_start();\n\n    \n\n    vic_alloc();\n\n    \n\n    vic_image_init();\n\n    \n\n    vic_populate_model_state();\n\n    \n\n    vic_init_output(&(dmy[0]));\n\n    \n\n    log_info(\n        \"Initialization is complete, print global param and options structures\");\n    print_global_param(&global_param);\n    print_option(&options);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_INIT]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_RUN]));\n\n    \n\n    for (current = 0; current < global_param.nrecs; current++) {\n        \n\n        vic_force();\n\n        \n\n        vic_image_run(&(dmy[current]));\n\n        \n\n        vic_write_output(&(dmy[current]));\n\n        \n\n        if (check_save_state_flag(current)) {\n            debug(\"writing state file for timestep %zu\", current);\n            vic_store(&(dmy[current]), state_filename);\n            debug(\"finished storing state file: %s\", state_filename)\n        }\n    }\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_RUN]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    vic_image_finalize();\n\n    \n\n    status =\n    if (status != MPI_SUCCESS) {\n        log_err(\"MPI error: %d\", status);\n    }\n\n    log_info(\"Completed running VIC %s\", VIC_DRIVER);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_ALL]));\n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        \n\n        write_vic_timing_table(global_timers, VIC_DRIVER);\n    }\n\n    return EXIT_SUCCESS;\n}", "label": "int\nmain(int    argc,\n     char **argv)\n{\n    int          status;\n    timer_struct global_timers[N_TIMERS];\n    char         state_filename[MAXSTRING];\n\n    \n\n    timer_start(&(global_timers[TIMER_VIC_ALL]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_INIT]));\n\n    \n\n    status = MPI_Init(&argc, &argv);\n    if (status != MPI_SUCCESS) {\n        fprintf(stderr, \"MPI error in main(): %d\\n\", status);\n        exit(EXIT_FAILURE);\n    }\n\n    \n\n    initialize_log();\n\n    \n\n    initialize_mpi();\n\n    \n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        cmd_proc(argc, argv, filenames.global);\n    }\n\n    \n\n    vic_image_start();\n\n    \n\n    vic_alloc();\n\n    \n\n    vic_image_init();\n\n    \n\n    vic_populate_model_state();\n\n    \n\n    vic_init_output(&(dmy[0]));\n\n    \n\n    log_info(\n        \"Initialization is complete, print global param and options structures\");\n    print_global_param(&global_param);\n    print_option(&options);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_INIT]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_RUN]));\n\n    \n\n    for (current = 0; current < global_param.nrecs; current++) {\n        \n\n        vic_force();\n\n        \n\n        vic_image_run(&(dmy[current]));\n\n        \n\n        vic_write_output(&(dmy[current]));\n\n        \n\n        if (check_save_state_flag(current)) {\n            debug(\"writing state file for timestep %zu\", current);\n            vic_store(&(dmy[current]), state_filename);\n            debug(\"finished storing state file: %s\", state_filename)\n        }\n    }\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_RUN]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    vic_image_finalize();\n\n    \n\n    status = MPI_Finalize();\n    if (status != MPI_SUCCESS) {\n        log_err(\"MPI error: %d\", status);\n    }\n\n    log_info(\"Completed running VIC %s\", VIC_DRIVER);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_ALL]));\n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        \n\n        write_vic_timing_table(global_timers, VIC_DRIVER);\n    }\n\n    return EXIT_SUCCESS;\n}"}
{"program": "nerscadmin_137", "code": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int *scnt, *rcnt;\n  int *sdpl, *rdpl;\n  int dsize;\n  int ssize, rsize;\n\n\n\n  scnt = malloc( sizeof(int)*nprocs );\n  sdpl = malloc( sizeof(int)*nprocs );\n  rcnt = malloc( sizeof(int)*nprocs );\n  rdpl = malloc( sizeof(int)*nprocs );\n  \n  for( i=0; i<nprocs; i++ )\n    {\n      scnt[i]=SIZE*(i+1)*(myrank+1);\n      rcnt[i]=SIZE*(i+1)*(myrank+1);\n      sdpl[i]=SIZE*((i*(i+1))/2)*(myrank+1);\n      rdpl[i]=SIZE*((i*(i+1))/2)*(myrank+1);\n    }\n  \n  ssize=0; for(i=0; i<nprocs; i++) ssize+=scnt[i];\n  rsize=0; for(i=0; i<nprocs; i++) rsize+=rcnt[i];\n  \n  sbuf = (char*) malloc( SIZE*dsize*ssize );\n  rbuf = (char*) malloc( SIZE*dsize*rsize );\n\n  for( i=0; i<REPEAT; i++ )\n    {\n    }\n\n  fprintf(stdout, \"DONE (rank %d)!\\n\", myrank);\n  \n  return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int *scnt, *rcnt;\n  int *sdpl, *rdpl;\n  int dsize;\n  int ssize, rsize;\n\n  MPI_Init( &argc, &argv );\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  MPI_Type_size(DATATYPE, &dsize);\n\n  scnt = malloc( sizeof(int)*nprocs );\n  sdpl = malloc( sizeof(int)*nprocs );\n  rcnt = malloc( sizeof(int)*nprocs );\n  rdpl = malloc( sizeof(int)*nprocs );\n  \n  for( i=0; i<nprocs; i++ )\n    {\n      scnt[i]=SIZE*(i+1)*(myrank+1);\n      rcnt[i]=SIZE*(i+1)*(myrank+1);\n      sdpl[i]=SIZE*((i*(i+1))/2)*(myrank+1);\n      rdpl[i]=SIZE*((i*(i+1))/2)*(myrank+1);\n    }\n  \n  ssize=0; for(i=0; i<nprocs; i++) ssize+=scnt[i];\n  rsize=0; for(i=0; i<nprocs; i++) rsize+=rcnt[i];\n  \n  sbuf = (char*) malloc( SIZE*dsize*ssize );\n  rbuf = (char*) malloc( SIZE*dsize*rsize );\n\n  for( i=0; i<REPEAT; i++ )\n    {\n      MPI_Alltoallv( sbuf, scnt, sdpl, DATATYPE,\n\t\t     rbuf, rcnt, rdpl, DATATYPE,\n\t\t     MPI_COMM_WORLD );\n    }\n\n  fprintf(stdout, \"DONE (rank %d)!\\n\", myrank);\n  \n  MPI_Finalize();\n  return 0;\n}"}
{"program": "lellisls_139", "code": "int main( int argc, char *argv[] ) {\n\n  float *vec, max, min, *maxTot, *minTot;\n  int i, pri, qtde, tamLocal, ierr, numProc, esteProc, iproc;\n  MPI_Status status;\n\n\n  maxTot = ( float* ) malloc( numProc * sizeof( float ) );\n  if( !maxTot ) {\n    printf( \"Impossivel alocar MaxTot\\n\" ); return( 0 );\n  }\n  minTot = ( float* ) malloc( numProc * sizeof( float ) );\n  if( !minTot ) {\n    printf( \"Impossivel alocar MinTot\\n\" ); return( 0 );\n  }\n  tamLocal = tam / numProc;\n  pri = esteProc * tamLocal;\n  if( esteProc == numProc - 1 ) {\n    qtde = tam - pri;\n  }\n  else {\n    qtde = floor( ( float ) tam / numProc );\n  }\n  vec = ( float* ) malloc( qtde * sizeof( float ) );\n  if( !vec ) {\n    printf( \"Impossivel alocar\\n\" ); return( 0 );\n  }\n  printf( \"no %d/%d: pri=%d, qtde=%d, ultimo=%d\\n\",\n          esteProc, numProc, pri, qtde, ( pri + qtde ) );\n  for( i = 0; i < qtde; i++ ) {\n    vec[ i ] = ( ( float ) ( i + pri ) - ( float ) tam / 2.0 );\n    vec[ i ] *= vec[ i ];\n  }\n  for( i = 0; i < qtde; i++ ) {\n    vec[ i ] = sqrt( vec[ i ] );\n  }\n  maxTot[ esteProc ] = vec[ 0 ];\n  minTot[ esteProc ] = vec[ 0 ];\n  for( i = 0; i < qtde; i++ ) {\n    if( vec[ i ] > maxTot[ esteProc ] ) {\n      maxTot[ esteProc ] = vec[ i ];\n    }\n    if( vec[ i ] < minTot[ esteProc ] ) {\n      minTot[ esteProc ] = vec[ i ];\n    }\n  }\n  printf( \"no %d/%d: min= %f, max=%f\\n\",\n          esteProc, numProc, minTot[ esteProc ], maxTot[ esteProc ] );\n  fflush( stdout );\n  if( esteProc != 0 ) { \n\n  }\n  else { \n\n    for( iproc = 1; iproc < numProc; iproc++ ) {\n    }\n  }\n  if( esteProc == 0 ) { \n\n    max = maxTot[ 0 ]; min = minTot[ 0 ];\n    for( i = 0; i < numProc; i++ ) {\n      printf( \"MESTRE: i=%d, min= %f, max=%f\\n\",\n              i, minTot[ i ], maxTot[ i ] );\n      fflush( stdout );\n      if( maxTot[ i ] > max ) {\n        max = maxTot[ i ];\n      }\n      if( minTot[ i ] < min ) {\n        min = minTot[ i ];\n      }\n    }\n    printf( \"Max=%f, Min=%f\\n\", max, min );\n  }\n  return( 0 );\n}", "label": "int main( int argc, char *argv[] ) {\n\n  float *vec, max, min, *maxTot, *minTot;\n  int i, pri, qtde, tamLocal, ierr, numProc, esteProc, iproc;\n  MPI_Status status;\n\n  MPI_Init( &argc, &argv );\n  MPI_Comm_size( MPI_COMM_WORLD, &numProc );\n  MPI_Comm_rank( MPI_COMM_WORLD, &esteProc );\n\n  maxTot = ( float* ) malloc( numProc * sizeof( float ) );\n  if( !maxTot ) {\n    printf( \"Impossivel alocar MaxTot\\n\" ); return( 0 );\n  }\n  minTot = ( float* ) malloc( numProc * sizeof( float ) );\n  if( !minTot ) {\n    printf( \"Impossivel alocar MinTot\\n\" ); return( 0 );\n  }\n  tamLocal = tam / numProc;\n  pri = esteProc * tamLocal;\n  if( esteProc == numProc - 1 ) {\n    qtde = tam - pri;\n  }\n  else {\n    qtde = floor( ( float ) tam / numProc );\n  }\n  vec = ( float* ) malloc( qtde * sizeof( float ) );\n  if( !vec ) {\n    printf( \"Impossivel alocar\\n\" ); return( 0 );\n  }\n  printf( \"no %d/%d: pri=%d, qtde=%d, ultimo=%d\\n\",\n          esteProc, numProc, pri, qtde, ( pri + qtde ) );\n  for( i = 0; i < qtde; i++ ) {\n    vec[ i ] = ( ( float ) ( i + pri ) - ( float ) tam / 2.0 );\n    vec[ i ] *= vec[ i ];\n  }\n  for( i = 0; i < qtde; i++ ) {\n    vec[ i ] = sqrt( vec[ i ] );\n  }\n  maxTot[ esteProc ] = vec[ 0 ];\n  minTot[ esteProc ] = vec[ 0 ];\n  for( i = 0; i < qtde; i++ ) {\n    if( vec[ i ] > maxTot[ esteProc ] ) {\n      maxTot[ esteProc ] = vec[ i ];\n    }\n    if( vec[ i ] < minTot[ esteProc ] ) {\n      minTot[ esteProc ] = vec[ i ];\n    }\n  }\n  printf( \"no %d/%d: min= %f, max=%f\\n\",\n          esteProc, numProc, minTot[ esteProc ], maxTot[ esteProc ] );\n  fflush( stdout );\n  if( esteProc != 0 ) { \n\n    MPI_Send( &maxTot[ esteProc ], 1, MPI_FLOAT, 0, 12, MPI_COMM_WORLD );\n    MPI_Send( &minTot[ esteProc ], 1, MPI_FLOAT, 0, 13, MPI_COMM_WORLD );\n  }\n  else { \n\n    for( iproc = 1; iproc < numProc; iproc++ ) {\n      MPI_Recv( &maxTot[ iproc ], 1, MPI_FLOAT, iproc,\n                12, MPI_COMM_WORLD, &status );\n      MPI_Recv( &minTot[ iproc ], 1, MPI_FLOAT, iproc,\n                13, MPI_COMM_WORLD, &status );\n    }\n  }\n  if( esteProc == 0 ) { \n\n    max = maxTot[ 0 ]; min = minTot[ 0 ];\n    for( i = 0; i < numProc; i++ ) {\n      printf( \"MESTRE: i=%d, min= %f, max=%f\\n\",\n              i, minTot[ i ], maxTot[ i ] );\n      fflush( stdout );\n      if( maxTot[ i ] > max ) {\n        max = maxTot[ i ];\n      }\n      if( minTot[ i ] < min ) {\n        min = minTot[ i ];\n      }\n    }\n    printf( \"Max=%f, Min=%f\\n\", max, min );\n  }\n  MPI_Finalize( );\n  return( 0 );\n}"}
{"program": "aleph7_140", "code": "int\nmain (int argc, char **argv)\n{\n    hid_t       file_id, dset_id, grp_id;\n    hid_t       fapl, sid, mem_dataspace;\n    hsize_t     dims[RANK], i;\n    herr_t      ret;\n    char\tfilename[1024];\n    int         mpi_size, mpi_rank;\n    MPI_Comm    comm  = MPI_COMM_WORLD;\n    MPI_Info    info  = MPI_INFO_NULL;\n    hsize_t     start[RANK];\n    hsize_t     count[RANK];\n    hsize_t     stride[RANK];\n    hsize_t     block[RANK];\n    DATATYPE   *data_array = NULL;\t\n\n\n\n    if(MAINPROCESS)\n\tTESTING(\"proper shutdown of HDF5 library\");\n \n    \n\n    fapl = H5Pcreate(H5P_FILE_ACCESS);\n    VRFY((fapl >= 0), \"H5Pcreate succeeded\");\n    ret = H5Pset_fapl_mpio(fapl, comm, info);\n    VRFY((ret >= 0), \"\");\n\n    h5_fixname(FILENAME[0], fapl, filename, sizeof filename);\n    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, fapl);\n    VRFY((file_id >= 0), \"H5Fcreate succeeded\");\n    grp_id = H5Gcreate2(file_id, \"Group\", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\n    VRFY((grp_id >= 0), \"H5Gcreate succeeded\");\n\n    dims[0] = ROW_FACTOR*mpi_size;\n    dims[1] = COL_FACTOR*mpi_size;\n    sid = H5Screate_simple (RANK, dims, NULL);\n    VRFY((sid >= 0), \"H5Screate_simple succeeded\");\n\n    dset_id = H5Dcreate2(grp_id, \"Dataset\", H5T_NATIVE_INT, sid, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\n    VRFY((dset_id >= 0), \"H5Dcreate succeeded\");\n\n    \n\n    data_array = (DATATYPE *)HDmalloc(dims[0]*dims[1]*sizeof(DATATYPE));\n    VRFY((data_array != NULL), \"data_array HDmalloc succeeded\");\n\n    \n\n    block[0] = dims[0]/mpi_size;\n    block[1] = dims[1];\n    stride[0] = block[0];\n    stride[1] = block[1];\n    count[0] = 1;\n    count[1] = 1;\n    start[0] = mpi_rank*block[0];\n    start[1] = 0;\n\n    \n\n    for(i=0 ; i<dims[0]*dims[1]; i++)\n        data_array[i] = mpi_rank + 1;\n\n    ret = H5Sselect_hyperslab(sid, H5S_SELECT_SET, start, stride, count, block);\n    VRFY((ret >= 0), \"H5Sset_hyperslab succeeded\");\n\n    \n\n    mem_dataspace = H5Screate_simple (RANK, block, NULL);\n    VRFY((mem_dataspace >= 0), \"\");\n\n    \n\n    ret = H5Dwrite(dset_id, H5T_NATIVE_INT, mem_dataspace, sid,\n                   H5P_DEFAULT, data_array);\n    VRFY((ret >= 0), \"H5Dwrite succeeded\");\n\n    \n\n    if(data_array) \n        HDfree(data_array);\n\n\n    nerrors += GetTestNumErrs();\n\n    if(MAINPROCESS) {\n        if(0 == nerrors)\n            PASSED()\n        else\n\t    H5_FAILED()\n    }\n\n    return (nerrors!=0);\n}", "label": "int\nmain (int argc, char **argv)\n{\n    hid_t       file_id, dset_id, grp_id;\n    hid_t       fapl, sid, mem_dataspace;\n    hsize_t     dims[RANK], i;\n    herr_t      ret;\n    char\tfilename[1024];\n    int         mpi_size, mpi_rank;\n    MPI_Comm    comm  = MPI_COMM_WORLD;\n    MPI_Info    info  = MPI_INFO_NULL;\n    hsize_t     start[RANK];\n    hsize_t     count[RANK];\n    hsize_t     stride[RANK];\n    hsize_t     block[RANK];\n    DATATYPE   *data_array = NULL;\t\n\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(comm, &mpi_size);\n    MPI_Comm_rank(comm, &mpi_rank);  \n\n    if(MAINPROCESS)\n\tTESTING(\"proper shutdown of HDF5 library\");\n \n    \n\n    fapl = H5Pcreate(H5P_FILE_ACCESS);\n    VRFY((fapl >= 0), \"H5Pcreate succeeded\");\n    ret = H5Pset_fapl_mpio(fapl, comm, info);\n    VRFY((ret >= 0), \"\");\n\n    h5_fixname(FILENAME[0], fapl, filename, sizeof filename);\n    file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, fapl);\n    VRFY((file_id >= 0), \"H5Fcreate succeeded\");\n    grp_id = H5Gcreate2(file_id, \"Group\", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\n    VRFY((grp_id >= 0), \"H5Gcreate succeeded\");\n\n    dims[0] = ROW_FACTOR*mpi_size;\n    dims[1] = COL_FACTOR*mpi_size;\n    sid = H5Screate_simple (RANK, dims, NULL);\n    VRFY((sid >= 0), \"H5Screate_simple succeeded\");\n\n    dset_id = H5Dcreate2(grp_id, \"Dataset\", H5T_NATIVE_INT, sid, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\n    VRFY((dset_id >= 0), \"H5Dcreate succeeded\");\n\n    \n\n    data_array = (DATATYPE *)HDmalloc(dims[0]*dims[1]*sizeof(DATATYPE));\n    VRFY((data_array != NULL), \"data_array HDmalloc succeeded\");\n\n    \n\n    block[0] = dims[0]/mpi_size;\n    block[1] = dims[1];\n    stride[0] = block[0];\n    stride[1] = block[1];\n    count[0] = 1;\n    count[1] = 1;\n    start[0] = mpi_rank*block[0];\n    start[1] = 0;\n\n    \n\n    for(i=0 ; i<dims[0]*dims[1]; i++)\n        data_array[i] = mpi_rank + 1;\n\n    ret = H5Sselect_hyperslab(sid, H5S_SELECT_SET, start, stride, count, block);\n    VRFY((ret >= 0), \"H5Sset_hyperslab succeeded\");\n\n    \n\n    mem_dataspace = H5Screate_simple (RANK, block, NULL);\n    VRFY((mem_dataspace >= 0), \"\");\n\n    \n\n    ret = H5Dwrite(dset_id, H5T_NATIVE_INT, mem_dataspace, sid,\n                   H5P_DEFAULT, data_array);\n    VRFY((ret >= 0), \"H5Dwrite succeeded\");\n\n    \n\n    if(data_array) \n        HDfree(data_array);\n\n    MPI_Finalize();\n\n    nerrors += GetTestNumErrs();\n\n    if(MAINPROCESS) {\n        if(0 == nerrors)\n            PASSED()\n        else\n\t    H5_FAILED()\n    }\n\n    return (nerrors!=0);\n}"}
{"program": "rahlk_141", "code": "main (int ac, char **av)\n{\n  int i, j;\n  MPI_Request r1, r2;\n  MPI_Status s1, s2;\n  int cnt = MSG_CNT;\n  int tag = 111;\n  int comm = MPI_COMM_WORLD;\n  int nprocs, rank;\n  float sendX[MSG_CNT] = { 1, 2, 3, 4 };\n  float recvX[MSG_CNT];\n\n  for (i = 0; i < 10; i++)\n    {\n      int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n      int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n    }\n}", "label": "main (int ac, char **av)\n{\n  int i, j;\n  MPI_Request r1, r2;\n  MPI_Status s1, s2;\n  int cnt = MSG_CNT;\n  int tag = 111;\n  int comm = MPI_COMM_WORLD;\n  int nprocs, rank;\n  float sendX[MSG_CNT] = { 1, 2, 3, 4 };\n  float recvX[MSG_CNT];\n\n  MPI_Init (&ac, &av);\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  for (i = 0; i < 10; i++)\n    {\n      int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n      int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n      MPI_Irecv (recvX, cnt, MPI_FLOAT, src, tag, comm, &r1);\n      MPI_Isend (sendX, cnt, MPI_FLOAT, dest, tag, comm, &r2);\n      MPI_Wait (&r1, &s1);\n      MPI_Wait (&r2, &s2);\n    }\n  MPI_Finalize ();\n}"}
{"program": "dash-project_142", "code": "int main( int argc, char* argv[] )\n{\n  int myrank, nprocs;\n  int val, val2;\n  int idx, idx2[2];\n  int flag;\n\n\n  MPI_Request req;\n  MPI_Request req2[2];\n  MPI_Status stat;\n\n\n\n  if( nprocs<2 ) {\n    fprintf(stderr, \"Need at least 2 procs to run this program\\n\");\n    return 1;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    break;\n\n  case 1:\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n\n    break;\n\n  case 1:\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    break;\n\n  case 1:\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    break;\n\n  case 1:\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    break;\n\n  case 1:\n    break;\n  }\n\n\n\n\n  fprintf(stderr, \"%5d: DONE\\n\", myrank);\n\n}", "label": "int main( int argc, char* argv[] )\n{\n  int myrank, nprocs;\n  int val, val2;\n  int idx, idx2[2];\n  int flag;\n\n\n  MPI_Request req;\n  MPI_Request req2[2];\n  MPI_Status stat;\n\n  MPI_Init( &argc, &argv );\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n\n  if( nprocs<2 ) {\n    fprintf(stderr, \"Need at least 2 procs to run this program\\n\");\n    MPI_Abort(MPI_COMM_WORLD, 1);\n    return 1;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    MPI_Send( &val, 1, MPI_INTEGER, 1, 33, MPI_COMM_WORLD);\n    break;\n\n  case 1:\n    MPI_Recv( &val, 1, MPI_INTEGER, 0, 33, MPI_COMM_WORLD, MPI_STATUS_IGNORE );\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    MPI_Isend( &val, 1, MPI_INTEGER, 1, 34, MPI_COMM_WORLD, &req);\n    MPI_Test( &req, &flag, MPI_STATUS_IGNORE );\n    MPI_Wait( &req, MPI_STATUS_IGNORE );\n\n    break;\n\n  case 1:\n    MPI_Recv( &val, 1, MPI_INTEGER, 0, 34, MPI_COMM_WORLD, &stat );\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    MPI_Isend( &val,  1, MPI_INTEGER, 1, 35, MPI_COMM_WORLD, &(req2[0]));\n    MPI_Isend( &val2, 1, MPI_INTEGER, 1, 36, MPI_COMM_WORLD, &(req2[1]));\n    MPI_Testany( 2, req2, &idx, &flag, MPI_STATUS_IGNORE );\n    MPI_Waitany( 2, req2, &idx, MPI_STATUS_IGNORE );\n    break;\n\n  case 1:\n    MPI_Recv( &val,  1, MPI_INTEGER, 0, 35, MPI_COMM_WORLD, &stat );\n    MPI_Recv( &val2, 1, MPI_INTEGER, 0, 36, MPI_COMM_WORLD, &stat );\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    MPI_Isend( &val,  1, MPI_INTEGER, 1, 35, MPI_COMM_WORLD, &(req2[0]));\n    MPI_Isend( &val2, 1, MPI_INTEGER, 1, 36, MPI_COMM_WORLD, &(req2[1]));\n    MPI_Testall( 2, req2, &flag, MPI_STATUSES_IGNORE );\n    MPI_Waitall( 2, req2, MPI_STATUSES_IGNORE );\n    break;\n\n  case 1:\n    MPI_Recv( &val,  1, MPI_INTEGER, 0, 35, MPI_COMM_WORLD, &stat );\n    MPI_Recv( &val2, 1, MPI_INTEGER, 0, 36, MPI_COMM_WORLD, &stat );\n    break;\n  }\n\n  \n\n  switch(myrank) {\n  case 0:\n    MPI_Isend( &val,  1, MPI_INTEGER, 1, 35, MPI_COMM_WORLD, &(req2[0]));\n    MPI_Isend( &val2, 1, MPI_INTEGER, 1, 36, MPI_COMM_WORLD, &(req2[1]));\n    MPI_Testsome( 2, req2, &idx, idx2, MPI_STATUSES_IGNORE );\n    MPI_Waitsome( 2, req2, &idx, idx2, MPI_STATUSES_IGNORE );\n    break;\n\n  case 1:\n    MPI_Recv( &val,  1, MPI_INTEGER, 0, 35, MPI_COMM_WORLD, &stat );\n    MPI_Recv( &val2, 1, MPI_INTEGER, 0, 36, MPI_COMM_WORLD, &stat );\n    break;\n  }\n\n\n\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  fprintf(stderr, \"%5d: DONE\\n\", myrank);\n\n  MPI_Finalize();\n}"}
{"program": "jiajuncao_144", "code": "int main(int argc, char ** argv)\n{\n  int pid, np;\n\n  \n  setbuf(stdout, NULL);\n  char hname[1000];\n\n\n  if(pid == 0)\n    system(\"echo \\\"Start time is: `date`\\\"\"); \n\n  if( argc < 3 ){\n    printf(\"%s Usage:\\n%s <nsamples> <seed>\\n\",argv[0],argv[0]);\n    exit(0);\n  }\n\n  int nSamples = atoi(argv[1]);\n\n  double a[] = {0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -2.00, -1.00,\n                      1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, -1.00,  0.00,\n                      2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00,  0.00,  1.00,\n                      3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00,  1.00,  2.00,\n                      4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00,  2.00,  3.00};\n\n  double b[] = {3.14, 4.00, 1.00, 1.00, 1.00, 3.00,  4.00, 1.00, 1.00, 1.00,\n                      4.14, 5.00, 2.00, 6.00, 2.00, 6.00,  5.00, 3.00, 2.00, 3.00,\n                      5.14, 6.00, 3.00, 7.00, 3.00, 9.00,  6.00, 4.00, 3.00, 5.00,\n                      6.14, 7.00, 4.00, 8.00, 4.00, 10.00, 7.00, 5.00, 5.00, 7.00,\n                      7.14, 8.00, 5.00, 9.00, 5.00, 12.00, 8.00, 7.00, 7.00, 9.00}; \n\n  int seed = atoi(argv[2])+pid;\n\n  double sum, mci[nFuncs];\n  int i,j, tid;\n\n  double (*f[nFuncs])(double)={f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, \n\t\t\t       f11, f12, f13, f14, f15, f16, f17, f18, \n\t\t\t       f19, f20, f21, f22, f23, f24, f25, f26,\n\t\t\t       f27, f28, f29, f30, f31, f32, f33, f34,\n\t\t\t       f35, f36, f37, f38, f39, f40, f41, f42,\n\t\t\t       f43, f44, f45, f46, f47, f48, f49, f50};\n\n  srand(seed);\n\n  if(pid == 0)\n  {\n    printf(\"%s : Monte Carlo Integration\\n\", getMyHostName(hname, 1000) );\n    printf(\"%s : Random number seed = %d\\n\", getMyHostName(hname, 1000), seed);\n    printf(\"%s : nSamples = %d\\n\", getMyHostName(hname, 1000), nSamples);\n    printf(\"%s : nProcs = %d\\n\", getMyHostName(hname, 1000), np);\n  }\n\n  for(j = 0; j < nFuncs; j++)\n  {\n    sum = 0.00;\n \n    for(i = pid; i < nSamples; i+=np)\n    {\n      sum += (*f[j])(a[j] + urand()*(b[j] - a[j]));\n    }   \n    if(pid == 0)\n    {\n      mci[j] = ((b[j]-a[j])*(mci[j]))/(double)nSamples;\n      printf(\"%s : Integral of f%d over %E to %E is %E\\n\", getMyHostName(hname, 1000), j+1, a[j], b[j], mci[j]);\n    }\n  }  \n\n\n    \n\n  return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n  int pid, np;\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n  MPI_Comm_size(MPI_COMM_WORLD, &np);  \n  \n  setbuf(stdout, NULL);\n  char hname[1000];\n\n\n  if(pid == 0)\n    system(\"echo \\\"Start time is: `date`\\\"\"); \n\n  if( argc < 3 ){\n    printf(\"%s Usage:\\n%s <nsamples> <seed>\\n\",argv[0],argv[0]);\n    exit(0);\n  }\n\n  int nSamples = atoi(argv[1]);\n\n  double a[] = {0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -2.00, -1.00,\n                      1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, -1.00,  0.00,\n                      2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00, 2.00,  0.00,  1.00,\n                      3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00, 3.00,  1.00,  2.00,\n                      4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00, 4.00,  2.00,  3.00};\n\n  double b[] = {3.14, 4.00, 1.00, 1.00, 1.00, 3.00,  4.00, 1.00, 1.00, 1.00,\n                      4.14, 5.00, 2.00, 6.00, 2.00, 6.00,  5.00, 3.00, 2.00, 3.00,\n                      5.14, 6.00, 3.00, 7.00, 3.00, 9.00,  6.00, 4.00, 3.00, 5.00,\n                      6.14, 7.00, 4.00, 8.00, 4.00, 10.00, 7.00, 5.00, 5.00, 7.00,\n                      7.14, 8.00, 5.00, 9.00, 5.00, 12.00, 8.00, 7.00, 7.00, 9.00}; \n\n  int seed = atoi(argv[2])+pid;\n\n  double sum, mci[nFuncs];\n  int i,j, tid;\n\n  double (*f[nFuncs])(double)={f1, f2, f3, f4, f5, f6, f7, f8, f9, f10, \n\t\t\t       f11, f12, f13, f14, f15, f16, f17, f18, \n\t\t\t       f19, f20, f21, f22, f23, f24, f25, f26,\n\t\t\t       f27, f28, f29, f30, f31, f32, f33, f34,\n\t\t\t       f35, f36, f37, f38, f39, f40, f41, f42,\n\t\t\t       f43, f44, f45, f46, f47, f48, f49, f50};\n\n  srand(seed);\n\n  if(pid == 0)\n  {\n    printf(\"%s : Monte Carlo Integration\\n\", getMyHostName(hname, 1000) );\n    printf(\"%s : Random number seed = %d\\n\", getMyHostName(hname, 1000), seed);\n    printf(\"%s : nSamples = %d\\n\", getMyHostName(hname, 1000), nSamples);\n    printf(\"%s : nProcs = %d\\n\", getMyHostName(hname, 1000), np);\n  }\n\n  for(j = 0; j < nFuncs; j++)\n  {\n    sum = 0.00;\n \n    for(i = pid; i < nSamples; i+=np)\n    {\n      sum += (*f[j])(a[j] + urand()*(b[j] - a[j]));\n    }   \n    MPI_Reduce(&sum, &(mci[j]), 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(pid == 0)\n    {\n      mci[j] = ((b[j]-a[j])*(mci[j]))/(double)nSamples;\n      printf(\"%s : Integral of f%d over %E to %E is %E\\n\", getMyHostName(hname, 1000), j+1, a[j], b[j], mci[j]);\n    }\n  }  \n\n\n    \n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_145", "code": "int main(int argc, char *argv[])\n{\n    MPI_Request request;\n    int size, rank;\n    int one = 1, two = 2, isum, sum;\n    assert(size == 2);\n#if defined(TEST_NBC_ROUTINES)\n\n    assert(isum == 2);\n    assert(sum == 4);\n    if (rank == 0)\n        printf(\" No errors\\n\");\n#endif\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Request request;\n    int size, rank;\n    int one = 1, two = 2, isum, sum;\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    assert(size == 2);\n#if defined(TEST_NBC_ROUTINES)\n    MPI_Iallreduce(&one,&isum,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD,&request);\n    MPI_Allreduce(&two,&sum,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);\n    MPI_Wait(&request,MPI_STATUS_IGNORE);\n\n    assert(isum == 2);\n    assert(sum == 4);\n    if (rank == 0)\n        printf(\" No errors\\n\");\n#endif\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "xyuan_146", "code": "int\nmain (int argc, char **argv)\n{\n  MPI_Comm            mpicomm;\n  int                 mpiret;\n  int                 mpisize, mpirank;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n  sc_dmatrix_t       *vtkvec;\n  p4est_tree_t       *tree;\n  sc_array_t         *quadrants;\n  size_t              zz, count;\n  p4est_quadrant_t   *q;\n  int                 i;\n#ifndef P4_TO_P8\n  const char          filename[] = \"p4est_balance_face\";\n#else\n  const char          filename[] = \"p8est_balance_edge\";\n#endif\n\n  \n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n#ifndef P4_TO_P8\n  connectivity = p4est_connectivity_new_unitsquare ();\n#else\n  connectivity = p8est_connectivity_new_unitcube ();\n#endif\n\n  p4est = p4est_new_ext (mpicomm, connectivity, 0, 2, 1,\n                         sizeof (balance_seeds_elem_t), init_fn, NULL);\n\n  p4est_refine (p4est, 1, refine_fn, init_fn);\n\n  p4est_vtk_write_header (p4est, NULL, 1. - 2. * SC_EPS,\n                          0, 0, 0, \"level\", NULL, filename);\n  vtkvec = sc_dmatrix_new (p4est->local_num_quadrants, P4EST_CHILDREN);\n  tree = p4est_tree_array_index (p4est->trees, 0);\n  quadrants = &(tree->quadrants);\n  count = quadrants->elem_count;\n  for (zz = 0; zz < count; zz++) {\n    q = p4est_quadrant_array_index (quadrants, zz);\n    for (i = 0; i < P4EST_CHILDREN; i++) {\n      vtkvec->e[zz][i] = (double)\n        ((balance_seeds_elem_t *) (q->p.user_data))->flag;\n    }\n  }\n  p4est_vtk_write_point_scalar (p4est, NULL, filename, \"level\", vtkvec->e[0]);\n  p4est_vtk_write_footer (p4est, filename);\n\n  sc_dmatrix_destroy (vtkvec);\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n\n  sc_finalize ();\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  MPI_Comm            mpicomm;\n  int                 mpiret;\n  int                 mpisize, mpirank;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n  sc_dmatrix_t       *vtkvec;\n  p4est_tree_t       *tree;\n  sc_array_t         *quadrants;\n  size_t              zz, count;\n  p4est_quadrant_t   *q;\n  int                 i;\n#ifndef P4_TO_P8\n  const char          filename[] = \"p4est_balance_face\";\n#else\n  const char          filename[] = \"p8est_balance_edge\";\n#endif\n\n  \n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n  mpiret = MPI_Comm_size (mpicomm, &mpisize);\n  SC_CHECK_MPI (mpiret);\n  mpiret = MPI_Comm_rank (mpicomm, &mpirank);\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n#ifndef P4_TO_P8\n  connectivity = p4est_connectivity_new_unitsquare ();\n#else\n  connectivity = p8est_connectivity_new_unitcube ();\n#endif\n\n  p4est = p4est_new_ext (mpicomm, connectivity, 0, 2, 1,\n                         sizeof (balance_seeds_elem_t), init_fn, NULL);\n\n  p4est_refine (p4est, 1, refine_fn, init_fn);\n\n  p4est_vtk_write_header (p4est, NULL, 1. - 2. * SC_EPS,\n                          0, 0, 0, \"level\", NULL, filename);\n  vtkvec = sc_dmatrix_new (p4est->local_num_quadrants, P4EST_CHILDREN);\n  tree = p4est_tree_array_index (p4est->trees, 0);\n  quadrants = &(tree->quadrants);\n  count = quadrants->elem_count;\n  for (zz = 0; zz < count; zz++) {\n    q = p4est_quadrant_array_index (quadrants, zz);\n    for (i = 0; i < P4EST_CHILDREN; i++) {\n      vtkvec->e[zz][i] = (double)\n        ((balance_seeds_elem_t *) (q->p.user_data))->flag;\n    }\n  }\n  p4est_vtk_write_point_scalar (p4est, NULL, filename, \"level\", vtkvec->e[0]);\n  p4est_vtk_write_footer (p4est, filename);\n\n  sc_dmatrix_destroy (vtkvec);\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n\n  sc_finalize ();\n\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "bmi-forum_148", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tUniqueList*\t\tlist;\n\t\tIndex\t\t\tidx;\n\t\tStream*\t\t\tstream;\n\t\t\n\t\tstream = Journal_Register( Info_Type, \"myStream\" );\n\t\t\n\t\tlist = UniqueList_New( sizeof(unsigned) );\n\t\tprintf( \"Adding:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\tadding %d at index %d\\n\", idx, UniqueList_Append( list, &idx ) );\n\t\t}\n\t\t\n\t\tprintf( \"Adding:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\tadding %d at index %d\\n\", idx, UniqueList_Append( list, &idx ) );\n\t\t}\n\t\t\n\t\tPrint( list, stream );\n\t\t\n\t\tprintf( \"List Data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\telements[%d]: %d\\n\", idx, UniqueList_ElementAt( list, unsigned, idx ) );\n\t\t}\n\t\t\n\t\tidx = 1000;\n\t\tUniqueList_Mutate( list, 0, &idx );\n\t\t\n\t\tprintf( \"Modified list data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\telements[%d]: %d\\n\", idx, UniqueList_ElementAt( list, unsigned, idx ) );\n\t\t}\n\t\t\n\t\tStg_Class_Delete( list );\n\t}\n\t\n\tBaseFoundation_Finalise();\n\tBaseIO_Finalise();\n\tBaseContainer_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tUniqueList*\t\tlist;\n\t\tIndex\t\t\tidx;\n\t\tStream*\t\t\tstream;\n\t\t\n\t\tstream = Journal_Register( Info_Type, \"myStream\" );\n\t\t\n\t\tlist = UniqueList_New( sizeof(unsigned) );\n\t\tprintf( \"Adding:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\tadding %d at index %d\\n\", idx, UniqueList_Append( list, &idx ) );\n\t\t}\n\t\t\n\t\tprintf( \"Adding:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\tadding %d at index %d\\n\", idx, UniqueList_Append( list, &idx ) );\n\t\t}\n\t\t\n\t\tPrint( list, stream );\n\t\t\n\t\tprintf( \"List Data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\telements[%d]: %d\\n\", idx, UniqueList_ElementAt( list, unsigned, idx ) );\n\t\t}\n\t\t\n\t\tidx = 1000;\n\t\tUniqueList_Mutate( list, 0, &idx );\n\t\t\n\t\tprintf( \"Modified list data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tprintf( \"\\telements[%d]: %d\\n\", idx, UniqueList_ElementAt( list, unsigned, idx ) );\n\t\t}\n\t\t\n\t\tStg_Class_Delete( list );\n\t}\n\t\n\tBaseFoundation_Finalise();\n\tBaseIO_Finalise();\n\tBaseContainer_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "biospi_149", "code": "int\nmain(int argc, char** argv)\n{\n   int ncid,varid;\n   int retval; \n\n   printf(\"\\n*** Testing illegal mode combinations\\n\");\n\n\n   printf(\"*** Testing create + MPIO + fletcher32\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4|NC_MPIIO, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_fletcher32(ncid,varid,NC_FLETCHER32);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n   printf(\"*** Testing create + MPIO + deflation\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4|NC_MPIIO, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_deflate(ncid,varid, NC_NOSHUFFLE, 1, 1);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n\n   SUMMARIZE_ERR;\n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char** argv)\n{\n   int ncid,varid;\n   int retval; \n\n   printf(\"\\n*** Testing illegal mode combinations\\n\");\n\n   MPI_Init(&argc,&argv);\n\n   printf(\"*** Testing create + MPIO + fletcher32\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4|NC_MPIIO, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_fletcher32(ncid,varid,NC_FLETCHER32);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n   printf(\"*** Testing create + MPIO + deflation\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4|NC_MPIIO, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_deflate(ncid,varid, NC_NOSHUFFLE, 1, 1);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n   MPI_Finalize();\n\n   SUMMARIZE_ERR;\n   FINAL_RESULTS;\n}"}
{"program": "gnu3ra_150", "code": "int main( int argc, char *argv[] )\n{\n    int i, j, dims[3], size;\n\n    \n    for (i=2; i<12; i++) {\n\tfor (j=0; j<2; j++) dims[j] = 0;\n\tif (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t     i*i, dims[0], dims[1]);\n\tfor (j=0; j<3; j++) dims[j] = 0;\n\tif (verbose) printf( \"Dims_create(%d,3) = (%d,%d,%d)\\n\", \n\t\t\t     i*i*i, dims[0], dims[1], dims[2]);\n    }\n    size = 2*5*7*11;\n    for (j=0; j<3; j++) dims[j] = 0;\n    if (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1]);\n    for (j=0; j<3; j++) dims[j] = 0;\n    if (verbose) printf( \"Dims_create(%d,3) = (%d,%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1], dims[2]);\n    size = 5*5*2*7*7*7;\n    for (j=0; j<3; j++) dims[j] = 0;\n    if (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1]);\n    for (j=0; j<3; j++) dims[j] = 0;\n    if (verbose) printf( \"Dims_create(%d,3) = (%d,%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1], dims[2]);\n    \n    size = 12;\n    for (j=0; j<3; j++) dims[j] = 0;\n    if (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1]);\n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int i, j, dims[3], size;\n\n    MPI_Init( &argc, &argv );\n    \n    for (i=2; i<12; i++) {\n\tfor (j=0; j<2; j++) dims[j] = 0;\n\tMPI_Dims_create( i * i, 2, dims );\n\tif (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t     i*i, dims[0], dims[1]);\n\tfor (j=0; j<3; j++) dims[j] = 0;\n\tMPI_Dims_create( i * i * i, 3, dims );\n\tif (verbose) printf( \"Dims_create(%d,3) = (%d,%d,%d)\\n\", \n\t\t\t     i*i*i, dims[0], dims[1], dims[2]);\n    }\n    size = 2*5*7*11;\n    for (j=0; j<3; j++) dims[j] = 0;\n    MPI_Dims_create( size, 2, dims );\n    if (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1]);\n    for (j=0; j<3; j++) dims[j] = 0;\n    MPI_Dims_create( size, 3, dims );\n    if (verbose) printf( \"Dims_create(%d,3) = (%d,%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1], dims[2]);\n    size = 5*5*2*7*7*7;\n    for (j=0; j<3; j++) dims[j] = 0;\n    MPI_Dims_create( size, 2, dims );\n    if (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1]);\n    for (j=0; j<3; j++) dims[j] = 0;\n    MPI_Dims_create( size, 3, dims );\n    if (verbose) printf( \"Dims_create(%d,3) = (%d,%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1], dims[2]);\n    \n    size = 12;\n    for (j=0; j<3; j++) dims[j] = 0;\n    MPI_Dims_create( size, 2, dims );\n    if (verbose) printf( \"Dims_create(%d,2) = (%d,%d)\\n\", \n\t\t\t size, dims[0], dims[1]);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_151", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tDictionary* dictionary;\n\tAbstractContext* abstractContext;\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\tBaseContext_Init( &argc, &argv );\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\t\n\n\tStream_SetFileBranch( Journal_GetTypedStream( ErrorStream_Type ), stJournal->stdOut );\n\tstJournal->firewallProducesAssert = False;\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\t\n\t\n\n\tabstractContext = _AbstractContext_New( sizeof(AbstractContext), \"TestContext\", MyDelete, MyPrint, NULL,\n\t\t\tNULL, NULL,\n\t\t_AbstractContext_Build, \n\t\t_AbstractContext_Initialise, \n\t\t_AbstractContext_Execute, \n\t\t_AbstractContext_Destroy, \n\t\t\"Context\", True, MySetDt, 0, 10, CommWorld, dictionary );\n\n\tif( rank == procToWatch ) {\n\t\tstream = Journal_Register( InfoStream_Type, AbstractContext_Type );\n\t\tStg_Component_Build( abstractContext, 0 \n, False );\n\t\tStg_Component_Initialise( abstractContext, 0 \n, False );\n\t\tContext_PrintConcise( abstractContext, stream );\n\t\tStg_Component_Execute( abstractContext, 0 \n, False );\n\t\tStg_Component_Destroy( abstractContext, 0 \n, False );\n\t}\n\t\n\t\n\n\tStg_Class_Delete( abstractContext );\n\tStg_Class_Delete( dictionary );\n\t\n\tBaseContext_Finalise();\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tDictionary* dictionary;\n\tAbstractContext* abstractContext;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\tBaseContext_Init( &argc, &argv );\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\t\n\n\tStream_SetFileBranch( Journal_GetTypedStream( ErrorStream_Type ), stJournal->stdOut );\n\tstJournal->firewallProducesAssert = False;\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\t\n\t\n\n\tabstractContext = _AbstractContext_New( sizeof(AbstractContext), \"TestContext\", MyDelete, MyPrint, NULL,\n\t\t\tNULL, NULL,\n\t\t_AbstractContext_Build, \n\t\t_AbstractContext_Initialise, \n\t\t_AbstractContext_Execute, \n\t\t_AbstractContext_Destroy, \n\t\t\"Context\", True, MySetDt, 0, 10, CommWorld, dictionary );\n\n\tif( rank == procToWatch ) {\n\t\tstream = Journal_Register( InfoStream_Type, AbstractContext_Type );\n\t\tStg_Component_Build( abstractContext, 0 \n, False );\n\t\tStg_Component_Initialise( abstractContext, 0 \n, False );\n\t\tContext_PrintConcise( abstractContext, stream );\n\t\tStg_Component_Execute( abstractContext, 0 \n, False );\n\t\tStg_Component_Destroy( abstractContext, 0 \n, False );\n\t}\n\t\n\t\n\n\tStg_Class_Delete( abstractContext );\n\tStg_Class_Delete( dictionary );\n\t\n\tBaseContext_Finalise();\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "goodinges_152", "code": "int main ( int argc, char *argv[] )\n{\n\tint mpisize, rank, tag;\n\n\tif (argc != 3) {\n\t\tfprintf(stderr, \"Wrong number of arguments!\\n\");\n\t}\n\n\tlong N = atoi(argv[1]);\n\tif (N%mpisize != 0)\n\t{\n\t\tfprintf(stderr, \"N should be a multiple of p\\n\");\n\t}\n\tlong maxIterations = atoi(argv[2]);\n\tint uSize = N/mpisize;\n\n\tdouble *residuals = calloc(N, sizeof(double));\n\tint i;\n\tdouble *residuals_reduced;\n\tif(rank==0)\n\t{\n\t\tresiduals_reduced = calloc(N, sizeof(double));\n\t}\n\tint finished = 0;\n\ttag = 99;\n\n\tdouble h = 1;\n\th = h/(N+1);\n\n\tMPI_Status status;\n\n\tdouble *u = calloc(uSize, sizeof(double));\n\n\t\n\n\t\n\n\n\n\n\t\n\n\tdouble initial_residual;\n\tdouble threshold;\n\tif(rank == 0)\n\t{\n\t\tinitial_residual = 0;\n\t\t\n\n\t\t\n\n\t\t\n\n\t\t\n\n\n\n\t\tfor(i=0;i<N;i++){\n\t\t\tinitial_residual += 1;\n\n\t\t}\n\t\tinitial_residual = sqrt(initial_residual);\n\t\t\n\n\t\tthreshold = initial_residual;\n\t\tthreshold /= 1000000;\n\n\t}\n\n\t\n\n\tdouble *pre_u = calloc(uSize + 2, sizeof(double));\n\n\tlong k;\n\tdouble h2 = h*h;\n\tdouble diag = 2/h2;\n\tdouble residual;\n\tfor(k=0;k<maxIterations ;k++){\n\t\tfor(i=0;i<uSize;i++){\n\t\t\tdouble sum = 1;\n\t\t\tsum -= (-1)*pre_u[i]/h2 + (-1)*pre_u[i+2]/h2;\n\t\t\tu[i] = sum/diag;\n\t\t}\n\n\t\tfor(i=0;i<uSize+2;i++){\n\t\t\tpre_u[i+1] = u[i];\n\t\t}\n\n\n\n\n\t\tif(finished == 1)\n\t\t{\n\t\t\tbreak;\n\t\t}\n\t\telse\n\t\t{\n\n\n\n\n\n\n\n\n\t\t\tif(rank!=0)\n\t\t\t{\n\t\t\t}\n\t\t\tif(rank!=mpisize-1)\n\t\t\t{\n\t\t\t}\n\t\t\tif(rank!=0)\n\t\t\t{\n\t\t\t}\n\t\t}\n\n\t}\n\n\t\n\n\tfor(i=0;i<uSize;i++)\n\t{\n\t\tint realI = rank*uSize + i;\n\t\tif(realI-1>=0)\n\t\t{\n\t\t\tresiduals[realI-1] -= u[i];\n\t\t}\n\t\tresiduals[realI] += 2*u[i];\n\t\tif(realI+1<N)\n\t\t{\n\t\t\tresiduals[realI+1] -= u[i];\n\t\t}\n\t}\n\t\n\n\t\n\n\n\tif(rank==0)\n\t{\n\n\t\tfor(i=0;i<N;i++){\n\t\t\tresiduals_reduced[i] = residuals_reduced[i]/h2 - 1;\n\t\t\t\n\n\t\t}\n\t\tresidual = 0;\n\t\tfor(i=0;i<N;i++){\n\t\t\tresidual += (residuals_reduced[i])*(residuals_reduced[i]);\n\t\t}\n\t\tresidual = sqrt(residual);\n\t\t\n\n\t\tif(residual<=threshold){\n\t\t\tfinished = 1;\n\t\t}\n\t}\n\t\n\tif(rank==0){\n\t\tprintf(\"%ld iterations\\n\",k);\n\t\tprintf(\"Residual: %12.10f\\n\",residual);\n\t}\n\n}", "label": "int main ( int argc, char *argv[] )\n{\n\tint mpisize, rank, tag;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpisize);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (argc != 3) {\n\t\tfprintf(stderr, \"Wrong number of arguments!\\n\");\n\t\tMPI_ABORT(MPI_COMM_WORLD,1);\n\t}\n\n\tlong N = atoi(argv[1]);\n\tif (N%mpisize != 0)\n\t{\n\t\tfprintf(stderr, \"N should be a multiple of p\\n\");\n\t\tMPI_ABORT(MPI_COMM_WORLD,1);\n\t}\n\tlong maxIterations = atoi(argv[2]);\n\tint uSize = N/mpisize;\n\n\tdouble *residuals = calloc(N, sizeof(double));\n\tint i;\n\tdouble *residuals_reduced;\n\tif(rank==0)\n\t{\n\t\tresiduals_reduced = calloc(N, sizeof(double));\n\t}\n\tint finished = 0;\n\ttag = 99;\n\n\tdouble h = 1;\n\th = h/(N+1);\n\n\tMPI_Status status;\n\n\tdouble *u = calloc(uSize, sizeof(double));\n\n\t\n\n\t\n\n\n\n\n\t\n\n\tdouble initial_residual;\n\tdouble threshold;\n\tif(rank == 0)\n\t{\n\t\tinitial_residual = 0;\n\t\t\n\n\t\t\n\n\t\t\n\n\t\t\n\n\n\n\t\tfor(i=0;i<N;i++){\n\t\t\tinitial_residual += 1;\n\n\t\t}\n\t\tinitial_residual = sqrt(initial_residual);\n\t\t\n\n\t\tthreshold = initial_residual;\n\t\tthreshold /= 1000000;\n\n\t}\n\n\t\n\n\tdouble *pre_u = calloc(uSize + 2, sizeof(double));\n\n\tlong k;\n\tdouble h2 = h*h;\n\tdouble diag = 2/h2;\n\tdouble residual;\n\tfor(k=0;k<maxIterations ;k++){\n\t\tfor(i=0;i<uSize;i++){\n\t\t\tdouble sum = 1;\n\t\t\tsum -= (-1)*pre_u[i]/h2 + (-1)*pre_u[i+2]/h2;\n\t\t\tu[i] = sum/diag;\n\t\t}\n\n\t\tfor(i=0;i<uSize+2;i++){\n\t\t\tpre_u[i+1] = u[i];\n\t\t}\n\n\n\n\n\t\tif(finished == 1)\n\t\t{\n\t\t\tbreak;\n\t\t}\n\t\telse\n\t\t{\n\n\n\n\n\n\n\n\n\t\t\tif(rank!=0)\n\t\t\t{\n\t\t\t\tMPI_Send(&u[0], 1, MPI_DOUBLE, rank-1, tag, MPI_COMM_WORLD);\n\t\t\t}\n\t\t\tif(rank!=mpisize-1)\n\t\t\t{\n\t\t\t\tMPI_Send(&u[uSize-1], 1, MPI_DOUBLE, rank+1, tag, MPI_COMM_WORLD);\n\t\t\t\tMPI_Recv(&pre_u[uSize+1], 1, MPI_DOUBLE, rank+1, tag, MPI_COMM_WORLD, &status);\n\t\t\t}\n\t\t\tif(rank!=0)\n\t\t\t{\n\t\t\t\tMPI_Recv(&pre_u[0], 1, MPI_DOUBLE, rank-1, tag, MPI_COMM_WORLD, &status);\n\t\t\t}\n\t\t}\n\n\t}\n\n\t\n\n\tfor(i=0;i<uSize;i++)\n\t{\n\t\tint realI = rank*uSize + i;\n\t\tif(realI-1>=0)\n\t\t{\n\t\t\tresiduals[realI-1] -= u[i];\n\t\t}\n\t\tresiduals[realI] += 2*u[i];\n\t\tif(realI+1<N)\n\t\t{\n\t\t\tresiduals[realI+1] -= u[i];\n\t\t}\n\t}\n\t\n\n\tMPI_Reduce(residuals, residuals_reduced, N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\t\n\n\n\tif(rank==0)\n\t{\n\n\t\tfor(i=0;i<N;i++){\n\t\t\tresiduals_reduced[i] = residuals_reduced[i]/h2 - 1;\n\t\t\t\n\n\t\t}\n\t\tresidual = 0;\n\t\tfor(i=0;i<N;i++){\n\t\t\tresidual += (residuals_reduced[i])*(residuals_reduced[i]);\n\t\t}\n\t\tresidual = sqrt(residual);\n\t\t\n\n\t\tif(residual<=threshold){\n\t\t\tfinished = 1;\n\t\t}\n\t}\n\t\n\tif(rank==0){\n\t\tprintf(\"%ld iterations\\n\",k);\n\t\tprintf(\"Residual: %12.10f\\n\",residual);\n\t}\n\n\tMPI_Finalize();\n}"}
{"program": "bjoern-leder_153", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,ii,status,bc;\n   wloop_parms_t wl;\n   double phi[2],phi_prime[2];\n   double wt,delta,eps;\n   FILE *log=NULL,*fin=NULL;   \n  \n  \n   \n   if (my_rank==0)\n   {\n      log=freopen(\"check1.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Consistency checks on the wloop routine\\n\");\n      printf(\"---------------------------------------\\n\\n\");\n      \n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\\n\",L0,L1,L2,L3);\n      fflush(log);\n\n      bc=find_opt(argc,argv,\"-bc\");\n\n      if (bc!=0)\n         error_root(sscanf(argv[bc+1],\"%d\",&bc)!=1,1,\"main [check1.c]\",\n                    \"Syntax: check1 [-bc <type>]\");\n   }\n   \n\n   error_root((bc!=0)&&(bc!=3),1,\"main [check1.c]\",\n               \"Only open and periodic boundary conditions supported\");\n   \n   read_wloop_parms();\n   if(my_rank==0)\n      fclose(fin);\n   wl=wloop_parms();\n   \n   phi[0]=0.0;\n   phi[1]=0.0;\n   phi_prime[0]=0.0;\n   phi_prime[1]=0.0;\n   set_bc_parms(bc,1.0,1.0,1.0,1.0,phi,phi_prime);\n   print_bc_parms();\n\n   geometry();\n   \n   print_wloop_parms();\n\n   message(\"HYP-smearing of temporal links\\n\\n\");\n   \n   hyp_time_links(wl.alpha_action[0],wl.alpha_action[1],wl.alpha_action[2],wl.proj,wl.proj_iter,0);\n   wt=(MPI_Wtime()-wt)/60.0;\n  \n   message(\"double precision hyp_time_links successfully calculated in %10.4f min.\\n\\n\",wt);\n\n   wloop_sum(1);\n   wt=(MPI_Wtime()-wt)/60.0;\n\n   free_wloop();\n   \n   status=0;\n   eps=sqrt(3.0)*((double)(NPROC0*NPROC1*L0*L1/4))*DBL_EPSILON;\n   for(ii=0; ii<wl.wls; ++ii)\n   {   \n      if (wl.wl_mat[ii]>eps)\n      {\n         delta=fabs(1.0-1.0/wl.wl_mat[ii]);\n         if (delta>eps)\n         {\n            message(\"delta[%2d] = %.6e\\n\",ii,delta);\n            status+=1;\n         }\n      }\n   }\n   error(status>0,1,\"main [check1.c]\",\n         \"double precision wloops have not been properly calculated!!\");\n   \n   message(\"double precision wloops successfully calculated in %10.4f min.\\n\\n\",wt);\n   \n   if(my_rank==0)\n      fclose(log);\n  \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,ii,status,bc;\n   wloop_parms_t wl;\n   double phi[2],phi_prime[2];\n   double wt,delta,eps;\n   FILE *log=NULL,*fin=NULL;   \n  \n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n  \n   \n   if (my_rank==0)\n   {\n      log=freopen(\"check1.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Consistency checks on the wloop routine\\n\");\n      printf(\"---------------------------------------\\n\\n\");\n      \n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\\n\",L0,L1,L2,L3);\n      fflush(log);\n\n      bc=find_opt(argc,argv,\"-bc\");\n\n      if (bc!=0)\n         error_root(sscanf(argv[bc+1],\"%d\",&bc)!=1,1,\"main [check1.c]\",\n                    \"Syntax: check1 [-bc <type>]\");\n   }\n   \n   MPI_Bcast(&bc,1,MPI_INT,0,MPI_COMM_WORLD);\n\n   error_root((bc!=0)&&(bc!=3),1,\"main [check1.c]\",\n               \"Only open and periodic boundary conditions supported\");\n   \n   read_wloop_parms();\n   if(my_rank==0)\n      fclose(fin);\n   wl=wloop_parms();\n   \n   phi[0]=0.0;\n   phi[1]=0.0;\n   phi_prime[0]=0.0;\n   phi_prime[1]=0.0;\n   set_bc_parms(bc,1.0,1.0,1.0,1.0,phi,phi_prime);\n   print_bc_parms();\n\n   geometry();\n   \n   print_wloop_parms();\n\n   message(\"HYP-smearing of temporal links\\n\\n\");\n   \n   MPI_Barrier(MPI_COMM_WORLD);\n   wt=MPI_Wtime(); \n   hyp_time_links(wl.alpha_action[0],wl.alpha_action[1],wl.alpha_action[2],wl.proj,wl.proj_iter,0);\n   MPI_Barrier(MPI_COMM_WORLD);\n   wt=(MPI_Wtime()-wt)/60.0;\n  \n   message(\"double precision hyp_time_links successfully calculated in %10.4f min.\\n\\n\",wt);\n\n   MPI_Barrier(MPI_COMM_WORLD);\n   wt=MPI_Wtime(); \n   wloop_sum(1);\n   MPI_Barrier(MPI_COMM_WORLD);\n   wt=(MPI_Wtime()-wt)/60.0;\n\n   free_wloop();\n   \n   status=0;\n   eps=sqrt(3.0)*((double)(NPROC0*NPROC1*L0*L1/4))*DBL_EPSILON;\n   for(ii=0; ii<wl.wls; ++ii)\n   {   \n      if (wl.wl_mat[ii]>eps)\n      {\n         delta=fabs(1.0-1.0/wl.wl_mat[ii]);\n         if (delta>eps)\n         {\n            message(\"delta[%2d] = %.6e\\n\",ii,delta);\n            status+=1;\n         }\n      }\n   }\n   error(status>0,1,\"main [check1.c]\",\n         \"double precision wloops have not been properly calculated!!\");\n   \n   message(\"double precision wloops successfully calculated in %10.4f min.\\n\\n\",wt);\n   \n   if(my_rank==0)\n      fclose(log);\n  \n   MPI_Finalize(); \n   exit(0);\n}"}
{"program": "daidong_156", "code": "int main(int argc, char **argv)\n{\n   int fd;\n   int ret;\n   double stime, etime, elapsed, slowest;\n   struct stat64 statbuf;\n   int nprocs;\n   off64_t offset, orig_offset;\n   char* new_path;\n\n   \n   \n\n   parse_args(argc, argv);\n\n\n   \n\n   if(!opt_create)\n   {\n      fd = open(opt_file, O_RDWR);  \n      if(fd < 0)\n      {\n         perror(\"open\");\n         exit(1);\n      }\n   }\n   else\n   {\n      \n\n      if(rank == 0)\n      {\n         fd = open(opt_file, O_RDWR|O_CREAT|O_EXCL, S_IRUSR|S_IWUSR);\n         if(fd < 0)\n         {\n            perror(\"open\");\n            exit(1);\n         }\n      }\n      else\n      {\n         fd = open(opt_file, O_RDWR);  \n         if(fd < 0)\n         {\n            perror(\"open\");\n            exit(1);\n         }\n      }\n   }\n\n   stime =\n\n   ret = 0;\n   if(opt_fstat)\n      ret = fstat64(fd, &statbuf);\n   else if(opt_lseek)\n   {\n      \n\n      orig_offset = lseek64(fd, 0, SEEK_CUR);\n      if(orig_offset < 0)\n         ret = -1;\n      else\n      {\n         \n\n         offset = lseek64(fd, 0, SEEK_END);\n         if(offset < 0)\n            ret = -1;\n         else\n         {\n            \n\n            offset = lseek64(fd, orig_offset, SEEK_SET);\n            if(offset < 0)\n                ret = -1;\n         }\n      }\n   }\n   else if(opt_realpath)\n   {\n      new_path = realpath(opt_file, NULL);\n      if(!new_path)\n        ret = -1;\n      else\n        free(new_path);\n   }\n   else\n      ret = stat64(opt_file, &statbuf);\n\n   if(ret != 0)\n   {\n      perror(\"stat64 or fstat64\");\n      exit(1);\n   }\n   \n   etime =\n\n   elapsed = etime-stime;\n   ret =\n   if(ret != 0)\n   {\n      exit(1);\n   }\n\n\n   slowest *= 1000.0;\n\n   if(rank == 0)\n   {\n      printf(\"opt_file: %s, opt_create: %d, opt_fstat: %d, opt_lseek: %d, opt_realpath: %d, nprocs: %d, time: %f ms\\n\", opt_file, opt_create, opt_fstat, opt_lseek, opt_realpath, nprocs, slowest);\n   }\n\n   return(0);\n}", "label": "int main(int argc, char **argv)\n{\n   int fd;\n   int ret;\n   double stime, etime, elapsed, slowest;\n   struct stat64 statbuf;\n   int nprocs;\n   off64_t offset, orig_offset;\n   char* new_path;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   \n   \n\n   parse_args(argc, argv);\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   \n\n   if(!opt_create)\n   {\n      fd = open(opt_file, O_RDWR);  \n      if(fd < 0)\n      {\n         perror(\"open\");\n         exit(1);\n      }\n   }\n   else\n   {\n      \n\n      if(rank == 0)\n      {\n         fd = open(opt_file, O_RDWR|O_CREAT|O_EXCL, S_IRUSR|S_IWUSR);\n         if(fd < 0)\n         {\n            perror(\"open\");\n            exit(1);\n         }\n         MPI_Barrier(MPI_COMM_WORLD);\n      }\n      else\n      {\n         MPI_Barrier(MPI_COMM_WORLD);\n         fd = open(opt_file, O_RDWR);  \n         if(fd < 0)\n         {\n            perror(\"open\");\n            exit(1);\n         }\n      }\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n   stime = MPI_Wtime();\n\n   ret = 0;\n   if(opt_fstat)\n      ret = fstat64(fd, &statbuf);\n   else if(opt_lseek)\n   {\n      \n\n      orig_offset = lseek64(fd, 0, SEEK_CUR);\n      if(orig_offset < 0)\n         ret = -1;\n      else\n      {\n         \n\n         offset = lseek64(fd, 0, SEEK_END);\n         if(offset < 0)\n            ret = -1;\n         else\n         {\n            \n\n            offset = lseek64(fd, orig_offset, SEEK_SET);\n            if(offset < 0)\n                ret = -1;\n         }\n      }\n   }\n   else if(opt_realpath)\n   {\n      new_path = realpath(opt_file, NULL);\n      if(!new_path)\n        ret = -1;\n      else\n        free(new_path);\n   }\n   else\n      ret = stat64(opt_file, &statbuf);\n\n   if(ret != 0)\n   {\n      perror(\"stat64 or fstat64\");\n      exit(1);\n   }\n   \n   etime = MPI_Wtime();\n\n   elapsed = etime-stime;\n   ret = MPI_Reduce(&elapsed, &slowest, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n   if(ret != 0)\n   {\n      fprintf(stderr, \"Error: MPI_Reduce() failure.\\n\");\n      exit(1);\n   }\n\n\n   slowest *= 1000.0;\n\n   if(rank == 0)\n   {\n      printf(\"opt_file: %s, opt_create: %d, opt_fstat: %d, opt_lseek: %d, opt_realpath: %d, nprocs: %d, time: %f ms\\n\", opt_file, opt_create, opt_fstat, opt_lseek, opt_realpath, nprocs, slowest);\n   }\n\n   MPI_Finalize();\n   return(0);\n}"}
{"program": "NLeSC_158", "code": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Request request;\n    MPI_Status status;\n\n\n\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n       for (i=0;i<10;i++) {\n          buffer[i] = 42+i;\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"Send to %d\\n\", i);\n\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Send failed! %d\\n\", error);\n          }\n       }\n    } else {\n       fprintf(stderr, \"IRecv from to %d\\n\", 0);\n\n       error =\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Irecv failed! %d\\n\", error);\n       } else {\n\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Wait failed! %d\\n\", error);\n          } else {\n             fprintf(stderr, \"Received from 0: {\");\n\n             for (i=0;i<10;i++) {\n                fprintf(stderr, \"%d \", buffer[i]);\n             }\n\n             fprintf(stderr, \"}\\n\");\n          }\n       }\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Request request;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n       for (i=0;i<10;i++) {\n          buffer[i] = 42+i;\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"Send to %d\\n\", i);\n\n          error = MPI_Send(buffer, 10, MPI_INTEGER, i, 0, MPI_COMM_WORLD);\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Send failed! %d\\n\", error);\n          }\n       }\n    } else {\n       fprintf(stderr, \"IRecv from to %d\\n\", 0);\n\n       error = MPI_Irecv(buffer, 10, MPI_INTEGER, 0, 0, MPI_COMM_WORLD, &request);\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Irecv failed! %d\\n\", error);\n       } else {\n\n          error = MPI_Wait(&request, &status);\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Wait failed! %d\\n\", error);\n          } else {\n             fprintf(stderr, \"Received from 0: {\");\n\n             for (i=0;i<10;i++) {\n                fprintf(stderr, \"%d \", buffer[i]);\n             }\n\n             fprintf(stderr, \"}\\n\");\n          }\n       }\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "rpereira-dev_160", "code": "int main(int argc, char **argv)\n{\n    int n = atoi(argv[1]);\n    int rank, iter;\n    char *buf;\n    double tend, tbeg;\n\n\n\n    buf = (char*)malloc(n*sizeof(char));\n\n    if (rank == \n0)\n    {\n        printf(\"%d bytes\\n\", n);\n        fflush(stdout);\n        memset(buf, 0, n);\n    }\n\n    tbeg =\n\n    for(iter = 0 ; iter < NITER ; iter++)\n    {\n        mpi_bcast(buf, n);\n    }\n\n    tend =\n\n    if (rank == 0)\n    {\n        printf(\"Telaps for %d bcast = %.3e s\\n\", NITER, tend - tbeg);\n    }\n\n    free(buf);\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int n = atoi(argv[1]);\n    int rank, iter;\n    char *buf;\n    double tend, tbeg;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    buf = (char*)malloc(n*sizeof(char));\n\n    if (rank == \n0)\n    {\n        printf(\"%d bytes\\n\", n);\n        fflush(stdout);\n        memset(buf, 0, n);\n    }\n\n    tbeg = MPI_Wtime();\n\n    for(iter = 0 ; iter < NITER ; iter++)\n    {\n        mpi_bcast(buf, n);\n    }\n\n    tend = MPI_Wtime();\n\n    if (rank == 0)\n    {\n        printf(\"Telaps for %d bcast = %.3e s\\n\", NITER, tend - tbeg);\n    }\n\n    free(buf);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "90jrong_163", "code": "int\nmain(int argc, char **argv)\n{\n#ifdef TEST_PNETCDF\n#endif\n   printf(\"\\n*** Testing some extra stuff.\\n\");\n   printf(\"*** Trying to open non-netCDF files of tiny length...\");\n   {\n#define DATA_LEN 32\n     int ncid,openstat;\n      char dummy_data[DATA_LEN];\n      FILE *file;\n      int i;\n\n      \n\n      for (i = 0; i < DATA_LEN; i++)\n\t dummy_data[i] = i;\n\n      for (i = DATA_LEN; i >= 0; i--)\n      {\n\t \n\n\t if (!(file = fopen(FILE_NAME, \"w+\"))) ERR;\n\t if (fwrite(dummy_data, 1, i, file) != i) ERR;\n\t if (fclose(file)) ERR;\n\n\t \n\n#ifdef TEST_PNETCDF\n        openstat = nc_open_par(FILE_NAME, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid);\n#else\n         openstat = nc_open(FILE_NAME, 0, &ncid);\n#endif\n\t \n\n\t if(openstat != NC_ENOTNC && openstat != 2) ERR;\n\n      }\n   }\n\n   SUMMARIZE_ERR;\n#ifndef USE_NETCDF4   \n   printf(\"*** Trying to create netCDF-4 file without netCDF-4...\");\n   {\n       int ncid;\n       \n       if (nc_create(FILE_NAME, NC_NETCDF4, &ncid) != NC_ENOTBUILT)\n\t   ERR;\n   }\n   SUMMARIZE_ERR;\n#endif \n\n   \n#ifdef TEST_PNETCDF\n#endif\n   \n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char **argv)\n{\n#ifdef TEST_PNETCDF\n   MPI_Init(&argc, &argv);\n#endif\n   printf(\"\\n*** Testing some extra stuff.\\n\");\n   printf(\"*** Trying to open non-netCDF files of tiny length...\");\n   {\n#define DATA_LEN 32\n     int ncid,openstat;\n      char dummy_data[DATA_LEN];\n      FILE *file;\n      int i;\n\n      \n\n      for (i = 0; i < DATA_LEN; i++)\n\t dummy_data[i] = i;\n\n      for (i = DATA_LEN; i >= 0; i--)\n      {\n\t \n\n\t if (!(file = fopen(FILE_NAME, \"w+\"))) ERR;\n\t if (fwrite(dummy_data, 1, i, file) != i) ERR;\n\t if (fclose(file)) ERR;\n\n\t \n\n#ifdef TEST_PNETCDF\n        openstat = nc_open_par(FILE_NAME, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid);\n#else\n         openstat = nc_open(FILE_NAME, 0, &ncid);\n#endif\n\t \n\n\t if(openstat != NC_ENOTNC && openstat != 2) ERR;\n\n      }\n   }\n\n   SUMMARIZE_ERR;\n#ifndef USE_NETCDF4   \n   printf(\"*** Trying to create netCDF-4 file without netCDF-4...\");\n   {\n       int ncid;\n       \n       if (nc_create(FILE_NAME, NC_NETCDF4, &ncid) != NC_ENOTBUILT)\n\t   ERR;\n   }\n   SUMMARIZE_ERR;\n#endif \n\n   \n#ifdef TEST_PNETCDF\n   MPI_Finalize();\n#endif\n   \n   FINAL_RESULTS;\n}"}
{"program": "jeffhammond_165", "code": "int main(int argc, char* argv[])\n{\n\n    MPI_Comm newcomm;\n\n    int rank, size;\n\n    for (int i=0; i<100; i++) {\n    }\n\n    {\n        MPI_Request * reqs = malloc(2*size*sizeof(MPI_Request));\n        int * temp = malloc(size*sizeof(int));\n        for (int i=0; i<1000; i++) {\n            for (int j=0; j<size; j++) {\n            }\n        }\n        free(temp);\n        free(reqs);\n    }\n\n    for (int i=0; i<100; i++) {\n    }\n\n    {\n        MPI_Request * reqs = malloc(2*size*sizeof(MPI_Request));\n        int * temp = malloc(size*sizeof(int));\n        for (int i=0; i<1000; i++) {\n            for (int j=0; j<size; j++) {\n            }\n        }\n        free(temp);\n        free(reqs);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm newcomm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &newcomm);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i=0; i<100; i++) {\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n\n    {\n        MPI_Request * reqs = malloc(2*size*sizeof(MPI_Request));\n        int * temp = malloc(size*sizeof(int));\n        for (int i=0; i<1000; i++) {\n            for (int j=0; j<size; j++) {\n                MPI_Isend(&rank, 1, MPI_INT, j, 0, MPI_COMM_WORLD, &(reqs[j]));\n                MPI_Irecv(&(temp[j]), 1, MPI_INT, j, 0, MPI_COMM_WORLD, &(reqs[size+j]));\n            }\n            MPI_Waitall(2*size, reqs, MPI_STATUSES_IGNORE);\n        }\n        free(temp);\n        free(reqs);\n    }\n\n    for (int i=0; i<100; i++) {\n        MPI_Barrier(newcomm);\n    }\n\n    {\n        MPI_Request * reqs = malloc(2*size*sizeof(MPI_Request));\n        int * temp = malloc(size*sizeof(int));\n        for (int i=0; i<1000; i++) {\n            for (int j=0; j<size; j++) {\n                MPI_Isend(&rank, 1, MPI_INT, j, 0, newcomm, &(reqs[j]));\n                MPI_Irecv(&(temp[j]), 1, MPI_INT, j, 0, newcomm, &(reqs[size+j]));\n            }\n            MPI_Waitall(2*size, reqs, MPI_STATUSES_IGNORE);\n        }\n        free(temp);\n        free(reqs);\n    }\n\n    MPI_Comm_free(&newcomm);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "bmi-forum_166", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* entryPoint;\n\tint value = 5;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\t\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, \"myStream\" );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_VoidPtr_CastType );\n\tif( rank == procToWatch ) {\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\t}\n\tEntryPoint_Append( entryPoint, \"Test0\", (void*)Test0, \"testCode\" );\n\tif( rank == procToWatch ) {\n\t\tHook_Index hookIndex;\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\n\t\tfor (hookIndex = 0; hookIndex < entryPoint->hooks->count; hookIndex++ ) {\n\t\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->data[%d]->name: %s\\n\", hookIndex,\n\t\t\t\tentryPoint->hooks->data[hookIndex]->name  );\n\t\t}\n\t}\n\n\t\n\n\tif( rank == procToWatch ) {\n\t\t((EntryPoint_VoidPtr_CallCast*) entryPoint->run)( entryPoint, &value );\n\t}\n\n\t\n\n\tStg_Class_Delete( entryPoint );\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* entryPoint;\n\tint value = 5;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\t\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, \"myStream\" );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_VoidPtr_CastType );\n\tif( rank == procToWatch ) {\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\t}\n\tEntryPoint_Append( entryPoint, \"Test0\", (void*)Test0, \"testCode\" );\n\tif( rank == procToWatch ) {\n\t\tHook_Index hookIndex;\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\n\t\tfor (hookIndex = 0; hookIndex < entryPoint->hooks->count; hookIndex++ ) {\n\t\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->data[%d]->name: %s\\n\", hookIndex,\n\t\t\t\tentryPoint->hooks->data[hookIndex]->name  );\n\t\t}\n\t}\n\n\t\n\n\tif( rank == procToWatch ) {\n\t\t((EntryPoint_VoidPtr_CallCast*) entryPoint->run)( entryPoint, &value );\n\t}\n\n\t\n\n\tStg_Class_Delete( entryPoint );\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "ghisvail_168", "code": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  pfft_complex *in, *out;\n  pfft_plan plan=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 2; n[1] = 2; n[2] = 4;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d);\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan = pfft_plan_dft_3d(n, in, out, comm_cart_2d,\n      PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan);\n  \n  \n\n  pfft_destroy_plan(plan);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  pfft_complex *in, *out;\n  pfft_plan plan=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 2; n[1] = 2; n[2] = 4;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d);\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan = pfft_plan_dft_3d(n, in, out, comm_cart_2d,\n      PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan);\n  \n  \n\n  pfft_destroy_plan(plan);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "castrinho8_171", "code": "int\nmain(int argc, char **argv)\n{\n\n\tint numprocs, myrank, i, proc, *sendcounts, *displs, recvcount;\n\tint *subarray, *data, *num_el_heap;\n\theap_t h;\n\tdouble mytime;\n\tlong numberofitems = -1, indiceroot;\n\n\n\tif(argc != 2 && !myrank)\n\t{\n\t\tprintf(\"Usage: ./combsort <number of items>\\n\");\n\t\treturn -1;\n\t}\n\n\tnumberofitems = atol(argv[1]);\n\n\tif(!myrank){\n\n\t\tif(numberofitems <= 0){\n\t\t\tprintf(\"The number of items must be a positive number.\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\t\n\n\t\tgenerate_data(&data,numberofitems);\n\n\t\tmytime =\n\t}\n\n\n   \n\n   \n\n   \n\n    \n\n    if( !myrank ){\n\n      sendcounts = malloc(sizeof(int) * numprocs);\n      assert(sendcounts);\n\n      displs = malloc(sizeof(int) * numprocs);\n      assert(displs);\n\n    \tfor(i=0; i < numprocs; i++)\n    \t{\n    \t\tsendcounts[i] = CALC_IT_PER_PROC(numberofitems, numprocs, i);\n    \t\tdispls[i] = i ? displs[i-1] + sendcounts[i-1] : 0;\n    \t}\n    }\n\n    recvcount = CALC_IT_PER_PROC(numberofitems, numprocs, myrank);\n    subarray = malloc(sizeof(int) * recvcount);\n    assert(subarray);\n\n\n   \n\n   \n\n\tomp_sort(subarray,(long) recvcount);\n\n\t#ifdef CHECKCHILD\n\tprintf(\"MPI Check. Proceso %i : %s\\n\",myrank, check_results(subarray,\n\t\t(long) recvcount) ? \"Correct\" : \"Incorrect\");\n\t#endif\n\n   \n\n   \n\n   \n\n\t\n\n\tif(myrank != 0) \n\n\t{\n\t\tsend_data(subarray,recvcount);\n\t}\n\telse \n\n\t{\n\t\th = new_heap(numprocs * CHUNK);\n\t\tnum_el_heap = malloc(sizeof(long) * numprocs);\n\t\tindiceroot = 0;\n\n\t\t\n\n\t\tnum_el_heap[0] = insert_root(subarray,recvcount,&indiceroot, h);\n\n\t\t\n\n\t\tfor(proc=1; proc<numprocs; proc++)\n\t\t\tnum_el_heap[proc] = receive_and_insert(proc, h);\n\n\t\t\n\n\t\t\n\n\t\tfor(i = 0; i < numberofitems; i++)\n\t\t{\n\t\t\tint v;\n\t\t\tpop_heap(h,&v,&proc); \n\n\t\t\tdata[i] = v;\n\n\t\t\tif ( ! (--num_el_heap[proc]))\n\t\t\t{\n\t\t\t\tnum_el_heap[proc] = proc ? receive_and_insert(proc,h) : insert_root(subarray,recvcount,&indiceroot,h);\n\t\t\t}\n\t\t}\n\n\t\tdispose_heap(h);\n\t}\n\n\tif(!myrank){\n\t    mytime = MPI_Wtime() - mytime;  \n\n\t    mytime = mytime *\n\n\t\t#ifdef CHECK\n\t\t\tprintf(\"FINAL Check: %s\\n\", check_results(data,numberofitems) ? \"Correct\" : \"Incorrect\");\n\t\t#endif\n\n\t\t\n\n\t\tprint_results(data,numberofitems);\n\t\tprintf(\"numchunk = %i\\n\", CHUNK);\n\t\tprintf(\"numberofthreads = %i\\n\",omp_thread_count());\n\t\tprintf(\"numberofprocs = %i\\n\",numprocs );\n\t\tprintf (\"numberofitems = %ld\\n\", numberofitems);\n\t\tprintf (\"Tiempo        = %f (segs) \\n\", mytime);\n\t}\n\n   return 0;\n}\n", "label": "int\nmain(int argc, char **argv)\n{\n\n\tint numprocs, myrank, i, proc, *sendcounts, *displs, recvcount;\n\tint *subarray, *data, *num_el_heap;\n\theap_t h;\n\tdouble mytime;\n\tlong numberofitems = -1, indiceroot;\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n\tif(argc != 2 && !myrank)\n\t{\n\t\tprintf(\"Usage: ./combsort <number of items>\\n\");\n\t\treturn -1;\n\t}\n\n\tnumberofitems = atol(argv[1]);\n\n\tif(!myrank){\n\n\t\tif(numberofitems <= 0){\n\t\t\tprintf(\"The number of items must be a positive number.\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\t\n\n\t\tgenerate_data(&data,numberofitems);\n\n\t\tmytime = MPI_Wtime();\n\t}\n\n\n   \n\n   \n\n   \n\n    \n\n    if( !myrank ){\n\n      sendcounts = malloc(sizeof(int) * numprocs);\n      assert(sendcounts);\n\n      displs = malloc(sizeof(int) * numprocs);\n      assert(displs);\n\n    \tfor(i=0; i < numprocs; i++)\n    \t{\n    \t\tsendcounts[i] = CALC_IT_PER_PROC(numberofitems, numprocs, i);\n    \t\tdispls[i] = i ? displs[i-1] + sendcounts[i-1] : 0;\n    \t}\n    }\n\n    recvcount = CALC_IT_PER_PROC(numberofitems, numprocs, myrank);\n    subarray = malloc(sizeof(int) * recvcount);\n    assert(subarray);\n\n    MPI_Scatterv( \tdata,\n        \t\t\tsendcounts,\n                    displs,\n                    MPI_INT,\n                    subarray,\n                    recvcount,\n                    MPI_INT,\n                    0,\n                    MPI_COMM_WORLD);\n\n   \n\n   \n\n\tomp_sort(subarray,(long) recvcount);\n\n\t#ifdef CHECKCHILD\n\tprintf(\"MPI Check. Proceso %i : %s\\n\",myrank, check_results(subarray,\n\t\t(long) recvcount) ? \"Correct\" : \"Incorrect\");\n\t#endif\n\n   \n\n   \n\n   \n\n\t\n\n\tif(myrank != 0) \n\n\t{\n\t\tsend_data(subarray,recvcount);\n\t}\n\telse \n\n\t{\n\t\th = new_heap(numprocs * CHUNK);\n\t\tnum_el_heap = malloc(sizeof(long) * numprocs);\n\t\tindiceroot = 0;\n\n\t\t\n\n\t\tnum_el_heap[0] = insert_root(subarray,recvcount,&indiceroot, h);\n\n\t\t\n\n\t\tfor(proc=1; proc<numprocs; proc++)\n\t\t\tnum_el_heap[proc] = receive_and_insert(proc, h);\n\n\t\t\n\n\t\t\n\n\t\tfor(i = 0; i < numberofitems; i++)\n\t\t{\n\t\t\tint v;\n\t\t\tpop_heap(h,&v,&proc); \n\n\t\t\tdata[i] = v;\n\n\t\t\tif ( ! (--num_el_heap[proc]))\n\t\t\t{\n\t\t\t\tnum_el_heap[proc] = proc ? receive_and_insert(proc,h) : insert_root(subarray,recvcount,&indiceroot,h);\n\t\t\t}\n\t\t}\n\n\t\tdispose_heap(h);\n\t}\n\n\tif(!myrank){\n\t    mytime = MPI_Wtime() - mytime;  \n\n\t    mytime = mytime * MPI_Wtick();\n\n\t\t#ifdef CHECK\n\t\t\tprintf(\"FINAL Check: %s\\n\", check_results(data,numberofitems) ? \"Correct\" : \"Incorrect\");\n\t\t#endif\n\n\t\t\n\n\t\tprint_results(data,numberofitems);\n\t\tprintf(\"numchunk = %i\\n\", CHUNK);\n\t\tprintf(\"numberofthreads = %i\\n\",omp_thread_count());\n\t\tprintf(\"numberofprocs = %i\\n\",numprocs );\n\t\tprintf (\"numberofitems = %ld\\n\", numberofitems);\n\t\tprintf (\"Tiempo        = %f (segs) \\n\", mytime);\n\t}\n\n\tMPI_Finalize();\n   return 0;\n}\n"}
{"program": "lstorchi_172", "code": "int main (int argc, char ** argv) \n{\n  int size, rank, rlen, i;\n  char myname[MPI_MAX_PROCESSOR_NAME]; \n  double time, ctime, pi, * points;\n\n  int n = -1, circle_count, myn, t_circle_count;\n\n\n  fprintf (stdout, \"I am proces %3d of %3d my name is %s (%s)\\n\", \n    rank, size, myname, argv[0]);\n  fflush(stdout);\n\n  if (argc == 2)\n    n = atoi(argv[1]);\n\n  if (n < 0)\n    n = 100;\n\n  if (size > 1) \n  {\n    ctime =\n    myn = n / size;\n    \n    points = (double *) malloc (sizeof (double) * myn * 2);\n    assert(points != NULL);\n    \n    if (rank == 0) \n    {\n      int torank;\n\n      srand (10);\n      for (torank=1; torank<size; torank++) \n      {\n        for (i=0; i<myn; i++) \n        {\n          points[2*i] = (double) rand() / ((double)RAND_MAX + 1.0e0);\n          points[(2*i)+1] = (double) rand() / ((double)RAND_MAX + 1.0e0);\n        }\n        \n        \n\n      }\n    \n      myn = myn + (n - (size * myn)); \n      circle_count = 0;  \n      for (i=0; i<myn; i++) \n      {\n        double x, y; \n      \n        x = (double) rand() / ((double)RAND_MAX + 1.0e0);\n        y = (double) rand() / ((double)RAND_MAX + 1.0e0);\n      \n        if ((pow(x, 2.0) + pow(y, 2.0)) < 1.0)\n          (circle_count)++;\n      }\n    }\n    else \n    {\n      MPI_Status status;\n    \n    \n      circle_count = 0;\n      for (i=0; i<myn; i++) \n        if ((pow(points[2*i], 2.0) + pow(points[(2*i)+1], 2.0)) < 1.0)\n          circle_count++;\n    }\n    \n    free (points);\n    \n    ctime = MPI_Wtime () - ctime;\n\n    if (rank == 0) \n    {\n      pi = 0.0e0;\n      pi = (double) t_circle_count / (double) n;\n      pi *= 4.0e0;\n      fprintf (stdout, \"Parallel PI = %.23f  %f s\\n\", pi, ctime);\n    }\n  }\n\n  if (rank == 0) \n  {\n    time = (double) clock();\n    srand(10);\n    pi = 0.0e0;\n    circle_count = 0;\n    sequential (n, &circle_count);\n    pi = (double) circle_count / (double) n;\n    pi *= 4.0e0;\n    time = (double) clock() - time;\n    \n    time /= CLOCKS_PER_SEC;\n\n    fprintf (stdout, \"Sequential PI = %.23f %f s\\n\", pi, time);\n  }\n\n\n  return EXIT_SUCCESS;\n}", "label": "int main (int argc, char ** argv) \n{\n  int size, rank, rlen, i;\n  char myname[MPI_MAX_PROCESSOR_NAME]; \n  double time, ctime, pi, * points;\n\n  int n = -1, circle_count, myn, t_circle_count;\n\n  MPI_Init (&argc, &argv);\n  MPI_Comm_size (MPI_COMM_WORLD, &size);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  MPI_Get_processor_name (myname, &rlen);\n\n  fprintf (stdout, \"I am proces %3d of %3d my name is %s (%s)\\n\", \n    rank, size, myname, argv[0]);\n  fflush(stdout);\n\n  if (argc == 2)\n    n = atoi(argv[1]);\n\n  if (n < 0)\n    n = 100;\n\n  if (size > 1) \n  {\n    MPI_Barrier (MPI_COMM_WORLD);\n    ctime = MPI_Wtime ();\n    myn = n / size;\n    \n    points = (double *) malloc (sizeof (double) * myn * 2);\n    assert(points != NULL);\n    \n    if (rank == 0) \n    {\n      int torank;\n\n      srand (10);\n      for (torank=1; torank<size; torank++) \n      {\n        for (i=0; i<myn; i++) \n        {\n          points[2*i] = (double) rand() / ((double)RAND_MAX + 1.0e0);\n          points[(2*i)+1] = (double) rand() / ((double)RAND_MAX + 1.0e0);\n        }\n        \n        \n\n        MPI_Send (points, myn*2, MPI_DOUBLE, torank, torank, MPI_COMM_WORLD);\n      }\n    \n      myn = myn + (n - (size * myn)); \n      circle_count = 0;  \n      for (i=0; i<myn; i++) \n      {\n        double x, y; \n      \n        x = (double) rand() / ((double)RAND_MAX + 1.0e0);\n        y = (double) rand() / ((double)RAND_MAX + 1.0e0);\n      \n        if ((pow(x, 2.0) + pow(y, 2.0)) < 1.0)\n          (circle_count)++;\n      }\n    }\n    else \n    {\n      MPI_Status status;\n    \n      MPI_Recv (points, myn*2, MPI_DOUBLE, 0, rank, MPI_COMM_WORLD, &status);\n    \n      circle_count = 0;\n      for (i=0; i<myn; i++) \n        if ((pow(points[2*i], 2.0) + pow(points[(2*i)+1], 2.0)) < 1.0)\n          circle_count++;\n    }\n    \n    free (points);\n    \n    MPI_Reduce (&circle_count, &t_circle_count, 1, MPI_INT, MPI_SUM, 0, \n        MPI_COMM_WORLD);\n    ctime = MPI_Wtime () - ctime;\n\n    if (rank == 0) \n    {\n      pi = 0.0e0;\n      pi = (double) t_circle_count / (double) n;\n      pi *= 4.0e0;\n      fprintf (stdout, \"Parallel PI = %.23f  %f s\\n\", pi, ctime);\n    }\n  }\n\n  if (rank == 0) \n  {\n    time = (double) clock();\n    srand(10);\n    pi = 0.0e0;\n    circle_count = 0;\n    sequential (n, &circle_count);\n    pi = (double) circle_count / (double) n;\n    pi *= 4.0e0;\n    time = (double) clock() - time;\n    \n    time /= CLOCKS_PER_SEC;\n\n    fprintf (stdout, \"Sequential PI = %.23f %f s\\n\", pi, time);\n  }\n\n  MPI_Finalize ();\n\n  return EXIT_SUCCESS;\n}"}
{"program": "annehutter_174", "code": "int main (int argc, \n char * argv[]) { \n    int size = 1;\n    int myRank = 0;\n\n    char iniFile[MAXLENGTH];\n    confObj_t simParam;\n    \n    double *redshift_list = NULL;\n    \n    grid_t *grid = NULL;\n    \n    sourcelist_t *sourcelist = NULL;\n    \n    integral_table_t *integralTable = NULL;\n    \n    photIonlist_t *photIonBgList = NULL;\n    \n    double t1, t2;\n    \n    int num_cycles = 0;\n    int restart = 0;\n        \n#ifdef __MPI\n    \n    t1 =\n    \n    fftw_mpi_init();\n#else\n    t1 = time(NULL);\n#endif\n    \n    \n\n    if (argc != 2 && argc != 3) {\n        printf(\"cifog: (C)  - Use at own risk...\\n\");\n        printf(\"USAGE:\\n\");\n        printf(\"cifog iniFile\\n\");\n        \n        printf(\"argc = %d\\n\", argc);\n        \n        exit(EXIT_FAILURE);\n    } else {\n        if(argc == 2)\n        {\n            strcpy(iniFile, argv[1]);\n        }else{\n            if(strcmp(argv[1], \"-c\") == 0)\n            {\n                restart = 1;\n            }\n            strcpy(iniFile, argv[2]);\n        }\n    }\n\n    cifog_init(iniFile, &simParam, &redshift_list, &grid, &integralTable, &photIonBgList, &num_cycles, restart, myRank);\n    \n    cifog(simParam, redshift_list, grid, sourcelist, integralTable, photIonBgList, num_cycles, myRank, size);\n    \n    cifog_deallocate(simParam, redshift_list, grid, integralTable, photIonBgList, myRank);\n\n#ifdef __MPI\n#endif\n    \n    if(myRank==0) printf(\"\\n********\\nFINISHED\\n********\\n\");\n    \n#ifdef __MPI\n    fftw_mpi_cleanup();\n        \n    t2 =\n    if(myRank == 0) printf(\"\\nExecution took %f s\\n\", t2-t1);\n#else\n    fftw_cleanup();\n    \n    t2 = time(NULL);\n    if(myRank == 0) printf(\"\\nExecution took %f s\\n\", t2-t1);\n#endif\n    \n    return 0;\n}", "label": "int main (int argc, \n char * argv[]) { \n    int size = 1;\n    int myRank = 0;\n\n    char iniFile[MAXLENGTH];\n    confObj_t simParam;\n    \n    double *redshift_list = NULL;\n    \n    grid_t *grid = NULL;\n    \n    sourcelist_t *sourcelist = NULL;\n    \n    integral_table_t *integralTable = NULL;\n    \n    photIonlist_t *photIonBgList = NULL;\n    \n    double t1, t2;\n    \n    int num_cycles = 0;\n    int restart = 0;\n        \n#ifdef __MPI\n    MPI_Init(&argc, &argv); \n    MPI_Comm_size(MPI_COMM_WORLD, &size); \n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank); \n    \n    t1 = MPI_Wtime();\n    \n    fftw_mpi_init();\n#else\n    t1 = time(NULL);\n#endif\n    \n    \n\n    if (argc != 2 && argc != 3) {\n        printf(\"cifog: (C)  - Use at own risk...\\n\");\n        printf(\"USAGE:\\n\");\n        printf(\"cifog iniFile\\n\");\n        \n        printf(\"argc = %d\\n\", argc);\n        \n        exit(EXIT_FAILURE);\n    } else {\n        if(argc == 2)\n        {\n            strcpy(iniFile, argv[1]);\n        }else{\n            if(strcmp(argv[1], \"-c\") == 0)\n            {\n                restart = 1;\n            }\n            strcpy(iniFile, argv[2]);\n        }\n    }\n\n    cifog_init(iniFile, &simParam, &redshift_list, &grid, &integralTable, &photIonBgList, &num_cycles, restart, myRank);\n    \n    cifog(simParam, redshift_list, grid, sourcelist, integralTable, photIonBgList, num_cycles, myRank, size);\n    \n    cifog_deallocate(simParam, redshift_list, grid, integralTable, photIonBgList, myRank);\n\n#ifdef __MPI\n    MPI_Barrier(MPI_COMM_WORLD);\n#endif\n    \n    if(myRank==0) printf(\"\\n********\\nFINISHED\\n********\\n\");\n    \n#ifdef __MPI\n    fftw_mpi_cleanup();\n        \n    t2 = MPI_Wtime();\n    if(myRank == 0) printf(\"\\nExecution took %f s\\n\", t2-t1);\n    MPI_Finalize();\n#else\n    fftw_cleanup();\n    \n    t2 = time(NULL);\n    if(myRank == 0) printf(\"\\nExecution took %f s\\n\", t2-t1);\n#endif\n    \n    return 0;\n}"}
{"program": "kdusling_175", "code": "int main(int argc, char **argv)\n{\n\n   \n\n   \n\n\n   if (np < 3)\n   {\n    printf(\"Error: np must be at least 3.\\n\");\n    return 0;\n   }\n\n   \n\n   if (myid == 0) { master_main(argc, argv); }\n  \n   \n\n   if (myid != 0) { slave_main(argc, argv); }\n\n   return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n   \n\n   MPI_Init(NULL, NULL);\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n   if (np < 3)\n   {\n    printf(\"Error: np must be at least 3.\\n\");\n    return 0;\n   }\n\n   \n\n   if (myid == 0) { master_main(argc, argv); }\n  \n   \n\n   if (myid != 0) { slave_main(argc, argv); }\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "tenstream_177", "code": "int main(int argc, char *argv[]) {\n  int        numprocs, myid, fcomm;\n\n  int    Nx=3, Ny=3, Nz=2;\n  double dx=500,dy=500;\n  double phi0=0, theta0=60;\n  double albedo_th=.005, albedo_sol=0.2;\n  char   atm_filename[] = \"afglus.dat\";\n  int    lsolar=1, lthermal=1;\n\n  int    Nz_merged;\n  double *edir, *edn, *eup, *abso;  \n\n\n  double *d_tlev = malloc(Nx*Ny*(Nz+1) * sizeof(double));\n  double *d_plev = malloc(Nx*Ny*(Nz+1) * sizeof(double));\n\n  double *d_lwc   = malloc(Nx*Ny*Nz * sizeof(double));\n  double *d_reliq = malloc(Nx*Ny*Nz * sizeof(double));\n  double *d_iwc   = malloc(Nx*Ny*Nz * sizeof(double));\n  double *d_reice = malloc(Nx*Ny*Nz * sizeof(double));\n\n  PetscInitialize(&argc,&argv,(char*)0,help);\n  PetscInitializeFortran();\n\n  fcomm =\n\n  \n\n  int nprocx = 1;\n  int *nxproc = malloc(sizeof(int));\n  nxproc[0] = Nx;\n\n  int nprocy = numprocs;\n  int *nyproc = malloc(nprocy*sizeof(int));\n  for(int j=0; j<nprocy; j++) {\n    nyproc[j] = Ny;\n  }\n\n\n  \n\n  for (int k=0; k<Nz+1; k++) {\n    for (int i=0; i<Nx; i++) {\n      for (int j=0; j<Ny; j++) {\n        int ind = j*Nx*(Nz+1) + i*(Nz+1) + k;     \n\n        d_plev[ind] = 1013 - k * 200/(Nz+1);  \n\n        d_tlev[ind] = 288 - k * 10/(Nz+1);    \n\n      }\n    }\n  }\n\n  \n\n  for (int k=0; k<Nz; k++) {\n    for (int i=0; i<Nx; i++) {\n      for (int j=0; j<Ny; j++) {\n        int ind = j*Nx*Nz + i*Nz + k;     \n\n        d_lwc  [ind] = .1;\n        d_reliq[ind] = 10;\n        d_iwc  [ind] =  0;\n        d_reice[ind] = 10;\n      }\n    }\n  }\n\n  \n\n  f2c_pprts_rrtmg(fcomm, &Nz, &Nx, &Ny, &dx, &dy, &phi0, &theta0,\n      &albedo_th, &albedo_sol, atm_filename, &lthermal, &lsolar,\n      &Nz_merged, &edir, &edn, &eup, &abso, d_plev, d_tlev,\n      d_lwc, d_reliq, d_iwc, d_reice, &nprocx, nxproc, &nprocy, nyproc);\n\n  \n\n  if(edir) {\n      fprintf(stdout, \" rank :: level :: edir edn eup abso\\n\");\n  } else {\n      fprintf(stdout, \" rank :: level :: edn eup abso\\n\");\n  }\n  \n\n  \n\n  for (int k=0; k<Nz_merged; k++) {\n      \n\n      \n\n      if(edir) {\n          fprintf(stdout, \"%d :: %d :: %f %f %f %f\\n\", myid, k, edir[k], edn[k], eup[k], abso[k]);\n      } else {\n          fprintf(stdout, \"%d :: %d :: %f %f %f\\n\", myid, k, edn[k], eup[k], abso[k]);\n      }\n  }\n  int k = Nz_merged;\n  if(edir) {\n      fprintf(stdout, \"%d :: %d :: %f %f %f %f\\n\", myid, k, edir[k], edn[k], eup[k], abso[k]);\n  } else {\n      fprintf(stdout, \"%d :: %d :: %f %f %f\\n\", myid, k, edn[k], eup[k], abso[k]);\n  }\n  \n\n  \n\n\n  int lfinalizepetsc = 0; \n\n  f2c_destroy_pprts_rrtmg(&lfinalizepetsc); \n\n\n  PetscFinalize(); \n\n  return(0);\n\n}", "label": "int main(int argc, char *argv[]) {\n  int        numprocs, myid, fcomm;\n\n  int    Nx=3, Ny=3, Nz=2;\n  double dx=500,dy=500;\n  double phi0=0, theta0=60;\n  double albedo_th=.005, albedo_sol=0.2;\n  char   atm_filename[] = \"afglus.dat\";\n  int    lsolar=1, lthermal=1;\n\n  int    Nz_merged;\n  double *edir, *edn, *eup, *abso;  \n\n\n  double *d_tlev = malloc(Nx*Ny*(Nz+1) * sizeof(double));\n  double *d_plev = malloc(Nx*Ny*(Nz+1) * sizeof(double));\n\n  double *d_lwc   = malloc(Nx*Ny*Nz * sizeof(double));\n  double *d_reliq = malloc(Nx*Ny*Nz * sizeof(double));\n  double *d_iwc   = malloc(Nx*Ny*Nz * sizeof(double));\n  double *d_reice = malloc(Nx*Ny*Nz * sizeof(double));\n\n  MPI_Init(&argc,&argv);\n  PetscInitialize(&argc,&argv,(char*)0,help);\n  PetscInitializeFortran();\n\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n  fcomm = MPI_Comm_c2f(MPI_COMM_WORLD);\n\n  \n\n  int nprocx = 1;\n  int *nxproc = malloc(sizeof(int));\n  nxproc[0] = Nx;\n\n  int nprocy = numprocs;\n  int *nyproc = malloc(nprocy*sizeof(int));\n  for(int j=0; j<nprocy; j++) {\n    nyproc[j] = Ny;\n  }\n\n\n  \n\n  for (int k=0; k<Nz+1; k++) {\n    for (int i=0; i<Nx; i++) {\n      for (int j=0; j<Ny; j++) {\n        int ind = j*Nx*(Nz+1) + i*(Nz+1) + k;     \n\n        d_plev[ind] = 1013 - k * 200/(Nz+1);  \n\n        d_tlev[ind] = 288 - k * 10/(Nz+1);    \n\n      }\n    }\n  }\n\n  \n\n  for (int k=0; k<Nz; k++) {\n    for (int i=0; i<Nx; i++) {\n      for (int j=0; j<Ny; j++) {\n        int ind = j*Nx*Nz + i*Nz + k;     \n\n        d_lwc  [ind] = .1;\n        d_reliq[ind] = 10;\n        d_iwc  [ind] =  0;\n        d_reice[ind] = 10;\n      }\n    }\n  }\n\n  \n\n  f2c_pprts_rrtmg(fcomm, &Nz, &Nx, &Ny, &dx, &dy, &phi0, &theta0,\n      &albedo_th, &albedo_sol, atm_filename, &lthermal, &lsolar,\n      &Nz_merged, &edir, &edn, &eup, &abso, d_plev, d_tlev,\n      d_lwc, d_reliq, d_iwc, d_reice, &nprocx, nxproc, &nprocy, nyproc);\n\n  \n\n  if(edir) {\n      fprintf(stdout, \" rank :: level :: edir edn eup abso\\n\");\n  } else {\n      fprintf(stdout, \" rank :: level :: edn eup abso\\n\");\n  }\n  \n\n  \n\n  for (int k=0; k<Nz_merged; k++) {\n      \n\n      \n\n      if(edir) {\n          fprintf(stdout, \"%d :: %d :: %f %f %f %f\\n\", myid, k, edir[k], edn[k], eup[k], abso[k]);\n      } else {\n          fprintf(stdout, \"%d :: %d :: %f %f %f\\n\", myid, k, edn[k], eup[k], abso[k]);\n      }\n  }\n  int k = Nz_merged;\n  if(edir) {\n      fprintf(stdout, \"%d :: %d :: %f %f %f %f\\n\", myid, k, edir[k], edn[k], eup[k], abso[k]);\n  } else {\n      fprintf(stdout, \"%d :: %d :: %f %f %f\\n\", myid, k, edn[k], eup[k], abso[k]);\n  }\n  \n\n  \n\n\n  int lfinalizepetsc = 0; \n\n  f2c_destroy_pprts_rrtmg(&lfinalizepetsc); \n\n\n  PetscFinalize(); \n\n  MPI_Finalize();\n  return(0);\n\n}"}
{"program": "qingu_182", "code": "int main(int argc, char **argv)\n{\n    int rank, size, i;\n    MPI_Group full_group, half_group;\n    int range[1][3];\n    MPI_Comm comm;\n\n\n    range[0][0] = 0;\n    range[0][1] = size / 2;\n    range[0][2] = 1;\n\n#if !defined(USE_STRICT_MPI) && defined(MPICH)\n    if (rank <= size / 2) {\n    }\n#endif \n\n\n\n    if (rank == 0)\n        printf(\" No Errors\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, i;\n    MPI_Group full_group, half_group;\n    int range[1][3];\n    MPI_Comm comm;\n\n    MPI_Init(NULL, NULL);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    MPI_Comm_group(MPI_COMM_WORLD, &full_group);\n    range[0][0] = 0;\n    range[0][1] = size / 2;\n    range[0][2] = 1;\n    MPI_Group_range_incl(full_group, 1, range, &half_group);\n\n#if !defined(USE_STRICT_MPI) && defined(MPICH)\n    if (rank <= size / 2) {\n        MPI_Comm_create_group(MPI_COMM_WORLD, half_group, 0, &comm);\n        MPI_Barrier(comm);\n        MPI_Comm_free(&comm);\n    }\n#endif \n\n\n    MPI_Group_free(&half_group);\n    MPI_Group_free(&full_group);\n\n    if (rank == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "akohlmey_183", "code": "int main(int narg, char **arg)\n{\n  \n\n\n\n  if (narg != 3) {\n    printf(\"Syntax: simpleC P in.lammps\\n\");\n    exit(1);\n  }\n\n  int me,nprocs;\n\n  int nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n  }\n\n  int lammps;\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm comm_lammps;\n\n  \n\n\n  FILE *fp;\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n    }\n  }\n\n  \n\n\n  void *lmp = NULL;\n  if (lammps == 1) lmp = lammps_open(0,NULL,comm_lammps,NULL);\n\n  int n;\n  char line[1024];\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    if (n == 0) break;\n    if (lammps == 1) lammps_command(lmp,line);\n  }\n\n  \n\n\n  double *x = NULL;\n  double *v = NULL;\n\n  if (lammps == 1) {\n    lammps_command(lmp,\"run 10\");\n\n    int natoms = lammps_get_natoms(lmp);\n    x = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(lmp,\"x\",1,3,x);\n    v = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(lmp,\"v\",1,3,v);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    lammps_scatter_atoms(lmp,\"x\",1,3,x);\n\n    lammps_command(lmp,\"run 1\");\n  }\n\n  \n\n\n  if (lammps == 1) {\n    double **f = (double **) lammps_extract_atom(lmp,\"f\");\n    printf(\"Force on 1 atom via extract_atom: %g\\n\",f[0][0]);\n\n    double *fx = (double *) lammps_extract_variable(lmp,\"fx\",\"all\");\n    printf(\"Force on 1 atom via extract_variable: %g\\n\",fx[0]);\n  }\n\n  \n\n\n  const char *strtwo = \"run 10\\nrun 20\";\n  if (lammps == 1) lammps_commands_string(lmp,strtwo);\n\n  const char *cmds[2];\n  cmds[0] = \"run 10\";\n  cmds[1] = \"run 20\";\n  if (lammps == 1) lammps_commands_list(lmp,2,cmds);\n\n  \n\n\n  int *type = NULL;\n\n  if (lammps == 1) {\n    int natoms = lammps_get_natoms(lmp);\n    type = (int *) malloc(natoms*sizeof(double));\n    int i;\n    for (i = 0; i < natoms; i++) type[i] = 1;\n\n    lammps_command(lmp,\"delete_atoms group all\");\n    lammps_create_atoms(lmp,natoms,NULL,type,x,v,NULL,0);\n    lammps_command(lmp,\"run 10\");\n  }\n\n  if (x) free(x);\n  if (v) free(v);\n  if (type) free(type);\n\n  \n\n\n  lammps_close(lmp);\n\n  \n\n\n  if (lammps == 1)\n}", "label": "int main(int narg, char **arg)\n{\n  \n\n\n  MPI_Init(&narg,&arg);\n\n  if (narg != 3) {\n    printf(\"Syntax: simpleC P in.lammps\\n\");\n    exit(1);\n  }\n\n  int me,nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD,&me);\n  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n\n  int nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  int lammps;\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm comm_lammps;\n  MPI_Comm_split(MPI_COMM_WORLD,lammps,0,&comm_lammps);\n\n  \n\n\n  FILE *fp;\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n      MPI_Abort(MPI_COMM_WORLD,1);\n    }\n  }\n\n  \n\n\n  void *lmp = NULL;\n  if (lammps == 1) lmp = lammps_open(0,NULL,comm_lammps,NULL);\n\n  int n;\n  char line[1024];\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);\n    if (n == 0) break;\n    MPI_Bcast(line,n,MPI_CHAR,0,MPI_COMM_WORLD);\n    if (lammps == 1) lammps_command(lmp,line);\n  }\n\n  \n\n\n  double *x = NULL;\n  double *v = NULL;\n\n  if (lammps == 1) {\n    lammps_command(lmp,\"run 10\");\n\n    int natoms = lammps_get_natoms(lmp);\n    x = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(lmp,\"x\",1,3,x);\n    v = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(lmp,\"v\",1,3,v);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    lammps_scatter_atoms(lmp,\"x\",1,3,x);\n\n    lammps_command(lmp,\"run 1\");\n  }\n\n  \n\n\n  if (lammps == 1) {\n    double **f = (double **) lammps_extract_atom(lmp,\"f\");\n    printf(\"Force on 1 atom via extract_atom: %g\\n\",f[0][0]);\n\n    double *fx = (double *) lammps_extract_variable(lmp,\"fx\",\"all\");\n    printf(\"Force on 1 atom via extract_variable: %g\\n\",fx[0]);\n  }\n\n  \n\n\n  const char *strtwo = \"run 10\\nrun 20\";\n  if (lammps == 1) lammps_commands_string(lmp,strtwo);\n\n  const char *cmds[2];\n  cmds[0] = \"run 10\";\n  cmds[1] = \"run 20\";\n  if (lammps == 1) lammps_commands_list(lmp,2,cmds);\n\n  \n\n\n  int *type = NULL;\n\n  if (lammps == 1) {\n    int natoms = lammps_get_natoms(lmp);\n    type = (int *) malloc(natoms*sizeof(double));\n    int i;\n    for (i = 0; i < natoms; i++) type[i] = 1;\n\n    lammps_command(lmp,\"delete_atoms group all\");\n    lammps_create_atoms(lmp,natoms,NULL,type,x,v,NULL,0);\n    lammps_command(lmp,\"run 10\");\n  }\n\n  if (x) free(x);\n  if (v) free(v);\n  if (type) free(type);\n\n  \n\n\n  lammps_close(lmp);\n\n  \n\n\n  if (lammps == 1) MPI_Comm_free(&comm_lammps);\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}"}
{"program": "xl483_184", "code": "int main(int argc, char** argv)\n{\n    int n    = 200;            \n\n    double p = 0.05;           \n\n    const char* ifname = NULL; \n\n    const char* ofname = NULL; \n\n\n    \n\n    extern char* optarg;\n    const char* optstring = \"hn:d:p:o:i:\";\n    int c;\n    while ((c = getopt(argc, argv, optstring)) != -1) {\n        switch (c) {\n        case 'h':\n            fprintf(stderr, \"%s\", usage);\n            return -1;\n        case 'n': n = atoi(optarg); break;\n        case 'p': p = atof(optarg); break;\n        case 'o': ofname = optarg;  break;\n        case 'i': ifname = optarg;  break;\n        }\n    }\n\n    int t, id;\n\n    if (t > n) {\n        fprintf(stderr, \"Number of workers exceeds dimension of matrix!\");\n        return 1;\n    }\n\n    c = (int) ceil(((double) n) / ((double) t));\n    int m = t * c;\n\n    if (id == 0) {\n        \n\n        int* restrict l = gen_graph(n, p);\n        if (ifname)\n            write_matrix(ifname,  n, l);\n\n        double time =\n        shortest_paths(n, m, c, t, l); \n        time -=\n\n        printf(\"%d,%d,%g,%g,%X\\n\", t, n, -time, p, fletcher16(l, n*n));\n\n        \n\n        if (ofname)\n            write_matrix(ofname, n, l);\n\n        \n\n        _mm_free(l);\n    } else {\n        int* restrict graph = (int*) _mm_malloc(m * m * sizeof(int), sizeof(long));\n        worker(m, c, t, id, graph);\n        _mm_free(graph);\n    }\n    \n\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n    int n    = 200;            \n\n    double p = 0.05;           \n\n    const char* ifname = NULL; \n\n    const char* ofname = NULL; \n\n\n    \n\n    extern char* optarg;\n    const char* optstring = \"hn:d:p:o:i:\";\n    int c;\n    while ((c = getopt(argc, argv, optstring)) != -1) {\n        switch (c) {\n        case 'h':\n            fprintf(stderr, \"%s\", usage);\n            return -1;\n        case 'n': n = atoi(optarg); break;\n        case 'p': p = atof(optarg); break;\n        case 'o': ofname = optarg;  break;\n        case 'i': ifname = optarg;  break;\n        }\n    }\n\n    int t, id;\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&t);\n    MPI_Comm_rank(MPI_COMM_WORLD,&id);\n\n    if (t > n) {\n        fprintf(stderr, \"Number of workers exceeds dimension of matrix!\");\n        return 1;\n    }\n\n    c = (int) ceil(((double) n) / ((double) t));\n    int m = t * c;\n\n    if (id == 0) {\n        \n\n        int* restrict l = gen_graph(n, p);\n        if (ifname)\n            write_matrix(ifname,  n, l);\n\n        double time = MPI_Wtime();\n        shortest_paths(n, m, c, t, l); \n        time -= MPI_Wtime();\n\n        printf(\"%d,%d,%g,%g,%X\\n\", t, n, -time, p, fletcher16(l, n*n));\n\n        \n\n        if (ofname)\n            write_matrix(ofname, n, l);\n\n        \n\n        _mm_free(l);\n    } else {\n        int* restrict graph = (int*) _mm_malloc(m * m * sizeof(int), sizeof(long));\n        worker(m, c, t, id, graph);\n        _mm_free(graph);\n    }\n    \n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "qingu_190", "code": "int main(int argc, char **argv)\n{\n    int errs = 0;\n    int rank;\n    MPI_Datatype t;\n    int count = 4;\n    int blocklength = 2;\n    MPI_Aint displacements[] = {0, 8, 16, 24};\n\n\n    if (!rank) {\n        {\n            int ni, na, nd, combiner;\n            int i[1024];\n            MPI_Aint a[1024];\n            MPI_Datatype d[1024];\n            int k;\n\n            check(ni == 2);\n            check(i[0] == 4);\n            check(i[1] == 2);\n\n            check(na == 4);\n            for (k=0; k < na; k++)\n                check(a[k] == (k * 8));\n\n            check(nd == 1);\n            check(d[0] == MPI_INT);\n        }\n\n    }\n\n    if (rank == 0) {\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int errs = 0;\n    int rank;\n    MPI_Datatype t;\n    int count = 4;\n    int blocklength = 2;\n    MPI_Aint displacements[] = {0, 8, 16, 24};\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (!rank) {\n        MPI_Type_create_hindexed_block(count, blocklength,\n                                        displacements, MPI_INT,\n                                        &t);\n        MPI_Type_commit(&t);\n        {\n            int ni, na, nd, combiner;\n            int i[1024];\n            MPI_Aint a[1024];\n            MPI_Datatype d[1024];\n            int k;\n            MPI_Type_get_envelope(t, &ni, &na, &nd, &combiner);\n            MPI_Type_get_contents(t, ni, na, nd, i, a, d);\n\n            check(ni == 2);\n            check(i[0] == 4);\n            check(i[1] == 2);\n\n            check(na == 4);\n            for (k=0; k < na; k++)\n                check(a[k] == (k * 8));\n\n            check(nd == 1);\n            check(d[0] == MPI_INT);\n        }\n\n        MPI_Type_free(&t);\n    }\n\n    if (rank == 0) {\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "gnu3ra_191", "code": "int main(int argc, char **argv)\n{\n    int i, mynod, nprocs, len, errs=0, sum_errs=0, verbose=0;\n    char *filename;\n    char * cb_config_string;\n    int cb_config_len;\n    ADIO_cb_name_array array;\n\n\n\n    \n    \n\n    if (!mynod) {\n\ti = 1;\n\t\n\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: noncontig_coll -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+1);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+1);\n    }\n\n    \n\n    cb_gather_name_array(MPI_COMM_WORLD,  &array);\n\n    \n\n    if (!mynod) {\n\t    if (array->namect < 2 ) {\n\t\t    fprintf(stderr, \"Run this test on two or more hosts\\n\");\n\t    }\n    }\n    \n\n    if (!mynod) {\n\t    cb_config_len = 0;\n\t    for (i=0; i < array->namect; i++) {\n\t\t    \n\n\t\t    cb_config_len += strlen(array->names[i]) + 1;\n\t    }\n\t    ++cb_config_len;\n    }\n    if ( (cb_config_string = malloc(cb_config_len)) == NULL ) {\n\t    perror(\"malloc\");\n    }\n\n    \n\n    errs += test_file(filename, mynod, nprocs, NULL, \"collective w/o hinting\", verbose);\n\n    \n\n    default_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: default order\", verbose);\n\n    \n\n    reverse_str(mynod, cb_config_len, array, cb_config_string); \n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: reverse order\", verbose);\n\n    \n\n    reverse_alternating_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string,\"collective w/ hinting: permutation1\", verbose);\n\n    \n\n    simple_shuffle_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: permutation2\", verbose);\n\n\t \n    if (!mynod) {\n\t    if (sum_errs) fprintf(stderr, \"Found %d error cases\\n\", sum_errs);\n\t    else printf(\" No Errors\\n\");\n    }\n    free(filename);\n    free(cb_config_string);\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int i, mynod, nprocs, len, errs=0, sum_errs=0, verbose=0;\n    char *filename;\n    char * cb_config_string;\n    int cb_config_len;\n    ADIO_cb_name_array array;\n\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod); \n\n    \n    \n\n    if (!mynod) {\n\ti = 1;\n\t\n\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: noncontig_coll -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+1);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+1, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+1);\n\tMPI_Bcast(filename, len+1, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n    \n\n    cb_gather_name_array(MPI_COMM_WORLD,  &array);\n\n    \n\n    if (!mynod) {\n\t    if (array->namect < 2 ) {\n\t\t    fprintf(stderr, \"Run this test on two or more hosts\\n\");\n\t\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t    }\n    }\n    \n\n    if (!mynod) {\n\t    cb_config_len = 0;\n\t    for (i=0; i < array->namect; i++) {\n\t\t    \n\n\t\t    cb_config_len += strlen(array->names[i]) + 1;\n\t    }\n\t    ++cb_config_len;\n    }\n    MPI_Bcast(&cb_config_len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if ( (cb_config_string = malloc(cb_config_len)) == NULL ) {\n\t    perror(\"malloc\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    \n\n    errs += test_file(filename, mynod, nprocs, NULL, \"collective w/o hinting\", verbose);\n\n    \n\n    default_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: default order\", verbose);\n\n    \n\n    reverse_str(mynod, cb_config_len, array, cb_config_string); \n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: reverse order\", verbose);\n\n    \n\n    reverse_alternating_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string,\"collective w/ hinting: permutation1\", verbose);\n\n    \n\n    simple_shuffle_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: permutation2\", verbose);\n\n    MPI_Allreduce(&errs, &sum_errs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\t \n    if (!mynod) {\n\t    if (sum_errs) fprintf(stderr, \"Found %d error cases\\n\", sum_errs);\n\t    else printf(\" No Errors\\n\");\n    }\n    free(filename);\n    free(cb_config_string);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ngcurrier_193", "code": "int\nmain (int argc, char **argv)\n{\n    int         mpi_size, mpi_rank;\n    MPI_Comm    comm  = MPI_COMM_WORLD;\n\n    \n\n\n    if(MAINPROCESS)\n\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    H5open();\n\n    if(mpi_rank == 0) {\n        char\tfilename[1024];\n        hid_t   file_id;\n\n        h5_fixname(FILENAME[0], H5P_DEFAULT, filename, sizeof filename);\n        file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);\n        VRFY((file_id >= 0), \"H5Fcreate succeeded\");\n        H5Fclose(file_id);\n        file_id = -1;\n    }\n\n    H5close();\n\n    if(MAINPROCESS) {\n        if(0 == nerrors)\n            PASSED()\n        else\n\t    H5_FAILED()\n    }\n\n    return (nerrors!=0);\n}", "label": "int\nmain (int argc, char **argv)\n{\n    int         mpi_size, mpi_rank;\n    MPI_Comm    comm  = MPI_COMM_WORLD;\n\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(comm, &mpi_size);\n    MPI_Comm_rank(comm, &mpi_rank);  \n\n    if(MAINPROCESS)\n\tTESTING(\"Usage of Serial HDF5 after MPI_Finalize() is called\");\n\n    MPI_Finalize();\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    H5open();\n\n    if(mpi_rank == 0) {\n        char\tfilename[1024];\n        hid_t   file_id;\n\n        h5_fixname(FILENAME[0], H5P_DEFAULT, filename, sizeof filename);\n        file_id = H5Fcreate(filename, H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);\n        VRFY((file_id >= 0), \"H5Fcreate succeeded\");\n        H5Fclose(file_id);\n        file_id = -1;\n    }\n\n    H5close();\n\n    if(MAINPROCESS) {\n        if(0 == nerrors)\n            PASSED()\n        else\n\t    H5_FAILED()\n    }\n\n    return (nerrors!=0);\n}"}
{"program": "d-meiser_194", "code": "int main(int argn, char **argv) {\n#ifdef BL_WITH_MPI\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  struct BLSimulationState simulationState;\n  struct Configuration conf;\n  struct BLParticleSource *particleSource;\n  struct BLDipoleOperator *dipoleOperator;\n  struct BLModeFunction *modeFunction;\n  struct BLDiagnostics *diagnostics = 0;\n  BL_STATUS stat;\n  int i, rank;\n\n#ifdef BL_WITH_MPI\n#else\n  rank = 0;\n  BL_UNUSED(rank);\n#endif\n\n  setDefaults(&conf);\n  processCommandLineArgs(&conf, argn, argv);\n  adjustNumPtclsForNumRanks(&conf);\n\n  stat = blEnsembleCreate(conf.maxNumParticles, INTERNAL_STATE_DIM,\n      &simulationState.ensemble);\n  simulationState.ensemble.ptclWeight = conf.ptclWeight;\n\n  if (stat != BL_SUCCESS) return stat;\n  simulationState.fieldState.q = 1.0;\n  simulationState.fieldState.p = 0.0;\n  if (stat != BL_SUCCESS) return stat;\n\n  particleSource = constructParticleSources(&conf);\n  diagnostics = constructDiagnostics(&conf);\n  dipoleOperator = blDipoleOperatorTLACreate(conf.dipoleMatrixElement);\n  modeFunction = constructModeFunction(&conf);\n\n  struct BLUpdate *fieldUpdate = blFieldUpdateCreate(0.0, conf.kappa, conf.kappa);\n  struct BLUpdate *atomPush = blPushUpdateCreate();\n  struct BLUpdate *atomFieldInteraction = blAtomFieldInteractionCreate(\n      simulationState.ensemble.maxNumPtcls,\n      simulationState.ensemble.internalStateSize,\n      dipoleOperator, modeFunction);\n  struct BLUpdate *sinks = blSinkBelowCreate(conf.simulationDomain.zmin);\n  struct BLUpdate *sources = blSourceCreate(particleSource);\n\n  for (i = 0; i < conf.numSteps; ++i) {\n    blUpdateTakeStep(sinks, i * conf.dt, conf.dt, &simulationState);\n    blUpdateTakeStep(sources, i * conf.dt, conf.dt, &simulationState);\n    blUpdateTakeStep(atomPush, i * conf.dt, 0.5 * conf.dt, &simulationState);\n    blUpdateTakeStep(fieldUpdate, i * conf.dt, 0.5 * conf.dt, &simulationState);\n    blUpdateTakeStep(atomFieldInteraction, i * conf.dt, conf.dt, &simulationState);\n    blUpdateTakeStep(fieldUpdate, (i + 1) * conf.dt, 0.5 * conf.dt, &simulationState);\n    blUpdateTakeStep(atomPush, (i + 1) * conf.dt, 0.5 * conf.dt, &simulationState);\n    blDiagnosticsProcess(diagnostics, i, &simulationState);\n  }\n\n  blUpdateDestroy(sources);\n  blUpdateDestroy(sinks);\n  blUpdateDestroy(atomFieldInteraction);\n  blUpdateDestroy(atomPush);\n  blUpdateDestroy(fieldUpdate);\n  blModeFunctionDestroy(modeFunction);\n  blDipoleOperatorDestroy(dipoleOperator);\n  blDiagnosticsDestroy(diagnostics);\n  blParticleSourceDestroy(particleSource);\n  blEnsembleDestroy(&simulationState.ensemble);\n\n#ifdef BL_WITH_MPI\n#endif\n  return BL_SUCCESS;\n}", "label": "int main(int argn, char **argv) {\n#ifdef BL_WITH_MPI\n  MPI_Init(&argn, &argv);\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  struct BLSimulationState simulationState;\n  struct Configuration conf;\n  struct BLParticleSource *particleSource;\n  struct BLDipoleOperator *dipoleOperator;\n  struct BLModeFunction *modeFunction;\n  struct BLDiagnostics *diagnostics = 0;\n  BL_STATUS stat;\n  int i, rank;\n\n#ifdef BL_WITH_MPI\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n#else\n  rank = 0;\n  BL_UNUSED(rank);\n#endif\n\n  setDefaults(&conf);\n  processCommandLineArgs(&conf, argn, argv);\n  adjustNumPtclsForNumRanks(&conf);\n\n  stat = blEnsembleCreate(conf.maxNumParticles, INTERNAL_STATE_DIM,\n      &simulationState.ensemble);\n  simulationState.ensemble.ptclWeight = conf.ptclWeight;\n\n  if (stat != BL_SUCCESS) return stat;\n  simulationState.fieldState.q = 1.0;\n  simulationState.fieldState.p = 0.0;\n  if (stat != BL_SUCCESS) return stat;\n\n  particleSource = constructParticleSources(&conf);\n  diagnostics = constructDiagnostics(&conf);\n  dipoleOperator = blDipoleOperatorTLACreate(conf.dipoleMatrixElement);\n  modeFunction = constructModeFunction(&conf);\n\n  struct BLUpdate *fieldUpdate = blFieldUpdateCreate(0.0, conf.kappa, conf.kappa);\n  struct BLUpdate *atomPush = blPushUpdateCreate();\n  struct BLUpdate *atomFieldInteraction = blAtomFieldInteractionCreate(\n      simulationState.ensemble.maxNumPtcls,\n      simulationState.ensemble.internalStateSize,\n      dipoleOperator, modeFunction);\n  struct BLUpdate *sinks = blSinkBelowCreate(conf.simulationDomain.zmin);\n  struct BLUpdate *sources = blSourceCreate(particleSource);\n\n  for (i = 0; i < conf.numSteps; ++i) {\n    blUpdateTakeStep(sinks, i * conf.dt, conf.dt, &simulationState);\n    blUpdateTakeStep(sources, i * conf.dt, conf.dt, &simulationState);\n    blUpdateTakeStep(atomPush, i * conf.dt, 0.5 * conf.dt, &simulationState);\n    blUpdateTakeStep(fieldUpdate, i * conf.dt, 0.5 * conf.dt, &simulationState);\n    blUpdateTakeStep(atomFieldInteraction, i * conf.dt, conf.dt, &simulationState);\n    blUpdateTakeStep(fieldUpdate, (i + 1) * conf.dt, 0.5 * conf.dt, &simulationState);\n    blUpdateTakeStep(atomPush, (i + 1) * conf.dt, 0.5 * conf.dt, &simulationState);\n    blDiagnosticsProcess(diagnostics, i, &simulationState);\n  }\n\n  blUpdateDestroy(sources);\n  blUpdateDestroy(sinks);\n  blUpdateDestroy(atomFieldInteraction);\n  blUpdateDestroy(atomPush);\n  blUpdateDestroy(fieldUpdate);\n  blModeFunctionDestroy(modeFunction);\n  blDipoleOperatorDestroy(dipoleOperator);\n  blDiagnosticsDestroy(diagnostics);\n  blParticleSourceDestroy(particleSource);\n  blEnsembleDestroy(&simulationState.ensemble);\n\n#ifdef BL_WITH_MPI\n  MPI_Finalize();\n#endif\n  return BL_SUCCESS;\n}"}
{"program": "opencb_198", "code": "int main (int argc, char *argv[]) {\n    int myrank;\n    MPI_File fd;\n    MPI_Offset len;\n    MPI_Status status;\n    \n    if (argc < 2) {\n        printf(\"Usage: program <filename>\\n\");\n        exit(1);\n    }\n    \n    char *filename = argv[1];\n    \n    \n    \n    size_t file_len = (size_t) len;\n    printf(\"File %s length = %llu bytes / %zu bytes\\n\", filename, len, file_len);\n    \n    uint8_t *map = malloc(len * sizeof(uint8_t));\n    \n\n    size_t num_variants;\n    int num_affected, num_unaffected;\n    size_t genotypes_offset = sizeof(size_t) + sizeof(uint32_t) + sizeof(uint32_t);\n    \n    \n    printf(\"%d %d %d %d\\n\", map[0], map[1], map[2], map[3]);\n    size_t *map_size_t = (size_t*) map;\n    uint32_t *map_uint32 = (uint32_t*) (map + sizeof(size_t));\n    num_variants = map_size_t[0];\n    num_affected = map_uint32[0];\n    num_unaffected = map_uint32[1];\n    printf(\"num variants = %zu, aff = %d, unaff = %d\\n\", num_variants, num_affected, num_unaffected);\n    \n    map = map + genotypes_offset;\n    \n    for (int i = 0; i < 100; i+=5) {\n        printf(\"%d %d %d %d %d \", map[i], map[i+1], map[i+2], map[i+3], map[i+4]);\n    }\n    \n    printf(\"\\n\");\n    \n    \n    \n    map = map - genotypes_offset;\n    free(map);\n    \n    return 0;\n}", "label": "int main (int argc, char *argv[]) {\n    int myrank;\n    MPI_File fd;\n    MPI_Offset len;\n    MPI_Status status;\n    \n    if (argc < 2) {\n        printf(\"Usage: program <filename>\\n\");\n        exit(1);\n    }\n    \n    char *filename = argv[1];\n    \n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    \n    MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fd);\n    MPI_File_get_size(fd, &len);\n    \n    size_t file_len = (size_t) len;\n    printf(\"File %s length = %llu bytes / %zu bytes\\n\", filename, len, file_len);\n    \n    uint8_t *map = malloc(len * sizeof(uint8_t));\n    \n    MPI_File_seek(fd, 0, MPI_SEEK_SET);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    size_t num_variants;\n    int num_affected, num_unaffected;\n    size_t genotypes_offset = sizeof(size_t) + sizeof(uint32_t) + sizeof(uint32_t);\n    \n    MPI_File_read(fd, map, len, MPI_BYTE, &status);\n    \n    printf(\"%d %d %d %d\\n\", map[0], map[1], map[2], map[3]);\n    size_t *map_size_t = (size_t*) map;\n    uint32_t *map_uint32 = (uint32_t*) (map + sizeof(size_t));\n    num_variants = map_size_t[0];\n    num_affected = map_uint32[0];\n    num_unaffected = map_uint32[1];\n    printf(\"num variants = %zu, aff = %d, unaff = %d\\n\", num_variants, num_affected, num_unaffected);\n    \n    map = map + genotypes_offset;\n    \n    for (int i = 0; i < 100; i+=5) {\n        printf(\"%d %d %d %d %d \", map[i], map[i+1], map[i+2], map[i+3], map[i+4]);\n    }\n    \n    printf(\"\\n\");\n    \n    MPI_File_close(&fd);\n    \n    MPI_Finalize();\n    \n    map = map - genotypes_offset;\n    free(map);\n    \n    return 0;\n}"}
{"program": "sg0_200", "code": "int main(int argc, char **argv)\n{\n    int rank, nprocs;\n    int g_A, ld, lo[2], hi[2];\n    double val=2., *A;\n    \n\n    int dims[2] = { 10, 10 };\n    int block[2] = { 2, 2 };\n    int map[4] = { 0, 5, 0, 5 };\n    const int nprow = block[0];\n    const int npcol = block[1];\n#if defined(USE_ELEMENTAL)\n    ElInitialize( &argc, &argv );\n    ElMPICommRank( MPI_COMM_WORLD, &rank );\n    ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n    \n\n    ElGlobalArraysConstruct_d( &eldga );\n    \n\n    ElGlobalArraysInitialize_d( eldga );\n#else  \n\n\n    MA_init(C_DBL, 1000, 1000);\n\n    GA_Initialize();\n#endif\n\n    assert (nprocs == (nprow * npcol));\n\n    \n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysCreateIrreg_d (eldga, 2, dims, \"array_A\", block, map, &g_A);\n    ElGlobalArraysFill_d (eldga, g_A, &val);\n#else\n    g_A = NGA_Create_irreg(C_DBL, 2, dims, \"array_A\", block, map);\n    GA_Fill(g_A, &val);\n#endif\n\n    \n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysDistribution_d( eldga, g_A, rank, lo, hi );\n    ElGlobalArraysAccess_d (eldga, g_A, lo, hi, &A, &ld);\n#else    \n    NGA_Distribution(g_A, rank, lo, hi);\n    NGA_Access (g_A, lo, hi, &A, &ld);\n#endif\n        \n    if (rank == 0)\n\tprintf (\"ldim = %d\\n\", ld);\n    if (rank == 0)\n\tprintf (\"BEFORE: \\n\");\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysPrint_d (eldga, g_A);\n#else\n    GA_Print (g_A);\n#endif\n    \n    int width = hi[0] - lo[0] + 1;\n    int height = hi[1] - lo[1] + 1;\n\n    for (int j = 0; j < width; j++)\n\tfor (int i = 0; i < height; i++)\n\t    A[i + j * ld] *= val;\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysRelease_d (eldga, g_A, lo, hi);\n#else\n    NGA_Release(g_A, lo, hi);\n#endif\n    \n    if (rank == 0)\n\tprintf (\"AFTER: \\n\");\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysPrint_d (eldga, g_A);\n#else\n    GA_Print (g_A);\n#endif   \n\n    \n\n    printf (\"rank #[%d] range --> lo: (%d, %d) and hi: (%d, %d)\\n\", \n\t    rank, lo[0], lo[1], hi[0], hi[1]);\n    \n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysDestroy_d (eldga, g_A);\n#else\n    GA_Destroy(g_A);    \n#endif\n    \n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysTerminate_d( eldga );\n    ElGlobalArraysDestruct_d( eldga );\n    ElFinalize();\n#else\n    GA_Terminate();\n#endif\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, nprocs;\n    int g_A, ld, lo[2], hi[2];\n    double val=2., *A;\n    \n\n    int dims[2] = { 10, 10 };\n    int block[2] = { 2, 2 };\n    int map[4] = { 0, 5, 0, 5 };\n    const int nprow = block[0];\n    const int npcol = block[1];\n#if defined(USE_ELEMENTAL)\n    ElInitialize( &argc, &argv );\n    ElMPICommRank( MPI_COMM_WORLD, &rank );\n    ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n    \n\n    ElGlobalArraysConstruct_d( &eldga );\n    \n\n    ElGlobalArraysInitialize_d( eldga );\n#else  \n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    MA_init(C_DBL, 1000, 1000);\n\n    GA_Initialize();\n#endif\n\n    assert (nprocs == (nprow * npcol));\n\n    \n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysCreateIrreg_d (eldga, 2, dims, \"array_A\", block, map, &g_A);\n    ElGlobalArraysFill_d (eldga, g_A, &val);\n#else\n    g_A = NGA_Create_irreg(C_DBL, 2, dims, \"array_A\", block, map);\n    GA_Fill(g_A, &val);\n#endif\n\n    \n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysDistribution_d( eldga, g_A, rank, lo, hi );\n    ElGlobalArraysAccess_d (eldga, g_A, lo, hi, &A, &ld);\n#else    \n    NGA_Distribution(g_A, rank, lo, hi);\n    NGA_Access (g_A, lo, hi, &A, &ld);\n#endif\n        \n    if (rank == 0)\n\tprintf (\"ldim = %d\\n\", ld);\n    if (rank == 0)\n\tprintf (\"BEFORE: \\n\");\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysPrint_d (eldga, g_A);\n#else\n    GA_Print (g_A);\n#endif\n    \n    int width = hi[0] - lo[0] + 1;\n    int height = hi[1] - lo[1] + 1;\n\n    for (int j = 0; j < width; j++)\n\tfor (int i = 0; i < height; i++)\n\t    A[i + j * ld] *= val;\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysRelease_d (eldga, g_A, lo, hi);\n#else\n    NGA_Release(g_A, lo, hi);\n#endif\n    \n    if (rank == 0)\n\tprintf (\"AFTER: \\n\");\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysPrint_d (eldga, g_A);\n#else\n    GA_Print (g_A);\n#endif   \n\n    \n\n    printf (\"rank #[%d] range --> lo: (%d, %d) and hi: (%d, %d)\\n\", \n\t    rank, lo[0], lo[1], hi[0], hi[1]);\n    \n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysDestroy_d (eldga, g_A);\n#else\n    GA_Destroy(g_A);    \n#endif\n    \n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysTerminate_d( eldga );\n    ElGlobalArraysDestruct_d( eldga );\n    ElFinalize();\n#else\n    GA_Terminate();\n    MPI_Finalize();\n#endif\n}"}
{"program": "Heathckliff_201", "code": "idx_t main(idx_t argc, char *argv[])\n{\n  idx_t mype, npes;\n  MPI_Comm comm;\n\n\n  if (argc != 2) {\n    if (mype == 0)\n      printf(\"Usage: %s <graph-file>\\n\", argv[0]);\n\n    exit(0);\n  }\n\n  TestParMetis_GPart(argv[1], comm); \n\n\n\n  return 0;\n}", "label": "idx_t main(idx_t argc, char *argv[])\n{\n  idx_t mype, npes;\n  MPI_Comm comm;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &mype);\n\n  if (argc != 2) {\n    if (mype == 0)\n      printf(\"Usage: %s <graph-file>\\n\", argv[0]);\n\n    MPI_Finalize();\n    exit(0);\n  }\n\n  TestParMetis_GPart(argv[1], comm); \n\n  MPI_Comm_free(&comm);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "PaulKh_203", "code": "int main (int argc, char **argv)\n{\n   int myproc, size, other_proc, nprocs, i, last;\n   double t0, t1, time;\n   double *a, *b;\n   double max_rate = 0.0, min_latency = 10e6;\n   MPI_Request request, request_a, request_b;\n   MPI_Status status;\n\n   a = (double *) malloc (1048576 * sizeof (double));\n   b = (double *) malloc (1048576 * sizeof (double));\n\n\n   for (i = 0; i < 1048576; i++) {\n      a[i] = (double) i;\n      b[i] = 0.0;\n   }\n\n\n\n   other_proc = (myproc + 1) % 2;\n\n\n\n\n\n   if (myproc == 0) printf(\"\\n  Synchronous ping-pong\\n\\n\");\n\n   for (size = 8; size <= 8388608; size *= 2) {\n\n\t  t0 =\n      if (myproc == 0) {\n      } else {\n      }\n      t1 =\n      time = 1.e6 * (t1 - t0);\n\n\t  if (myproc == 0 && time > 0.000001) {\n\t  printf(\" %7d bytes took %9.0f usec (%8.3f MB/sec)\\n\", size, time, 2.0 * size / time);\n\n\t  } else if (myproc == 0) {\n\t\t\t  printf(\" %7d bytes took less than the timer accuracy\\n\", size);\n\t  }\n\n   }\n\n\n   if (myproc == 0) printf(\"\\n  Asynchronous ping-pong\\n\\n\");\n\n    for (size = 8; size <= 8388608; size *= 2) {\n\n\n\n\t  t0 =\n       if (myproc == 0) {\n       } else {\n       }\n       t1 =\n       time = 1.e6 * (t1 - t0);\n\n\t  if (myproc == 0 && time > 0.000001) {\n\t  printf(\" %7d bytes took %9.0f usec (%8.3f MB/sec)\\n\", size, time, 2.0 * size / time);\n\n\t  } else if (myproc == 0) {\n\t\t\t  printf(\" %7d bytes took less than the timer accuracy\\n\", size);\n\t  }\n\n    }\n\n\n}", "label": "int main (int argc, char **argv)\n{\n   int myproc, size, other_proc, nprocs, i, last;\n   double t0, t1, time;\n   double *a, *b;\n   double max_rate = 0.0, min_latency = 10e6;\n   MPI_Request request, request_a, request_b;\n   MPI_Status status;\n\n   a = (double *) malloc (1048576 * sizeof (double));\n   b = (double *) malloc (1048576 * sizeof (double));\n\n\n   for (i = 0; i < 1048576; i++) {\n      a[i] = (double) i;\n      b[i] = 0.0;\n   }\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myproc);\n\n\n   other_proc = (myproc + 1) % 2;\n\n\n\n\n\n   if (myproc == 0) printf(\"\\n  Synchronous ping-pong\\n\\n\");\n\n   for (size = 8; size <= 8388608; size *= 2) {\n\n\t  t0 = MPI_Wtime();\n      if (myproc == 0) {\n         MPI_Send(a, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD);\n         MPI_Recv(b, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD, &status);\n      } else {\n         MPI_Recv(b, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD, &status);\n         MPI_Send(b, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD);\n      }\n      t1 = MPI_Wtime();\n      time = 1.e6 * (t1 - t0);\n\n\t  if (myproc == 0 && time > 0.000001) {\n\t  printf(\" %7d bytes took %9.0f usec (%8.3f MB/sec)\\n\", size, time, 2.0 * size / time);\n\n\t  } else if (myproc == 0) {\n\t\t\t  printf(\" %7d bytes took less than the timer accuracy\\n\", size);\n\t  }\n\n   }\n\n\n   if (myproc == 0) printf(\"\\n  Asynchronous ping-pong\\n\\n\");\n\n    for (size = 8; size <= 8388608; size *= 2) {\n\n\n      MPI_Irecv(b, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD, &request);\n\n\t  t0 = MPI_Wtime();\n       if (myproc == 0) {\n\t   MPI_Send(a, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD);\n\t   MPI_Wait(&request, &status);\n       } else {\n\t   MPI_Wait(&request, &status);\n\t   MPI_Send(b, size/8, MPI_DOUBLE, other_proc, 0, MPI_COMM_WORLD);\n       }\n       t1 = MPI_Wtime();\n       time = 1.e6 * (t1 - t0);\n\n\t  if (myproc == 0 && time > 0.000001) {\n\t  printf(\" %7d bytes took %9.0f usec (%8.3f MB/sec)\\n\", size, time, 2.0 * size / time);\n\n\t  } else if (myproc == 0) {\n\t\t\t  printf(\" %7d bytes took less than the timer accuracy\\n\", size);\n\t  }\n\n    }\n\n\n   MPI_Finalize();\n}"}
{"program": "aksiazek_204", "code": "int main(int argc, char** argv) {\n\tint world_rank;\n\tint world_size;\n\n\tif (world_size < 2) {\n\t\tfprintf(stderr, \"World size must be greater than 1 for %s\\n\", argv[0]);\n\t}\n\n\tint iterations = 10000;\n\n\tdouble start =\n\tfor (int i = 0; i < iterations; i++) {\n\t\tcomputations();\n\t}\n\tdouble end =\n\tdouble elapsed_barrier = end - start;\n\n\tstart =\n\tfor (int i = 0; i < iterations; i++) {\n\t\tcomputations();\n\t}\n\tend =\n\tdouble elapsed_no_barrier = end - start;\n\t\n\tif (world_rank == 0)\n\t\tfprintf(stdout, \"Barrier: %lf [s], no barrier: %lf [s]\\n\", elapsed_barrier, elapsed_no_barrier);\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tMPI_Init(&argc, &argv);\n\tint world_rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\tint world_size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n\tif (world_size < 2) {\n\t\tfprintf(stderr, \"World size must be greater than 1 for %s\\n\", argv[0]);\n\t\tMPI_Abort(MPI_COMM_WORLD, 1); \n\t}\n\n\tint iterations = 10000;\n\n\tdouble start = MPI_Wtime();\n\tfor (int i = 0; i < iterations; i++) {\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t\tcomputations();\n\t}\n\tdouble end = MPI_Wtime();\n\tdouble elapsed_barrier = end - start;\n\n\tstart = MPI_Wtime();\n\tfor (int i = 0; i < iterations; i++) {\n\t\tcomputations();\n\t}\n\tend = MPI_Wtime();\n\tdouble elapsed_no_barrier = end - start;\n\t\n\tif (world_rank == 0)\n\t\tfprintf(stdout, \"Barrier: %lf [s], no barrier: %lf [s]\\n\", elapsed_barrier, elapsed_no_barrier);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "krahser_205", "code": "int main(int argc,char*argv[]){\n double *A,*B,*C,*W,*Buffer,*results;\n int i,j,k,N;\n int check=1;\n double temp,total;\n double totaltick, communicationtick;\n\n \n\n  if (argc < 2){\n   printf(\"\\n Faltan argumentos:: N dimension de la matriz \\n\");\n\n   return 0;\n  }\n   N=atoi(argv[1]);\n\n\n\n\n\n int worker_id, n_proc, chunk, size_chunk;\n chunk=N/n_proc;\n size_chunk=chunk*N;\n\n  B=(double*)malloc(sizeof(double)*N*N);\n  Buffer=(double*)malloc(sizeof(double)*size_chunk);\n  \n  for(i=0;i<size_chunk;i++) Buffer[i]=0;\n  \n  results=(double*)malloc(sizeof(double)*3);\n\n  if(worker_id == MANAGER){\n \n\n      A=(double*)malloc(sizeof(double)*N*N);\n      C=(double*)malloc(sizeof(double)*N*N);\n      W=(double*)malloc(sizeof(double)*N*N);\n      \n\n      for(i=0;i<N;i++){\n\tfor(j=0;j<N;j++){\n\t\tA[i*N+j]= 1.0;\n\t\t\n\n\t\tB[i+N*j]= 1.0;\n\t\tC[i*N+j]= 1.0;\n\t\tW[i*N+j]= 1.0;\n    }\n  }   \n\n  }\n  else {\n      A=(double*)malloc(sizeof(double)*size_chunk);\n      W=(double*)malloc(sizeof(double)*size_chunk);\n  }\n\n \n totaltick = dwalltime();\n\n\n results[0]=dwalltime() - totaltick;\n\n\n\n\n\n \n\n for(i=0;i<chunk;i++){\n      for(j=0;j<N;j++){\n\t   temp += W[i*chunk+j];\n\t   total += A[i*chunk+j] * W[i*chunk+j];\n\t}\n  }\n\n\ncommunicationtick = dwalltime();\n\n\nresults[1]=dwalltime() - communicationtick;\n\ntotal/=temp;\n  \n\n\n\n\n\n  for(i=0;i<chunk;i++){\n      for(j=0;j<N;j++)\n\t    for(k=0;k<N;k++)\n\t    Buffer[i*chunk+j]+=sqrt((pow(A[i*chunk+k]-total,2))*(pow(B[j*N+k]-total,2)));\n  }\n\n\n\n\ncommunicationtick = dwalltime();\n\n\n results[2]=dwalltime() - communicationtick;\n\n totaltick = dwalltime() - totaltick;\n\n\n for(i=0;i<3;i++)\nMPI_Allreduce(results+i, results+i, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n\n  \n\n  if(worker_id == MANAGER){\n      for(i=0;i<N*N;i++)\n\t      check= check&&(C[i]==0.0);\n  if(check){\n   printf(\"Tiempo total en segundos: %f \\n\", totaltick);\n   for(i=0,communicationtick=0;i<3;i++) communicationtick+=results[i];\n   printf(\"Tiempo de comunicacion en segundos: %f \\n\", communicationtick);\n   printf(\"Resultado correcto\\n\");\n  }else{\n   printf(\"Resultado erroneo\\n\");\n  }\n free(C);\n }\n free(A);\n free(B);\n free(Buffer);\n free(results);\n free(W);\n return(0);\n}", "label": "int main(int argc,char*argv[]){\n double *A,*B,*C,*W,*Buffer,*results;\n int i,j,k,N;\n int check=1;\n double temp,total;\n double totaltick, communicationtick;\n\n \n\n  if (argc < 2){\n   printf(\"\\n Faltan argumentos:: N dimension de la matriz \\n\");\n\n   return 0;\n  }\n   N=atoi(argv[1]);\n\n\n\n\n\n int worker_id, n_proc, chunk, size_chunk;\n MPI_Init(&argc, &argv);\n MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n MPI_Comm_rank(MPI_COMM_WORLD, &worker_id);\n chunk=N/n_proc;\n size_chunk=chunk*N;\n\n  B=(double*)malloc(sizeof(double)*N*N);\n  Buffer=(double*)malloc(sizeof(double)*size_chunk);\n  \n  for(i=0;i<size_chunk;i++) Buffer[i]=0;\n  \n  results=(double*)malloc(sizeof(double)*3);\n\n  if(worker_id == MANAGER){\n \n\n      A=(double*)malloc(sizeof(double)*N*N);\n      C=(double*)malloc(sizeof(double)*N*N);\n      W=(double*)malloc(sizeof(double)*N*N);\n      \n\n      for(i=0;i<N;i++){\n\tfor(j=0;j<N;j++){\n\t\tA[i*N+j]= 1.0;\n\t\t\n\n\t\tB[i+N*j]= 1.0;\n\t\tC[i*N+j]= 1.0;\n\t\tW[i*N+j]= 1.0;\n    }\n  }   \n\n  }\n  else {\n      A=(double*)malloc(sizeof(double)*size_chunk);\n      W=(double*)malloc(sizeof(double)*size_chunk);\n  }\n\n \n totaltick = dwalltime();\n\n MPI_Scatter (A, size_chunk, MPI_DOUBLE, A, size_chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n MPI_Scatter (W, size_chunk, MPI_DOUBLE, W, size_chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n results[0]=dwalltime() - totaltick;\n\n\n\n\n\n \n\n for(i=0;i<chunk;i++){\n      for(j=0;j<N;j++){\n\t   temp += W[i*chunk+j];\n\t   total += A[i*chunk+j] * W[i*chunk+j];\n\t}\n  }\n\n\ncommunicationtick = dwalltime();\n\n  MPI_Allreduce(&temp, &temp, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n  MPI_Allreduce(&total, &total, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n  MPI_Scatter (A, size_chunk, MPI_DOUBLE, A, size_chunk, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n  MPI_Bcast(B, N*N, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n\nresults[1]=dwalltime() - communicationtick;\n\ntotal/=temp;\n  \n\n\n\n\n\n  for(i=0;i<chunk;i++){\n      for(j=0;j<N;j++)\n\t    for(k=0;k<N;k++)\n\t    Buffer[i*chunk+j]+=sqrt((pow(A[i*chunk+k]-total,2))*(pow(B[j*N+k]-total,2)));\n  }\n\n\n\n\ncommunicationtick = dwalltime();\n\n MPI_Gather(Buffer, size_chunk, MPI_DOUBLE, C, size_chunk, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n\n results[2]=dwalltime() - communicationtick;\n\n totaltick = dwalltime() - totaltick;\n\n\n for(i=0;i<3;i++)\nMPI_Allreduce(results+i, results+i, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n\n  \n\n  if(worker_id == MANAGER){\n      for(i=0;i<N*N;i++)\n\t      check= check&&(C[i]==0.0);\n  if(check){\n   printf(\"Tiempo total en segundos: %f \\n\", totaltick);\n   for(i=0,communicationtick=0;i<3;i++) communicationtick+=results[i];\n   printf(\"Tiempo de comunicacion en segundos: %f \\n\", communicationtick);\n   printf(\"Resultado correcto\\n\");\n  }else{\n   printf(\"Resultado erroneo\\n\");\n  }\n free(C);\n }\n free(A);\n free(B);\n free(Buffer);\n free(results);\n free(W);\n MPI_Finalize();\n return(0);\n}"}
{"program": "oscartt89_208", "code": "int main(int ac, char **argv) {\n\n   int myID;\n\n   if(ac != 3){\n     terror(\"Use: ./scheduler SCRIPT_FILENAME TASK_PER_NODE\");\n   }\n\n\n   if (myID == 0){\n      master(argv[1]); \n\n   }\n   else{\n      worker(atoi(argv[2]));\n   }\n   \n\n\n   return 0;\n}", "label": "int main(int ac, char **argv) {\n\n   int myID;\n\n   if(ac != 3){\n     terror(\"Use: ./scheduler SCRIPT_FILENAME TASK_PER_NODE\");\n   }\n\n   MPI_Init(&ac, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myID);\n\n   if (myID == 0){\n      master(argv[1]); \n\n   }\n   else{\n      worker(atoi(argv[2]));\n   }\n   \n   MPI_Finalize(); \n\n\n   return 0;\n}"}
{"program": "rpereira-dev_210", "code": "gc, char **argv)\n{\n    int i, array_size, size, rank;\n    float *my_values, *tmp_values, *full_vector;\n    double tbeg, tend, telaps;\n\n       \n    \n    if( BUFFER_SIZE % size != 0 )\n    {\n        fprintf(stdout, \"Le nombre de processus n'est pas un multiple de la taille du tableau.\\n\");\n        abort();\n    }    \n    \n    if( rank == 0)\n    {\n        full_vector = malloc( BUFFER_SIZE * sizeof( float ) );\t\n        assert( full_vector != NULL );\n\n        \n\n        for( i = 0; i < BUFFER_SIZE; i++ )\n\t\tfull_vector[i] = i;\n    } \n    \n    array_size = BUFFER_SIZE / size; \n    my_values = malloc( ( array_size + 2 ) * sizeof( float ) ); \n\n    assert( my_values != NULL );\n    tmp_values = malloc( ( array_size + 2 ) * sizeof( float ) ); \n\n    assert( tmp_values != NULL );\n    \n    \n\n    void * begin_my_value = ( char * ) my_values + sizeof( float );\n    \n    \n \n    if ( rank == 0 )\n    {\n        memset(full_vector, 0, BUFFER_SIZE*sizeof(float));\n    }\n\n    tbeg =\n\n    for( i = 0; i < MAX_REPEAT_NB; i++)\n    { \n\t\n\n        communications( my_values );\n\n\t\n\n        convolution( my_values, tmp_values );\n    }\n\n    tend =\n    telaps = tend - tbeg;\n    if (rank == 0)\n    {\n        printf(\"Pt2pt Telaps = %g s\\n\", telaps);\n    }\n\n    \n \n\n    \n\n    if( rank == 0 )\n\t    free(full_vector);\t\n    free(my_values);\n    free(tmp_values);\n\n\n    return EXIT_SUCCESS;\n}\n", "label": "gc, char **argv)\n{\n    int i, array_size, size, rank;\n    float *my_values, *tmp_values, *full_vector;\n    double tbeg, tend, telaps;\n\n    MPI_Init( &argc, &argv );\n       \n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    if( BUFFER_SIZE % size != 0 )\n    {\n        fprintf(stdout, \"Le nombre de processus n'est pas un multiple de la taille du tableau.\\n\");\n        abort();\n    }    \n    \n    if( rank == 0)\n    {\n        full_vector = malloc( BUFFER_SIZE * sizeof( float ) );\t\n        assert( full_vector != NULL );\n\n        \n\n        for( i = 0; i < BUFFER_SIZE; i++ )\n\t\tfull_vector[i] = i;\n    } \n    \n    array_size = BUFFER_SIZE / size; \n    my_values = malloc( ( array_size + 2 ) * sizeof( float ) ); \n\n    assert( my_values != NULL );\n    tmp_values = malloc( ( array_size + 2 ) * sizeof( float ) ); \n\n    assert( tmp_values != NULL );\n    \n    \n\n    void * begin_my_value = ( char * ) my_values + sizeof( float );\n    \n    \n \n    MPI_Scatter(full_vector, array_size, MPI_FLOAT, begin_my_value, array_size, MPI_FLOAT, 0, MPI_COMM_WORLD);  \n    if ( rank == 0 )\n    {\n        memset(full_vector, 0, BUFFER_SIZE*sizeof(float));\n    }\n\n    tbeg = MPI_Wtime();\n\n    for( i = 0; i < MAX_REPEAT_NB; i++)\n    { \n\t\n\n        communications( my_values );\n\n\t\n\n        convolution( my_values, tmp_values );\n    }\n\n    tend = MPI_Wtime();\n    telaps = tend - tbeg;\n    if (rank == 0)\n    {\n        printf(\"Pt2pt Telaps = %g s\\n\", telaps);\n    }\n\n    \n \n    MPI_Gather( begin_my_value, array_size, MPI_FLOAT, full_vector, array_size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    \n\n    if( rank == 0 )\n\t    free(full_vector);\t\n    free(my_values);\n    free(tmp_values);\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}\n"}
{"program": "tcsiwula_211", "code": "int main(int argc, char **argv)\n{\n   int n, loc_n, p, my_rank;\n   int *loc_mat, *loc_dist, *loc_pred, *global_dist, *global_pred;\n   MPI_Datatype blk_col_mpi_t;\n   n = Read_n(my_rank, MPI_COMM_WORLD);\n   loc_n = n/p;\n   loc_mat = malloc(n*loc_n*sizeof(int));\n   global_dist = malloc(n*sizeof(int));\n   global_pred = malloc(n*sizeof(int));\n   loc_dist = malloc(loc_n*sizeof(int));\n   loc_pred = malloc(loc_n*sizeof(int));\n   blk_col_mpi_t = Build_blk_col_type(n, loc_n);\n   Read_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, MPI_COMM_WORLD);\n   Dijkstra(loc_mat, loc_dist, loc_pred, loc_n, n, my_rank);\n   Print_dists(loc_dist, n, my_rank, loc_n, global_dist);\n   Print_paths(loc_pred, n, global_pred, my_rank, loc_n);\n   free(loc_mat);\n   free(global_dist);\n   free(global_pred);\n   free(loc_dist);\n   free(loc_pred);\n   return 0;\n}", "label": "int main(int argc, char **argv)\n{\n   int n, loc_n, p, my_rank;\n   int *loc_mat, *loc_dist, *loc_pred, *global_dist, *global_pred;\n   MPI_Datatype blk_col_mpi_t;\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   n = Read_n(my_rank, MPI_COMM_WORLD);\n   loc_n = n/p;\n   loc_mat = malloc(n*loc_n*sizeof(int));\n   global_dist = malloc(n*sizeof(int));\n   global_pred = malloc(n*sizeof(int));\n   loc_dist = malloc(loc_n*sizeof(int));\n   loc_pred = malloc(loc_n*sizeof(int));\n   blk_col_mpi_t = Build_blk_col_type(n, loc_n);\n   Read_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, MPI_COMM_WORLD);\n   Dijkstra(loc_mat, loc_dist, loc_pred, loc_n, n, my_rank);\n   Print_dists(loc_dist, n, my_rank, loc_n, global_dist);\n   Print_paths(loc_pred, n, global_pred, my_rank, loc_n);\n   free(loc_mat);\n   free(global_dist);\n   free(global_pred);\n   free(loc_dist);\n   free(loc_pred);\n   MPI_Type_free(&blk_col_mpi_t);\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "auag92_212", "code": "void main(int argc, char *argv[]){\n\n  numworkers = numtasks - 1;\n\n  if(taskid == MASTER){ \n\n\n    allocate_memory();\n    phi_initialize();\n    fluid_initialize();\n    gs_allocate();\n    for (t=0; t < phi_timesteps; t++) {\n    #ifdef growth\n      neuman_boundary(phi_old, MESHX);\n      neuman_boundary(mu_old, MESHX);\n      concentration(phi_old, mu_old, conc, MESHX);\n      neuman_boundary(conc, MESHX);\n      laplacian(phi_old, lap_phi, MESHX);\n      laplacian(mu_old,  lap_mu, MESHX);\n      solverloop();\n      update(phi_old, phi_new, MESHX);\n      update(mu_old, mu_new, MESHX);\n      if((t%save_phi) == 0) {\n       write2file_phi(t, MESHX,phi_old);\n      }\n    #endif\n    #ifdef FLUID\n      if (t>20) {\n        fluid_solver();\n        if((t%save_fluid) ==0) {\n             write2file_fluid (t,u_old,v_old,MESHX);\n        }\n      }\n    #endif\n      printf(\"t=%d\\n\",t);\n    }\n    free_memory();\n  } else { \n\n    gs_allocate();\n    for (t=0; t < phi_timesteps; t++) {\n      if (t>20) {\n        gs_mpi();\n      }\n    }\n    free(P);\n    free(rhs_fn);\n    free(a_x);\n    free(a_y);\n  }\n}", "label": "void main(int argc, char *argv[]){\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n  numworkers = numtasks - 1;\n  MPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n\n  if(taskid == MASTER){ \n\n\n    allocate_memory();\n    phi_initialize();\n    fluid_initialize();\n    gs_allocate();\n    for (t=0; t < phi_timesteps; t++) {\n    #ifdef growth\n      neuman_boundary(phi_old, MESHX);\n      neuman_boundary(mu_old, MESHX);\n      concentration(phi_old, mu_old, conc, MESHX);\n      neuman_boundary(conc, MESHX);\n      laplacian(phi_old, lap_phi, MESHX);\n      laplacian(mu_old,  lap_mu, MESHX);\n      solverloop();\n      update(phi_old, phi_new, MESHX);\n      update(mu_old, mu_new, MESHX);\n      if((t%save_phi) == 0) {\n       write2file_phi(t, MESHX,phi_old);\n      }\n    #endif\n    #ifdef FLUID\n      if (t>20) {\n        fluid_solver();\n        if((t%save_fluid) ==0) {\n             write2file_fluid (t,u_old,v_old,MESHX);\n        }\n      }\n    #endif\n      printf(\"t=%d\\n\",t);\n    }\n    free_memory();\n  } else { \n\n    gs_allocate();\n    for (t=0; t < phi_timesteps; t++) {\n      if (t>20) {\n        gs_mpi();\n      }\n    }\n    free(P);\n    free(rhs_fn);\n    free(a_x);\n    free(a_y);\n  }\n  MPI_Finalize();\n}"}
{"program": "formap_216", "code": "int main(int argc, char *argv[]) {\n\t\n\tint myid, numprocs;\n    int tag,source,destination,count;\n    int buffer[ndata*2];\n    MPI_Status status;\n    MPI_Request request;\n    \n    int iter = 20;\n    \n    \n    if (myid == 0 && numprocs == 2) \n    {\n\t\tint recvID = 1;\n\t\tdouble acum = 0;\n\t\tint i, tam = 3;\n\t\tdouble startT;\n\t\tfor (i = 0; i < ndata*2; ++i) \n\t\t{\n\t\t\tbuffer[i]=i;\n\t\t}\n\t\twhile (tam < ndata)\n\t\t{\n\t\t\tdouble startTime =\n\t\t\t\n\t\t\t\n\t\t\tdouble endTime =\n\t\t\tdouble elapsed = endTime - startTime;\n\t\t\tacum += elapsed;\n\t\t\tprintf(\"%d, %f, elapsed: %f\\n\",tam,acum,elapsed);\n\t\t\tfflush(stdout);\n\t\t\ttam += 2;\n\t\t}\n\t\tprintf(\"total: %f\\nmean: %f\\n\", acum, acum/iter);\t\n\t}\n\telse if (numprocs == 2) \n\t{\n\t\tint i, tam=3;\n\t\twhile (tam < ndata) \n\t\t{\n\t\t\ttam += 2;\n\t\t}\n\t}\n\telse \n\t{\n\t\tprintf(\"Need only 2 threads\\n\");\n\t}\n    \n\n    \n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n\t\n\tint myid, numprocs;\n    int tag,source,destination,count;\n    int buffer[ndata*2];\n    MPI_Status status;\n    MPI_Request request;\n    \n    int iter = 20;\n    \n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    \n    if (myid == 0 && numprocs == 2) \n    {\n\t\tint recvID = 1;\n\t\tdouble acum = 0;\n\t\tint i, tam = 3;\n\t\tdouble startT;\n\t\tfor (i = 0; i < ndata*2; ++i) \n\t\t{\n\t\t\tbuffer[i]=i;\n\t\t}\n\t\twhile (tam < ndata)\n\t\t{\n\t\t\tdouble startTime = MPI_Wtime();\n\t\t\t\n\t\t\tMPI_Rsend(&buffer,tam,MPI_INT,recvID,0,MPI_COMM_WORLD);\n\t\t\t\n\t\t\tdouble endTime = MPI_Wtime();\n\t\t\tdouble elapsed = endTime - startTime;\n\t\t\tacum += elapsed;\n\t\t\tprintf(\"%d, %f, elapsed: %f\\n\",tam,acum,elapsed);\n\t\t\tfflush(stdout);\n\t\t\ttam += 2;\n\t\t}\n\t\tprintf(\"total: %f\\nmean: %f\\n\", acum, acum/iter);\t\n\t}\n\telse if (numprocs == 2) \n\t{\n\t\tint i, tam=3;\n\t\twhile (tam < ndata) \n\t\t{\n\t\t\tMPI_Recv(&buffer,ndata,MPI_INT,0,0,MPI_COMM_WORLD,&status);\n\t\t\ttam += 2;\n\t\t}\n\t}\n\telse \n\t{\n\t\tprintf(\"Need only 2 threads\\n\");\n\t}\n    \n   \tMPI_Finalize();\n\n    \n    return 0;\n}"}
{"program": "lesina_220", "code": "int main( int argc, char* argv[] ) {\n        const int N = 100;\n        double x[N], TotalSum, ProcSum = 0.0;\n        int ProcRank, ProcNum, k, i1, i2;\n        MPI_Status Status;\n\n        \n\n\n        \n\n        if ( ProcRank == 0 ) {\n                for( i1 = 0; i1 < N; ++i1 ) {\n                        x[i1] = i1;\n                }\n        }\n\n        \n\n\n        \n\n        \n\n        k = N / ProcNum;\n        i1 = k * ProcRank;\n        i2 = k * ( ProcRank + 1 );\n        if ( ProcRank == ProcNum-1 ) i2 = N;\n        for ( int i = i1; i < i2; i++ ) {\n                ProcSum = ProcSum + x[i];\n        }\n\n\n        \n\n        if ( ProcRank == 0 ) {\n                printf(\"\\nTotal Sum = %10.2f\",TotalSum);\n        }\n        return 0;\n}\n", "label": "int main( int argc, char* argv[] ) {\n        const int N = 100;\n        double x[N], TotalSum, ProcSum = 0.0;\n        int ProcRank, ProcNum, k, i1, i2;\n        MPI_Status Status;\n\n        \n\n        MPI_Init( &argc, &argv );\n        MPI_Comm_size( MPI_COMM_WORLD, &ProcNum );\n        MPI_Comm_rank( MPI_COMM_WORLD, &ProcRank );\n\n        \n\n        if ( ProcRank == 0 ) {\n                for( i1 = 0; i1 < N; ++i1 ) {\n                        x[i1] = i1;\n                }\n        }\n\n        \n\n        MPI_Bcast( x, N, MPI_DOUBLE, 0, MPI_COMM_WORLD );\n\n        \n\n        \n\n        k = N / ProcNum;\n        i1 = k * ProcRank;\n        i2 = k * ( ProcRank + 1 );\n        if ( ProcRank == ProcNum-1 ) i2 = N;\n        for ( int i = i1; i < i2; i++ ) {\n                ProcSum = ProcSum + x[i];\n        }\n\n        MPI_Reduce( &ProcSum, &TotalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD );\n\n        \n\n        if ( ProcRank == 0 ) {\n                printf(\"\\nTotal Sum = %10.2f\",TotalSum);\n        }\n        MPI_Finalize();\n        return 0;\n}\n"}
{"program": "lorenzgerber_222", "code": "int main(int argc, char* argv[]) {\n   double*     x;\n   int         i, n;\n   MPI_Status  status;\n   double      start, finish;\n\n   comm = MPI_COMM_WORLD;\n\n   Get_arg(argc, argv, &n);\n\n   x = malloc(n*sizeof(double));\n\n   if (my_rank == 0)\n      for (i = 0; i < n; i++) x[i] = i;\n   else \n\n      for (i = 0; i < n; i++) x[i] = -1;\n\n   start =\n   if (my_rank == 0) \n      for (i = 0; i < n; i++)\n   else \n\n      for (i = 0; i < n; i++)\n   finish =\n   printf(\"Proc %d > First comm took %e seconds\\n\", my_rank,\n         finish-start);\n   fflush(stdout);\n\n   start =\n   if (my_rank == 0)\n   else  \n\n   finish =\n   printf(\"Proc %d > Second comm took %e seconds\\n\", my_rank,\n         finish-start);\n   fflush(stdout);\n\n   free(x);\n   return 0;\n}", "label": "int main(int argc, char* argv[]) {\n   double*     x;\n   int         i, n;\n   MPI_Status  status;\n   double      start, finish;\n\n   MPI_Init(&argc, &argv);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &comm_sz);\n   MPI_Comm_rank(comm, &my_rank);\n\n   Get_arg(argc, argv, &n);\n\n   x = malloc(n*sizeof(double));\n\n   if (my_rank == 0)\n      for (i = 0; i < n; i++) x[i] = i;\n   else \n\n      for (i = 0; i < n; i++) x[i] = -1;\n\n   MPI_Barrier(comm);\n   start = MPI_Wtime();\n   if (my_rank == 0) \n      for (i = 0; i < n; i++)\n         MPI_Send(&x[i], 1, MPI_DOUBLE, 1, 0, comm);\n   else \n\n      for (i = 0; i < n; i++)\n         MPI_Recv(&x[i], 1, MPI_DOUBLE, 0, 0, comm, &status);\n   finish = MPI_Wtime();\n   printf(\"Proc %d > First comm took %e seconds\\n\", my_rank,\n         finish-start);\n   fflush(stdout);\n\n   MPI_Barrier(comm);\n   start = MPI_Wtime();\n   if (my_rank == 0)\n      MPI_Send(x, n, MPI_DOUBLE, 1, 0, comm);\n   else  \n\n      MPI_Recv(x, n, MPI_DOUBLE, 0, 0, comm, &status);\n   finish = MPI_Wtime();\n   printf(\"Proc %d > Second comm took %e seconds\\n\", my_rank,\n         finish-start);\n   fflush(stdout);\n\n   free(x);\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "biospi_224", "code": "int\nmain(int argc, char **argv)\n{\n    int ret;\n    int exit_value = EXIT_SUCCESS;\n    struct options *opts = NULL;\n\n#ifndef STANDALONE\n    \n\n    h5tools_init();\n#endif\n\n    output = stdout;\n\n    \n\n    ret =\n\n    if (ret != MPI_SUCCESS) {\n        fprintf(stderr, \"%s: MPI_Comm_size call failed\\n\", progname);\n\n        if (ret == MPI_ERR_COMM)\n            fprintf(stderr, \"invalid MPI communicator\\n\");\n        else\n            fprintf(stderr, \"invalid argument\\n\");\n\n        exit_value = EXIT_FAILURE;\n        goto finish;\n    }\n\n    ret =\n\n    if (ret != MPI_SUCCESS) {\n        fprintf(stderr, \"%s: MPI_Comm_rank call failed\\n\", progname);\n\n        if (ret == MPI_ERR_COMM)\n            fprintf(stderr, \"invalid MPI communicator\\n\");\n        else\n            fprintf(stderr, \"invalid argument\\n\");\n\n        exit_value = EXIT_FAILURE;\n        goto finish;\n    }\n\n    pio_comm_g = MPI_COMM_WORLD;\n\n    h5_set_info_object();\n    opts = parse_command_line(argc, argv);\n\n    if (!opts) {\n        exit_value = EXIT_FAILURE;\n        goto finish;\n    }\n\n    if (opts->output_file) {\n        if ((output = fopen(opts->output_file, \"w\")) == NULL) {\n            fprintf(stderr, \"%s: cannot open output file\\n\", progname);\n            perror(opts->output_file);\n            goto finish;\n        }\n    }\n\n    if ((pio_debug_level == 0 && comm_world_rank_g == 0) || pio_debug_level > 0)\n        report_parameters(opts);\n\n    run_test_loop(opts);\n\nfinish:\n    free(opts);\n    return exit_value;\n}", "label": "int\nmain(int argc, char **argv)\n{\n    int ret;\n    int exit_value = EXIT_SUCCESS;\n    struct options *opts = NULL;\n\n#ifndef STANDALONE\n    \n\n    h5tools_init();\n#endif\n\n    output = stdout;\n\n    \n\n    MPI_Init(&argc, &argv);\n    ret = MPI_Comm_size(MPI_COMM_WORLD, &comm_world_nprocs_g);\n\n    if (ret != MPI_SUCCESS) {\n        fprintf(stderr, \"%s: MPI_Comm_size call failed\\n\", progname);\n\n        if (ret == MPI_ERR_COMM)\n            fprintf(stderr, \"invalid MPI communicator\\n\");\n        else\n            fprintf(stderr, \"invalid argument\\n\");\n\n        exit_value = EXIT_FAILURE;\n        goto finish;\n    }\n\n    ret = MPI_Comm_rank(MPI_COMM_WORLD, &comm_world_rank_g);\n\n    if (ret != MPI_SUCCESS) {\n        fprintf(stderr, \"%s: MPI_Comm_rank call failed\\n\", progname);\n\n        if (ret == MPI_ERR_COMM)\n            fprintf(stderr, \"invalid MPI communicator\\n\");\n        else\n            fprintf(stderr, \"invalid argument\\n\");\n\n        exit_value = EXIT_FAILURE;\n        goto finish;\n    }\n\n    pio_comm_g = MPI_COMM_WORLD;\n\n    h5_set_info_object();\n    opts = parse_command_line(argc, argv);\n\n    if (!opts) {\n        exit_value = EXIT_FAILURE;\n        goto finish;\n    }\n\n    if (opts->output_file) {\n        if ((output = fopen(opts->output_file, \"w\")) == NULL) {\n            fprintf(stderr, \"%s: cannot open output file\\n\", progname);\n            perror(opts->output_file);\n            goto finish;\n        }\n    }\n\n    if ((pio_debug_level == 0 && comm_world_rank_g == 0) || pio_debug_level > 0)\n        report_parameters(opts);\n\n    run_test_loop(opts);\n\nfinish:\n    MPI_Finalize();\n    free(opts);\n    return exit_value;\n}"}
{"program": "mnakao_228", "code": "int main(int argc, char **argv){\n  int i, me, target;\n  unsigned int size;\n  double t, t_max;\n  MPI_Win win;\n\n  target = 1 - me;\n\n  init_buf(send_buf, me);\n\n  if(me==0) print_items();\n\n  for(size=1;size<MAX_SIZE+1;size*=2){\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n\tt = wtime();\n\n      if(me == 0){\n\twhile(send_buf[0] == '0' || send_buf[size-1] == '0'){}  \n\n\tsend_buf[0] = '0'; send_buf[size-1] = '0';              \n\n      } \n      else {\n\twhile(send_buf[0] == '1' || send_buf[size-1] == '1'){}  \n\n\tsend_buf[0] = '1'; send_buf[size-1] = '1';              \n\n      }\n\n    } \n\n\n    t = wtime() - t;\n    if(me == 0)\n      print_results(size, t_max);\n  }\n\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int i, me, target;\n  unsigned int size;\n  double t, t_max;\n  MPI_Win win;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Win_create(&send_buf, sizeof(char)*MAX_SIZE, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n  MPI_Win_fence(0, win);\n  target = 1 - me;\n\n  init_buf(send_buf, me);\n\n  if(me==0) print_items();\n\n  for(size=1;size<MAX_SIZE+1;size*=2){\n    MPI_Barrier(MPI_COMM_WORLD);\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n\tt = wtime();\n\n      if(me == 0){\n\tMPI_Get(send_buf, size, MPI_CHAR, target, 0, size, MPI_CHAR, win);\t\n\tMPI_Win_fence(0, win);\n\twhile(send_buf[0] == '0' || send_buf[size-1] == '0'){}  \n\n\tsend_buf[0] = '0'; send_buf[size-1] = '0';              \n\n\tMPI_Win_fence(0, win);\n      } \n      else {\n\tMPI_Win_fence(0, win);\n\tMPI_Get(send_buf, size, MPI_CHAR, target, 0, size, MPI_CHAR, win);\n\tMPI_Win_fence(0, win);\n\twhile(send_buf[0] == '1' || send_buf[size-1] == '1'){}  \n\n\tsend_buf[0] = '1'; send_buf[size-1] = '1';              \n\n      }\n\n    } \n\n\n    t = wtime() - t;\n    MPI_Reduce(&t, &t_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n    if(me == 0)\n      print_results(size, t_max);\n  }\n\n  MPI_Win_free(&win);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "cc-hpc-itwm_230", "code": "int main (int argc, char *argv[])\n{\n\n  \n  int nProc, iProc;\n\n\n  \n\n  const int NTHREADS = 1;\n\n  \n\n  const int NWAY     = 2;\n\n  \n\n  const int left  = LEFT(iProc, nProc);\n\n  \n\n  const int right = RIGHT(iProc, nProc);\n\n  \n\n  double* array = malloc (NWAY * (NTHREADS+2) * 2 * VLEN * sizeof (double));\n  ASSERT (array != 0);\n\n  \n\n  int buffer_id = 0;\n\n  \n\n  data_init (NTHREADS, iProc, buffer_id, array);\n\n\n  double time = -now();\n\n  for (int k = 0; k < NITER; ++k)\n  {\n    for (int i = 0; i < nProc; ++i)\n    {\n      MPI_Request request;\n      int slice_id    = 1;\n      int left_halo   = 0;\n\n      \n\n\n      \n\n\n      \n\n      data_compute (NTHREADS, array, 1 - buffer_id, buffer_id, slice_id);\n\n      \n\n\n      \n\n      buffer_id = 1 - buffer_id;\n\n    }\n  }\n\n  time += now();\n\n\n\n\n  printf (\"# mpi %s nProc %d vlen %i niter %d nthreads %i nway %i time %g\\n\"\n         , argv[0], nProc, VLEN, NITER, NTHREADS, NWAY, time\n         );\n  \n\n  free (array);\n\n  return EXIT_SUCCESS;\n}", "label": "int main (int argc, char *argv[])\n{\n\n  MPI_Init (&argc, &argv);\n  \n  int nProc, iProc;\n\n  MPI_Comm_rank (MPI_COMM_WORLD, &iProc);\n  MPI_Comm_size (MPI_COMM_WORLD, &nProc);\n\n  \n\n  const int NTHREADS = 1;\n\n  \n\n  const int NWAY     = 2;\n\n  \n\n  const int left  = LEFT(iProc, nProc);\n\n  \n\n  const int right = RIGHT(iProc, nProc);\n\n  \n\n  double* array = malloc (NWAY * (NTHREADS+2) * 2 * VLEN * sizeof (double));\n  ASSERT (array != 0);\n\n  \n\n  int buffer_id = 0;\n\n  \n\n  data_init (NTHREADS, iProc, buffer_id, array);\n\n  MPI_Barrier (MPI_COMM_WORLD);\n\n  double time = -now();\n\n  for (int k = 0; k < NITER; ++k)\n  {\n    for (int i = 0; i < nProc; ++i)\n    {\n      MPI_Request request;\n      int slice_id    = 1;\n      int left_halo   = 0;\n\n      \n\n      MPI_Isend ( &array_ELEM_right (buffer_id, left_halo + 1, 0), VLEN, MPI_DOUBLE\n\t         , right, i, MPI_COMM_WORLD, &request);\n\n      \n\n      MPI_Recv ( &array_ELEM_right (buffer_id, left_halo, 0), VLEN, MPI_DOUBLE\n\t\t , left, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      \n\n      data_compute (NTHREADS, array, 1 - buffer_id, buffer_id, slice_id);\n\n      \n\n      MPI_Wait (&request, MPI_STATUS_IGNORE);\n\n      \n\n      buffer_id = 1 - buffer_id;\n\n    }\n  }\n\n  time += now();\n\n\n\n\n  printf (\"# mpi %s nProc %d vlen %i niter %d nthreads %i nway %i time %g\\n\"\n         , argv[0], nProc, VLEN, NITER, NTHREADS, NWAY, time\n         );\n  \n  MPI_Finalize();\n\n  free (array);\n\n  return EXIT_SUCCESS;\n}"}
{"program": "cot_231", "code": "nt\nmain(int    argc,\n     char **argv)\n{\n\tint\tcomm_rank\t= -1;\n\tint\tcomm_size\t= -1;\n\tchar\thost_name[1024]\t= \"\";\n        int dest;\n        int len            = LEN;\n        int iterations     = LOOPS;\n\n\n\n        if (gethostname(host_name, 1023) < 0) {\n                perror(\"gethostname\");\n                exit(1);\n        }\n\n        if (comm_size != 2) {\n                fprintf(stderr, \"This program requires 2 MPI processes, aborting...\\n\");\n                goto out;\n        }\n\n        fprintf(stdout, \"(%s): My rank is %d\\n\", host_name, comm_rank);\n\n\tdest = (comm_rank+1)%2;\n\n        main_buffer = malloc(len);\n\tfill_buffer(main_buffer, len);\n\n\tint i;\n\n\tfor(i = 0; i< WARMUP; i++) {\n\t\tif(! comm_rank) {\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t} else {\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t}\n\t}\n\n\tdouble t1, t2;\n\tt1 =\n\tfor(i = 0; i< iterations; i++) {\n\t\tif(! comm_rank) {\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t} else {\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t}\n\t}\n\n\tt2 =\n\n\tif(! comm_rank) {\n\t\tdouble latency = 1e6*(t2-t1)/(2*iterations);\n\t\tprintf(\"%d\\t%d\\t%lf\\n\",iterations, len, latency);\n\t}\n out:\n\tfree(main_buffer);\n\n        return 0;\n}\n", "label": "nt\nmain(int    argc,\n     char **argv)\n{\n\tint\tcomm_rank\t= -1;\n\tint\tcomm_size\t= -1;\n\tchar\thost_name[1024]\t= \"\";\n        int dest;\n        int len            = LEN;\n        int iterations     = LOOPS;\n\n\tMPI_Init(&argc,&argv);\n\n        MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n        MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n        if (gethostname(host_name, 1023) < 0) {\n                perror(\"gethostname\");\n                exit(1);\n        }\n\n        if (comm_size != 2) {\n                fprintf(stderr, \"This program requires 2 MPI processes, aborting...\\n\");\n                goto out;\n        }\n\n        fprintf(stdout, \"(%s): My rank is %d\\n\", host_name, comm_rank);\n\n\tdest = (comm_rank+1)%2;\n\n        main_buffer = malloc(len);\n\tfill_buffer(main_buffer, len);\n\n\tint i;\n\n\tfor(i = 0; i< WARMUP; i++) {\n\t\tif(! comm_rank) {\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t} else {\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t}\n\t}\n\n\tdouble t1, t2;\n\tt1 = MPI_Wtime();\n        MPI_Barrier(MPI_COMM_WORLD);\n\tfor(i = 0; i< iterations; i++) {\n\t\tif(! comm_rank) {\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t} else {\n\t\t\tRECV(main_buffer, len, dest, 0);\n\t\t\tSEND(main_buffer, len, dest, 0);\n\t\t}\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tt2 = MPI_Wtime();\n\n\tif(! comm_rank) {\n\t\tdouble latency = 1e6*(t2-t1)/(2*iterations);\n\t\tprintf(\"%d\\t%d\\t%lf\\n\",iterations, len, latency);\n\t}\n out:\n\tfree(main_buffer);\n        MPI_Finalize();\n\n        return 0;\n}\n"}
{"program": "qingu_232", "code": "int main(int argc, char **argv)\n{\n    MPI_Comm c0, c1, ic;\n    MPI_Group g0, g1, gworld;\n    int a, b, c, d;\n    int rank, size, remote_leader, tag;\n    int ranks[2];\n    int errs = 0;\n\n    tag = 5;\n    c0 = c1 = ic = MPI_COMM_NULL;\n    g0 = g1 = gworld = MPI_GROUP_NULL;\n\n\n\n    if (size < 33) {\n        printf(\"ERROR: this test requires at least 33 processes\\n\");\n        return 1;\n    }\n\n    \n\n    a = 32;\n    b = 24;\n\n    \n\n    c = 25;\n    d = 26;\n\n\n    ranks[0] = a;\n    ranks[1] = b;\n\n    ranks[0] = c;\n    ranks[1] = d;\n\n    if (rank == a || rank == b) {\n        remote_leader = c;\n    }\n    else if (rank == c || rank == d) {\n        remote_leader = a;\n    }\n\n\n    if (c0 != MPI_COMM_NULL)\n    if (c1 != MPI_COMM_NULL)\n    if (ic != MPI_COMM_NULL)\n\n\n    if (rank == 0) {\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    MPI_Comm c0, c1, ic;\n    MPI_Group g0, g1, gworld;\n    int a, b, c, d;\n    int rank, size, remote_leader, tag;\n    int ranks[2];\n    int errs = 0;\n\n    tag = 5;\n    c0 = c1 = ic = MPI_COMM_NULL;\n    g0 = g1 = gworld = MPI_GROUP_NULL;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size < 33) {\n        printf(\"ERROR: this test requires at least 33 processes\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n        return 1;\n    }\n\n    \n\n    a = 32;\n    b = 24;\n\n    \n\n    c = 25;\n    d = 26;\n\n    MPI_Comm_group(MPI_COMM_WORLD, &gworld);\n\n    ranks[0] = a;\n    ranks[1] = b;\n    MPI_Group_incl(gworld, 2, ranks, &g0);\n    MPI_Comm_create(MPI_COMM_WORLD, g0, &c0);\n\n    ranks[0] = c;\n    ranks[1] = d;\n    MPI_Group_incl(gworld, 2, ranks, &g1);\n    MPI_Comm_create(MPI_COMM_WORLD, g1, &c1);\n\n    if (rank == a || rank == b) {\n        remote_leader = c;\n        MPI_Intercomm_create(c0, 0, MPI_COMM_WORLD, remote_leader, tag, &ic);\n    }\n    else if (rank == c || rank == d) {\n        remote_leader = a;\n        MPI_Intercomm_create(c1, 0, MPI_COMM_WORLD, remote_leader, tag, &ic);\n    }\n\n    MPI_Group_free(&g0);\n    MPI_Group_free(&g1);\n    MPI_Group_free(&gworld);\n\n    if (c0 != MPI_COMM_NULL)\n        MPI_Comm_free(&c0);\n    if (c1 != MPI_COMM_NULL)\n        MPI_Comm_free(&c1);\n    if (ic != MPI_COMM_NULL)\n        MPI_Comm_free(&ic);\n\n\n    MPI_Reduce((rank == 0 ? MPI_IN_PLACE : &errs), &errs,\n               1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "my-zhang_233", "code": "int main(int argc, char *argv[]) {\n\tint ID, P; \n\tint tag = 1;\n\tMPI_Status status;\n\n\n\tif (ID == 0) { \n\t\tdouble t = \n\t\tfor (int i = 0; i < M; i++) { \n\t\t}\n\t\tprintf(\"send/recv throughput %lf/s\\n\", \n\t\t\t\t(double)M / (MPI_Wtime() - t)); \n\t\tprintf(\"send/recv latency %lf/s\\n\", \n\t\t\t\t(MPI_Wtime() - t) / (double) M); \n\n  \t}\n\telse { \n\t\tfor (int i = 0; i < M; i++) { \n\t\t}\n\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tint ID, P; \n\tint tag = 1;\n\tMPI_Status status;\n\n\tMPI_Init (&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &ID);\t\n\tMPI_Comm_size(MPI_COMM_WORLD, &P); \n\n\tif (ID == 0) { \n\t\tdouble t = MPI_Wtime(); \n\t\tfor (int i = 0; i < M; i++) { \n\t\t\tMPI_Send(NULL, 0, MPI_INT, 1, tag, MPI_COMM_WORLD); \n\t\t\tMPI_Recv(NULL, 0, MPI_INT, 1, tag, \n\t\t\t\t\tMPI_COMM_WORLD, &status); \n\t\t}\n\t\tprintf(\"send/recv throughput %lf/s\\n\", \n\t\t\t\t(double)M / (MPI_Wtime() - t)); \n\t\tprintf(\"send/recv latency %lf/s\\n\", \n\t\t\t\t(MPI_Wtime() - t) / (double) M); \n\n  \t}\n\telse { \n\t\tfor (int i = 0; i < M; i++) { \n\t\t\tMPI_Recv(NULL, 0, MPI_INT, 0, tag, \n\t\t\t\t\tMPI_COMM_WORLD, &status); \n\t\t\tMPI_Send(NULL, 0, MPI_INT, 0, tag, MPI_COMM_WORLD); \n\t\t}\n\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "grantathon_234", "code": "int main(int argc, char *argv[])\n{\n    unsigned int dim_x = 80, dim_y = 40, time_steps = 80;\n\n\tif (argc > 1)\n\t\ttime_steps = strtoul(argv[1], NULL, 0);\n\n\tif (argc > 2)\n\t\tdim_x = strtoul(argv[2], NULL, 0);\n\n\tif (argc > 3)\n\t\tdim_y = strtoul(argv[3], NULL, 0);\n\n\tif(dim_x < 9 || dim_y < 9)\n\t{\n\t\tprintf(\"Invalid dim_x / dim_y!\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n    struct timespec begin, end;\n    unsigned char *grid = NULL;\n\n\n    if(comm_rank == 0)\n    {\n\t\tsize_t size = sizeof(unsigned char) * dim_x * dim_y;\n\t\tgrid = malloc(size);\n\n\t\tif(grid == NULL)\n\t\t\texit(EXIT_FAILURE);\n\n\t\tmemset(grid, 0, size);\n\n\t\tr_pentomino(grid, dim_x, dim_y, dim_x/2, dim_y/2);\n\n\t\tprintf(\"\\nGame of Life: time_steps = %u; dim_x = %u; dim_y = %u; processes = %d \\n\\n\", time_steps, dim_x, dim_y, comm_size);\n\n\t\tprint_gol(grid, dim_x, dim_y);\n\n\t\tprintf(\"\\n\\n\");\n\n\t\tclock_gettime(CLOCK_REALTIME, &begin);\n    }\n\n    unsigned int living_cells = gol(grid, dim_x, dim_y, time_steps);\n\n    if(comm_rank == 0)\n    {\n\t\tclock_gettime(CLOCK_REALTIME, &end);\n\n\t\tprint_gol(grid, dim_x, dim_y);\n\n\t\tprintf(\"Living Cells after %u time steps: %u\\n\", time_steps, living_cells);\n\n\t\tprintf(\"\\nProcessing Time: %.3lf seconds\\n\", ts_to_double(ts_diff(begin, end)));\n\n\t\tfree(grid);\n    }\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    unsigned int dim_x = 80, dim_y = 40, time_steps = 80;\n\n\tif (argc > 1)\n\t\ttime_steps = strtoul(argv[1], NULL, 0);\n\n\tif (argc > 2)\n\t\tdim_x = strtoul(argv[2], NULL, 0);\n\n\tif (argc > 3)\n\t\tdim_y = strtoul(argv[3], NULL, 0);\n\n\tif(dim_x < 9 || dim_y < 9)\n\t{\n\t\tprintf(\"Invalid dim_x / dim_y!\\n\");\n\t\texit(EXIT_FAILURE);\n\t}\n\n    struct timespec begin, end;\n    unsigned char *grid = NULL;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n    if(comm_rank == 0)\n    {\n\t\tsize_t size = sizeof(unsigned char) * dim_x * dim_y;\n\t\tgrid = malloc(size);\n\n\t\tif(grid == NULL)\n\t\t\texit(EXIT_FAILURE);\n\n\t\tmemset(grid, 0, size);\n\n\t\tr_pentomino(grid, dim_x, dim_y, dim_x/2, dim_y/2);\n\n\t\tprintf(\"\\nGame of Life: time_steps = %u; dim_x = %u; dim_y = %u; processes = %d \\n\\n\", time_steps, dim_x, dim_y, comm_size);\n\n\t\tprint_gol(grid, dim_x, dim_y);\n\n\t\tprintf(\"\\n\\n\");\n\n\t\tclock_gettime(CLOCK_REALTIME, &begin);\n    }\n\n    unsigned int living_cells = gol(grid, dim_x, dim_y, time_steps);\n\n    if(comm_rank == 0)\n    {\n\t\tclock_gettime(CLOCK_REALTIME, &end);\n\n\t\tprint_gol(grid, dim_x, dim_y);\n\n\t\tprintf(\"Living Cells after %u time steps: %u\\n\", time_steps, living_cells);\n\n\t\tprintf(\"\\nProcessing Time: %.3lf seconds\\n\", ts_to_double(ts_diff(begin, end)));\n\n\t\tfree(grid);\n    }\n\n    MPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "qingu_235", "code": "int main(int argc, char *argv[])\n{\n    int          rank;\n    int          errors = 0, all_errors = 0;\n    int          buf;\n    MPI_Win      win;\n\n\n\n\n    \n\n    CHECK_ERR(MPI_Win_unlock(0, win));\n\n\n\n    if (rank == 0 && all_errors == 0) printf(\" No Errors\\n\");\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int          rank;\n    int          errors = 0, all_errors = 0;\n    int          buf;\n    MPI_Win      win;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Win_create(&buf, sizeof(int), sizeof(int),\n                    MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n    MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);\n\n    \n\n    CHECK_ERR(MPI_Win_unlock(0, win));\n\n    MPI_Win_free(&win);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0) printf(\" No Errors\\n\");\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ClaudioNahmad_236", "code": "int main(int argc, char **argv)\n{\n    int iter, err, rank, size;\n    MPI_Comm comm, merged;\n\n    \n\n\n    printf(\"parent*******************************\\n\");\n    printf(\"parent: Launching MPI*\\n\");\n\n\n    for (iter = 0; iter < 100; ++iter) {\n        printf(\"parent: MPI_Comm_spawn #%d return : %d\\n\", iter, err);\n\n        printf(\"parent: MPI_Comm_spawn #%d rank %d, size %d\\n\",\n               iter, rank, size);\n    }\n\n    printf(\"parent: End .\\n\" );\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int iter, err, rank, size;\n    MPI_Comm comm, merged;\n\n    \n\n\n    printf(\"parent*******************************\\n\");\n    printf(\"parent: Launching MPI*\\n\");\n\n    MPI_Init( &argc, &argv);\n\n    for (iter = 0; iter < 100; ++iter) {\n        MPI_Comm_spawn(EXE_TEST, NULL, 1, MPI_INFO_NULL,\n                       0, MPI_COMM_WORLD, &comm, &err);\n        printf(\"parent: MPI_Comm_spawn #%d return : %d\\n\", iter, err);\n\n        MPI_Intercomm_merge(comm, 0, &merged);\n        MPI_Comm_rank(merged, &rank);\n        MPI_Comm_size(merged, &size);\n        printf(\"parent: MPI_Comm_spawn #%d rank %d, size %d\\n\",\n               iter, rank, size);\n        MPI_Comm_free(&merged);\n        MPI_Comm_disconnect(&comm);\n    }\n\n    MPI_Finalize();\n    printf(\"parent: End .\\n\" );\n    return 0;\n}"}
{"program": "gabrielivascu_240", "code": "int main (int argc, char **argv)\n{\n  int num_tasks, rank;\n  unsigned char *matrix = NULL;\n  unsigned char *initial = NULL;\n  unsigned char *buffer = NULL;\n  unsigned int size = 0;\n  unsigned int counter = 0;\n  unsigned int layer, num_layers, expected_layers;\n  boolean to_print = FALSE;\n  boolean to_check = FALSE;\n  struct timespec start, end;\n  double init_time, rotate_time;\n  MPI_Status status;\n\n  \n\n  if (argc < 2 || argc > 5) {\n    print_usage (argv[0]);\n    exit (1);\n  }\n\n  \n\n  parse_command_line_args (argc, argv, &size, &to_print, &to_check);\n\n\n  \n\n  if (rank == num_tasks - 1)\n    printf (\"Initializing %dx%d matrix...\\n\", size, size);\n  DIE (clock_gettime (CLOCK_MONOTONIC, &start) == -1, \"clock_gettime\");\n  matrix = matrix_generate (size);\n  DIE (clock_gettime (CLOCK_MONOTONIC, &end) == -1, \"clock_gettime\");\n\n  \n\n  init_time = end.tv_sec - start.tv_sec + (end.tv_nsec - start.tv_nsec) / 1000000000.0;\n\n  \n\n  buffer = malloc (size * 4 * sizeof (unsigned char));\n  DIE (buffer == NULL, \"malloc\");\n\n  num_layers = size / 2;\n  expected_layers = num_layers % 2 == 0 ? num_layers : num_layers - 1;\n\n  if (rank == num_tasks - 1) {\n    \n\n    if (to_print == TRUE) {\n      printf (\"\\nInitial matrix:\\n\");\n      matrix_print (matrix, size);\n    }\n\n    \n\n    if (to_check == TRUE)\n      initial = matrix_duplicate (matrix, size);\n\n    printf (\"\\nRotating matrix...\\n\");\n    DIE (clock_gettime (CLOCK_MONOTONIC, &start) == -1, \"clock_gettime\");\n\n    \n\n    while (TRUE) {\n      if (counter == expected_layers)\n        break;\n\n      matrix_set_layer (matrix, size, status.MPI_TAG, buffer);\n      counter++;\n    }\n\n    \n\n    if (num_layers % 2 == 1) {\n      matrix_get_rotated_layer (matrix, size, num_layers / 2, buffer);\n      matrix_set_layer (matrix, size, num_layers / 2, buffer);\n    }\n\n    DIE (clock_gettime (CLOCK_MONOTONIC, &end) == -1, \"clock_gettime\");\n\n    \n\n    rotate_time = end.tv_sec - start.tv_sec + (end.tv_nsec - start.tv_nsec) / 1000000000.0;\n\n    \n\n    if (to_print == TRUE) {\n      printf (\"\\nRotated matrix:\\n\");\n      matrix_print (matrix, size);\n    }\n\n    \n\n    if (to_check == TRUE) {\n      printf (\"\\nChecking correctness...\\n\");\n      if (matrix_is_rotation (matrix, initial, size) == TRUE)\n        printf (\"\\nThe rotated matrix is correct.\\n\");\n      else\n        printf (\"\\nThe rotated matrix is NOT correct.\\n\");\n    }\n\n    \n\n    printf (\"\\nInit time: %lf\\n\", init_time);\n    printf (\"Rotate time: %lf\\n\", rotate_time);\n  } else {\n    DIE (clock_gettime (CLOCK_MONOTONIC, &start) == -1, \"clock_gettime\");\n\n    \n\n    for (layer = rank; layer < num_layers / 2; layer += num_tasks - 1) {\n      matrix_get_rotated_layer (matrix, size, layer, buffer);\n\n      matrix_get_rotated_layer (matrix, size, num_layers - 1 - layer, buffer);\n    }\n\n    DIE (clock_gettime (CLOCK_MONOTONIC, &end) == -1, \"clock_gettime\");\n    rotate_time = end.tv_sec - start.tv_sec + (end.tv_nsec - start.tv_nsec) / 1000000000.0;\n    printf(\"[%d] My rotate time: %lf\\n\", rank, rotate_time);\n  }\n\n\n  free (matrix);\n  free (initial);\n  free (buffer);\n\n  return 0;\n}", "label": "int main (int argc, char **argv)\n{\n  int num_tasks, rank;\n  unsigned char *matrix = NULL;\n  unsigned char *initial = NULL;\n  unsigned char *buffer = NULL;\n  unsigned int size = 0;\n  unsigned int counter = 0;\n  unsigned int layer, num_layers, expected_layers;\n  boolean to_print = FALSE;\n  boolean to_check = FALSE;\n  struct timespec start, end;\n  double init_time, rotate_time;\n  MPI_Status status;\n\n  \n\n  if (argc < 2 || argc > 5) {\n    print_usage (argv[0]);\n    exit (1);\n  }\n\n  \n\n  parse_command_line_args (argc, argv, &size, &to_print, &to_check);\n\n  MPI_Init (&argc, &argv);\n  MPI_Comm_size (MPI_COMM_WORLD, &num_tasks);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\n  \n\n  if (rank == num_tasks - 1)\n    printf (\"Initializing %dx%d matrix...\\n\", size, size);\n  DIE (clock_gettime (CLOCK_MONOTONIC, &start) == -1, \"clock_gettime\");\n  matrix = matrix_generate (size);\n  DIE (clock_gettime (CLOCK_MONOTONIC, &end) == -1, \"clock_gettime\");\n\n  \n\n  init_time = end.tv_sec - start.tv_sec + (end.tv_nsec - start.tv_nsec) / 1000000000.0;\n\n  \n\n  buffer = malloc (size * 4 * sizeof (unsigned char));\n  DIE (buffer == NULL, \"malloc\");\n\n  num_layers = size / 2;\n  expected_layers = num_layers % 2 == 0 ? num_layers : num_layers - 1;\n\n  if (rank == num_tasks - 1) {\n    \n\n    if (to_print == TRUE) {\n      printf (\"\\nInitial matrix:\\n\");\n      matrix_print (matrix, size);\n    }\n\n    \n\n    if (to_check == TRUE)\n      initial = matrix_duplicate (matrix, size);\n\n    printf (\"\\nRotating matrix...\\n\");\n    DIE (clock_gettime (CLOCK_MONOTONIC, &start) == -1, \"clock_gettime\");\n\n    \n\n    while (TRUE) {\n      if (counter == expected_layers)\n        break;\n\n      MPI_Recv (buffer, size * 4, MPI_UNSIGNED_CHAR, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n      matrix_set_layer (matrix, size, status.MPI_TAG, buffer);\n      counter++;\n    }\n\n    \n\n    if (num_layers % 2 == 1) {\n      matrix_get_rotated_layer (matrix, size, num_layers / 2, buffer);\n      matrix_set_layer (matrix, size, num_layers / 2, buffer);\n    }\n\n    DIE (clock_gettime (CLOCK_MONOTONIC, &end) == -1, \"clock_gettime\");\n\n    \n\n    rotate_time = end.tv_sec - start.tv_sec + (end.tv_nsec - start.tv_nsec) / 1000000000.0;\n\n    \n\n    if (to_print == TRUE) {\n      printf (\"\\nRotated matrix:\\n\");\n      matrix_print (matrix, size);\n    }\n\n    \n\n    if (to_check == TRUE) {\n      printf (\"\\nChecking correctness...\\n\");\n      if (matrix_is_rotation (matrix, initial, size) == TRUE)\n        printf (\"\\nThe rotated matrix is correct.\\n\");\n      else\n        printf (\"\\nThe rotated matrix is NOT correct.\\n\");\n    }\n\n    \n\n    printf (\"\\nInit time: %lf\\n\", init_time);\n    printf (\"Rotate time: %lf\\n\", rotate_time);\n  } else {\n    DIE (clock_gettime (CLOCK_MONOTONIC, &start) == -1, \"clock_gettime\");\n\n    \n\n    for (layer = rank; layer < num_layers / 2; layer += num_tasks - 1) {\n      matrix_get_rotated_layer (matrix, size, layer, buffer);\n      MPI_Send (buffer, size * 4, MPI_UNSIGNED_CHAR, num_tasks - 1, layer, MPI_COMM_WORLD);\n\n      matrix_get_rotated_layer (matrix, size, num_layers - 1 - layer, buffer);\n      MPI_Send (buffer, size * 4, MPI_UNSIGNED_CHAR, num_tasks - 1, num_layers - 1 - layer, MPI_COMM_WORLD);\n    }\n\n    DIE (clock_gettime (CLOCK_MONOTONIC, &end) == -1, \"clock_gettime\");\n    rotate_time = end.tv_sec - start.tv_sec + (end.tv_nsec - start.tv_nsec) / 1000000000.0;\n    printf(\"[%d] My rotate time: %lf\\n\", rank, rotate_time);\n  }\n\n  MPI_Finalize ();\n\n  free (matrix);\n  free (initial);\n  free (buffer);\n\n  return 0;\n}"}
{"program": "ElofssonLab_242", "code": "int\nmain(int argc, char **argv)\n{\n  int              status   = eslOK;\n\n  ESL_GETOPTS     *go  = NULL;\t\n\n  struct cfg_s     cfg;         \n\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.qfile      = NULL;\n  cfg.dbfile     = NULL;\n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n  cfg.firstseq_key = NULL;\n  cfg.n_targetseq  = -1;\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.qfile, &cfg.dbfile);    \n\n\n  \n\n#ifndef eslAUGMENT_SSI\n  if (esl_opt_IsUsed(go, \"--restrictdb_stkey\") || esl_opt_IsUsed(go, \"--restrictdb_n\")  || esl_opt_IsUsed(go, \"--ssifile\")  )\n    p7_Fail(\"Unable to use range-control options unless an SSI index file is available. See 'esl_sfetch --index'\\n\");\n#else\n  if (esl_opt_IsUsed(go, \"--restrictdb_stkey\") )\n    if ((cfg.firstseq_key = esl_opt_GetString(go, \"--restrictdb_stkey\")) == NULL)  p7_Fail(\"Failure capturing --restrictdb_stkey\\n\");\n\n  if (esl_opt_IsUsed(go, \"--restrictdb_n\") )\n    cfg.n_targetseq = esl_opt_GetInteger(go, \"--restrictdb_n\");\n\n  if ( cfg.n_targetseq != -1 && cfg.n_targetseq < 1 )\n    p7_Fail(\"--restrictdb_n must be >= 1\\n\");\n#endif\n\n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  int              status   = eslOK;\n\n  ESL_GETOPTS     *go  = NULL;\t\n\n  struct cfg_s     cfg;         \n\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.qfile      = NULL;\n  cfg.dbfile     = NULL;\n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n  cfg.firstseq_key = NULL;\n  cfg.n_targetseq  = -1;\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.qfile, &cfg.dbfile);    \n\n\n  \n\n#ifndef eslAUGMENT_SSI\n  if (esl_opt_IsUsed(go, \"--restrictdb_stkey\") || esl_opt_IsUsed(go, \"--restrictdb_n\")  || esl_opt_IsUsed(go, \"--ssifile\")  )\n    p7_Fail(\"Unable to use range-control options unless an SSI index file is available. See 'esl_sfetch --index'\\n\");\n#else\n  if (esl_opt_IsUsed(go, \"--restrictdb_stkey\") )\n    if ((cfg.firstseq_key = esl_opt_GetString(go, \"--restrictdb_stkey\")) == NULL)  p7_Fail(\"Failure capturing --restrictdb_stkey\\n\");\n\n  if (esl_opt_IsUsed(go, \"--restrictdb_n\") )\n    cfg.n_targetseq = esl_opt_GetInteger(go, \"--restrictdb_n\");\n\n  if ( cfg.n_targetseq != -1 && cfg.n_targetseq < 1 )\n    p7_Fail(\"--restrictdb_n must be >= 1\\n\");\n#endif\n\n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n      MPI_Init(&argc, &argv);\n      MPI_Comm_rank(MPI_COMM_WORLD, &(cfg.my_rank));\n      MPI_Comm_size(MPI_COMM_WORLD, &(cfg.nproc));\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n      MPI_Finalize();\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}"}
{"program": "softwarejimenez_243", "code": "int main (int argc, char ** argv) {\n    int xsize=0, ysize=0, colmax=0,size=0;\n    pixel * src_total;\n    src_total= (pixel *) calloc ( MAX_PIXELS ,sizeof(pixel));\n    struct timespec stime, etime;\n\n    int rank, np;\n\n\n\n    MPI_Comm com = MPI_COMM_WORLD;\n\n    \n\n\n    if (rank==0){\n        if (argc != 3) {\n        \tfprintf(stderr, \"Usage: %s infile outfile\\n\", argv[0]);\n        \texit(1);\n        }\n\n        \n\n        if(read_ppm (argv[1], &xsize, &ysize, &colmax, (char *) src_total) != 0){\n        \tfprintf(stderr, \"Error reading file\\n\");\n        \texit(1);\n        }\n        size=xsize*ysize;\n        if (colmax > 255) {\n        \tfprintf(stderr, \"Too large maximum color-component value\\n\");\n        \texit(1);\n        }\n        \n\n        clock_gettime(CLOCK_REALTIME, &stime);\n    }\n    thresfilter(size, src_total);\n    if (rank==0){\n        clock_gettime(CLOCK_REALTIME, &etime);\n        printf(\"Filtering took: %g secs\\n\", (etime.tv_sec  - stime.tv_sec) +1e-9*(etime.tv_nsec  - stime.tv_nsec)) ;\n\n        \n\n        \n\n\n        if(write_ppm (argv[2], xsize, ysize, (char *)src_total) != 0){\n        \tfprintf(stderr, \"Error writing file\\n\");\n        \texit(1);\n        }\n    }\n    free (src_total);\n    return(0);\n}", "label": "int main (int argc, char ** argv) {\n    int xsize=0, ysize=0, colmax=0,size=0;\n    pixel * src_total;\n    src_total= (pixel *) calloc ( MAX_PIXELS ,sizeof(pixel));\n    struct timespec stime, etime;\n\n    int rank, np;\n    MPI_Init (&argc, &argv);      \n\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);        \n\n    MPI_Comm_size (MPI_COMM_WORLD, &np);        \n\n    MPI_Comm com = MPI_COMM_WORLD;\n\n    \n\n\n    if (rank==0){\n        if (argc != 3) {\n        \tfprintf(stderr, \"Usage: %s infile outfile\\n\", argv[0]);\n        \texit(1);\n        }\n\n        \n\n        if(read_ppm (argv[1], &xsize, &ysize, &colmax, (char *) src_total) != 0){\n        \tfprintf(stderr, \"Error reading file\\n\");\n        \texit(1);\n        }\n        size=xsize*ysize;\n        if (colmax > 255) {\n        \tfprintf(stderr, \"Too large maximum color-component value\\n\");\n        \texit(1);\n        }\n        \n\n        clock_gettime(CLOCK_REALTIME, &stime);\n    }\n    thresfilter(size, src_total);\n    if (rank==0){\n        clock_gettime(CLOCK_REALTIME, &etime);\n        printf(\"Filtering took: %g secs\\n\", (etime.tv_sec  - stime.tv_sec) +1e-9*(etime.tv_nsec  - stime.tv_nsec)) ;\n\n        \n\n        \n\n\n        if(write_ppm (argv[2], xsize, ysize, (char *)src_total) != 0){\n        \tfprintf(stderr, \"Error writing file\\n\");\n        \texit(1);\n        }\n    }\n    free (src_total);\n    MPI_Finalize();\n    return(0);\n}"}
{"program": "teuben_244", "code": "int main(int argc, char* argv[]){\n  int my_rank;\n  int p;\n  int source;\n  int dest;\n  int tag=0;\n  char message[100];\n  char my_name[20];\n  MPI_Status status;\n\n  \n\n  \n  \n\n  \n  \n\n\n  \n\n  gethostname(my_name, 20);    \n\n  \n\n  sleep(15);\n\n  if (my_rank == 0) {\n    printf(\"MPIHello running on %i processors.\\n\", p);\n    printf(\"Greetings from processor %i, on host %s.\\n\", my_rank, my_name);\n    for (source=1; source<p; source++) {\n      printf(\"%s\", message);\n    }\n  } else if (my_rank != 0) {\n    sprintf(message, \"Greetings from processor %i, on host %s.\\n\", my_rank, my_name);\n    dest=0;\n  }\n  \n\n  sleep(15);\n}", "label": "int main(int argc, char* argv[]){\n  int my_rank;\n  int p;\n  int source;\n  int dest;\n  int tag=0;\n  char message[100];\n  char my_name[20];\n  MPI_Status status;\n\n  \n\n  MPI_Init(&argc, &argv);\n  \n  \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  \n  \n\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  \n\n  gethostname(my_name, 20);    \n\n  \n\n  sleep(15);\n\n  if (my_rank == 0) {\n    printf(\"MPIHello running on %i processors.\\n\", p);\n    printf(\"Greetings from processor %i, on host %s.\\n\", my_rank, my_name);\n    for (source=1; source<p; source++) {\n      MPI_Recv(message, 100, MPI_CHAR, source, tag, MPI_COMM_WORLD, &status);\n      printf(\"%s\", message);\n    }\n  } else if (my_rank != 0) {\n    sprintf(message, \"Greetings from processor %i, on host %s.\\n\", my_rank, my_name);\n    dest=0;\n    MPI_Send(message, strlen(message)+1, MPI_CHAR, dest, tag, MPI_COMM_WORLD); \n  }\n  \n\n  sleep(15);\n  MPI_Finalize();\n}"}
{"program": "gromgull_245", "code": "int main(int argc, char **argv)\n{\n   time_t startTime;\n   time_t endTime;\n   Parameters *parameters;\n   int processRank;\n   int numProcesses;\n\n   \n\n\n   if (processRank == 0) \n   { \n\n      startTime = time(NULL);\n      printf(\"MPI SUBDUE %s\\n\\n\", SUBDUE_VERSION);\n      parameters = GetParameters(argc, argv, processRank);\n      parameters->numPartitions = numProcesses - 1;\n      PrintParameters(parameters);\n      SubdueMaster(parameters);\n      FreeParameters(parameters);\n      endTime = time(NULL);\n      printf(\"SUBDUE done (elapsed time = %lu seconds).\\n\",\n             (endTime - startTime));\n   } \n   else \n   { \n\n      parameters = GetParameters(argc, argv, processRank);\n      parameters->numPartitions = numProcesses - 1;\n      SubdueChild(parameters);\n      FreeParameters(parameters);\n   }\n\n   \n\n\n   return 0;\n}", "label": "int main(int argc, char **argv)\n{\n   time_t startTime;\n   time_t endTime;\n   Parameters *parameters;\n   int processRank;\n   int numProcesses;\n\n   \n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &processRank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n   if (processRank == 0) \n   { \n\n      startTime = time(NULL);\n      printf(\"MPI SUBDUE %s\\n\\n\", SUBDUE_VERSION);\n      parameters = GetParameters(argc, argv, processRank);\n      parameters->numPartitions = numProcesses - 1;\n      PrintParameters(parameters);\n      SubdueMaster(parameters);\n      FreeParameters(parameters);\n      endTime = time(NULL);\n      printf(\"SUBDUE done (elapsed time = %lu seconds).\\n\",\n             (endTime - startTime));\n   } \n   else \n   { \n\n      parameters = GetParameters(argc, argv, processRank);\n      parameters->numPartitions = numProcesses - 1;\n      SubdueChild(parameters);\n      FreeParameters(parameters);\n   }\n\n   \n\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "syftalent_246", "code": "int main(int argc, char **argv) {\n    int my_rank, shared_rank;\n    void *mybase = NULL;\n    MPI_Win win;\n    MPI_Info win_info;\n    MPI_Comm shared_comm;\n    int i;\n    int shm_win_size = 1024 * 1024 * 1024 * sizeof(char); \n\n\n\n\n    for (i = 0; i < 2; i++) {\n        if (i == 0) {\n        }\n        else {\n            win_info = MPI_INFO_NULL;\n        }\n\n\n\n        \n\n\n\n\n\n        \n\n        if (my_rank % 2 == 0)\n        else\n\n\n        if (shared_rank % 2 == 0)\n        else\n\n\n        \n\n        if (my_rank % 2 == 0)\n        else\n\n\n        \n\n        if (shared_rank % 2 == 0)\n        else\n\n\n\n        if (i == 0)\n    }\n\n    if (my_rank == 0)\n        printf(\" No Errors\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int my_rank, shared_rank;\n    void *mybase = NULL;\n    MPI_Win win;\n    MPI_Info win_info;\n    MPI_Comm shared_comm;\n    int i;\n    int shm_win_size = 1024 * 1024 * 1024 * sizeof(char); \n\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    for (i = 0; i < 2; i++) {\n        if (i == 0) {\n            MPI_Info_create(&win_info);\n            MPI_Info_set(win_info, (char*)\"alloc_shm\", (char*)\"true\");\n        }\n        else {\n            win_info = MPI_INFO_NULL;\n        }\n\n        MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, my_rank, MPI_INFO_NULL, &shared_comm);\n\n        MPI_Comm_rank(shared_comm, &shared_rank);\n\n        \n\n        MPI_Win_allocate(shm_win_size, sizeof(char), win_info, MPI_COMM_WORLD, &mybase, &win);\n\n        MPI_Win_free(&win);\n\n        MPI_Win_allocate_shared(shm_win_size, sizeof(char), win_info, shared_comm, &mybase, &win);\n\n        MPI_Win_free(&win);\n\n        \n\n        if (my_rank % 2 == 0)\n            MPI_Win_allocate(shm_win_size, sizeof(char), win_info, MPI_COMM_WORLD, &mybase, &win);\n        else\n            MPI_Win_allocate(0, sizeof(char), win_info, MPI_COMM_WORLD, &mybase, &win);\n\n        MPI_Win_free(&win);\n\n        if (shared_rank % 2 == 0)\n            MPI_Win_allocate_shared(shm_win_size, sizeof(char), win_info, shared_comm, &mybase, &win);\n        else\n            MPI_Win_allocate_shared(0, sizeof(char), win_info, shared_comm, &mybase, &win);\n\n        MPI_Win_free(&win);\n\n        \n\n        if (my_rank % 2 == 0)\n            MPI_Win_allocate(shm_win_size, sizeof(char), win_info, MPI_COMM_WORLD, &mybase, &win);\n        else\n            MPI_Win_allocate(shm_win_size/2, sizeof(char), win_info, MPI_COMM_WORLD, &mybase, &win);\n\n        MPI_Win_free(&win);\n\n        \n\n        if (shared_rank % 2 == 0)\n            MPI_Win_allocate_shared(shm_win_size, sizeof(char), win_info, shared_comm, &mybase, &win);\n        else\n            MPI_Win_allocate_shared(shm_win_size/2, sizeof(char), win_info, shared_comm, &mybase, &win);\n\n        MPI_Win_free(&win);\n\n        MPI_Comm_free(&shared_comm);\n\n        if (i == 0)\n            MPI_Info_free(&win_info);\n    }\n\n    if (my_rank == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "pkrusche_249", "code": "int main(int argc, char **argv){\n\n    int nloc(int p, int s, int n);\n    void mpilu(int M, int N, int s, int t, int n, int *pi, double **a);\n    int p, pid, M, N, s, t, n, nlr, nlc, i, j, iglob, jglob, *pi;\n    double **a, time0, time1;\n\n\n\n    if (pid==0){\n        printf(\"Please enter number of processor rows M:\\n\");\n\t\tfflush(stdout);\n        scanf(\"%d\",&M);\n        printf(\"Please enter number of processor columns N:\\n\");\n\t\tfflush(stdout);\n        scanf(\"%d\",&N);\n        if (M*N != p)\n        printf(\"Please enter matrix size n:\\n\");\n\t\tfflush(stdout);\n        scanf(\"%d\",&n);\n    }\n\n    \n\n    s= pid%M;  \n\n    t= pid/M;  \n\n\n    \n\n    nlr=  nloc(M,s,n); \n\n    nlc=  nloc(N,t,n); \n\n    a= matallocd(nlr,nlc);\n    pi= vecalloci(nlr);\n  \n    if (s==0 && t==0){\n        printf(\"LU decomposition of %d by %d matrix\\n\",n,n);\n        printf(\"using the %d by %d cyclic distribution\\n\",M,N);\n    }\n    for (i=0; i<nlr; i++){\n        iglob= i*M+s;         \n\n        iglob= (iglob-1+n)%n; \n\n        for (j=0; j<nlc; j++){\n            jglob= j*N+t;     \n\n            a[i][j]= (iglob<=jglob ? 0.5*iglob+1 : 0.5*(jglob+1) );\n        }\n    }\n  \n    if (s==0 && t==0)\n        printf(\"Start of LU decomposition\\n\");\n\n \n    mpilu(M,N,s,t,n,pi,a);\n\n    if (s==0 && t==0){\n        printf(\"End of LU decomposition\\n\");\n        printf(\"This took only %.6lf seconds.\\n\", time1-time0);\n        printf(\"\\nThe output permutation is:\\n\"); fflush(stdout);\n    }\n\n    if (t==0){\n        for (i=0; i<nlr; i++){\n            iglob=i*M+s;\n            printf(\"i=%d, pi=%d, proc=(%d,%d)\\n\",iglob,pi[i],s,t);\n        }\n        fflush(stdout);\n    }\n\n    if (s==0 && t==0){  \n        printf(\"\\nThe output matrix is:\\n\"); fflush(stdout);\n    }   \n    for (i=0; i<nlr; i++){\n        iglob=i*M+s;\n        for (j=0; j<nlc; j++){\n            jglob=j*N+t;\n            printf(\"i=%d, j=%d, a=%f, proc=(%d,%d)\\n\",\n                   iglob,jglob,a[i][j],s,t);\n        }\n    }\n\n    vecfreei(pi);\n    matfreed(a);\n\n    exit(0);\n\n}", "label": "int main(int argc, char **argv){\n\n    int nloc(int p, int s, int n);\n    void mpilu(int M, int N, int s, int t, int n, int *pi, double **a);\n    int p, pid, M, N, s, t, n, nlr, nlc, i, j, iglob, jglob, *pi;\n    double **a, time0, time1;\n\n    MPI_Init(&argc,&argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD,&p); \n    MPI_Comm_rank(MPI_COMM_WORLD,&pid); \n\n    if (pid==0){\n        printf(\"Please enter number of processor rows M:\\n\");\n\t\tfflush(stdout);\n        scanf(\"%d\",&M);\n        printf(\"Please enter number of processor columns N:\\n\");\n\t\tfflush(stdout);\n        scanf(\"%d\",&N);\n        if (M*N != p)\n            MPI_Abort(MPI_COMM_WORLD,-5);\n        printf(\"Please enter matrix size n:\\n\");\n\t\tfflush(stdout);\n        scanf(\"%d\",&n);\n    }\n    MPI_Bcast(&M,1,MPI_INT,0,MPI_COMM_WORLD);\n    MPI_Bcast(&N,1,MPI_INT,0,MPI_COMM_WORLD);\n    MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);   \n\n    \n\n    s= pid%M;  \n\n    t= pid/M;  \n\n\n    \n\n    nlr=  nloc(M,s,n); \n\n    nlc=  nloc(N,t,n); \n\n    a= matallocd(nlr,nlc);\n    pi= vecalloci(nlr);\n  \n    if (s==0 && t==0){\n        printf(\"LU decomposition of %d by %d matrix\\n\",n,n);\n        printf(\"using the %d by %d cyclic distribution\\n\",M,N);\n    }\n    for (i=0; i<nlr; i++){\n        iglob= i*M+s;         \n\n        iglob= (iglob-1+n)%n; \n\n        for (j=0; j<nlc; j++){\n            jglob= j*N+t;     \n\n            a[i][j]= (iglob<=jglob ? 0.5*iglob+1 : 0.5*(jglob+1) );\n        }\n    }\n  \n    if (s==0 && t==0)\n        printf(\"Start of LU decomposition\\n\");\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    time0=MPI_Wtime();\n \n    mpilu(M,N,s,t,n,pi,a);\n    MPI_Barrier(MPI_COMM_WORLD);\n    time1=MPI_Wtime();\n\n    if (s==0 && t==0){\n        printf(\"End of LU decomposition\\n\");\n        printf(\"This took only %.6lf seconds.\\n\", time1-time0);\n        printf(\"\\nThe output permutation is:\\n\"); fflush(stdout);\n    }\n\n    if (t==0){\n        for (i=0; i<nlr; i++){\n            iglob=i*M+s;\n            printf(\"i=%d, pi=%d, proc=(%d,%d)\\n\",iglob,pi[i],s,t);\n        }\n        fflush(stdout);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (s==0 && t==0){  \n        printf(\"\\nThe output matrix is:\\n\"); fflush(stdout);\n    }   \n    for (i=0; i<nlr; i++){\n        iglob=i*M+s;\n        for (j=0; j<nlc; j++){\n            jglob=j*N+t;\n            printf(\"i=%d, j=%d, a=%f, proc=(%d,%d)\\n\",\n                   iglob,jglob,a[i][j],s,t);\n        }\n    }\n\n    vecfreei(pi);\n    matfreed(a);\n    MPI_Finalize();\n\n    exit(0);\n\n}"}
{"program": "qingu_250", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = builtin_float_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in builtin float test.\\n\", \n\t\t\t\terr);\n    errs += err;\n\n    err = vector_of_vectors_test();\n    if (err && verbose) fprintf(stderr, \n\t\t\t\t\"%d errors in vector of vectors test.\\n\", err);\n    errs += err;\n\n    err = optimizable_vector_of_basics_test();\n    if (err && verbose) fprintf(stderr, \n\t\t\t\t\"%d errors in vector of basics test.\\n\", err);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = builtin_float_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in builtin float test.\\n\", \n\t\t\t\terr);\n    errs += err;\n\n    err = vector_of_vectors_test();\n    if (err && verbose) fprintf(stderr, \n\t\t\t\t\"%d errors in vector of vectors test.\\n\", err);\n    errs += err;\n\n    err = optimizable_vector_of_basics_test();\n    if (err && verbose) fprintf(stderr, \n\t\t\t\t\"%d errors in vector of basics test.\\n\", err);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "NYU-CAL_255", "code": "int main( int argc , char * argv[] ){\n \n#if USE_MPI\n#endif\n   struct domain theDomain = {0};\n   struct profiler prof;\n   theDomain.prof = &prof;\n   start_clock( &theDomain ); \n   read_par_file( &theDomain );\n  \n   if(theDomain.rank==0)\n      print_welcome();\n\n   int error = mpiSetup(&theDomain,argc,argv);\n   if( error==1 ) return(0);\n\n   if(theDomain.rank==0) remove(\"abort\");\n\n   setupGrid( &theDomain );   \n   setupDomain( &theDomain );\n \n   setupCells( &theDomain );\n\n\n\n   if( theDomain.Nr > 1 ){\n      exchangeData( &theDomain , 0 );\n      if( !theDomain.theParList.R_Periodic)\n         boundary_trans( &theDomain , 1);\n   }\n   if( theDomain.Nz > 1 ){\n      exchangeData( &theDomain , 1 );\n      if( !theDomain.theParList.Z_Periodic)\n         boundary_trans( &theDomain , 2);\n   }\n\n   if( theDomain.rank==0 && !(theDomain.theParList.restart_flag) ){\n      FILE * rFile = fopen(\"report.dat\",\"w\");\n      fclose(rFile);\n   }\n\n   while( !(theDomain.final_step) ){\n      \n      prof_tick(&prof, PROF_DT);\n      double dt = getmindt( &theDomain );\n      check_dt( &theDomain , &dt );\n      prof_tock(&prof, PROF_DT);\n      \n      prof_tick(&prof, PROF_OUTPUT);\n      possiblyOutput( &theDomain , 0 );\n      prof_tock(&prof, PROF_OUTPUT);\n      \n      prof_tick(&prof, PROF_TIMESTEP);\n      timestep( &theDomain , dt );\n      prof_tock(&prof, PROF_TIMESTEP);\n   }\n\n   possiblyOutput( &theDomain , 1 );\n   generate_log( &theDomain );\n#if USE_MPI\n#endif\n   freeDomain( &theDomain );\n#if USE_MPI\n#endif\n\n   return(0);\n\n}", "label": "int main( int argc , char * argv[] ){\n \n#if USE_MPI\n   MPI_Init(&argc,&argv);\n#endif\n   struct domain theDomain = {0};\n   struct profiler prof;\n   theDomain.prof = &prof;\n   start_clock( &theDomain ); \n   read_par_file( &theDomain );\n  \n   if(theDomain.rank==0)\n      print_welcome();\n\n   int error = mpiSetup(&theDomain,argc,argv);\n   if( error==1 ) return(0);\n\n   if(theDomain.rank==0) remove(\"abort\");\n\n   setupGrid( &theDomain );   \n   setupDomain( &theDomain );\n \n   setupCells( &theDomain );\n\n\n\n   if( theDomain.Nr > 1 ){\n      exchangeData( &theDomain , 0 );\n      if( !theDomain.theParList.R_Periodic)\n         boundary_trans( &theDomain , 1);\n   }\n   if( theDomain.Nz > 1 ){\n      exchangeData( &theDomain , 1 );\n      if( !theDomain.theParList.Z_Periodic)\n         boundary_trans( &theDomain , 2);\n   }\n\n   if( theDomain.rank==0 && !(theDomain.theParList.restart_flag) ){\n      FILE * rFile = fopen(\"report.dat\",\"w\");\n      fclose(rFile);\n   }\n\n   while( !(theDomain.final_step) ){\n      \n      prof_tick(&prof, PROF_DT);\n      double dt = getmindt( &theDomain );\n      check_dt( &theDomain , &dt );\n      prof_tock(&prof, PROF_DT);\n      \n      prof_tick(&prof, PROF_OUTPUT);\n      possiblyOutput( &theDomain , 0 );\n      prof_tock(&prof, PROF_OUTPUT);\n      \n      prof_tick(&prof, PROF_TIMESTEP);\n      timestep( &theDomain , dt );\n      prof_tock(&prof, PROF_TIMESTEP);\n   }\n\n   possiblyOutput( &theDomain , 1 );\n   generate_log( &theDomain );\n#if USE_MPI\n   MPI_Barrier(theDomain.theComm);\n#endif\n   freeDomain( &theDomain );\n#if USE_MPI\n   MPI_Finalize();\n#endif\n\n   return(0);\n\n}"}
{"program": "cmcantalupo_256", "code": "int main(int argc, char **argv)\n{\n    int size = 0;\n    int rank = 0;\n\n    int err =\n    if (!err) {\n        err =\n    }\n    if (!err) {\n        err =\n    }\n    if (!err && !rank) {\n        printf(\"MPI_COMM_WORLD size: %d\\n\", size);\n    }\n\n    int num_iter = 10;\n    double stream_big_o = 1.0;\n\n    if (!rank) {\n        printf(\"Beginning loop of %d iterations.\\n\", num_iter);\n        fflush(stdout);\n    }\n    for (int i = 0; !err && i < num_iter; ++i) {\n        err = geopm_prof_epoch();\n        if (!err) {\n            err = tutorial_stream_profiled(stream_big_o, 0);\n        }\n        if (!err) {\n            err =\n        }\n        if (!err && !rank) {\n            printf(\"Iteration=%.3d\\r\", i);\n            fflush(stdout);\n        }\n    }\n    if (!err && !rank) {\n        printf(\"Completed loop.                    \\n\");\n        fflush(stdout);\n    }\n\n    int err_fin =\n    err = err ? err : err_fin;\n\n    return err;\n}", "label": "int main(int argc, char **argv)\n{\n    int size = 0;\n    int rank = 0;\n\n    int err = MPI_Init(&argc, &argv);\n    if (!err) {\n        err = MPI_Comm_size(MPI_COMM_WORLD, &size);\n    }\n    if (!err) {\n        err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    }\n    if (!err && !rank) {\n        printf(\"MPI_COMM_WORLD size: %d\\n\", size);\n    }\n\n    int num_iter = 10;\n    double stream_big_o = 1.0;\n\n    if (!rank) {\n        printf(\"Beginning loop of %d iterations.\\n\", num_iter);\n        fflush(stdout);\n    }\n    for (int i = 0; !err && i < num_iter; ++i) {\n        err = geopm_prof_epoch();\n        if (!err) {\n            err = tutorial_stream_profiled(stream_big_o, 0);\n        }\n        if (!err) {\n            err = MPI_Barrier(MPI_COMM_WORLD);\n        }\n        if (!err && !rank) {\n            printf(\"Iteration=%.3d\\r\", i);\n            fflush(stdout);\n        }\n    }\n    if (!err && !rank) {\n        printf(\"Completed loop.                    \\n\");\n        fflush(stdout);\n    }\n\n    int err_fin = MPI_Finalize();\n    err = err ? err : err_fin;\n\n    return err;\n}"}
{"program": "germasch_258", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n\n  \n\n  mrc_domain_set_param_int3(domain, \"m\", (int [3]) { 160, 1, 1 });\n  mrc_domain_set_param_int(domain, \"bcx\", BC_PERIODIC);\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  mrc_crds_set_param_int(crds, \"sw\", BND);\n  mrc_crds_set_param_double3(crds, \"l\", (double[3]) { -8., 0., 0. });\n  mrc_crds_set_param_double3(crds, \"h\", (double[3]) {  8., 1., 1. });\n\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n\n  struct mrc_fld *x = mrc_domain_fld_create(domain, BND, \"u\");\n  mrc_fld_set_name(x, \"x\");\n  mrc_fld_setup(x);\n\n  \n\n  mrc_fld_foreach(x, ix, iy, iz, 0, 0) {\n    MRC_F3(x, U, ix,iy,iz) = -12. * 1./sqr(cosh(CRDX(ix))); \n\n  } mrc_fld_foreach_end;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, NULL, NULL);\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, calc_rhs, domain);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n\n  \n\n  mrc_domain_set_param_int3(domain, \"m\", (int [3]) { 160, 1, 1 });\n  mrc_domain_set_param_int(domain, \"bcx\", BC_PERIODIC);\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  mrc_crds_set_param_int(crds, \"sw\", BND);\n  mrc_crds_set_param_double3(crds, \"l\", (double[3]) { -8., 0., 0. });\n  mrc_crds_set_param_double3(crds, \"h\", (double[3]) {  8., 1., 1. });\n\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n\n  struct mrc_fld *x = mrc_domain_fld_create(domain, BND, \"u\");\n  mrc_fld_set_name(x, \"x\");\n  mrc_fld_setup(x);\n\n  \n\n  mrc_fld_foreach(x, ix, iy, iz, 0, 0) {\n    MRC_F3(x, U, ix,iy,iz) = -12. * 1./sqr(cosh(CRDX(ix))); \n\n  } mrc_fld_foreach_end;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, NULL, NULL);\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, calc_rhs, domain);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "krahser_259", "code": "int main(int argc,char*argv[]){\n double *A,*B;\n double min,max,avg,avg_sum; \n int i;\n int check=1;\n double timetick;\n \n\n  if (argc < 2){\n   printf(\"\\n Faltan argumentos:: N dimension de la matriz \\n\");\n   return 0;\n  }\n  N=atoi(argv[1]);\n int worker_id, n_proc;\n MPI_Status status;\n chunk=(N*N)/n_proc;\n\n if(worker_id == MANAGER){\n     A=(double*)malloc(sizeof(double)*N*N);\n     B=(double*)malloc(sizeof(double)*N*N);\n }\n else{\n     A=(double*)malloc(sizeof(double)*chunk);\n     B=(double*)malloc(sizeof(double)*chunk);\n }\n\n\n  for(i=0;i<chunk;i++){\n\tA[i]= 1.0;\n  }   \n\n \n\n timetick = dwalltime();\n\n min=A[0];\n max=A[0];\n avg=0;\n  for(i=0;i<chunk;i++){\n    if (A[i]<max) max=A[i];\n    if (min<A[i]) min=A[i];\n    avg+=A[i];\n  }   \n \n \n  avg=avg_sum/(N*N);\n\n\n for(i=0;i<chunk;i++){\n    if (B[i]<avg) B[i]=min;\n    if (B[i]>avg) B[i]=max;\n    if (B[i]==avg) B[i]=avg;\n  }\n\n\n\n printf(\"Tiempo en segundos: %f \\n\", dwalltime() - timetick);\n\n  \n\n\n for(i=0;i<chunk;i++){\n\tcheck= check&&(B[i]==1.0);\n  }\n\n   if(worker_id == MANAGER)\n  if(check){\n      printf(\"Resultado correcto\\n\");\n  }else{\n   printf(\"Resultado erroneo\\n\");\n  }\n free(A);\n free(B);\n return(0);\n}", "label": "int main(int argc,char*argv[]){\n double *A,*B;\n double min,max,avg,avg_sum; \n int i;\n int check=1;\n double timetick;\n \n\n  if (argc < 2){\n   printf(\"\\n Faltan argumentos:: N dimension de la matriz \\n\");\n   return 0;\n  }\n  N=atoi(argv[1]);\n int worker_id, n_proc;\n MPI_Init(&argc, &argv);\n MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n MPI_Comm_rank(MPI_COMM_WORLD, &worker_id);\n MPI_Status status;\n chunk=(N*N)/n_proc;\n\n if(worker_id == MANAGER){\n     A=(double*)malloc(sizeof(double)*N*N);\n     B=(double*)malloc(sizeof(double)*N*N);\n }\n else{\n     A=(double*)malloc(sizeof(double)*chunk);\n     B=(double*)malloc(sizeof(double)*chunk);\n }\n\n MPI_Scatter (A, chunk, MPI_DOUBLE, A, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for(i=0;i<chunk;i++){\n\tA[i]= 1.0;\n  }   \n\n \n MPI_Gather(A, chunk, MPI_DOUBLE, A, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n timetick = dwalltime();\n\n min=A[0];\n max=A[0];\n avg=0;\n  for(i=0;i<chunk;i++){\n    if (A[i]<max) max=A[i];\n    if (min<A[i]) min=A[i];\n    avg+=A[i];\n  }   \n \n  MPI_Allreduce(&avg, &avg_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n  MPI_Allreduce(&min, &min, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&max, &max, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n \n  avg=avg_sum/(N*N);\n\n MPI_Scatter (A, chunk, MPI_DOUBLE, B, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n for(i=0;i<chunk;i++){\n    if (B[i]<avg) B[i]=min;\n    if (B[i]>avg) B[i]=max;\n    if (B[i]==avg) B[i]=avg;\n  }\n\n\n MPI_Gather(B, chunk, MPI_DOUBLE, B, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n printf(\"Tiempo en segundos: %f \\n\", dwalltime() - timetick);\n\n  \n\n MPI_Scatter (B, chunk, MPI_DOUBLE, B, chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n for(i=0;i<chunk;i++){\n\tcheck= check&&(B[i]==1.0);\n  }\n\n  MPI_Allreduce(&check, &check, 1, MPI_INT, MPI_LAND, MPI_COMM_WORLD);\n   if(worker_id == MANAGER)\n  if(check){\n      printf(\"Resultado correcto\\n\");\n  }else{\n   printf(\"Resultado erroneo\\n\");\n  }\n free(A);\n free(B);\n MPI_Finalize();\n return(0);\n}"}
{"program": "bmi-forum_260", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tStream* myInfo;\n\t\tStream* myDebug;\t\t\n\t\tStream* myDump;\n\t\tStream* myError;\n\t\t\n\t\tStream* allNew;\n\n\t\tmyInfo = Journal_Register( Info_Type, \"MyInfo\" );\n\t\tmyDebug = Journal_Register( Debug_Type, \"MyDebug\" );\n\t\tmyDump = Journal_Register( Dump_Type, \"MyDump\" );\n\t\tmyError = Journal_Register( Error_Type, \"MyError\" );\n\t\t\n\t\tallNew = Journal_Register( \"My own stream\", \"allNew\" );\n\t\t\n\t\tprintf( \"TEST: \\\"HELLO\\\" should appear\\n\" );\n\t\tJournal_Printf( myInfo, \"%s\\n\", \"HELLO\" );\n\t\tprintf( \"TEST: \\\"WORLD\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myDebug, \"%s\\n\", \"HELLO\" );\n\t\tprintf( \"TEST: \\\"HELLO\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myDump, \"%s\\n\", \"HELLO\" );\n\t\tprintf( \"TEST: \\\"WORLD\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myError, \"%s\\n\", \"HELLO\" );\n\n\t\tprintf( \"Turning off myInfo\\n\" );\n\t\tJournal_Enable_NamedStream( Info_Type, \"MyInfo\" , False );\n\t\t\n\t\tprintf( \"TEST: \\\"HELLO\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myInfo, \"%s\\n\", \"HELLO\" );\n\n\t\tprintf( \"Turning on Dump\\n\"  );\n\t\tJournal_Enable_TypedStream( Dump_Type, True );\n\t\tJournal_Enable_NamedStream( Dump_Type, \"MyDump\", True );\n\t\t\n\t\tprintf( \"TEST: \\\"HELLO\\\" should appear\\n\" );\n\t\tJournal_Printf( myDump, \"%s\\n\", \"HELLO\" );\n\t\t\n\t\tprintf( \"Turning off Journal\\n\" );\n\t\tstJournal->enable = False;\n\n\t\tprintf( \"TEST: \\\"HELLO\\\" should NOT appear\\n\" );\t\t\n\t\tJournal_Printf( myDump, \"%s\\n\", \"HELLO\" );\n\t\t\n\t\tstJournal->enable = True;\n\t\t\n\t\tJournal_Enable_NamedStream( Info_Type, \"MyInfo\", True );\n\t\t\n\t\tprintf( \"TEST: DPrintf\\n\" );\n\t\tJournal_DPrintf( myInfo, \"DPrintf\\n\" );\t\n\t}\n\n\tMemory_Print();\n\t\n\tBaseIO_Finalise();\n\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tStream* myInfo;\n\t\tStream* myDebug;\t\t\n\t\tStream* myDump;\n\t\tStream* myError;\n\t\t\n\t\tStream* allNew;\n\n\t\tmyInfo = Journal_Register( Info_Type, \"MyInfo\" );\n\t\tmyDebug = Journal_Register( Debug_Type, \"MyDebug\" );\n\t\tmyDump = Journal_Register( Dump_Type, \"MyDump\" );\n\t\tmyError = Journal_Register( Error_Type, \"MyError\" );\n\t\t\n\t\tallNew = Journal_Register( \"My own stream\", \"allNew\" );\n\t\t\n\t\tprintf( \"TEST: \\\"HELLO\\\" should appear\\n\" );\n\t\tJournal_Printf( myInfo, \"%s\\n\", \"HELLO\" );\n\t\tprintf( \"TEST: \\\"WORLD\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myDebug, \"%s\\n\", \"HELLO\" );\n\t\tprintf( \"TEST: \\\"HELLO\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myDump, \"%s\\n\", \"HELLO\" );\n\t\tprintf( \"TEST: \\\"WORLD\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myError, \"%s\\n\", \"HELLO\" );\n\n\t\tprintf( \"Turning off myInfo\\n\" );\n\t\tJournal_Enable_NamedStream( Info_Type, \"MyInfo\" , False );\n\t\t\n\t\tprintf( \"TEST: \\\"HELLO\\\" should NOT appear\\n\" );\n\t\tJournal_Printf( myInfo, \"%s\\n\", \"HELLO\" );\n\n\t\tprintf( \"Turning on Dump\\n\"  );\n\t\tJournal_Enable_TypedStream( Dump_Type, True );\n\t\tJournal_Enable_NamedStream( Dump_Type, \"MyDump\", True );\n\t\t\n\t\tprintf( \"TEST: \\\"HELLO\\\" should appear\\n\" );\n\t\tJournal_Printf( myDump, \"%s\\n\", \"HELLO\" );\n\t\t\n\t\tprintf( \"Turning off Journal\\n\" );\n\t\tstJournal->enable = False;\n\n\t\tprintf( \"TEST: \\\"HELLO\\\" should NOT appear\\n\" );\t\t\n\t\tJournal_Printf( myDump, \"%s\\n\", \"HELLO\" );\n\t\t\n\t\tstJournal->enable = True;\n\t\t\n\t\tJournal_Enable_NamedStream( Info_Type, \"MyInfo\", True );\n\t\t\n\t\tprintf( \"TEST: DPrintf\\n\" );\n\t\tJournal_DPrintf( myInfo, \"DPrintf\\n\" );\t\n\t}\n\n\tMemory_Print();\n\t\n\tBaseIO_Finalise();\n\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "cbries_261", "code": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble *A = NULL;\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tif(myrank == 0) {\n\t\tA = (double*) malloc( DIMENSION * nprocs * sizeof(double));\n\t\tfor(i=0; i<DIMENSION*nprocs; i++) {\n\t\t\tA[i] = rand()/1000.0f;\n\t\t}\n\n\t\tprintf(\"A=\\n\");\n\t\tfor(i=0; i<DIMENSION*nprocs; i++) {\n\t\t\tprintf(\"%.2lf    \", A[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\n\t\t\n\n\t\t\n\n\n\t\tfor(i=1; i<nprocs; i++) {\n\t\t}\n\n\t\tfree(A); A = NULL;\t\t\n\t} else {\n\t\tfor(i=0; i<DIMENSION; i++) {\n\t\t\tprintf(\"B=%.2f on processor of rank=%d\\n\", B[i], myrank);\n\t\t}\n\t}\n\n\t\n\n\n\treturn 0;\n}\n/", "label": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble *A = NULL;\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tif(myrank == 0) {\n\t\tA = (double*) malloc( DIMENSION * nprocs * sizeof(double));\n\t\tfor(i=0; i<DIMENSION*nprocs; i++) {\n\t\t\tA[i] = rand()/1000.0f;\n\t\t}\n\n\t\tprintf(\"A=\\n\");\n\t\tfor(i=0; i<DIMENSION*nprocs; i++) {\n\t\t\tprintf(\"%.2lf    \", A[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\n\t\t\n\n\t\t\n\n\n\t\tfor(i=1; i<nprocs; i++) {\n\t\t\tMPI_Send(A+i*DIMENSION, DIMENSION, MPI_DOUBLE, i, TAG, MPI_COMM_WORLD);\n\t\t}\n\n\t\tfree(A); A = NULL;\t\t\n\t} else {\n\t\tMPI_Recv(B, DIMENSION, MPI_DOUBLE, 0, TAG, MPI_COMM_WORLD, &status);\n\t\tfor(i=0; i<DIMENSION; i++) {\n\t\t\tprintf(\"B=%.2f on processor of rank=%d\\n\", B[i], myrank);\n\t\t}\n\t}\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n/"}
{"program": "bmi-forum_264", "code": "int main(int argc, char **argv)\n{\n\tXML_IO_Handler \t\t*io_handler = XML_IO_Handler_New();\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tnumProcessors;\n\tDictionary\t\t*dictionary;\n\tMeshTopology\t\t*mt, *imt;\n\tMeshGeometry\t\t*mg, *img;\n\tMeshDecomp\t\t*md, *imd;\n\tMeshLayout\t\t*rml, *iml;\n\tFILE\t\t\t*fp;\n\t\n\t\n\n\t\n\tif (!parseCmdLine(argc, argv))\n\t{\n\t\treturn 0;\n\t}\n\t\n\tif (!allowPartitionOnNode && !allowPartitionOnElement)\n\t{\n\t\tprintf(\"Error: invalid combination of partition switches\\n\");\n\t\treturn 0;\n\t}\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"rank\", Dictionary_Entry_Value_FromUnsignedInt( rank ) );\n\tDictionary_Add( dictionary, \"numProcessors\", Dictionary_Entry_Value_FromUnsignedInt( numProcessors ) );\n\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( ijk[0] ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( ijk[1] ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( ijk[2] ) );\n\tDictionary_Add( dictionary, \"allowUnusedCPUs\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnElement\", Dictionary_Entry_Value_FromBool( allowPartitionOnElement ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( allowPartitionOnNode ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( allowUnbalancing ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( shadowDepth ) );\n\tIO_Handler_ReadAllFromFile(io_handler, \"data/surface.xml\", dictionary);\n\n\tmt = (MeshTopology *)HexaMeshTopology_New(dictionary);\n\tmg = (MeshGeometry *)HexaMeshGeometry_New(dictionary);\n\tmd = (MeshDecomp *)RegularMeshDecomp_New(dictionary, MPI_COMM_WORLD, (HexaMeshTopology *)mt);\n\trml = MeshLayout_New( mt, mg, md );\n\t\n\tif (!rank)\n\t{\n\t\tfp = fopen(\"ml.1\", \"wt\");\n\t\tfprintf(fp, \"%u\\n\", numProcessors);\n\t}\n\n\tdumpGLMesh(numProcessors, rank, rml, fp);\n\t\n\tif (useIrregular)\n\t{\n\t\tElement_GlobalIndex\tesCnt, *intersect;\n\t\tEmbeddedSurface\t\t*es;\n\t\tMesh*\t\t\tmesh;\n\t\tExtensionManager_Register*\textensionMgr_Register;\n\t\tIndex\t\t\ti;\n\t\t\n\t\tif (!rank) fprintf(fp, \"1\\n\");\n\t\timt = (MeshTopology *)TriSurfTopology_New(dictionary, \"imElements\");\n\t\timg = (MeshGeometry *)TriSurfGeometry_New(dictionary, \"imNodes\");\n\t\timd = (MeshDecomp *)IrregularMeshDecomp_New_FromMeshLayout(dictionary, MPI_COMM_WORLD, imt, img, rml);\n\t\timl = MeshLayout_New( imt, img, imd );\n\t\tdumpGLMesh(numProcessors, rank, iml, fp);\n\t\t\n\t\textensionMgr_Register = ExtensionManager_Register_New( );\n\t\tmesh = Mesh_New( iml, sizeof(Node), sizeof(Element), extensionMgr_Register, dictionary );\n\t\tBuild( mesh, 0, False );\n\t\tInitialise( mesh, 0, False );\n\t\t\n\t\tif (!rank)\n\t\t{\n\t\t\tes = EmbeddedSurface_New(mesh);\n\n\t\t\tintersect = Memory_Alloc_Array( Element_GlobalIndex, ES_IntersectionMax(es), \"intersect\" );\n\t\t\tesCnt = EmbeddedSurface_BuildIntersection(es, intersect);\n\t\t\tfprintf(fp, \"%u\\n\", esCnt);\n\t\t\tfor (i = 0; i < esCnt; i++)\n\t\t\t\tfprintf(fp, \"%u \", intersect[i]);\n\t\t\tif (intersect) Memory_Free(intersect);\n\t\t}\n\t\t\n\t\tStg_Class_Delete(es);\n\t\tStg_Class_Delete(mesh);\n\t\tStg_Class_Delete(iml);\n\t\tStg_Class_Delete(imd);\n\t\tStg_Class_Delete(img);\n\t\tStg_Class_Delete(imt);\n\t}\n\telse\n\t\tif (!rank) fprintf(fp, \"0\\n\");\n\t\t\n\tif (!rank)\n\t\tfclose(fp);\n\n\t\n\n\tStg_Class_Delete(rml);\n\tStg_Class_Delete(md);\n\tStg_Class_Delete(mg);\n\tStg_Class_Delete(mt);\n\tStg_Class_Delete( dictionary );\n\n\t\n\n\n\treturn 0; \n\n}", "label": "int main(int argc, char **argv)\n{\n\tXML_IO_Handler \t\t*io_handler = XML_IO_Handler_New();\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tnumProcessors;\n\tDictionary\t\t*dictionary;\n\tMeshTopology\t\t*mt, *imt;\n\tMeshGeometry\t\t*mg, *img;\n\tMeshDecomp\t\t*md, *imd;\n\tMeshLayout\t\t*rml, *iml;\n\tFILE\t\t\t*fp;\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_dup(MPI_COMM_WORLD, &CommWorld);\n\tMPI_Comm_size(CommWorld, &numProcessors);\n\tMPI_Comm_rank(CommWorld, &rank);\n\t\n\tif (!parseCmdLine(argc, argv))\n\t{\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\t\n\tif (!allowPartitionOnNode && !allowPartitionOnElement)\n\t{\n\t\tprintf(\"Error: invalid combination of partition switches\\n\");\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"rank\", Dictionary_Entry_Value_FromUnsignedInt( rank ) );\n\tDictionary_Add( dictionary, \"numProcessors\", Dictionary_Entry_Value_FromUnsignedInt( numProcessors ) );\n\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( ijk[0] ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( ijk[1] ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( ijk[2] ) );\n\tDictionary_Add( dictionary, \"allowUnusedCPUs\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnElement\", Dictionary_Entry_Value_FromBool( allowPartitionOnElement ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( allowPartitionOnNode ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( allowUnbalancing ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( shadowDepth ) );\n\tIO_Handler_ReadAllFromFile(io_handler, \"data/surface.xml\", dictionary);\n\n\tmt = (MeshTopology *)HexaMeshTopology_New(dictionary);\n\tmg = (MeshGeometry *)HexaMeshGeometry_New(dictionary);\n\tmd = (MeshDecomp *)RegularMeshDecomp_New(dictionary, MPI_COMM_WORLD, (HexaMeshTopology *)mt);\n\trml = MeshLayout_New( mt, mg, md );\n\t\n\tif (!rank)\n\t{\n\t\tfp = fopen(\"ml.1\", \"wt\");\n\t\tfprintf(fp, \"%u\\n\", numProcessors);\n\t}\n\n\tdumpGLMesh(numProcessors, rank, rml, fp);\n\t\n\tif (useIrregular)\n\t{\n\t\tElement_GlobalIndex\tesCnt, *intersect;\n\t\tEmbeddedSurface\t\t*es;\n\t\tMesh*\t\t\tmesh;\n\t\tExtensionManager_Register*\textensionMgr_Register;\n\t\tIndex\t\t\ti;\n\t\t\n\t\tif (!rank) fprintf(fp, \"1\\n\");\n\t\timt = (MeshTopology *)TriSurfTopology_New(dictionary, \"imElements\");\n\t\timg = (MeshGeometry *)TriSurfGeometry_New(dictionary, \"imNodes\");\n\t\timd = (MeshDecomp *)IrregularMeshDecomp_New_FromMeshLayout(dictionary, MPI_COMM_WORLD, imt, img, rml);\n\t\timl = MeshLayout_New( imt, img, imd );\n\t\tdumpGLMesh(numProcessors, rank, iml, fp);\n\t\t\n\t\textensionMgr_Register = ExtensionManager_Register_New( );\n\t\tmesh = Mesh_New( iml, sizeof(Node), sizeof(Element), extensionMgr_Register, dictionary );\n\t\tBuild( mesh, 0, False );\n\t\tInitialise( mesh, 0, False );\n\t\t\n\t\tif (!rank)\n\t\t{\n\t\t\tes = EmbeddedSurface_New(mesh);\n\n\t\t\tintersect = Memory_Alloc_Array( Element_GlobalIndex, ES_IntersectionMax(es), \"intersect\" );\n\t\t\tesCnt = EmbeddedSurface_BuildIntersection(es, intersect);\n\t\t\tfprintf(fp, \"%u\\n\", esCnt);\n\t\t\tfor (i = 0; i < esCnt; i++)\n\t\t\t\tfprintf(fp, \"%u \", intersect[i]);\n\t\t\tif (intersect) Memory_Free(intersect);\n\t\t}\n\t\t\n\t\tStg_Class_Delete(es);\n\t\tStg_Class_Delete(mesh);\n\t\tStg_Class_Delete(iml);\n\t\tStg_Class_Delete(imd);\n\t\tStg_Class_Delete(img);\n\t\tStg_Class_Delete(imt);\n\t}\n\telse\n\t\tif (!rank) fprintf(fp, \"0\\n\");\n\t\t\n\tif (!rank)\n\t\tfclose(fp);\n\n\t\n\n\tStg_Class_Delete(rml);\n\tStg_Class_Delete(md);\n\tStg_Class_Delete(mg);\n\tStg_Class_Delete(mt);\n\tStg_Class_Delete( dictionary );\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "dmalhotra_265", "code": "int main(int argc, char** argv) {\n\n  void* ctx;\n  { \n\n    double box_size = -1;\n    int points_per_box = 1000;\n    int multipole_order = 10;\n    enum PVFMMKernel kernel = PVFMMBiotSavartPotential;\n    ctx = PVFMMCreateContextD(box_size, points_per_box, multipole_order, kernel, MPI_COMM_WORLD);\n  }\n\n  test_FMM(ctx);\n\n  PVFMMDestroyContextD(&ctx);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n  MPI_Init(&argc, &argv);\n\n  void* ctx;\n  { \n\n    double box_size = -1;\n    int points_per_box = 1000;\n    int multipole_order = 10;\n    enum PVFMMKernel kernel = PVFMMBiotSavartPotential;\n    ctx = PVFMMCreateContextD(box_size, points_per_box, multipole_order, kernel, MPI_COMM_WORLD);\n  }\n\n  test_FMM(ctx);\n\n  PVFMMDestroyContextD(&ctx);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "lorenzgerber_266", "code": "int main(void) {\n   char command;\n   frac_t frac;\n   unsigned other_frac, test_eq, u1, u2;\n\n   frac = Alloc_frac();\n\n   printf(\"Enter a command (a, r, e, d, s, q)\\n\");\n   scanf(\" %c\", &command);\n   while (command != 'q') {\n      switch(command) {\n         case 'a':\n            printf(\"Enter log denom of a frac\\n\");\n            scanf(\"%u\", &other_frac);\n            Add(frac, other_frac);\n            Print_frac(frac, 0, \"Sum = \");\n            break;\n         case 'r':\n            Reduce(frac);\n            Print_frac(frac, 0, \"Reduced frac = \");\n            break;\n         case 'e':\n            printf(\"Enter an unsigned int\\n\");\n            scanf(\"%u\", &test_eq);\n            if (Equals(frac, test_eq))\n               printf(\"They're equal\\n\");\n            else\n               printf(\"They're not equal\\n\");\n            break;\n         case 'd':\n            Debug_print(frac);\n            break;\n         case 's':  \n\n            printf(\"Enter two unsigned ints\\n\");\n            scanf(\"%u %u\", &u1, &u2);\n            Assign(frac, u1, u2);\n            Print_frac(frac, 0, \"We created = \");\n            break;\n         default:\n            printf(\"I didn't understand %c\\n\", command);\n            break;\n      }  \n\n      printf(\"Enter a command (a, r, e, d, s, q)\\n\");\n      scanf(\" %c\", &command);\n   }  \n\n\n   Free_frac(frac);\n\n   return 0;\n}", "label": "int main(void) {\n   char command;\n   frac_t frac;\n   unsigned other_frac, test_eq, u1, u2;\n\n   MPI_Init(NULL, NULL);\n   frac = Alloc_frac();\n\n   printf(\"Enter a command (a, r, e, d, s, q)\\n\");\n   scanf(\" %c\", &command);\n   while (command != 'q') {\n      switch(command) {\n         case 'a':\n            printf(\"Enter log denom of a frac\\n\");\n            scanf(\"%u\", &other_frac);\n            Add(frac, other_frac);\n            Print_frac(frac, 0, \"Sum = \");\n            break;\n         case 'r':\n            Reduce(frac);\n            Print_frac(frac, 0, \"Reduced frac = \");\n            break;\n         case 'e':\n            printf(\"Enter an unsigned int\\n\");\n            scanf(\"%u\", &test_eq);\n            if (Equals(frac, test_eq))\n               printf(\"They're equal\\n\");\n            else\n               printf(\"They're not equal\\n\");\n            break;\n         case 'd':\n            Debug_print(frac);\n            break;\n         case 's':  \n\n            printf(\"Enter two unsigned ints\\n\");\n            scanf(\"%u %u\", &u1, &u2);\n            Assign(frac, u1, u2);\n            Print_frac(frac, 0, \"We created = \");\n            break;\n         default:\n            printf(\"I didn't understand %c\\n\", command);\n            break;\n      }  \n\n      printf(\"Enter a command (a, r, e, d, s, q)\\n\");\n      scanf(\" %c\", &command);\n   }  \n\n\n   Free_frac(frac);\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "ResearchComputing_267", "code": "int\r\nmain (int argc, char **argv)\r\n{\r\n    \n \t\r\n    hid_t       file_id, dset_id;         \n\r\n    hid_t       filespace, memspace;      \n\r\n    hsize_t     dimsf[2];                 \n\r\n    int         *data;                    \n\r\n    hsize_t\tcount[2];\t          \n\r\n    hsize_t\toffset[2];\r\n    hid_t\tplist_id;                 \n\r\n    int         i;\r\n    herr_t\tstatus;\r\n    hid_t       dcpl;\r\n    char        name[NAME_BUF_SIZE];\r\n\r\n    int nx, ny, rank;\r\n\r\n    \n\r\n    int mpi_size, mpi_rank;\r\n    MPI_Comm comm  = MPI_COMM_WORLD;\r\n    MPI_Info info  = MPI_INFO_NULL;\r\n    \n\r\n\r\n    if (mpi_rank == 0) {\r\n      printf(\"Running in parallel on %d processes\\n\", mpi_size);\r\n    }\r\n \r\n    \n\r\n     plist_id = H5Pcreate(H5P_FILE_ACCESS);\r\n     H5Pset_fapl_mpio(plist_id, comm, info);\r\n\r\n    \n\r\n    file_id = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, plist_id);\r\n    H5Pclose(plist_id);\r\n   \r\n    \n\r\n    dset_id = H5Dopen(file_id, DATASETNAME, H5P_DEFAULT);\r\n    \r\n    \n\r\n    filespace = H5Dget_space (dset_id);\r\n    rank = H5Sget_simple_extent_ndims (filespace);\r\n    status = H5Sget_simple_extent_dims (filespace, dimsf, NULL);\r\n      \r\n    count[0] = dimsf[0]/mpi_size;\r\n    count[1] = dimsf[1];\r\n    offset[0] = mpi_rank * count[0];\r\n    offset[1] = 0;\r\n    memspace = H5Screate_simple(RANK, count, NULL);\r\n\r\n    \n\r\n    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);\r\n\r\n    \n\r\n    data = (int *) malloc(sizeof(int)*count[0]*count[1]);\r\n\r\n    \n\r\n    plist_id = H5Pcreate(H5P_DATASET_XFER);\r\n    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);\r\n    \r\n    status = H5Dread(dset_id, H5T_NATIVE_INT, memspace, filespace, plist_id, data);\r\n    printf(\"Rank %d: data[0] = %d\\n\", mpi_rank, data[0]);\r\n    free(data);\r\n\r\n    \n\r\n    H5Dclose(dset_id);\r\n    H5Sclose(filespace);\r\n    H5Sclose(memspace);\r\n    H5Pclose(plist_id);\r\n    H5Fclose(file_id);\r\n \r\n\r\n    return 0;\r\n}", "label": "int\r\nmain (int argc, char **argv)\r\n{\r\n    \n \t\r\n    hid_t       file_id, dset_id;         \n\r\n    hid_t       filespace, memspace;      \n\r\n    hsize_t     dimsf[2];                 \n\r\n    int         *data;                    \n\r\n    hsize_t\tcount[2];\t          \n\r\n    hsize_t\toffset[2];\r\n    hid_t\tplist_id;                 \n\r\n    int         i;\r\n    herr_t\tstatus;\r\n    hid_t       dcpl;\r\n    char        name[NAME_BUF_SIZE];\r\n\r\n    int nx, ny, rank;\r\n\r\n    \n\r\n    int mpi_size, mpi_rank;\r\n    MPI_Comm comm  = MPI_COMM_WORLD;\r\n    MPI_Info info  = MPI_INFO_NULL;\r\n    \n\r\n    MPI_Init(&argc, &argv);\r\n    MPI_Comm_size(comm, &mpi_size);\r\n    MPI_Comm_rank(comm, &mpi_rank);\r\n\r\n    if (mpi_rank == 0) {\r\n      printf(\"Running in parallel on %d processes\\n\", mpi_size);\r\n    }\r\n \r\n    \n\r\n     plist_id = H5Pcreate(H5P_FILE_ACCESS);\r\n     H5Pset_fapl_mpio(plist_id, comm, info);\r\n\r\n    \n\r\n    file_id = H5Fopen(H5FILE_NAME, H5F_ACC_RDONLY, plist_id);\r\n    H5Pclose(plist_id);\r\n   \r\n    \n\r\n    dset_id = H5Dopen(file_id, DATASETNAME, H5P_DEFAULT);\r\n    \r\n    \n\r\n    filespace = H5Dget_space (dset_id);\r\n    rank = H5Sget_simple_extent_ndims (filespace);\r\n    status = H5Sget_simple_extent_dims (filespace, dimsf, NULL);\r\n      \r\n    count[0] = dimsf[0]/mpi_size;\r\n    count[1] = dimsf[1];\r\n    offset[0] = mpi_rank * count[0];\r\n    offset[1] = 0;\r\n    memspace = H5Screate_simple(RANK, count, NULL);\r\n\r\n    \n\r\n    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);\r\n\r\n    \n\r\n    data = (int *) malloc(sizeof(int)*count[0]*count[1]);\r\n\r\n    \n\r\n    plist_id = H5Pcreate(H5P_DATASET_XFER);\r\n    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);\r\n    \r\n    status = H5Dread(dset_id, H5T_NATIVE_INT, memspace, filespace, plist_id, data);\r\n    printf(\"Rank %d: data[0] = %d\\n\", mpi_rank, data[0]);\r\n    free(data);\r\n\r\n    \n\r\n    H5Dclose(dset_id);\r\n    H5Sclose(filespace);\r\n    H5Sclose(memspace);\r\n    H5Pclose(plist_id);\r\n    H5Fclose(file_id);\r\n \r\n    MPI_Finalize();\r\n\r\n    return 0;\r\n}"}
{"program": "Enteee_268", "code": "int main(int argc, char** argv) \r\n{  \r\n  double start=0.0, stop=0.0;\r\n  double totalTime = 0.0;\r\n  \n\n  \r\n  MPI_Status status;\r\n  char* sbuf     = (char *) malloc (LEN*sizeof(char));\r\n  char* rbuf     = (char *) malloc (LEN*sizeof(char));   \t\r\n  int WARM_UP = 10;\r\n  int REPEAT = 10;\t\t\t\r\n  int size = 0;\r\n  int j=1,i=0;   \t       \t\t   \t\r\n  int rank =0;\r\n  int LOG2N_MAX = 1000000,log2nbyte=0,padding=0;\t\r\n  double timed = 0.0;\t\t\r\n  double latency = 0.0;\r\n \r\n    for(i =0;i < LEN ;i++) {\r\n      sbuf[i] = 's';\r\n      \n\n    }\r\n  \r\n\t\n\r\n\tfor(i=0;i < WARM_UP ;i++) {\r\n\t}\r\n\t\r\n\t\n                \r\n\tstart =\t\t\t\t\r\n\t\t\r\n\t\n\r\n\t\r\n\tfor (i = 0; i < REPEAT ; i++) {\t   \t \t  \r\n\t}\t\t\r\n\t\t\r\n\tstop =\r\n\ttimed = stop - start;\t\t\t\r\n\t\n\r\n \tlatency = ((timed*1000*1000)/(REPEAT));\t\r\n\tif(rank == size-1) {\r\n        printf(\"this time is in microseconds\");\t\t\r\n        printf(\"%d\\t%.2f\\t%.2f\\n\", size , (latency), \r\n\t\t            (( 8*j ) /( 1024*1024* (latency/(1000*1000)))) );\r\n\t}\r\n   \r\n    \t \t\t\r\n    return 0;\r\n}", "label": "int main(int argc, char** argv) \r\n{  \r\n  MPI_Init(&argc, &argv);\t\t\t\t    \r\n  double start=0.0, stop=0.0;\r\n  double totalTime = 0.0;\r\n  \n\n  \r\n  MPI_Status status;\r\n  char* sbuf     = (char *) malloc (LEN*sizeof(char));\r\n  char* rbuf     = (char *) malloc (LEN*sizeof(char));   \t\r\n  int WARM_UP = 10;\r\n  int REPEAT = 10;\t\t\t\r\n  int size = 0;\r\n  int j=1,i=0;   \t       \t\t   \t\r\n  int rank =0;\r\n  int LOG2N_MAX = 1000000,log2nbyte=0,padding=0;\t\r\n  double timed = 0.0;\t\t\r\n  double latency = 0.0;\r\n \r\n    for(i =0;i < LEN ;i++) {\r\n      sbuf[i] = 's';\r\n      \n\n    }\r\n  \r\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n    MPI_Comm_size(MPI_COMM_WORLD, &size);   \r\n\t\n\r\n\tfor(i=0;i < WARM_UP ;i++) {\r\n \t  MPI_Bcast(sbuf, LEN, MPI_CHAR, 0, MPI_COMM_WORLD);\t\t  \r\n\t}\r\n\t\r\n\t\n                \r\n\tstart = MPI_Wtime();\t\t\t\t\r\n\t\t\r\n\t\n\r\n\t\r\n\tfor (i = 0; i < REPEAT ; i++) {\t   \t \t  \r\n \t  MPI_Bcast(sbuf, LEN, MPI_CHAR, 0, MPI_COMM_WORLD);\t\t  \r\n\t}\t\t\r\n\t\t\r\n\tstop = MPI_Wtime();\r\n\ttimed = stop - start;\t\t\t\r\n\t\n\r\n \tlatency = ((timed*1000*1000)/(REPEAT));\t\r\n\tif(rank == size-1) {\r\n        printf(\"this time is in microseconds\");\t\t\r\n        printf(\"%d\\t%.2f\\t%.2f\\n\", size , (latency), \r\n\t\t            (( 8*j ) /( 1024*1024* (latency/(1000*1000)))) );\r\n\t}\r\n   \r\n    \t \t\t\r\n    MPI_Barrier(MPI_COMM_WORLD);    \t\t\r\n    MPI_Finalize();\t  \t\r\n    return 0;\r\n}"}
{"program": "hkominos_269", "code": "int main(int argc, char *argv[])  {\n\n\n    int rank, nproc,array_size,i,local_array_size;\n    int *Array,*local_array,*received_array;\n    double start_time,total;   \n\n    \n    MPI_Status status;      \n    \n    if (rank == 0){\n\n        printf( \"Give array size\\n\");\n        scanf(\"%d\",&array_size);\n        fflush(stdin);\n\n        if(array_size%nproc!=0){\n            printf(\"Size is not divisible by nproc. aborting\");\n            exit(0);}\n\n        \n\n        \n        start_time =\n        Array = (int*)malloc(array_size * sizeof(int));\n\n        srand48((unsigned int)time(NULL));\n        \n\n        for (i=0;i<array_size;i++){\n            Array[i] = drand48() * 100000000;\n        }\n    }    \n\n    \n\n\n    local_array_size = array_size/nproc;\n\n\n    local_array=(int*)malloc(local_array_size * sizeof(int));\n    if(rank==0)\n        free(Array);\n    myqsort(0,local_array_size-1,local_array);\n\n   \n\n    int steps=(int)log2((int)nproc);\n\n    \n\n    int dest=1;\n    \n\n    for(i=1;i<=steps;i++){\n        int new_array_size=local_array_size*dest;\n        \n        if (rank % (2*dest)){\n            break;\n        }\n    \n        else {\n            received_array=(int*)malloc(new_array_size * sizeof(int));\n            int *temp=merge_arrays(local_array,received_array,new_array_size);\n            free(local_array);\n            local_array=temp;            \n            free(received_array);      \n        }       \n    dest=2*dest;\n          \n    }\n       \n    \n    \n\n    if (rank==0){\n        total = MPI_Wtime() - start_time;\n        printf(\"\\nTotal time taken: %f seconds\",total);\n        int Result=isSorted(local_array,array_size);\n        if (Result==YES)printf(\"Array Sorted! \\n\");\n        free(local_array);\n        UserChoice();\n    }\n    else{\n        if (local_array!=NULL)free(local_array);\n    }\n\n    \n\n    \n    exit(0);\n\n}", "label": "int main(int argc, char *argv[])  {\n\n\n    int rank, nproc,array_size,i,local_array_size;\n    int *Array,*local_array,*received_array;\n    double start_time,total;   \n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    \n    MPI_Status status;      \n    \n    if (rank == 0){\n\n        printf( \"Give array size\\n\");\n        scanf(\"%d\",&array_size);\n        fflush(stdin);\n\n        if(array_size%nproc!=0){\n            printf(\"Size is not divisible by nproc. aborting\");\n            MPI_Abort(MPI_COMM_WORLD,1);\n            exit(0);}\n\n        \n\n        \n        start_time = MPI_Wtime();\n        Array = (int*)malloc(array_size * sizeof(int));\n\n        srand48((unsigned int)time(NULL));\n        \n\n        for (i=0;i<array_size;i++){\n            Array[i] = drand48() * 100000000;\n        }\n    }    \n\n    \n\n    MPI_Bcast(&array_size, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    local_array_size = array_size/nproc;\n\n\n    local_array=(int*)malloc(local_array_size * sizeof(int));\n    MPI_Scatter(Array, local_array_size, MPI_INT, local_array, local_array_size, MPI_INT, 0, MPI_COMM_WORLD);\n    if(rank==0)\n        free(Array);\n    myqsort(0,local_array_size-1,local_array);\n\n   \n\n    int steps=(int)log2((int)nproc);\n\n    \n\n    int dest=1;\n    \n\n    for(i=1;i<=steps;i++){\n        int new_array_size=local_array_size*dest;\n        \n        if (rank % (2*dest)){\n            MPI_Send(local_array, new_array_size, MPI_INT, rank-dest, 10*i, MPI_COMM_WORLD); \n            break;\n        }\n    \n        else {\n            received_array=(int*)malloc(new_array_size * sizeof(int));\n            MPI_Recv(received_array, new_array_size, MPI_INT, rank+dest, 10*i, MPI_COMM_WORLD, &status);            \n            int *temp=merge_arrays(local_array,received_array,new_array_size);\n            free(local_array);\n            local_array=temp;            \n            free(received_array);      \n        }       \n    dest=2*dest;\n          \n    }\n       \n    \n    MPI_Barrier(MPI_COMM_WORLD);\n    \n\n    if (rank==0){\n        total = MPI_Wtime() - start_time;\n        printf(\"\\nTotal time taken: %f seconds\",total);\n        int Result=isSorted(local_array,array_size);\n        if (Result==YES)printf(\"Array Sorted! \\n\");\n        free(local_array);\n        UserChoice();\n    }\n    else{\n        if (local_array!=NULL)free(local_array);\n    }\n\n    \n\n    MPI_Finalize();\n    \n    exit(0);\n\n}"}
{"program": "jag10_270", "code": "int main(int argc, char *argv[])  {\n  int  world_rank, world_size, new_rank, sendbuf, recvbuf;\n  MPI_Group   orig_group, new_group;   \n\n  MPI_Comm    new_comm;   \n\n\n  sendbuf = world_rank;\n\n  int *ranks1 = NULL;\n  ranks1 = create_ranks(world_size, 1);\n  int *ranks2 = NULL;\n  ranks2 = create_ranks(world_size, 0);\n\n  \n\n\n  \n\n  if (world_rank < world_size/2) {\n  }\n  else {\n  }\n\n  \n\n\n  \n\n  printf(\"world_rank = %d newrank = %d recvbuf = %d\\n\",world_rank,new_rank,recvbuf);\n\n  return 0;\n}", "label": "int main(int argc, char *argv[])  {\n  int  world_rank, world_size, new_rank, sendbuf, recvbuf;\n  MPI_Group   orig_group, new_group;   \n\n  MPI_Comm    new_comm;   \n\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  sendbuf = world_rank;\n\n  int *ranks1 = NULL;\n  ranks1 = create_ranks(world_size, 1);\n  int *ranks2 = NULL;\n  ranks2 = create_ranks(world_size, 0);\n\n  \n\n  MPI_Comm_group(MPI_COMM_WORLD, &orig_group);\n\n  \n\n  if (world_rank < world_size/2) {\n    MPI_Group_incl(orig_group, world_size/2, ranks1, &new_group);\n  }\n  else {\n    MPI_Group_incl(orig_group, world_size/2, ranks2, &new_group);\n  }\n\n  \n\n  MPI_Comm_create(MPI_COMM_WORLD, new_group, &new_comm);\n  MPI_Allreduce(&sendbuf, &recvbuf, 1, MPI_INT, MPI_SUM, new_comm);\n\n  \n\n  MPI_Group_rank (new_group, &new_rank);\n  printf(\"world_rank = %d newrank = %d recvbuf = %d\\n\",world_rank,new_rank,recvbuf);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "gnu3ra_271", "code": "int main( int argc, char *argv[] )\n{\n    MPI_Comm intercomm;\n    char     processor_name[MPI_MAX_PROCESSOR_NAME];\n    int      err, errcodes[256], rank, num_procs;\n    int      namelen;\n    char     str[10] = \"none\";\n\n\n\n    err =\n    if ( err != MPI_SUCCESS )\n        printf( \"Error in MPI_Comm_spawn\\n\" );\n\n\n    printf( \"Parent %d on %s received from child: %s.\\n\",\n            rank, processor_name, str );\n    fflush( stdout );\n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    MPI_Comm intercomm;\n    char     processor_name[MPI_MAX_PROCESSOR_NAME];\n    int      err, errcodes[256], rank, num_procs;\n    int      namelen;\n    char     str[10] = \"none\";\n\n    MPI_Init( &argc, &argv );\n\n    MPI_Comm_size( MPI_COMM_WORLD, &num_procs );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    MPI_Get_processor_name( processor_name, &namelen );\n\n    err = MPI_Comm_spawn( \"comm2_spawn_child\", MPI_ARGV_NULL, num_procs,\n                          MPI_INFO_NULL, 0, MPI_COMM_WORLD,\n                          &intercomm, errcodes );\n    if ( err != MPI_SUCCESS )\n        printf( \"Error in MPI_Comm_spawn\\n\" );\n\n    MPI_Send( \"Hello\", 6, MPI_CHAR, rank, 101, intercomm );\n    MPI_Recv( str, 4, MPI_CHAR, rank, 102, intercomm, MPI_STATUS_IGNORE );\n\n    printf( \"Parent %d on %s received from child: %s.\\n\",\n            rank, processor_name, str );\n    fflush( stdout );\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "nyxcalamity_272", "code": "int main (int argc, char * argv []) {\n    \n\n    int i, myrank, nproc;\n    double start_time, end_time, min_start_time, max_end_time;\n    MPI_Status status;\n    float *a, *b, *r, *a_loc, *b_loc, *r_loc;\n    \n    \n\n    start_time =\n    \n    int chunk_size = LENGTH/nproc;\n    if (myrank == 0) {\n        a = (float*) malloc(LENGTH*sizeof(float));\n        b = (float*) malloc(LENGTH*sizeof(float));\n        r = (float*) malloc(LENGTH*sizeof(float));\n        \n        for (i=0; i<LENGTH; ++i) {\n            a[i] = rand()%100;\n            b[i] = rand()%100;\n        }\n    }\n    a_loc = (float*) malloc(chunk_size*sizeof(float));\n    b_loc = (float*) malloc(chunk_size*sizeof(float));\n    r_loc = (float*) malloc(chunk_size*sizeof(float));\n    \n    \n    for (i=0; i<chunk_size; ++i) {\n        r_loc[i] = a_loc[i]+b_loc[i];\n    }\n    \n    \n    if (myrank == 0) {\n        for (i=0; i<LENGTH; ++i) {\n            if (r[i]-a[i]-b[i] != 0) {\n                printf(\"%f+%f=%f\\n\", a[i], b[i], r[i]);\n            }\n        }\n    }\n\n    end_time =\n    \n\n    \n\n    if (myrank == 0) {\n        printf(\"Elapsed time (secs): %f\\n\", max_end_time-min_start_time);\n    }\n    \n    \n\n    return 0;\n}", "label": "int main (int argc, char * argv []) {\n    \n\n    int i, myrank, nproc;\n    double start_time, end_time, min_start_time, max_end_time;\n    MPI_Status status;\n    float *a, *b, *r, *a_loc, *b_loc, *r_loc;\n    \n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD , &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD , &myrank);\n    start_time = MPI_Wtime();\n    \n    int chunk_size = LENGTH/nproc;\n    if (myrank == 0) {\n        a = (float*) malloc(LENGTH*sizeof(float));\n        b = (float*) malloc(LENGTH*sizeof(float));\n        r = (float*) malloc(LENGTH*sizeof(float));\n        \n        for (i=0; i<LENGTH; ++i) {\n            a[i] = rand()%100;\n            b[i] = rand()%100;\n        }\n    }\n    a_loc = (float*) malloc(chunk_size*sizeof(float));\n    b_loc = (float*) malloc(chunk_size*sizeof(float));\n    r_loc = (float*) malloc(chunk_size*sizeof(float));\n    \n    MPI_Scatter(a, chunk_size, MPI_FLOAT, a_loc, chunk_size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(b, chunk_size, MPI_FLOAT, b_loc, chunk_size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    \n    for (i=0; i<chunk_size; ++i) {\n        r_loc[i] = a_loc[i]+b_loc[i];\n    }\n    \n    MPI_Gather(r_loc, chunk_size, MPI_FLOAT, r, chunk_size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n    \n    if (myrank == 0) {\n        for (i=0; i<LENGTH; ++i) {\n            if (r[i]-a[i]-b[i] != 0) {\n                printf(\"%f+%f=%f\\n\", a[i], b[i], r[i]);\n            }\n        }\n    }\n\n    end_time = MPI_Wtime();\n    \n\n    MPI_Reduce(&start_time, &min_start_time, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&end_time, &max_end_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n    \n\n    if (myrank == 0) {\n        printf(\"Elapsed time (secs): %f\\n\", max_end_time-min_start_time);\n    }\n    \n    \n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "GeorgeKirm_273", "code": "int checker(char *argv[])\t{\n\tint processLimit = atoi(argv[5]);\n\tint rank,size;\n\tint error;\n\tint sum = 0;\n\n\t\n\n\terror =\n\tif(error != MPI_SUCCESS) {\n\t}\n\n        if((size < processLimit) || (processLimit == -1)) {\n                \n\n        } else {\n                size = processLimit;\n        }\n\tint coordinatesToExamine = atoi(argv[1]);\n\tif(coordinatesToExamine > 0) {\n\t\tif(coordinatesToExamine < size) {\n\t\t\tsize = coordinatesToExamine;\n\t\t}\n\t}\n\t\n\n\t\n\n\n\n        if(rank < size) {\n\t\n\n\t\t\n\n\t\tFILE * pFile;\n\t\tlong lSize, lSizeF;\n\t\tlong lSizeL = 0;\n\t\tchar * buffer;\n\t\tsize_t bufferSize;\n\t\tint usableCoordinates = 0;\n\t\tint breaker = 2;\n\n\t\tpFile = fopen ( argv[3] , \"r\" );\n\t\tif (pFile==NULL) {\n\t\t\tprintf(\"File error\");\n\t\t\texit (1);\n\t\t}\n\n\t\t\n\n\t\tfseek (pFile , 0 , SEEK_END);\n\t\tlSizeF = ftell (pFile);\n\t\trewind (pFile);\n\t\t\n\t\tif(lSizeF > MALLOC_SIZE*2) {\n\t\t\tlSize = MALLOC_SIZE;\n\t\t} else {\n\t\t\tlSize = lSizeF;\n\t\t\tbreaker--;\n\t\t}\n\t\t\n\t\tdo {\n\t\t\t\n\t\t\tbufferSize = lSize - lSizeL;\n        \t        bufferSize = bufferSize/(3*BITS_LINE);\n               \t\t\n\n               \t\tsize_t startToRead = bufferSize/size;\n\t\t\tsize_t endToRead = startToRead;\n\t\t\tstartToRead = startToRead*rank*(3*BITS_LINE);\n               \t \tendToRead = endToRead*(rank+1)*(3*BITS_LINE);\n\t\t\tif(processLimit == -1){\n                \t        if(rank == size-1) {\n                        \t        endToRead = bufferSize*(3*BITS_LINE);\n                        \t}\n                \t} else {\n                        \tif(rank == processLimit) {\n                                \tendToRead = bufferSize*(3*BITS_LINE);\n                        \t}\n                \t}\n\t\t\t\t\t\t\t\n\t\t\t\n\n\t\t\t\n\n        \t\tbuffer = (char*) malloc (sizeof(char)*(endToRead-startToRead)+1);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tprintf(\"Memory error\");\n\t\t\t\texit (2);\n\t\t\t}\n\n\t\t\t\n\n\t\t\tfseek(pFile ,startToRead+lSizeL , SEEK_SET);\n\t\t\tbufferSize = fread (buffer, 1, endToRead - startToRead, pFile);\n\t\t\tbuffer[endToRead-startToRead] = '\\0';\n\n\n\n\n\n\t\t\n\n\n\t\t\n\n\n\t\t\t#if DEBUG1\n\t\t\t\tprintf(\"rank %d\\n\",rank);\n\t\t\t\tprintf(\"size %d\\n\",size);\n\t\t\t\tchar name[100];\n\t\t\t\tint len;\n\t\t\t\tprintf(\"Processor name is: %s\\n\",name);\n\t\t\t#endif\n\n\t\t\tnumberOfThreads(atoi(argv[4]));\n\t\t\t\n\n\t\t\tusableCoordinates = checkerOMP(rank, buffer, bufferSize) + usableCoordinates;\n\t\t\t\n\n\t\t\tfree (buffer);\n\t\t\tlSizeL = lSize;\n\t\t\tlSize = lSize + MALLOC_SIZE;\n\t\t\tif( lSize > lSizeF ) {\n\t\t\t\tlSize = lSizeF;\n\t\t\t\tbreaker--;\n\t\t\t}\n\t\t} while((breaker > 0));\n\n\n\n\n\n\t}\n\n\n\n\tif(rank == 0) {\n\t\tprintf(\"\\nNumber of usable cordinates = %d\\n\", sum);\n\t}\n\treturn rank;\n}", "label": "int checker(char *argv[])\t{\n\tint processLimit = atoi(argv[5]);\n\tint rank,size;\n\tint error;\n\tint sum = 0;\n\n\t\n\n\terror = MPI_Init(NULL, NULL);\n\tif(error != MPI_SUCCESS) {\n\t\tMPI_Abort(MPI_COMM_WORLD,error);\n\t}\n\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\tMPI_Comm_size(MPI_COMM_WORLD,&size);\n\n        if((size < processLimit) || (processLimit == -1)) {\n                \n\n        } else {\n                size = processLimit;\n        }\n\tint coordinatesToExamine = atoi(argv[1]);\n\tif(coordinatesToExamine > 0) {\n\t\tif(coordinatesToExamine < size) {\n\t\t\tsize = coordinatesToExamine;\n\t\t}\n\t}\n\t\n\n\t\n\n\n\n        if(rank < size) {\n\t\n\n\t\t\n\n\t\tFILE * pFile;\n\t\tlong lSize, lSizeF;\n\t\tlong lSizeL = 0;\n\t\tchar * buffer;\n\t\tsize_t bufferSize;\n\t\tint usableCoordinates = 0;\n\t\tint breaker = 2;\n\n\t\tpFile = fopen ( argv[3] , \"r\" );\n\t\tif (pFile==NULL) {\n\t\t\tprintf(\"File error\");\n\t\t\texit (1);\n\t\t}\n\n\t\t\n\n\t\tfseek (pFile , 0 , SEEK_END);\n\t\tlSizeF = ftell (pFile);\n\t\trewind (pFile);\n\t\t\n\t\tif(lSizeF > MALLOC_SIZE*2) {\n\t\t\tlSize = MALLOC_SIZE;\n\t\t} else {\n\t\t\tlSize = lSizeF;\n\t\t\tbreaker--;\n\t\t}\n\t\t\n\t\tdo {\n\t\t\t\n\t\t\tbufferSize = lSize - lSizeL;\n        \t        bufferSize = bufferSize/(3*BITS_LINE);\n               \t\t\n\n               \t\tsize_t startToRead = bufferSize/size;\n\t\t\tsize_t endToRead = startToRead;\n\t\t\tstartToRead = startToRead*rank*(3*BITS_LINE);\n               \t \tendToRead = endToRead*(rank+1)*(3*BITS_LINE);\n\t\t\tif(processLimit == -1){\n                \t        if(rank == size-1) {\n                        \t        endToRead = bufferSize*(3*BITS_LINE);\n                        \t}\n                \t} else {\n                        \tif(rank == processLimit) {\n                                \tendToRead = bufferSize*(3*BITS_LINE);\n                        \t}\n                \t}\n\t\t\t\t\t\t\t\n\t\t\t\n\n\t\t\t\n\n        \t\tbuffer = (char*) malloc (sizeof(char)*(endToRead-startToRead)+1);\n\t\t\tif (buffer == NULL) {\n\t\t\t\tprintf(\"Memory error\");\n\t\t\t\texit (2);\n\t\t\t}\n\n\t\t\t\n\n\t\t\tfseek(pFile ,startToRead+lSizeL , SEEK_SET);\n\t\t\tbufferSize = fread (buffer, 1, endToRead - startToRead, pFile);\n\t\t\tbuffer[endToRead-startToRead] = '\\0';\n\n\n\n\n\n\t\t\n\n\n\t\t\n\n\n\t\t\t#if DEBUG1\n\t\t\t\tprintf(\"rank %d\\n\",rank);\n\t\t\t\tprintf(\"size %d\\n\",size);\n\t\t\t\tchar name[100];\n\t\t\t\tint len;\n\t\t\t\tMPI_Get_processor_name(name, &len);\n\t\t\t\tprintf(\"Processor name is: %s\\n\",name);\n\t\t\t#endif\n\n\t\t\tnumberOfThreads(atoi(argv[4]));\n\t\t\t\n\n\t\t\tusableCoordinates = checkerOMP(rank, buffer, bufferSize) + usableCoordinates;\n\t\t\t\n\n\t\t\tfree (buffer);\n\t\t\tlSizeL = lSize;\n\t\t\tlSize = lSize + MALLOC_SIZE;\n\t\t\tif( lSize > lSizeF ) {\n\t\t\t\tlSize = lSizeF;\n\t\t\t\tbreaker--;\n\t\t\t}\n\t\t} while((breaker > 0));\n\n\n\n\t\tMPI_Reduce(&usableCoordinates,&sum,1,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n\n\n\t}\n\n\n\n\tMPI_Finalize();\n\tif(rank == 0) {\n\t\tprintf(\"\\nNumber of usable cordinates = %d\\n\", sum);\n\t}\n\treturn rank;\n}"}
{"program": "bmi-forum_274", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tDictionary*\t\tdictionary;\n\tGeometry*\t\tgeometry;\n\tTopology*\t\ttopology;\n\tElementLayout*\t\teLayout;\n\tElement_GlobalIndex\te_I;\n\tIndex\t\t\ti;\n\tXML_IO_Handler*\t\tio_handler;\n\t\n\t\n\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\t\n\tio_handler = XML_IO_Handler_New();\n\tdictionary = Dictionary_New();\n\tIO_Handler_ReadAllFromFile( io_handler, \"data/trisurf.xml\", dictionary );\n\t\n\tgeometry = (Geometry*)IrregGeometry_New( \"irregGeometry\", dictionary, \"geometry\" );\n\ttopology = (Topology*)IrregTopology_New( \"IrregTopology\", dictionary, \"topology\" );\n\teLayout = (ElementLayout*)IrregEL_New( \"IrregEL\", dictionary, geometry, topology, \"elements\" );\n\t\n\t\n\n\t\n\tprintf( \"Element corner indices:\\n\" );\n\tfor( e_I = 0; e_I < eLayout->elementCount; e_I++ ) {\n\t\tIndex* corners = Memory_Alloc_Array( Index, eLayout->elementCornerCount, \"corners\" );\n\t\t\n\t\teLayout->buildCornerIndices( eLayout, e_I, corners );\n\t\t\n\t\tprintf( \"\\tElement %u : { %u\", e_I, corners[0] );\n\t\tfor( i = 1; i < eLayout->elementCornerCount; i++ )\n\t\t\tprintf( \", %u\", corners[i] );\n\t\tprintf( \" }\\n\" );\n\t}\n\tprintf( \"\\n\" );\n\t\n\tprintf( \"Corner element indices:\\n\" );\n\tfor( i = 0; i < eLayout->cornerCount; i++ ) {\n\t\tElement_GlobalIndex\telementCnt = eLayout->cornerElementCount( eLayout, i );\n\t\tElement_GlobalIndex*\telements = Memory_Alloc_Array( Element_GlobalIndex, elementCnt, \"elements\" );\n\t\t\n\t\teLayout->buildCornerElements( eLayout, i, elements );\n\t\t\n\t\tprintf( \"\\tCorner %u : { %u\", i, elements[0] );\n\t\tfor( e_I = 1; e_I < elementCnt; e_I++ )\n\t\t\tprintf( \", %u\", elements[e_I] );\n\t\tprintf( \" }\\n\" );\n\t}\n\tprintf( \"\\n\" );\n\t\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( eLayout );\n\tStg_Class_Delete( geometry );\n\tStg_Class_Delete( topology );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tDictionary*\t\tdictionary;\n\tGeometry*\t\tgeometry;\n\tTopology*\t\ttopology;\n\tElementLayout*\t\teLayout;\n\tElement_GlobalIndex\te_I;\n\tIndex\t\t\ti;\n\tXML_IO_Handler*\t\tio_handler;\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\t\n\tio_handler = XML_IO_Handler_New();\n\tdictionary = Dictionary_New();\n\tIO_Handler_ReadAllFromFile( io_handler, \"data/trisurf.xml\", dictionary );\n\t\n\tgeometry = (Geometry*)IrregGeometry_New( \"irregGeometry\", dictionary, \"geometry\" );\n\ttopology = (Topology*)IrregTopology_New( \"IrregTopology\", dictionary, \"topology\" );\n\teLayout = (ElementLayout*)IrregEL_New( \"IrregEL\", dictionary, geometry, topology, \"elements\" );\n\t\n\t\n\n\t\n\tprintf( \"Element corner indices:\\n\" );\n\tfor( e_I = 0; e_I < eLayout->elementCount; e_I++ ) {\n\t\tIndex* corners = Memory_Alloc_Array( Index, eLayout->elementCornerCount, \"corners\" );\n\t\t\n\t\teLayout->buildCornerIndices( eLayout, e_I, corners );\n\t\t\n\t\tprintf( \"\\tElement %u : { %u\", e_I, corners[0] );\n\t\tfor( i = 1; i < eLayout->elementCornerCount; i++ )\n\t\t\tprintf( \", %u\", corners[i] );\n\t\tprintf( \" }\\n\" );\n\t}\n\tprintf( \"\\n\" );\n\t\n\tprintf( \"Corner element indices:\\n\" );\n\tfor( i = 0; i < eLayout->cornerCount; i++ ) {\n\t\tElement_GlobalIndex\telementCnt = eLayout->cornerElementCount( eLayout, i );\n\t\tElement_GlobalIndex*\telements = Memory_Alloc_Array( Element_GlobalIndex, elementCnt, \"elements\" );\n\t\t\n\t\teLayout->buildCornerElements( eLayout, i, elements );\n\t\t\n\t\tprintf( \"\\tCorner %u : { %u\", i, elements[0] );\n\t\tfor( e_I = 1; e_I < elementCnt; e_I++ )\n\t\t\tprintf( \", %u\", elements[e_I] );\n\t\tprintf( \" }\\n\" );\n\t}\n\tprintf( \"\\n\" );\n\t\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( eLayout );\n\tStg_Class_Delete( geometry );\n\tStg_Class_Delete( topology );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "chichunchen_275", "code": "int main (int argc, char *argv[])\n{\n    int   ntasks,               \n\n          rank,                 \n\n          pc,                   \n\n          pcsum,                \n\n          foundone,             \n\n          maxprime,             \n\n          mystart,              \n\n          stride;               \n\n    \n    long long int   n,          \n\n                    limit;\n\n    double start_time,end_time;\n\n\n \n\n    mystart = (rank*2)+1;       \n\n    stride = ntasks*2;          \n\n    pc=0;                       \n\n    foundone = 0;               \n\n\n    sscanf(argv[1], \"%llu\", &limit);\n\n    \n\n    if (rank == FIRST) {\n        \n\n        printf(\"Starting. Numbers to be scanned= %lld\\n\", limit);\n        pc = 4;                  \n\n        for (n=mystart; n<=limit; n=n+stride) {\n            if (isprime(n)) {\n                pc++;\n                foundone = n;\n                \n\n            }\n        }\n        \n\n        printf(\"Done. Largest prime is %d Total primes %d\\n\",maxprime,pcsum);\n        \n\n    }\n\n\n    \n\n    if (rank > FIRST) {\n        for (n=mystart; n<=limit; n=n+stride) {\n            if (isprime(n)) {\n                pc++;\n                foundone = n;\n                \n\n            }\n        }\n    }\n\n}", "label": "int main (int argc, char *argv[])\n{\n    int   ntasks,               \n\n          rank,                 \n\n          pc,                   \n\n          pcsum,                \n\n          foundone,             \n\n          maxprime,             \n\n          mystart,              \n\n          stride;               \n\n    \n    long long int   n,          \n\n                    limit;\n\n    double start_time,end_time;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    MPI_Comm_size(MPI_COMM_WORLD,&ntasks);\n\n \n\n    mystart = (rank*2)+1;       \n\n    stride = ntasks*2;          \n\n    pc=0;                       \n\n    foundone = 0;               \n\n\n    sscanf(argv[1], \"%llu\", &limit);\n\n    \n\n    if (rank == FIRST) {\n        \n\n        printf(\"Starting. Numbers to be scanned= %lld\\n\", limit);\n        pc = 4;                  \n\n        for (n=mystart; n<=limit; n=n+stride) {\n            if (isprime(n)) {\n                pc++;\n                foundone = n;\n                \n\n            }\n        }\n        MPI_Reduce(&pc,&pcsum,1,MPI_INT,MPI_SUM,FIRST,MPI_COMM_WORLD);\n        MPI_Reduce(&foundone,&maxprime,1,MPI_INT,MPI_MAX,FIRST,MPI_COMM_WORLD);\n        \n\n        printf(\"Done. Largest prime is %d Total primes %d\\n\",maxprime,pcsum);\n        \n\n    }\n\n\n    \n\n    if (rank > FIRST) {\n        for (n=mystart; n<=limit; n=n+stride) {\n            if (isprime(n)) {\n                pc++;\n                foundone = n;\n                \n\n            }\n        }\n        MPI_Reduce(&pc,&pcsum,1,MPI_INT,MPI_SUM,FIRST,MPI_COMM_WORLD);\n        MPI_Reduce(&foundone,&maxprime,1,MPI_INT,MPI_MAX,FIRST,MPI_COMM_WORLD);\n    }\n\n    MPI_Finalize();\n}"}
{"program": "fintler_276", "code": "int main(int argc, char *argv[])\n{\n\tint i, myid, numprocs;\n\tint io_rank=0;\n\tchar buffer[BUFLEN];\n\tint report_error=1;\n\tMPI_Status status;\n\n\n\tif (numprocs!=2) {\n\t\tif (myid==io_rank) {\n\t\t\tprintf(\"ping pong test requires exactly 2 nodes.\\n\");\n\t\t}\n\t\treturn -1;\n\t}\n\n\ti=100;\n\twhile (i--) {\n\tif (myid == 0) {\n\t\tchar result[BUFLEN];\n\n\t\tsprintf(buffer, \"Hello %d\", i);\n\t\tsprintf(result, \"%s%s\",buffer, buffer);\n\n\t\trandom_wait();\n\t\t\n\n\t\trandom_wait();\n\t\t\n\n\t\tif (strcmp(result, buffer)) {\n\t\t\tif (report_error) {\n\t\t\t\tprintf(\"Error!\\n\");\n\t\t\t\treport_error=0;\n\t\t\t}\n\t\t}\n\t\tprintf(\".\");\n\t\tfflush(stdout);\n\t} \n\telse {\n\t\tint len;\n\n\t\trandom_wait();\n\t\tlen=strlen(buffer);\n\t\tstrncpy(buffer+len, buffer, len);\n\t\tbuffer[len*2]='\\0';\n\t\t\n\n\t\trandom_wait();\n\t}\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint i, myid, numprocs;\n\tint io_rank=0;\n\tchar buffer[BUFLEN];\n\tint report_error=1;\n\tMPI_Status status;\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n\tif (numprocs!=2) {\n\t\tif (myid==io_rank) {\n\t\t\tprintf(\"ping pong test requires exactly 2 nodes.\\n\");\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\ti=100;\n\twhile (i--) {\n\tif (myid == 0) {\n\t\tchar result[BUFLEN];\n\n\t\tsprintf(buffer, \"Hello %d\", i);\n\t\tsprintf(result, \"%s%s\",buffer, buffer);\n\n\t\tMPI_Send(buffer, BUFLEN, MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n\t\trandom_wait();\n\t\t\n\n\t\tMPI_Recv(buffer, BUFLEN/2, MPI_CHAR, 1, 0, MPI_COMM_WORLD, &status);\n\t\trandom_wait();\n\t\t\n\n\t\tif (strcmp(result, buffer)) {\n\t\t\tif (report_error) {\n\t\t\t\tprintf(\"Error!\\n\");\n\t\t\t\treport_error=0;\n\t\t\t}\n\t\t}\n\t\tprintf(\".\");\n\t\tfflush(stdout);\n\t} \n\telse {\n\t\tint len;\n\n\t\tMPI_Recv(buffer, BUFLEN/2, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n\t\trandom_wait();\n\t\tlen=strlen(buffer);\n\t\tstrncpy(buffer+len, buffer, len);\n\t\tbuffer[len*2]='\\0';\n\t\t\n\n\t\tMPI_Send(buffer, BUFLEN, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n\t\trandom_wait();\n\t}\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "tkhirianov_277", "code": "int main(int argc, char **argv)\n{\n\n    MPI_Status status;\n    int data_size;\n\n    if (global_rank == 0)\n    {\n        for (data_size = 1; data_size < MAX_DATA_SIZE; data_size *= 2)\n        {\n            double time0 =\n            \n\n            \n\n\n            double time1 =\n            printf(\"data size =%d \\ttime = %f\\t speed = %f B/s\\n\", data_size, time1-time0, data_size*sizeof(double)/(time1-time0));\n        }\n    }\n    else if (global_rank == 1)\n    {\n        for (data_size = 1; data_size < MAX_DATA_SIZE; data_size *= 2)\n        {\n            \n\n            \n\n        }\n    }\n\n    printf(\"Process %d of %d finished.\\n\", global_rank, num_proc);\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &global_rank);\n    MPI_Status status;\n    int data_size;\n\n    if (global_rank == 0)\n    {\n        for (data_size = 1; data_size < MAX_DATA_SIZE; data_size *= 2)\n        {\n            double time0 = MPI_Wtime();\n            \n\n            MPI_Send(trash, data_size, MPI_DOUBLE, 1, TAG, MPI_COMM_WORLD);\n            \n\n            MPI_Recv(trash, data_size, MPI_DOUBLE, MPI_ANY_SOURCE, TAG, MPI_COMM_WORLD, &status);\n\n            double time1 = MPI_Wtime();\n            printf(\"data size =%d \\ttime = %f\\t speed = %f B/s\\n\", data_size, time1-time0, data_size*sizeof(double)/(time1-time0));\n        }\n    }\n    else if (global_rank == 1)\n    {\n        for (data_size = 1; data_size < MAX_DATA_SIZE; data_size *= 2)\n        {\n            \n\n            MPI_Recv(trash, data_size, MPI_DOUBLE, MPI_ANY_SOURCE, TAG, MPI_COMM_WORLD, &status);\n            \n\n            MPI_Send(trash, data_size, MPI_DOUBLE, 0, TAG, MPI_COMM_WORLD);\n        }\n    }\n\n    printf(\"Process %d of %d finished.\\n\", global_rank, num_proc);\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "plusinfo_279", "code": "int main(int argc, char *argv[])\n{\n   int myrank, numprocs, bufsize, *buf, count,i;\n\n   MPI_File thefile;\n   MPI_Status status;\n   MPI_Offset filesize;\n\n\n\n   filesize = filesize / sizeof(int); \n\n   bufsize = filesize / numprocs + 1; \n\n\n   buf = (int *) malloc (bufsize * sizeof(int));\n   printf(\"process %d read %d ints \\n\", myrank, count);\n   for(i=0;i<count;i++)\n        printf(\"%4d\",buf[i]);\n   printf(\"\\n\");\n   return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n   int myrank, numprocs, bufsize, *buf, count,i;\n\n   MPI_File thefile;\n   MPI_Status status;\n   MPI_Offset filesize;\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n   MPI_File_open(MPI_COMM_WORLD, \"testfile\", MPI_MODE_RDONLY,MPI_INFO_NULL, &thefile);\n   MPI_File_get_size(thefile, &filesize); \n\n\n   filesize = filesize / sizeof(int); \n\n   bufsize = filesize / numprocs + 1; \n\n\n   buf = (int *) malloc (bufsize * sizeof(int));\n   MPI_File_set_view(thefile, myrank * bufsize * sizeof(int),MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);\n   MPI_File_read(thefile, buf, bufsize, MPI_INT, &status);\n   MPI_Get_count(&status, MPI_INT, &count);\n   printf(\"process %d read %d ints \\n\", myrank, count);\n   for(i=0;i<count;i++)\n        printf(\"%4d\",buf[i]);\n   printf(\"\\n\");\n   MPI_File_close(&thefile);\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "scafacos_280", "code": "int main(int argc, char** argv)\n{\n      if(me==0)printf(\"Testing IPCs (%d MPI processes)\\n\\n\",nproc);\n      ARMCI_Init();\n      test();\n      ARMCI_Finalize();\n      return 0;\n\n}", "label": "int main(int argc, char** argv)\n{\n      MPI_Init(&argc, &argv);\n      MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n      MPI_Comm_rank(MPI_COMM_WORLD, &me);\n      if(me==0)printf(\"Testing IPCs (%d MPI processes)\\n\\n\",nproc);\n      ARMCI_Init();\n      test();\n      ARMCI_Finalize();\n      MPI_Finalize();\n      return 0;\n\n}"}
{"program": "qingu_283", "code": "int main(int argc, char *argv[])\n{\n    int myid, numprocs, next, namelen;\n    char buffer[BUFLEN], processor_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Status status;\n\n\n    fprintf(stderr,\"Process %d on %s\\n\", myid, processor_name);\n    fprintf(stderr,\"Process %d of %d\\n\", myid, numprocs);\n    strcpy(buffer,\"hello there\");\n    if (myid == numprocs-1)\n\tnext = 0;\n    else\n\tnext = myid+1;\n\n    if (myid == 0)\n    {\n        printf(\"%d sending '%s' \\n\",myid,buffer);fflush(stdout);\n\tprintf(\"%d receiving \\n\",myid);fflush(stdout);\n\tprintf(\"%d received '%s' \\n\",myid,buffer);fflush(stdout);\n\t\n\n    }\n    else\n    {\n        printf(\"%d receiving  \\n\",myid);fflush(stdout);\n\tprintf(\"%d received '%s' \\n\",myid,buffer);fflush(stdout);\n\t\n\n\tprintf(\"%d sent '%s' \\n\",myid,buffer);fflush(stdout);\n    }\n    return (0);\n}", "label": "int main(int argc, char *argv[])\n{\n    int myid, numprocs, next, namelen;\n    char buffer[BUFLEN], processor_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Status status;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    MPI_Get_processor_name(processor_name,&namelen);\n\n    fprintf(stderr,\"Process %d on %s\\n\", myid, processor_name);\n    fprintf(stderr,\"Process %d of %d\\n\", myid, numprocs);\n    strcpy(buffer,\"hello there\");\n    if (myid == numprocs-1)\n\tnext = 0;\n    else\n\tnext = myid+1;\n\n    if (myid == 0)\n    {\n        printf(\"%d sending '%s' \\n\",myid,buffer);fflush(stdout);\n\tMPI_Send(buffer, strlen(buffer)+1, MPI_CHAR, next, 99, MPI_COMM_WORLD);\n\tprintf(\"%d receiving \\n\",myid);fflush(stdout);\n\tMPI_Recv(buffer, BUFLEN, MPI_CHAR, MPI_ANY_SOURCE, 99, MPI_COMM_WORLD,\n\t\t &status);\n\tprintf(\"%d received '%s' \\n\",myid,buffer);fflush(stdout);\n\t\n\n    }\n    else\n    {\n        printf(\"%d receiving  \\n\",myid);fflush(stdout);\n\tMPI_Recv(buffer, BUFLEN, MPI_CHAR, MPI_ANY_SOURCE, 99, MPI_COMM_WORLD,\n\t\t &status);\n\tprintf(\"%d received '%s' \\n\",myid,buffer);fflush(stdout);\n\t\n\n\tMPI_Send(buffer, strlen(buffer)+1, MPI_CHAR, next, 99, MPI_COMM_WORLD);\n\tprintf(\"%d sent '%s' \\n\",myid,buffer);fflush(stdout);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Finalize();\n    return (0);\n}"}
{"program": "dash-project_285", "code": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double a=1.01;\n\n\n\n  sleep(1.0);\n\n\n\n  PMPI_Type_size(DATATYPE, &dsize);\n\n  bar1();\n  bar2();\n\n\n}", "label": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double a=1.01;\n\n  MPI_Init( &argc, &argv );\n\n  MPI_Pcontrol(1, \"main\");\n\n  sleep(1.0);\n\n\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  PMPI_Type_size(DATATYPE, &dsize);\n\n  bar1();\n  bar2();\n\n  MPI_Pcontrol(-1, \"main\");\n\n  MPI_Finalize();\n}"}
{"program": "nhbrown_288", "code": "int main(int argc, const char *argv[])\n{\n  clock_t start = clock();\n  \n  \n\n  int world_rank, world_size;\n  \n  \n\n  \n  int N = 0; \n\n  unsigned long seed = 0; \n  \n\n  double dt = 0.0; \n\n  double end_time = 0.0; \n\n  \n  \n\n  switch(argc)\n  {\n    case 4 : \n\n      seed = (unsigned long) time(NULL);\n      N = atoi(argv[1]);\n      dt = atof(argv[2]);\n      end_time = atof(argv[3]);\n      break;\n      \n    case 5 : \n\n      seed = atol(argv[1]);\n      N = atoi(argv[2]);\n      dt = atof(argv[3]);\n      end_time = atof(argv[4]);\n      break;\n\n    default : \n\n      printf(\"Invalid input for start.c!\\n\");\n      exit(0);\n  }\n  \n  \n\n  if(N <= 0 || dt <= 0 || end_time <= 0)\n  {\n    fprintf(stderr, \"Negative values are not allowed!\\n\");\n    exit(0);\n  }\n  else if((N * DIM) % world_size != 0)\n  {\n    fprintf(stderr, \"N * %d must be divisible by world size and ((N * %d) / world size) must be divisible by 3!\\n\", DIM, DIM);\n    exit(0);\n  }\n  \n  createNames(); \n\n  \n  callocArrays(N);\n  \n  if(world_rank == 0)\n  {\n    printLog(seed, N, M, R, G, dt, end_time); \n\n  }\n  \n  startPlummer(seed, N, DIM, mass, pos, vel, M, R); \n\n  \n  if(world_rank == 0)\n  {\n    printInitialConditions(N, DIM, mass, pos, vel); \n\n  }\n  \n  int proc_elem = (N * DIM) / world_size;\n  startHermite(N, DIM, dt, end_time, mass, pos, vel, acc, jerk, world_rank, world_size, proc_elem); \n\n  \n  freeArrays();\n  \n  \n\n  clock_t end = clock();\n  double cpu_time = ((double) (end - start)) / CLOCKS_PER_SEC;\n  \n  if(world_rank == 0)\n  {\n    printf(\"CPU time used: %f\", cpu_time);\n  }\n  \n\n  \n  return 0;\n}", "label": "int main(int argc, const char *argv[])\n{\n  clock_t start = clock();\n  \n  \n\n  int world_rank, world_size;\n  \n  \n\n  MPI_Init(NULL, NULL);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  \n  int N = 0; \n\n  unsigned long seed = 0; \n  \n\n  double dt = 0.0; \n\n  double end_time = 0.0; \n\n  \n  \n\n  switch(argc)\n  {\n    case 4 : \n\n      seed = (unsigned long) time(NULL);\n      N = atoi(argv[1]);\n      dt = atof(argv[2]);\n      end_time = atof(argv[3]);\n      break;\n      \n    case 5 : \n\n      seed = atol(argv[1]);\n      N = atoi(argv[2]);\n      dt = atof(argv[3]);\n      end_time = atof(argv[4]);\n      break;\n\n    default : \n\n      printf(\"Invalid input for start.c!\\n\");\n      exit(0);\n  }\n  \n  \n\n  if(N <= 0 || dt <= 0 || end_time <= 0)\n  {\n    fprintf(stderr, \"Negative values are not allowed!\\n\");\n    exit(0);\n  }\n  else if((N * DIM) % world_size != 0)\n  {\n    fprintf(stderr, \"N * %d must be divisible by world size and ((N * %d) / world size) must be divisible by 3!\\n\", DIM, DIM);\n    exit(0);\n  }\n  \n  createNames(); \n\n  \n  callocArrays(N);\n  \n  if(world_rank == 0)\n  {\n    printLog(seed, N, M, R, G, dt, end_time); \n\n  }\n  \n  startPlummer(seed, N, DIM, mass, pos, vel, M, R); \n\n  \n  if(world_rank == 0)\n  {\n    printInitialConditions(N, DIM, mass, pos, vel); \n\n  }\n  \n  int proc_elem = (N * DIM) / world_size;\n  startHermite(N, DIM, dt, end_time, mass, pos, vel, acc, jerk, world_rank, world_size, proc_elem); \n\n  \n  freeArrays();\n  \n  \n\n  clock_t end = clock();\n  double cpu_time = ((double) (end - start)) / CLOCKS_PER_SEC;\n  \n  if(world_rank == 0)\n  {\n    printf(\"CPU time used: %f\", cpu_time);\n  }\n  \n  MPI_Finalize(); \n\n  \n  return 0;\n}"}
{"program": "eliask_289", "code": "main(int argc, char *argv[])\n{\n  int streamnum, nstreams, seed, *stream, i, myid, nprocs;\n  double rn;\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n  streamnum = myid;\n  nstreams = nprocs;\t\t\n\n  seed = make_sprng_seed();\t\n\n\n  \n\n  printf(\"Process %d: seed = %16d\\n\", myid, seed);\n\n  stream = init_sprng(streamnum,nstreams,seed,SPRNG_DEFAULT);\t\n\n  printf(\"Process %d: Print information about stream:\\n\",myid);\n  print_sprng(stream);\n\n  \n\n\n  for (i=0;i<3;i++)\n  {\n    rn = sprng(stream);\t\t\n\n    printf(\"process %d, random number %d: %f\\n\", myid, i+1, rn);\n  }\n\n  free_sprng(stream);           \n\n\n\n}", "label": "main(int argc, char *argv[])\n{\n  int streamnum, nstreams, seed, *stream, i, myid, nprocs;\n  double rn;\n\n\n\n  \n\n\n  MPI_Init(&argc, &argv);\t\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\t\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs); \n\n\n  \n\n\n  streamnum = myid;\n  nstreams = nprocs;\t\t\n\n  seed = make_sprng_seed();\t\n\n\n  \n\n  printf(\"Process %d: seed = %16d\\n\", myid, seed);\n\n  stream = init_sprng(streamnum,nstreams,seed,SPRNG_DEFAULT);\t\n\n  printf(\"Process %d: Print information about stream:\\n\",myid);\n  print_sprng(stream);\n\n  \n\n\n  for (i=0;i<3;i++)\n  {\n    rn = sprng(stream);\t\t\n\n    printf(\"process %d, random number %d: %f\\n\", myid, i+1, rn);\n  }\n\n  free_sprng(stream);           \n\n\n  MPI_Finalize();\t\t\n\n}"}
{"program": "qingu_290", "code": "int main(int argc, char *argv[])\n{\n    int myrank;\n    MPI_Datatype subarray;\n    int array_size[] = {X, Y, Z};\n    int array_subsize[] = {X/2, Y/2, Z};\n    int array_start[] = {0, 0, 0};\n    int i, j, k;\n    int errs = 0;\n\n\n    for (i = 0; i < X; ++i) {\n        for (j = 0; j < Y; ++j) {\n            for (k = 0; k < Z; ++k) {\n                if (myrank == 0)\n                    array[i][j][k] = 2.0;\n                else\n                    array[i][j][k] = -2.0;\n            }\n        }\n    }\n\n\n    if(myrank == 0)\n    else {\n        for (i = array_start[0]; i < array_subsize[0]; ++i) {\n            for (j = array_start[1]; j < array_subsize[1]; ++j) {\n                for (k = array_start[2]; k < array_subsize[2]; ++k) {\n                    if (array[i][j][k] != 2.0)\n                        ++errs;\n                }\n            }\n        }\n    }\n\n\n    if (myrank == 0) {\n        if (errs)\n            printf(\"Found %d errors\\n\", errs);\n        else\n            printf(\" No Errors\\n\");\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int myrank;\n    MPI_Datatype subarray;\n    int array_size[] = {X, Y, Z};\n    int array_subsize[] = {X/2, Y/2, Z};\n    int array_start[] = {0, 0, 0};\n    int i, j, k;\n    int errs = 0;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n    for (i = 0; i < X; ++i) {\n        for (j = 0; j < Y; ++j) {\n            for (k = 0; k < Z; ++k) {\n                if (myrank == 0)\n                    array[i][j][k] = 2.0;\n                else\n                    array[i][j][k] = -2.0;\n            }\n        }\n    }\n\n    MPI_Type_create_subarray(3, array_size, array_subsize, array_start, MPI_ORDER_C,\n                             MPI_DOUBLE, &subarray);\n    MPI_Type_commit(&subarray);\n\n    if(myrank == 0)\n        MPI_Send(array, 1, subarray, 1, 0, MPI_COMM_WORLD);\n    else {\n        MPI_Recv(array, 1, subarray, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (i = array_start[0]; i < array_subsize[0]; ++i) {\n            for (j = array_start[1]; j < array_subsize[1]; ++j) {\n                for (k = array_start[2]; k < array_subsize[2]; ++k) {\n                    if (array[i][j][k] != 2.0)\n                        ++errs;\n                }\n            }\n        }\n    }\n\n    MPI_Type_free(&subarray);\n\n    MPI_Allreduce(MPI_IN_PLACE, &errs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    if (myrank == 0) {\n        if (errs)\n            printf(\"Found %d errors\\n\", errs);\n        else\n            printf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_291", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* entryPoint;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_0_CastType );\n\tif( rank == procToWatch ) {\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\t}\n\tEntryPoint_Append( entryPoint, \"Test0\", (void*)Test0, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test1\", (void*)Test1, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test2\", (void*)Test2, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test3\", (void*)Test3, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test4\", (void*)Test4, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test5\", (void*)Test5, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test6\", (void*)Test6, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test7\", (void*)Test7, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test8\", (void*)Test8, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test9\", (void*)Test9, \"testCode\" );\n\tif( rank == procToWatch ) {\n\t\tHook_Index hookIndex;\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\n\t\tfor (hookIndex = 0; hookIndex < entryPoint->hooks->count; hookIndex++ ) {\n\t\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->data[%d]->name: %s\\n\", hookIndex,\n\t\t\t\tentryPoint->hooks->data[hookIndex]->name  );\n\t\t}\n\t}\n\n\t\n\n\tif( rank == procToWatch ) {\n\t\t((EntryPoint_0_CallCast*) entryPoint->run)( entryPoint );\n\t}\n\n\t\n\n\tStg_Class_Delete( entryPoint );\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* entryPoint;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_0_CastType );\n\tif( rank == procToWatch ) {\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\t}\n\tEntryPoint_Append( entryPoint, \"Test0\", (void*)Test0, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test1\", (void*)Test1, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test2\", (void*)Test2, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test3\", (void*)Test3, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test4\", (void*)Test4, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test5\", (void*)Test5, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test6\", (void*)Test6, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test7\", (void*)Test7, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test8\", (void*)Test8, \"testCode\" );\n\tEntryPoint_Append( entryPoint, \"Test9\", (void*)Test9, \"testCode\" );\n\tif( rank == procToWatch ) {\n\t\tHook_Index hookIndex;\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->_size: %u\\n\", entryPoint->hooks->_size );\n\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->count: %u\\n\", entryPoint->hooks->count );\n\n\t\tfor (hookIndex = 0; hookIndex < entryPoint->hooks->count; hookIndex++ ) {\n\t\t\tJournal_Printf( (void*) stream, \"entryPoint->hooks->data[%d]->name: %s\\n\", hookIndex,\n\t\t\t\tentryPoint->hooks->data[hookIndex]->name  );\n\t\t}\n\t}\n\n\t\n\n\tif( rank == procToWatch ) {\n\t\t((EntryPoint_0_CallCast*) entryPoint->run)( entryPoint );\n\t}\n\n\t\n\n\tStg_Class_Delete( entryPoint );\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "lapesd_294", "code": "int \nmain (int argc, char **argv)\n{\n\tint nprocs, rank;\n\tint sqrnprocs;\n\tMPI_File file;\n\tMPI_Status status;\n\tMPI_Offset off;\n\tint mode = MPI_MODE_CREATE | MPI_MODE_RDWR;\n\tint s;\n\tdouble **bw;\n\tdouble **br;\n\tint i, j;\n\tint ret;\n\tssize_t total;\n\n\tassert (argc == 2);\n\n\tsqrnprocs = sqrt (nprocs);\n\n\tret =\n        assert (ret == MPI_SUCCESS);\n\n\ts = SIZE / sqrnprocs;\n\t\n\tbw = array2d (s, s);\n\tfillarray2d (bw, s, s, nprocs, rank);\n\ttotal = 0;\n\tfor (i = 0; i < s; i++)\n\t{\n\t\toff = ((rank / sqrnprocs * s + i) * SIZE * sizeof (double)) + ((rank % sqrnprocs * s) * sizeof (double));\n\t\tret =\n\t\tassert (ret == MPI_SUCCESS);\n                total += s * sizeof (double);\n\t}\n\tfprintf (stdout, \"Rank %d: %ld bytes written.\\n\", rank, total);\n\n\tbr = array2d (s, s);\n\ttotal = 0;\n\tfor (i = 0; i < s; i++)\n\t{\n\t\toff = ((rank / sqrnprocs * s + i) * SIZE * sizeof (double)) + ((rank % sqrnprocs * s) * sizeof (double));\n\t\tret =\n                assert (ret == MPI_SUCCESS);\n                total += s * sizeof (double);\n\t}\n\tfprintf (stdout, \"Rank %d: %ld bytes read.\\n\", rank, total);\t\n\n\tprintarray2d (br, s, s, rank);\n\n\n\n\texit (EXIT_FAILURE);\n}", "label": "int \nmain (int argc, char **argv)\n{\n\tint nprocs, rank;\n\tint sqrnprocs;\n\tMPI_File file;\n\tMPI_Status status;\n\tMPI_Offset off;\n\tint mode = MPI_MODE_CREATE | MPI_MODE_RDWR;\n\tint s;\n\tdouble **bw;\n\tdouble **br;\n\tint i, j;\n\tint ret;\n\tssize_t total;\n\n\tassert (argc == 2);\n\n\tMPI_Init (&argc, &argv);\n\tMPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n\tMPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\tsqrnprocs = sqrt (nprocs);\n\n\tret = MPI_File_open (MPI_COMM_WORLD, argv[1], mode, MPI_INFO_NULL, &file);\n        assert (ret == MPI_SUCCESS);\n\n\ts = SIZE / sqrnprocs;\n\t\n\tbw = array2d (s, s);\n\tfillarray2d (bw, s, s, nprocs, rank);\n\ttotal = 0;\n\tfor (i = 0; i < s; i++)\n\t{\n\t\toff = ((rank / sqrnprocs * s + i) * SIZE * sizeof (double)) + ((rank % sqrnprocs * s) * sizeof (double));\n\t\tret = MPI_File_write_at (file, off, bw[i], s, MPI_DOUBLE, &status);\n\t\tassert (ret == MPI_SUCCESS);\n                total += s * sizeof (double);\n\t}\n\tfprintf (stdout, \"Rank %d: %ld bytes written.\\n\", rank, total);\n\tMPI_Barrier (MPI_COMM_WORLD);\n\n\tbr = array2d (s, s);\n\ttotal = 0;\n\tfor (i = 0; i < s; i++)\n\t{\n\t\toff = ((rank / sqrnprocs * s + i) * SIZE * sizeof (double)) + ((rank % sqrnprocs * s) * sizeof (double));\n\t\tret = MPI_File_read_at (file, off, br[i], s, MPI_DOUBLE, &status);\n                assert (ret == MPI_SUCCESS);\n                total += s * sizeof (double);\n\t}\n\tfprintf (stdout, \"Rank %d: %ld bytes read.\\n\", rank, total);\t\n\tMPI_Barrier (MPI_COMM_WORLD);\n\n\tprintarray2d (br, s, s, rank);\n\tMPI_Barrier (MPI_COMM_WORLD);\n\n\tMPI_File_close (&file);\n\n\tMPI_Finalize ();\n\n\texit (EXIT_FAILURE);\n}"}
{"program": "tfogal_295", "code": "int\nmain(int argc, char* argv[])\n{\n  if(rank() == 0 && argc < 2) {\n    fprintf(stderr, \"Need at least one argument: the binary to run.\\n\");\n  }\n  ROOT(printf(\"Running on %zu procs.\\n\", size()));\n\n  \n\n  char* subv[argc]; memset(subv, 0, sizeof(char*)*argc);\n  char* command;\n  assert(argc > 0);\n  rebuild_args((size_t)argc, argv, &command, subv);\n\n  MPI_Comm intercomm; \n\n  int errors[size()];\n  if(setenv(\"MEANING_OF_EVERYTHING\", \"42\", 1) != 0) {\n    fprintf(stderr, \"[%zu] failed setting env var.\\n\", rank());\n  }\n  int spawn =\n  if(spawn != MPI_SUCCESS) {\n    fprintf(stderr, \"[%zu] spawn error: %d\\n\", rank(), spawn);\n  }\n  for(size_t i=0; rank()==0 && i < size(); ++i) {\n    if(errors[i] != MPI_SUCCESS) {\n      printf(\"process %zu error: %d\\n\", i, errors[i]);\n    }\n  }\n\n  return 0;\n}", "label": "int\nmain(int argc, char* argv[])\n{\n  MPI_Init(&argc, &argv);\n  if(rank() == 0 && argc < 2) {\n    fprintf(stderr, \"Need at least one argument: the binary to run.\\n\");\n    MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n  }\n  ROOT(printf(\"Running on %zu procs.\\n\", size()));\n\n  \n\n  char* subv[argc]; memset(subv, 0, sizeof(char*)*argc);\n  char* command;\n  assert(argc > 0);\n  rebuild_args((size_t)argc, argv, &command, subv);\n\n  MPI_Comm intercomm; \n\n  int errors[size()];\n  if(setenv(\"MEANING_OF_EVERYTHING\", \"42\", 1) != 0) {\n    fprintf(stderr, \"[%zu] failed setting env var.\\n\", rank());\n  }\n  int spawn = MPI_Comm_spawn(command, subv, (int)size(), MPI_INFO_NULL, 0,\n                             MPI_COMM_WORLD, &intercomm, errors);\n  if(spawn != MPI_SUCCESS) {\n    fprintf(stderr, \"[%zu] spawn error: %d\\n\", rank(), spawn);\n  }\n  for(size_t i=0; rank()==0 && i < size(); ++i) {\n    if(errors[i] != MPI_SUCCESS) {\n      printf(\"process %zu error: %d\\n\", i, errors[i]);\n    }\n  }\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bmi-forum_296", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\tPlane\t\t\t*planeRefs[CACHE_SIZE];\n\t\n\tStream *myStream = NULL;\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tMemoryPool *pool = NULL;\n\t\tPlane *p = NULL;\n\t\tint i = 0, passed = 0;\n\n\t\tpool = MemoryPool_New( Plane, CACHE_SIZE );\n\t\tmyStream = Journal_Register( InfoStream_Type, \"MemoryPoolStream\" );\n\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing memory allocation from the Memory Pool.. \" );\n\t\tfor( i=0; i<CACHE_SIZE; i++ ){\n\t\t\tp = NULL;\n\t\t\tp = MemoryPool_NewObject( Plane, pool );\n\t\t\tif( !p ){\n\t\t\t\tpassed = 0;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tplaneRefs[i] = p;\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t\t\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing out of memory.. \" );\n\t\tfor( i=0; i<10; i++ ){\n\t\t\tp = MemoryPool_NewObject( Plane, pool );\n\t\t\tif( p ){\n\t\t\t\tpassed = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t\t\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing memory deallocations.. \" );\n\t\tfor( i=0; i<CACHE_SIZE; i++ ){\n\t\t\tif(!MemoryPool_DeleteObject( pool, planeRefs[i] )){\n\t\t\t\tpassed = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t\t\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing illegal memory deallocations.. \" );\n\t\t{\n\t\t\tint *junkRefs[CACHE_SIZE];\n\t\t\tfor( i=0; i<CACHE_SIZE/4; i++ ){\n\t\t\t\tjunkRefs[i] = (int*)(junkRefs+i+1);\n\t\t\t\t\n\t\t\t\tif(MemoryPool_DeleteObject( pool, junkRefs[i] )){\n\t\t\t\t\tpassed = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t}\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\tPlane\t\t\t*planeRefs[CACHE_SIZE];\n\t\n\tStream *myStream = NULL;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tMemoryPool *pool = NULL;\n\t\tPlane *p = NULL;\n\t\tint i = 0, passed = 0;\n\n\t\tpool = MemoryPool_New( Plane, CACHE_SIZE );\n\t\tmyStream = Journal_Register( InfoStream_Type, \"MemoryPoolStream\" );\n\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing memory allocation from the Memory Pool.. \" );\n\t\tfor( i=0; i<CACHE_SIZE; i++ ){\n\t\t\tp = NULL;\n\t\t\tp = MemoryPool_NewObject( Plane, pool );\n\t\t\tif( !p ){\n\t\t\t\tpassed = 0;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tplaneRefs[i] = p;\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t\t\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing out of memory.. \" );\n\t\tfor( i=0; i<10; i++ ){\n\t\t\tp = MemoryPool_NewObject( Plane, pool );\n\t\t\tif( p ){\n\t\t\t\tpassed = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t\t\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing memory deallocations.. \" );\n\t\tfor( i=0; i<CACHE_SIZE; i++ ){\n\t\t\tif(!MemoryPool_DeleteObject( pool, planeRefs[i] )){\n\t\t\t\tpassed = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t\t\n\t\tpassed = 1;\n\t\tJournal_Printf( myStream, \"Testing illegal memory deallocations.. \" );\n\t\t{\n\t\t\tint *junkRefs[CACHE_SIZE];\n\t\t\tfor( i=0; i<CACHE_SIZE/4; i++ ){\n\t\t\t\tjunkRefs[i] = (int*)(junkRefs+i+1);\n\t\t\t\t\n\t\t\t\tif(MemoryPool_DeleteObject( pool, junkRefs[i] )){\n\t\t\t\t\tpassed = 0;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tJournal_Printf( myStream, \"%s\\n\", passed?\"Passed\\n\":\"Failed\\n\" );\n\t}\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "j2sg_297", "code": "int main(int argc, char *argv[])\n{\n   int idp;              \n\n   int np;               \n\n   char msg[MAXSTRING];  \n\n\n\n   if(idp == 0) {\n      while(--np) {\n         printf(\"%s\\n\", msg);\n      }\n   } else {\n      sprintf(msg, \"Hello from process %d/%d\", idp, np);\n   }\n\n\n   return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n   int idp;              \n\n   int np;               \n\n   char msg[MAXSTRING];  \n\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &idp);\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n   if(idp == 0) {\n      while(--np) {\n         MPI_Recv(msg, MAXSTRING, MPI_CHAR, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         printf(\"%s\\n\", msg);\n      }\n   } else {\n      sprintf(msg, \"Hello from process %d/%d\", idp, np);\n      MPI_Send(msg, strlen(msg) + 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n   }\n\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "hubonit_298", "code": "int main(int argc,char *argv[])\n{\n    int    n, myid, numprocs, i;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int    namelen;\n    char   processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n    fprintf(stdout,\"Process %d of %d is on %s\\n\", myid, numprocs, processor_name);\n    fflush(stdout);\n\n    int a[40];\n    n = 40;\n    if (myid == 0) startwtime =\n\n\n   for ( i = myid + 1 ; i <= n ; i += numprocs)\n   {\n      printf(\"%d\\n\", Fibonacci(i));\n   }\n\n\n    if (myid == 0) {\n\tendwtime =\n\n\tprintf(\"wall clock time = %f\\n\", endwtime-startwtime);\t       \n\tfflush(stdout);\n    }\n\n    return 0;\n}", "label": "int main(int argc,char *argv[])\n{\n    int    n, myid, numprocs, i;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int    namelen;\n    char   processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    MPI_Get_processor_name(processor_name,&namelen);\n\n    fprintf(stdout,\"Process %d of %d is on %s\\n\", myid, numprocs, processor_name);\n    fflush(stdout);\n\n    int a[40];\n    n = 40;\n    if (myid == 0) startwtime = MPI_Wtime();\n\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   for ( i = myid + 1 ; i <= n ; i += numprocs)\n   {\n      printf(\"%d\\n\", Fibonacci(i));\n   }\n\n\n    if (myid == 0) {\n\tendwtime = MPI_Wtime();\n\n\tprintf(\"wall clock time = %f\\n\", endwtime-startwtime);\t       \n\tfflush(stdout);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "blue42u_302", "code": "static adlb_code run()\n{\n  adlb_code ac;\n\n  fprintf(stderr, \"Initializing...\\n\");\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n  int rc =\n  ADLB_CHECK_MSG(rc == MPI_SUCCESS, \"error setting up MPI\");\n\n  fprintf(stderr, \"Running benchmarks...\\n\");\n  report_hdr();\n\n  \n\n  tgt_mix tgts[] = {UNTARGETED, RANK_TARGETED, NODE_TARGETED,\n          EQUAL_MIX, RANK_SOFT_TARGETED, \n};\n  int ntgts = sizeof(tgts)/sizeof(tgts[0]);\n  prio_mix prios[] = {EQUAL, UNIFORM_RANDOM};\n  int nprios = sizeof(prios)/sizeof(prios[0]);\n\n  for (int exp_iter = 0; exp_iter < 5; exp_iter++)\n  {\n    bool report = exp_iter > 0;\n\n    for (int tgt_idx = 0; tgt_idx < ntgts; tgt_idx++)\n    {\n      for (int prio_idx = 0; prio_idx < nprios; prio_idx++)\n      {\n        for (int init_qlen = min_init_qlen; init_qlen <= max_init_qlen;\n                init_qlen = init_qlen == 0 ? 1 : init_qlen * qlen_growth)\n        {\n          ac = expt(prios[prio_idx], tgts[tgt_idx], init_qlen, report);\n          ADLB_CHECK(ac);\n        }\n      }\n    }\n  }\n\n  fprintf(stderr, \"Finalizing...\\n\");\n\n\n  fprintf(stderr, \"Done.\\n\");\n\n  return ADLB_SUCCESS;\n}", "label": "static adlb_code run()\n{\n  adlb_code ac;\n\n  fprintf(stderr, \"Initializing...\\n\");\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n  int rc = MPI_Init(&mpi_argc, &mpi_argv);\n  ADLB_CHECK_MSG(rc == MPI_SUCCESS, \"error setting up MPI\");\n\n  fprintf(stderr, \"Running benchmarks...\\n\");\n  report_hdr();\n\n  \n\n  tgt_mix tgts[] = {UNTARGETED, RANK_TARGETED, NODE_TARGETED,\n          EQUAL_MIX, RANK_SOFT_TARGETED, \n};\n  int ntgts = sizeof(tgts)/sizeof(tgts[0]);\n  prio_mix prios[] = {EQUAL, UNIFORM_RANDOM};\n  int nprios = sizeof(prios)/sizeof(prios[0]);\n\n  for (int exp_iter = 0; exp_iter < 5; exp_iter++)\n  {\n    bool report = exp_iter > 0;\n\n    for (int tgt_idx = 0; tgt_idx < ntgts; tgt_idx++)\n    {\n      for (int prio_idx = 0; prio_idx < nprios; prio_idx++)\n      {\n        for (int init_qlen = min_init_qlen; init_qlen <= max_init_qlen;\n                init_qlen = init_qlen == 0 ? 1 : init_qlen * qlen_growth)\n        {\n          ac = expt(prios[prio_idx], tgts[tgt_idx], init_qlen, report);\n          ADLB_CHECK(ac);\n        }\n      }\n    }\n  }\n\n  fprintf(stderr, \"Finalizing...\\n\");\n\n  MPI_Finalize();\n\n  fprintf(stderr, \"Done.\\n\");\n\n  return ADLB_SUCCESS;\n}"}
{"program": "yhamoudi_304", "code": "int main(int argc, char **argv) {\n    char filename[100] = \"average.log.\" ;\n    FILE *f ;\n    int width, height, myid, nbproc, nbiter ;\n    double p ;\n    Process process ;\n    \n\n    if(myid==ONLY) {\n        scanf(\"%d\",&width) ;\n        scanf(\"%d\",&height) ;\n        scanf(\"%lf\",&p) ;\n        scanf(\"%d\",&nbiter) ;\n    }\n    \n\n    \n\n    process = initProcess(myid,nbproc,width,height,p,nbiter) ;\n    if(process) { \n\n        int ok = 1 ;\n        if(argc == 2 && !strcmp(\"-log\",argv[1])) { \n\n            sprintf(&filename[sizeof(\"average.log.\")-1], \"%d\", myid) ;\n            f = fopen(filename,\"w\") ;\n            printProcess(process,f) ;\n            fclose(f) ;\n        }\n        else if(argc != 1) { \n\n            if(process->myid == ONLY) {\n                printf(\"Syntax: ./%s\\n\",argv[0]) ;\n                printf(\"        ./%s -log\\n\",argv[0]) ;\n            }\n            ok = 0;\n        }\n        if(ok) { \n\n            inputOutput(process) ;\n            delProcess(process) ;\n        }\n    }\n    return 0 ;\n}", "label": "int main(int argc, char **argv) {\n    MPI_Init(&argc,&argv) ;\n    char filename[100] = \"average.log.\" ;\n    FILE *f ;\n    int width, height, myid, nbproc, nbiter ;\n    double p ;\n    Process process ;\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid) ;\n    MPI_Comm_size(MPI_COMM_WORLD, &nbproc) ;\n    \n\n    if(myid==ONLY) {\n        scanf(\"%d\",&width) ;\n        scanf(\"%d\",&height) ;\n        scanf(\"%lf\",&p) ;\n        scanf(\"%d\",&nbiter) ;\n    }\n    \n\n    MPI_Bcast(&width, 1, MPI_INT, ONLY, MPI_COMM_WORLD) ;\n    MPI_Bcast(&height, 1, MPI_INT, ONLY, MPI_COMM_WORLD) ;\n    MPI_Bcast(&p, 1, MPI_DOUBLE, ONLY, MPI_COMM_WORLD) ;\n    MPI_Bcast(&nbiter, 1, MPI_INT, ONLY, MPI_COMM_WORLD) ;\n    \n\n    process = initProcess(myid,nbproc,width,height,p,nbiter) ;\n    if(process) { \n\n        int ok = 1 ;\n        if(argc == 2 && !strcmp(\"-log\",argv[1])) { \n\n            sprintf(&filename[sizeof(\"average.log.\")-1], \"%d\", myid) ;\n            f = fopen(filename,\"w\") ;\n            printProcess(process,f) ;\n            fclose(f) ;\n        }\n        else if(argc != 1) { \n\n            if(process->myid == ONLY) {\n                printf(\"Syntax: ./%s\\n\",argv[0]) ;\n                printf(\"        ./%s -log\\n\",argv[0]) ;\n            }\n            ok = 0;\n        }\n        if(ok) { \n\n            inputOutput(process) ;\n            delProcess(process) ;\n        }\n    }\n    MPI_Finalize() ;\n    return 0 ;\n}"}
{"program": "milfeld2_305", "code": "int main(int argc, char * argv[]){\n\nint i,j;                  \n\n\nint rank, nranks;         \n\n\nint nsec = 10;            \n\n\nint cpuid;                \n\n\n\n\n\n\n\n\n   \n\n   \n\n                                      \n\n\n   mpi_report_mask();                 \n\n\n   load_cpu_nsec( nsec );             \n\n\n\n}", "label": "int main(int argc, char * argv[]){\n\nint i,j;                  \n\n\nint rank, nranks;         \n\n\nint nsec = 10;            \n\n\nint cpuid;                \n\n\n\n\n\n\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   \n\n   \n\n                                      \n\n\n   mpi_report_mask();                 \n\n\n   load_cpu_nsec( nsec );             \n\n\n   MPI_Finalize();\n\n}"}
{"program": "mpip_307", "code": "int main(int argc, char **argv)\n{\n  int np[2], inplace, loops;\n  ptrdiff_t n[3];\n  unsigned opt, tune, destroy_input;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  inplace = 0;\n  opt     = PFFT_ESTIMATE;\n  tune    = PFFT_NO_TUNE;\n  destroy_input = PFFT_PRESERVE_INPUT;\n  loops   = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  init_parameters(argc, argv, n, np, &loops, &inplace, &opt, &tune, &destroy_input);\n\n  measure_pfft(n, np, MPI_COMM_WORLD, loops, inplace, opt | tune | destroy_input);\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2], inplace, loops;\n  ptrdiff_t n[3];\n  unsigned opt, tune, destroy_input;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  inplace = 0;\n  opt     = PFFT_ESTIMATE;\n  tune    = PFFT_NO_TUNE;\n  destroy_input = PFFT_PRESERVE_INPUT;\n  loops   = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  init_parameters(argc, argv, n, np, &loops, &inplace, &opt, &tune, &destroy_input);\n\n  measure_pfft(n, np, MPI_COMM_WORLD, loops, inplace, opt | tune | destroy_input);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bsc-performance-tools_309", "code": "int main (int argc, char *argv[])\n{\n\tint res;\n\tint ntasks;\n\tint idtask;\n\n\tres =\n\n\tres =\n\n\tres =\n\n\tmerger_pre (ntasks);\n\tProcessArgs (idtask, argc, argv);\n\tmerger_post (ntasks, idtask);\n\n\tres =\n\n\treturn 0;\n}", "label": "int main (int argc, char *argv[])\n{\n\tint res;\n\tint ntasks;\n\tint idtask;\n\n\tres = MPI_Init (&argc, &argv);\n\tMPI_CHECK (res, MPI_Init, \"Failed to initialize MPI\");\n\n\tres = MPI_Comm_size (MPI_COMM_WORLD, &ntasks);\n\tMPI_CHECK (res, MPI_Comm_size, \"Failed to call MPI_Comm_size\");\n\n\tres = MPI_Comm_rank (MPI_COMM_WORLD, &idtask);\n\tMPI_CHECK (res, MPI_Comm_size, \"Failed to call MPI_Comm_rank\");\n\n\tmerger_pre (ntasks);\n\tProcessArgs (idtask, argc, argv);\n\tmerger_post (ntasks, idtask);\n\n\tres = MPI_Finalize ();\n\tMPI_CHECK (res, MPI_Finalize, \"Failed to uninitialize MPI\");\n\n\treturn 0;\n}"}
{"program": "linhbngo_310", "code": "int main(int argc, char *argv[])\n{\n    int rank, size;    \n    int i; \n    int recvcounts[4] = {1,2,3,4}; \n\n                                   \n\n    int displs[4] = {0,0,0,0}; \n\n    int *sendbuf; \n\n    int *recvbuf; \n\n\n\n    \n\n    if (rank == 0){\n      recvbuf = malloc(sizeof(int) * (10));\n\n      for (i = 0; i < 10; i ++)\n        recvbuf[i] = -1;\n    }\n    \n    \n\n    sendbuf = malloc(sizeof(int) * (rank + 1));\n    for (i = 0; i < (rank + 1); i++){\n        sendbuf[i] = rank;\n    }\n    \n    \n\n    for (i = 1; i < 4; i++) {\n        displs[i] = displs[i-1] + recvcounts[i-1] - 1;\n    }\n\n    \n\n\n    \n\n    if (rank == 0){\n      for (i = 0; i < 10; i++) {\n        printf(\"%d \", recvbuf[i]);\n      }\n      printf(\"\\n\");\n      free(recvbuf);\n    }\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, size;    \n    int i; \n    int recvcounts[4] = {1,2,3,4}; \n\n                                   \n\n    int displs[4] = {0,0,0,0}; \n\n    int *sendbuf; \n\n    int *recvbuf; \n\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    if (rank == 0){\n      recvbuf = malloc(sizeof(int) * (10));\n\n      for (i = 0; i < 10; i ++)\n        recvbuf[i] = -1;\n    }\n    \n    \n\n    sendbuf = malloc(sizeof(int) * (rank + 1));\n    for (i = 0; i < (rank + 1); i++){\n        sendbuf[i] = rank;\n    }\n    \n    \n\n    for (i = 1; i < 4; i++) {\n        displs[i] = displs[i-1] + recvcounts[i-1] - 1;\n    }\n\n    \n\n    MPI_Gatherv(sendbuf, rank + 1, MPI_INT, recvbuf, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n    \n\n    if (rank == 0){\n      for (i = 0; i < 10; i++) {\n        printf(\"%d \", recvbuf[i]);\n      }\n      printf(\"\\n\");\n      free(recvbuf);\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "axessim_311", "code": "int main(int argc, char **argv)\n{\n\n\n  int mpirank;\n\n  int nbprocess;\n\n  if(nbprocess != 3)\n  {\n    print(mpirank, \"Please relaunch with three process.\\n\");\n    return EXIT_FAILURE;\n  }\n\n\n  \n\n\n  hid_t plist_id = H5Pcreate(H5P_FILE_ACCESS);\n\n  MPI_Info info = MPI_INFO_NULL;\n\n  \n\n\n  herr_t status = H5Pset_fapl_mpio(plist_id, MPI_COMM_WORLD, info);\n\n  \n\n  hid_t hdf = H5Fcreate(\"ut_paral2.h5\", H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);\n\n  \n\n  H5Pclose(plist_id);\n\n\n\n  char *message;\n\n\n\n  message = test_assym_low(hdf, mpirank);\n\n  if(message!=NULL)\n  {\n    printf(\"%d** %s\\n\", mpirank, message);\n    return EXIT_FAILURE;\n  }\n\n\n\n\n\n\n\n  message = test_assym(hdf, mpirank);\n\n  if(message!=NULL)\n  {\n    printf(\"%d** %s\\n\", mpirank, message);\n    return EXIT_FAILURE;\n  }\n\n\n  H5Fclose(hdf);\n\n\n  print(mpirank, \"SUCCESS\\n\");\n\n  return EXIT_SUCCESS;\n}", "label": "int main(int argc, char **argv)\n{\n\n  MPI_Init(&argc, &argv);\n\n  int mpirank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpirank);\n\n  int nbprocess;\n  MPI_Comm_size(MPI_COMM_WORLD, &nbprocess);\n\n  if(nbprocess != 3)\n  {\n    print(mpirank, \"Please relaunch with three process.\\n\");\n    return EXIT_FAILURE;\n  }\n\n\n  \n\n\n  hid_t plist_id = H5Pcreate(H5P_FILE_ACCESS);\n\n  MPI_Info info = MPI_INFO_NULL;\n\n  \n\n\n  herr_t status = H5Pset_fapl_mpio(plist_id, MPI_COMM_WORLD, info);\n\n  \n\n  hid_t hdf = H5Fcreate(\"ut_paral2.h5\", H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);\n\n  \n\n  H5Pclose(plist_id);\n\n\n\n  char *message;\n\n\n\n  message = test_assym_low(hdf, mpirank);\n\n  if(message!=NULL)\n  {\n    printf(\"%d** %s\\n\", mpirank, message);\n    return EXIT_FAILURE;\n  }\n\n\n\n\n\n\n\n  message = test_assym(hdf, mpirank);\n\n  if(message!=NULL)\n  {\n    printf(\"%d** %s\\n\", mpirank, message);\n    return EXIT_FAILURE;\n  }\n\n\n  H5Fclose(hdf);\n\n  MPI_Finalize();\n\n  print(mpirank, \"SUCCESS\\n\");\n\n  return EXIT_SUCCESS;\n}"}
{"program": "Norky_313", "code": "int main(int argc, char *argv[])\n{\n  int rank;\n  int nproc;\n  FILE *input;\n\n  if ( nproc<2 && rank==0 )\n    {\n      fprintf(stderr,\"%s:  At least 2 MPI processes required!\\n\",argv[0]);\n      exit(-1);\n    }\n  if ( rank==0 )\n    {\n      if ( argc>1 )\n\t{\n\t  input = fopen(argv[1],\"r\");\n\t}\n      else\n\t{\n\t  input = stdin;\n\t}\n      mastermind(nproc-1,input);\n    }\n  else\n    {\n      minion(rank);\n    }\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  int rank;\n  int nproc;\n  FILE *input;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&nproc);\n  if ( nproc<2 && rank==0 )\n    {\n      fprintf(stderr,\"%s:  At least 2 MPI processes required!\\n\",argv[0]);\n      exit(-1);\n    }\n  if ( rank==0 )\n    {\n      if ( argc>1 )\n\t{\n\t  input = fopen(argv[1],\"r\");\n\t}\n      else\n\t{\n\t  input = stdin;\n\t}\n      mastermind(nproc-1,input);\n    }\n  else\n    {\n      minion(rank);\n    }\n  MPI_Finalize();\n  return(0);\n}"}
{"program": "icmhpc_314", "code": "int main(int argc, char **argv)\n{\n\n  OMPthreads = omp_get_max_threads();\n\n  simulationInit(argc, argv);\n\n  for (step = 0; step < nsteps; step++) {\n\n    if (!(step % statOutStep))\n      printStepNum();\n\n    decompositionExecute();\n    octBuild();\n    createExportList();\n    computeStep();\n\n    if (!(step % statOutStep))\n      statisticsPrint();\n\n    if (simStart)\n      simTime += secondsPerStep / 3600.0;\t\n\n\n    if (!(step % vtkOutStep)) {\n      if (vtkout)\n        ioWriteStepVTK(step);\n      if (povout)\n        ioWriteStepPovRay(step, 0);\n      if (vnfout)\n        ioWriteFields(step);\n    }\n\n    updateCellPositions();\n    updateCellStates();\n    commCleanup();\n    octFree();\n\n    if (!(step % rstOutStep))\n      saveRstFile();\n  }\n\n\n  decompositionFinalize();\n  randomStreamFree();\n  cellsCleanup();\n\n  if (MPIrank == 0)\n    printf(\"\\nEnd of simulation run.\\n\");\n\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &MPIsize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &MPIrank);\n\n  OMPthreads = omp_get_max_threads();\n\n  simulationInit(argc, argv);\n\n  for (step = 0; step < nsteps; step++) {\n\n    if (!(step % statOutStep))\n      printStepNum();\n\n    decompositionExecute();\n    octBuild();\n    createExportList();\n    computeStep();\n\n    if (!(step % statOutStep))\n      statisticsPrint();\n\n    if (simStart)\n      simTime += secondsPerStep / 3600.0;\t\n\n\n    if (!(step % vtkOutStep)) {\n      if (vtkout)\n        ioWriteStepVTK(step);\n      if (povout)\n        ioWriteStepPovRay(step, 0);\n      if (vnfout)\n        ioWriteFields(step);\n    }\n\n    updateCellPositions();\n    updateCellStates();\n    commCleanup();\n    octFree();\n\n    if (!(step % rstOutStep))\n      saveRstFile();\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  decompositionFinalize();\n  randomStreamFree();\n  cellsCleanup();\n\n  if (MPIrank == 0)\n    printf(\"\\nEnd of simulation run.\\n\");\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "JBorrow_315", "code": "int main(int argc, char **argv)\n{\n  double t0, t1;\n\n\n  if(NTask <= 1)\n    {\n      if(ThisTask == 0)\n\tprintf\n\t  (\"Note: This is a massively parallel code, but you are running with 1 processor only.\\nCompared to an equivalent serial code, there is some unnecessary overhead.\\n\");\n    }\n\n  for(PTask = 0; NTask > (1 << PTask); PTask++);\n\n  if(argc < 2)\n    {\n      if(ThisTask == 0)\n\t{\n\t  printf(\"Parameters are missing.\\n\");\n\t  printf(\"Call with <ParameterFile> [<RestartFlag>]\\n\");\n\t}\n      endrun(0);\n    }\n\n  strcpy(ParameterFile, argv[1]);\n\n  if(argc >= 3)\n    RestartFlag = atoi(argv[2]);\n  else\n    RestartFlag = 0;\n\n  All.CPU_TreeConstruction = All.CPU_TreeWalk = All.CPU_Gravity = All.CPU_Potential = All.CPU_Domain =\n    All.CPU_Snapshot = All.CPU_Total = All.CPU_CommSum = All.CPU_Imbalance = All.CPU_Hydro =\n    All.CPU_HydCompWalk = All.CPU_HydCommSumm = All.CPU_HydImbalance =\n    All.CPU_EnsureNgb = All.CPU_Predict = All.CPU_TimeLine = All.CPU_PM = All.CPU_Peano = 0;\n\n  CPUThisRun = 0;\n\n  t0 = second();\n\n  begrun();\t\t\t\n\n\n  t1 = second();\n  CPUThisRun += timediff(t0, t1);\n  All.CPU_Total += timediff(t0, t1);\n\n  run();\t\t\t\n\n\n\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  double t0, t1;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &ThisTask);\n  MPI_Comm_size(MPI_COMM_WORLD, &NTask);\n\n  if(NTask <= 1)\n    {\n      if(ThisTask == 0)\n\tprintf\n\t  (\"Note: This is a massively parallel code, but you are running with 1 processor only.\\nCompared to an equivalent serial code, there is some unnecessary overhead.\\n\");\n    }\n\n  for(PTask = 0; NTask > (1 << PTask); PTask++);\n\n  if(argc < 2)\n    {\n      if(ThisTask == 0)\n\t{\n\t  printf(\"Parameters are missing.\\n\");\n\t  printf(\"Call with <ParameterFile> [<RestartFlag>]\\n\");\n\t}\n      endrun(0);\n    }\n\n  strcpy(ParameterFile, argv[1]);\n\n  if(argc >= 3)\n    RestartFlag = atoi(argv[2]);\n  else\n    RestartFlag = 0;\n\n  All.CPU_TreeConstruction = All.CPU_TreeWalk = All.CPU_Gravity = All.CPU_Potential = All.CPU_Domain =\n    All.CPU_Snapshot = All.CPU_Total = All.CPU_CommSum = All.CPU_Imbalance = All.CPU_Hydro =\n    All.CPU_HydCompWalk = All.CPU_HydCommSumm = All.CPU_HydImbalance =\n    All.CPU_EnsureNgb = All.CPU_Predict = All.CPU_TimeLine = All.CPU_PM = All.CPU_Peano = 0;\n\n  CPUThisRun = 0;\n\n  t0 = second();\n\n  begrun();\t\t\t\n\n\n  t1 = second();\n  CPUThisRun += timediff(t0, t1);\n  All.CPU_Total += timediff(t0, t1);\n\n  run();\t\t\t\n\n\n  MPI_Finalize();\t\t\n\n\n  return 0;\n}"}
{"program": "gnu3ra_316", "code": "int main(int argc, char ** argv) {\n  int    rank, nproc, i, test_iter;\n  int   *my_data, *buf;\n  void **base_ptrs;\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCI test with %d processes\\n\", nproc);\n\n  buf = malloc(DATA_SZ);\n  base_ptrs = malloc(sizeof(void*)*nproc);\n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + iteration %d\\n\", test_iter);\n\n    \n\n    ARMCI_Malloc(base_ptrs, DATA_SZ);\n    my_data = base_ptrs[rank];\n\n    \n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank*test_iter;\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Get(base_ptrs[(rank+1) % nproc], buf, DATA_SZ, (rank+1) % nproc);\n\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (buf[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: GET expected %d, got %d\\n\", rank, (rank+1) % nproc, buf[i]);\n      }\n    }\n\n    ARMCI_Barrier(); \n\n\n    \n\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank*test_iter;\n    ARMCI_Put(buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: PUT expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    \n\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank;\n    \n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank;\n    ARMCI_Access_end(my_data);\n    ARMCI_Barrier();\n\n    int scale = test_iter;\n    ARMCI_Acc(ARMCI_ACC_INT, &scale, buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != rank + ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: ACC expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n        \n\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Free(my_data);\n  }\n\n  free(buf);\n  free(base_ptrs);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int    rank, nproc, i, test_iter;\n  int   *my_data, *buf;\n  void **base_ptrs;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCI test with %d processes\\n\", nproc);\n\n  buf = malloc(DATA_SZ);\n  base_ptrs = malloc(sizeof(void*)*nproc);\n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + iteration %d\\n\", test_iter);\n\n    \n\n    ARMCI_Malloc(base_ptrs, DATA_SZ);\n    my_data = base_ptrs[rank];\n\n    \n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank*test_iter;\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Get(base_ptrs[(rank+1) % nproc], buf, DATA_SZ, (rank+1) % nproc);\n\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (buf[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: GET expected %d, got %d\\n\", rank, (rank+1) % nproc, buf[i]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n      }\n    }\n\n    ARMCI_Barrier(); \n\n\n    \n\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank*test_iter;\n    ARMCI_Put(buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: PUT expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    \n\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank;\n    \n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank;\n    ARMCI_Access_end(my_data);\n    ARMCI_Barrier();\n\n    int scale = test_iter;\n    ARMCI_Acc(ARMCI_ACC_INT, &scale, buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != rank + ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: ACC expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n        \n\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Free(my_data);\n  }\n\n  free(buf);\n  free(base_ptrs);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "jeffhammond_317", "code": "int main(int argc, char* argv[])\n{\n\n    int n = (argc>1) ? atoi(argv[1]) : 1000;\n\n    int wrank, wsize;\n\n    int nrank, nsize;\n\n    char * buf1 = NULL;\n    char * buf2 = NULL;\n\n    memset(buf1, nrank==0 ? 'Z' : 'A', n);\n    memset(buf2, nrank==0 ? 'Z' : 'A', n);\n\n    double t0, t1, dt;\n    for (int r=0; r<20; r++) {\n        t0 =\n        t1 =\n        dt = t1-t0;\n        printf(\"%d: MPI_Bcast: %lf seconds, %lf MB/s \\n\", wrank, dt, n*(1.e-6/dt));\n        fflush(stdout);\n\n        t0 =\n        SMP_Bcast(buf2, n, MPI_CHAR, 0, MPI_COMM_NODE);\n        t1 =\n        dt = t1-t0;\n        printf(\"%d: SMP_Bcast: %lf seconds, %lf MB/s \\n\", wrank, dt, n*(1.e-6/dt));\n        fflush(stdout);\n\n        if (r==0) {\n            char * tmp = malloc(n);\n            memset(tmp, 'Z', n);\n            int err1 = memcmp(tmp, buf1, n);\n            int err2 = memcmp(tmp, buf2, n);\n            if (err1>0 || err2>0) {\n            }\n        }\n    }\n\n\n\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    MPI_Init(&argc,&argv);\n    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &MPI_COMM_NODE);\n\n    int n = (argc>1) ? atoi(argv[1]) : 1000;\n\n    int wrank, wsize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &wrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &wsize);\n\n    int nrank, nsize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &nrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nsize);\n\n    char * buf1 = NULL;\n    char * buf2 = NULL;\n    MPI_Alloc_mem(n, MPI_INFO_NULL, &buf1);\n    MPI_Alloc_mem(n, MPI_INFO_NULL, &buf2);\n\n    memset(buf1, nrank==0 ? 'Z' : 'A', n);\n    memset(buf2, nrank==0 ? 'Z' : 'A', n);\n\n    double t0, t1, dt;\n    for (int r=0; r<20; r++) {\n        MPI_Barrier(MPI_COMM_WORLD);\n        t0 = MPI_Wtime();\n        MPI_Bcast(buf1, n, MPI_CHAR, 0, MPI_COMM_NODE);\n        t1 = MPI_Wtime();\n        dt = t1-t0;\n        printf(\"%d: MPI_Bcast: %lf seconds, %lf MB/s \\n\", wrank, dt, n*(1.e-6/dt));\n        fflush(stdout);\n\n        MPI_Barrier(MPI_COMM_WORLD);\n        t0 = MPI_Wtime();\n        SMP_Bcast(buf2, n, MPI_CHAR, 0, MPI_COMM_NODE);\n        t1 = MPI_Wtime();\n        dt = t1-t0;\n        printf(\"%d: SMP_Bcast: %lf seconds, %lf MB/s \\n\", wrank, dt, n*(1.e-6/dt));\n        fflush(stdout);\n\n        if (r==0) {\n            char * tmp = malloc(n);\n            memset(tmp, 'Z', n);\n            int err1 = memcmp(tmp, buf1, n);\n            int err2 = memcmp(tmp, buf2, n);\n            if (err1>0 || err2>0) {\n                printf(\"%d: errors: MPI (%d), SMP (%d) \\n\", wrank, err1, err2);\n            }\n        }\n    }\n\n    MPI_Free_mem(buf1);\n    MPI_Free_mem(buf2);\n\n    MPI_Comm_free(&MPI_COMM_NODE);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "mcolmant_321", "code": "main(int argc, char **argv)\n{\n    int rank;\n    char* host;\n\n    host = (char*) malloc(HOST_NAME_MAX * sizeof(char));\n    gethostname(host, HOST_NAME_MAX);\n\n    MASTER(MPI started);\n    printf(\"Process with rank %d running on Node %s Core %d/%d\\n\",rank ,host, sched_getcpu(),get_cpu_id());\n\n    MASTER(Enter OpenMP parallel region);\n#pragma omp parallel\n    {\n#pragma omp master\n        {\n            pid_t pid = getppid();\n            char cmd[1024];\n            sprintf(cmd, \"pstree -p -H %d %d\",pid, pid);\n            system(cmd);\n        }\n#ifdef _OPENMP\n#pragma omp critical\n        {\n            printf (\"Rank %d Thread %d running on Node %s core %d/%d with pid %d and tid %d\\n\",rank,omp_get_thread_num(), host, sched_getcpu(),get_cpu_id(), getpid(),gettid());\n        }\n#endif\n\n    }\n\n    free(host);\n}", "label": "main(int argc, char **argv)\n{\n    int rank;\n    char* host;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    host = (char*) malloc(HOST_NAME_MAX * sizeof(char));\n    gethostname(host, HOST_NAME_MAX);\n\n    MASTER(MPI started);\n    MPI_Barrier(MPI_COMM_WORLD);\n    printf(\"Process with rank %d running on Node %s Core %d/%d\\n\",rank ,host, sched_getcpu(),get_cpu_id());\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MASTER(Enter OpenMP parallel region);\n    MPI_Barrier(MPI_COMM_WORLD);\n#pragma omp parallel\n    {\n#pragma omp master\n        {\n            pid_t pid = getppid();\n            char cmd[1024];\n            sprintf(cmd, \"pstree -p -H %d %d\",pid, pid);\n            system(cmd);\n        }\n#ifdef _OPENMP\n#pragma omp critical\n        {\n            printf (\"Rank %d Thread %d running on Node %s core %d/%d with pid %d and tid %d\\n\",rank,omp_get_thread_num(), host, sched_getcpu(),get_cpu_id(), getpid(),gettid());\n        }\n#endif\n\n    }\n\n    free(host);\n    MPI_Finalize();\n}"}
{"program": "formap_322", "code": "int main(int argc, char *argv[]) {\n\t\n\tint myid, numprocs;\n    int tag,source,destination,count;\n    int buffer[ndata*2];\n    MPI_Status status;\n    MPI_Request request;\n    \n    int iter = 20;\n    \n    \n    if (myid == 0 && numprocs == 2) \n    {\n\t\tint recvID = 1;\n\t\tdouble acum = 0;\n\t\tint i, tam = 3;\n\t\tdouble startT;\n\t\tfor (i = 0; i < ndata*2; ++i) \n\t\t{\n\t\t\tbuffer[i] = i;\n\t\t}\n\t\twhile (tam < ndata) \n\t\t{\n\t\t\tdouble startTime =\n\t\t\t\n\t\t\t\n\t\t\tdouble endTime =\n\t\t\tdouble elapsed = endTime - startTime;\n\t\t\tacum += elapsed;\n\t\t\tprintf(\"%d, %f, elapsed: %f\\n\",tam,acum,elapsed);fflush(stdout);\n\t\t\ttam += 2;\n\t\t}\n\t\tprintf(\"total: %f\\nmean: %f\\n\", acum, acum/iter);\t\n\t}\n\telse if (numprocs == 2) \n\t{\n\t\tint i, tam = 3;\n\t\tsleep(10);\n\t\twhile (tam < ndata) \n\t\t{\n\t\t\ttam += 2;\n\t\t}\n\t}\n\telse \n\t{\n\t\tprintf(\"Need only 2 threads\\n\");\n\t}\n    \n\n    \n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n\t\n\tint myid, numprocs;\n    int tag,source,destination,count;\n    int buffer[ndata*2];\n    MPI_Status status;\n    MPI_Request request;\n    \n    int iter = 20;\n    \n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    \n    if (myid == 0 && numprocs == 2) \n    {\n\t\tint recvID = 1;\n\t\tdouble acum = 0;\n\t\tint i, tam = 3;\n\t\tdouble startT;\n\t\tfor (i = 0; i < ndata*2; ++i) \n\t\t{\n\t\t\tbuffer[i] = i;\n\t\t}\n\t\twhile (tam < ndata) \n\t\t{\n\t\t\tdouble startTime = MPI_Wtime();\n\t\t\t\n\t\t\tMPI_Send(&buffer,tam,MPI_INT,recvID,0,MPI_COMM_WORLD);\n\t\t\t\n\t\t\tdouble endTime = MPI_Wtime();\n\t\t\tdouble elapsed = endTime - startTime;\n\t\t\tacum += elapsed;\n\t\t\tprintf(\"%d, %f, elapsed: %f\\n\",tam,acum,elapsed);fflush(stdout);\n\t\t\ttam += 2;\n\t\t}\n\t\tprintf(\"total: %f\\nmean: %f\\n\", acum, acum/iter);\t\n\t}\n\telse if (numprocs == 2) \n\t{\n\t\tint i, tam = 3;\n\t\tsleep(10);\n\t\twhile (tam < ndata) \n\t\t{\n\t\t\tMPI_Recv(&buffer,ndata,MPI_INT,0,0,MPI_COMM_WORLD,&status);\n\t\t\ttam += 2;\n\t\t}\n\t}\n\telse \n\t{\n\t\tprintf(\"Need only 2 threads\\n\");\n\t}\n    \n   \tMPI_Finalize();\n\n    \n    return 0;\n}"}
{"program": "bjoern-leder_324", "code": "int main(int argc,char *argv[])\n{\n   int my_rank;\n   double d1,d2,d3,d4,d5;\n   su3_dble *udb,**usv;\n   FILE *flog=NULL;\n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n\n      printf(\"\\n\");\n      printf(\"Renormalization of the link variables\\n\");\n      printf(\"-------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n   }\n\n   start_ranlux(0,123456);\n   geometry();\n   alloc_wud(1);\n   usv=reserve_wud(1);\n   udb=udfld();\n   \n   random_ud();\n   check_ud(&d1,&d2);\n   \n   if (my_rank==0)\n   {\n      printf(\"Random double-precision gauge field:\\n\");\n      printf(\"|u^dag*u-1| = %.2e\\n\",d1);\n      printf(\"|det{u}-1| = %.2e\\n\\n\",d2);      \n   }\n\n   cm3x3_assign(4*VOLUME,udb,usv[0]);   \n   tilt_ud(50.0*DBL_EPSILON);\n   check_ud(&d1,&d2);\n   renormalize_ud();\n   d3=cmp_ud(usv[0]);\n   check_ud(&d4,&d5);   \n   \n   if (my_rank==0)\n   {\n      printf(\"Tilt double-precision gauge field:\\n\");\n      printf(\"|u^dag*u-1| = %.2e\\n\",d1);\n      printf(\"|det{u}-1| = %.2e\\n\",d2);\n      printf(\"Difference after renormalization = %.2e\\n\",d3);            \n      printf(\"|u^dag*u-1| = %.2e\\n\",d4);\n      printf(\"|det{u}-1| = %.2e\\n\\n\",d5);\n   }\n\n   random_ud();\n   cm3x3_assign(4*VOLUME,udb,usv[0]); \n   renormalize_ud();\n   d1=cmp_ud(usv[0]);\n   \n   if (my_rank==0)\n   {\n      printf(\"Renormalization of a fresh random double-precision field:\\n\");\n      printf(\"Difference after renormalization = %.2e\\n\\n\",d1);       \n      fclose(flog);\n   }\n   \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank;\n   double d1,d2,d3,d4,d5;\n   su3_dble *udb,**usv;\n   FILE *flog=NULL;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n\n      printf(\"\\n\");\n      printf(\"Renormalization of the link variables\\n\");\n      printf(\"-------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n   }\n\n   start_ranlux(0,123456);\n   geometry();\n   alloc_wud(1);\n   usv=reserve_wud(1);\n   udb=udfld();\n   \n   random_ud();\n   check_ud(&d1,&d2);\n   \n   if (my_rank==0)\n   {\n      printf(\"Random double-precision gauge field:\\n\");\n      printf(\"|u^dag*u-1| = %.2e\\n\",d1);\n      printf(\"|det{u}-1| = %.2e\\n\\n\",d2);      \n   }\n\n   cm3x3_assign(4*VOLUME,udb,usv[0]);   \n   tilt_ud(50.0*DBL_EPSILON);\n   check_ud(&d1,&d2);\n   renormalize_ud();\n   d3=cmp_ud(usv[0]);\n   check_ud(&d4,&d5);   \n   \n   if (my_rank==0)\n   {\n      printf(\"Tilt double-precision gauge field:\\n\");\n      printf(\"|u^dag*u-1| = %.2e\\n\",d1);\n      printf(\"|det{u}-1| = %.2e\\n\",d2);\n      printf(\"Difference after renormalization = %.2e\\n\",d3);            \n      printf(\"|u^dag*u-1| = %.2e\\n\",d4);\n      printf(\"|det{u}-1| = %.2e\\n\\n\",d5);\n   }\n\n   random_ud();\n   cm3x3_assign(4*VOLUME,udb,usv[0]); \n   renormalize_ud();\n   d1=cmp_ud(usv[0]);\n   \n   if (my_rank==0)\n   {\n      printf(\"Renormalization of a fresh random double-precision field:\\n\");\n      printf(\"Difference after renormalization = %.2e\\n\\n\",d1);       \n      fclose(flog);\n   }\n   \n   MPI_Finalize();\n   exit(0);\n}"}
{"program": "tcsiwula_325", "code": "int main(void) {\n   int      p, my_rank;\n   MPI_Comm comm;\n   int      my_contrib;\n   int      sum;\n\n   comm = MPI_COMM_WORLD;\n\n   \n\n   srandom(my_rank);\n   my_contrib = random() % MAX_CONTRIB;\n\n   Print_results(\"Process Values\", my_contrib, my_rank, p, comm);\n\n   sum = Global_sum(my_contrib, my_rank, p, comm);\n\n   Print_results(\"Process Totals\", sum, my_rank, p, comm);\n\n   return 0;\n}", "label": "int main(void) {\n   int      p, my_rank;\n   MPI_Comm comm;\n   int      my_contrib;\n   int      sum;\n\n   MPI_Init(NULL, NULL);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &p);\n   MPI_Comm_rank(comm, &my_rank);\n\n   \n\n   srandom(my_rank);\n   my_contrib = random() % MAX_CONTRIB;\n\n   Print_results(\"Process Values\", my_contrib, my_rank, p, comm);\n\n   sum = Global_sum(my_contrib, my_rank, p, comm);\n\n   Print_results(\"Process Totals\", sum, my_rank, p, comm);\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "dboudour2002_326", "code": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS     *go = NULL;\t\n\n  ESL_STOPWATCH   *w  = esl_stopwatch_Create();\n  struct cfg_s     cfg;\n\n  \n\n  impl_Init();\n\n  cfg.alifile     = NULL;\n  cfg.hmmfile     = NULL;\n\n  \n\n  process_commandline(argc, argv, &go, &cfg.hmmfile, &cfg.alifile);    \n\n  \n\n  cfg.ofp         = NULL;\t           \n  cfg.fmt         = eslMSAFILE_UNKNOWN;    \n \n  cfg.afp         = NULL;\t           \n  cfg.abc         = NULL;\t           \n  cfg.hmmfp       = NULL;\t           \n  cfg.postmsafile = esl_opt_GetString(go, \"-O\"); \n\n  cfg.postmsafp   = NULL;                  \n\n  cfg.nali       = 0;\t\t           \n\n  cfg.nnamed     = 0;\t\t           \n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n  cfg.do_stall   = esl_opt_GetBoolean(go, \"--stall\");\n  cfg.hmmName    = esl_opt_GetString(go, \"-n\"); \n\n\n  if (esl_opt_IsOn(go, \"--informat\")) {\n    cfg.fmt = eslx_msafile_EncodeFormat(esl_opt_GetString(go, \"--informat\"));\n    if (cfg.fmt == eslMSAFILE_UNKNOWN) p7_Fail(\"%s is not a recognized input sequence file format\\n\", esl_opt_GetString(go, \"--informat\"));\n  }\n\n\n  \n\n  while (cfg.do_stall); \n\n  \n\n  esl_stopwatch_Start(w);\n\n  \n\n#ifdef HAVE_MPI\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n\n      if (cfg.my_rank > 0)  mpi_worker(go, &cfg);\n      else \t\t    mpi_master(go, &cfg);\n\n      esl_stopwatch_Stop(w);\n      esl_stopwatch_MPIReduce(w, 0, MPI_COMM_WORLD);\n    }\n  else\n#endif \n\n    {\n      usual_master(go, &cfg);\n      esl_stopwatch_Stop(w);\n    }\n\n  if (cfg.my_rank == 0) {\n    fputc('\\n', cfg.ofp);\n    esl_stopwatch_Display(cfg.ofp, w, \"# CPU time: \");\n  }\n\n  \n\n  if (cfg.my_rank == 0) {\n    if (esl_opt_IsOn(go, \"-o\")) { fclose(cfg.ofp); }\n    if (cfg.afp)   eslx_msafile_Close(cfg.afp);\n    if (cfg.abc)   esl_alphabet_Destroy(cfg.abc);\n    if (cfg.hmmfp) fclose(cfg.hmmfp);\n  }\n  esl_getopts_Destroy(go);\n  esl_stopwatch_Destroy(w);\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS     *go = NULL;\t\n\n  ESL_STOPWATCH   *w  = esl_stopwatch_Create();\n  struct cfg_s     cfg;\n\n  \n\n  impl_Init();\n\n  cfg.alifile     = NULL;\n  cfg.hmmfile     = NULL;\n\n  \n\n  process_commandline(argc, argv, &go, &cfg.hmmfile, &cfg.alifile);    \n\n  \n\n  cfg.ofp         = NULL;\t           \n  cfg.fmt         = eslMSAFILE_UNKNOWN;    \n \n  cfg.afp         = NULL;\t           \n  cfg.abc         = NULL;\t           \n  cfg.hmmfp       = NULL;\t           \n  cfg.postmsafile = esl_opt_GetString(go, \"-O\"); \n\n  cfg.postmsafp   = NULL;                  \n\n  cfg.nali       = 0;\t\t           \n\n  cfg.nnamed     = 0;\t\t           \n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n  cfg.do_stall   = esl_opt_GetBoolean(go, \"--stall\");\n  cfg.hmmName    = esl_opt_GetString(go, \"-n\"); \n\n\n  if (esl_opt_IsOn(go, \"--informat\")) {\n    cfg.fmt = eslx_msafile_EncodeFormat(esl_opt_GetString(go, \"--informat\"));\n    if (cfg.fmt == eslMSAFILE_UNKNOWN) p7_Fail(\"%s is not a recognized input sequence file format\\n\", esl_opt_GetString(go, \"--informat\"));\n  }\n\n\n  \n\n  while (cfg.do_stall); \n\n  \n\n  esl_stopwatch_Start(w);\n\n  \n\n#ifdef HAVE_MPI\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n      MPI_Init(&argc, &argv);\n      MPI_Comm_rank(MPI_COMM_WORLD, &(cfg.my_rank));\n      MPI_Comm_size(MPI_COMM_WORLD, &(cfg.nproc));\n\n      if (cfg.my_rank > 0)  mpi_worker(go, &cfg);\n      else \t\t    mpi_master(go, &cfg);\n\n      esl_stopwatch_Stop(w);\n      esl_stopwatch_MPIReduce(w, 0, MPI_COMM_WORLD);\n      MPI_Finalize();\n    }\n  else\n#endif \n\n    {\n      usual_master(go, &cfg);\n      esl_stopwatch_Stop(w);\n    }\n\n  if (cfg.my_rank == 0) {\n    fputc('\\n', cfg.ofp);\n    esl_stopwatch_Display(cfg.ofp, w, \"# CPU time: \");\n  }\n\n  \n\n  if (cfg.my_rank == 0) {\n    if (esl_opt_IsOn(go, \"-o\")) { fclose(cfg.ofp); }\n    if (cfg.afp)   eslx_msafile_Close(cfg.afp);\n    if (cfg.abc)   esl_alphabet_Destroy(cfg.abc);\n    if (cfg.hmmfp) fclose(cfg.hmmfp);\n  }\n  esl_getopts_Destroy(go);\n  esl_stopwatch_Destroy(w);\n  return 0;\n}"}
{"program": "florian-burger_328", "code": "int main(int argc,char *argv[])\n{\n  int tslice,j,k;\n  char conf_filename[50];\n  \n#ifdef MPI\n#endif\n  \n  \n\n  read_input(\"LapH.input\");\n  \n  tmlqcd_mpi_init(argc, argv);\n  \n  if(g_proc_id==0) {\n#ifdef SSE\n    printf(\"# The code was compiled with SSE instructions\\n\");\n#endif\n#ifdef SSE2\n    printf(\"# The code was compiled with SSE2 instructions\\n\");\n#endif\n#ifdef SSE3\n    printf(\"# The code was compiled with SSE3 instructions\\n\");\n#endif\n#ifdef P4\n    printf(\"# The code was compiled for Pentium4\\n\");\n#endif\n#ifdef OPTERON\n    printf(\"# The code was compiled for AMD Opteron\\n\");\n#endif\n#ifdef _GAUGE_COPY\n    printf(\"# The code was compiled with -D_GAUGE_COPY\\n\");\n#endif\n#ifdef BGL\n    printf(\"# The code was compiled for Blue Gene/L\\n\");\n#endif\n#ifdef BGP\n    printf(\"# The code was compiled for Blue Gene/P\\n\");\n#endif\n#ifdef _USE_HALFSPINOR\n    printf(\"# The code was compiled with -D_USE_HALFSPINOR\\n\");\n#endif    \n#ifdef _USE_SHMEM\n    printf(\"# the code was compiled with -D_USE_SHMEM\\n\");\n#  ifdef _PERSISTENT\n#  endif\n#endif\n#ifdef MPI\n#  ifdef _NON_BLOCKING\n#  endif\n#endif\n    printf(\"\\n\");\n    fflush(stdout);\n  }\n  \n\n#ifndef WITHLAPH\n  printf(\" Error: WITHLAPH not defined\");\n  exit(0);\n#endif\n#ifdef MPI\n#ifndef _INDEX_INDEP_GEOM\n  printf(\" Error: _INDEX_INDEP_GEOM not defined\");\n  exit(0);\n#endif\n#ifndef _USE_TSPLITPAR\n  printf(\" Error: _USE_TSPLITPAR not defined\");\n  exit(0);\n#endif\n#endif\n#ifdef FIXEDVOLUME\n  printf(\" Error: FIXEDVOLUME not allowed\");\n  exit(0);\n#endif\n\n  \n  init_gauge_field(VOLUMEPLUSRAND + g_dbw2rand, 0);\n  init_geometry_indices(VOLUMEPLUSRAND + g_dbw2rand);\n\n  if(g_proc_id == 0) {\n    fprintf(stdout,\"The number of processes is %d \\n\",g_nproc);\n    printf(\"# The lattice size is %d x %d x %d x %d\\n\",\n\t   (int)(T*g_nproc_t), (int)(LX*g_nproc_x), (int)(LY*g_nproc_y), (int)(g_nproc_z*LZ));\n    printf(\"# The local lattice size is %d x %d x %d x %d\\n\", \n\t   (int)(T), (int)(LX), (int)(LY),(int) LZ);\n    printf(\"# Computing LapH eigensystem \\n\");\n\n    fflush(stdout);\n  }\n  \n  \n\n  geometry();\n\n  start_ranlux(1, 123456);\n\n  \n\n  sprintf(conf_filename, \"%s.%.4d\", gauge_input_filename, nstore);\n  if (g_cart_id == 0) {\n    printf(\"#\\n# Trying to read gauge field from file %s in %s precision.\\n\",\n\t   conf_filename, (gauge_precision_read_flag == 32 ? \"single\" : \"double\"));\n    fflush(stdout);\n  }\n  if( (j = read_gauge_field(conf_filename,g_gauge_field)) !=0) {\n    fprintf(stderr, \"Error %d while reading gauge field from %s\\n Aborting...\\n\", j, conf_filename);\n    exit(-2);\n  }\n\n  \n  if (g_cart_id == 0) {\n    printf(\"# Finished reading gauge field.\\n\");\n    fflush(stdout);\n  }\n  \n#ifdef MPI\n  \n\n  xchange_gauge(g_gauge_field);\n#endif\n  \n  \n\n  init_jacobi_field(SPACEVOLUME+SPACERAND,3);\n\n#ifdef MPI\n  {\n     \n\n    volatile int i_gdb = 8;\n    char hostname[256];\n    gethostname(hostname, sizeof(hostname));\n    printf(\"PID %d on %s ready for attach\\n\", getpid(), hostname);\n    fflush(stdout);\n    if(g_cart_id == 0){\n      while (0 == i_gdb){\n\tsleep(5);\n      }\n    }\n  }\n\n#endif\n\n  for (k=0 ; k<3 ; k++)\n    random_jacobi_field(g_jacobi_field[k],SPACEVOLUME);\n\n\n  \n\n  \n  for(tslice=0; tslice<T; tslice++){ \n    eigenvalues_Jacobi(&no_eigenvalues,5000, eigenvalue_precision,0,tslice,nstore);\n  }\n  \n#ifdef MPI\n#endif\n  return(0);\n}", "label": "int main(int argc,char *argv[])\n{\n  int tslice,j,k;\n  char conf_filename[50];\n  \n#ifdef MPI\n  MPI_Init(&argc, &argv);\n#endif\n  \n  \n\n  read_input(\"LapH.input\");\n  \n  tmlqcd_mpi_init(argc, argv);\n  \n  if(g_proc_id==0) {\n#ifdef SSE\n    printf(\"# The code was compiled with SSE instructions\\n\");\n#endif\n#ifdef SSE2\n    printf(\"# The code was compiled with SSE2 instructions\\n\");\n#endif\n#ifdef SSE3\n    printf(\"# The code was compiled with SSE3 instructions\\n\");\n#endif\n#ifdef P4\n    printf(\"# The code was compiled for Pentium4\\n\");\n#endif\n#ifdef OPTERON\n    printf(\"# The code was compiled for AMD Opteron\\n\");\n#endif\n#ifdef _GAUGE_COPY\n    printf(\"# The code was compiled with -D_GAUGE_COPY\\n\");\n#endif\n#ifdef BGL\n    printf(\"# The code was compiled for Blue Gene/L\\n\");\n#endif\n#ifdef BGP\n    printf(\"# The code was compiled for Blue Gene/P\\n\");\n#endif\n#ifdef _USE_HALFSPINOR\n    printf(\"# The code was compiled with -D_USE_HALFSPINOR\\n\");\n#endif    \n#ifdef _USE_SHMEM\n    printf(\"# the code was compiled with -D_USE_SHMEM\\n\");\n#  ifdef _PERSISTENT\n    printf(\"# the code was compiled for persistent MPI calls (halfspinor only)\\n\");\n#  endif\n#endif\n#ifdef MPI\n#  ifdef _NON_BLOCKING\n    printf(\"# the code was compiled for non-blocking MPI calls (spinor and gauge)\\n\");\n#  endif\n#endif\n    printf(\"\\n\");\n    fflush(stdout);\n  }\n  \n\n#ifndef WITHLAPH\n  printf(\" Error: WITHLAPH not defined\");\n  exit(0);\n#endif\n#ifdef MPI\n#ifndef _INDEX_INDEP_GEOM\n  printf(\" Error: _INDEX_INDEP_GEOM not defined\");\n  exit(0);\n#endif\n#ifndef _USE_TSPLITPAR\n  printf(\" Error: _USE_TSPLITPAR not defined\");\n  exit(0);\n#endif\n#endif\n#ifdef FIXEDVOLUME\n  printf(\" Error: FIXEDVOLUME not allowed\");\n  exit(0);\n#endif\n\n  \n  init_gauge_field(VOLUMEPLUSRAND + g_dbw2rand, 0);\n  init_geometry_indices(VOLUMEPLUSRAND + g_dbw2rand);\n\n  if(g_proc_id == 0) {\n    fprintf(stdout,\"The number of processes is %d \\n\",g_nproc);\n    printf(\"# The lattice size is %d x %d x %d x %d\\n\",\n\t   (int)(T*g_nproc_t), (int)(LX*g_nproc_x), (int)(LY*g_nproc_y), (int)(g_nproc_z*LZ));\n    printf(\"# The local lattice size is %d x %d x %d x %d\\n\", \n\t   (int)(T), (int)(LX), (int)(LY),(int) LZ);\n    printf(\"# Computing LapH eigensystem \\n\");\n\n    fflush(stdout);\n  }\n  \n  \n\n  geometry();\n\n  start_ranlux(1, 123456);\n\n  \n\n  sprintf(conf_filename, \"%s.%.4d\", gauge_input_filename, nstore);\n  if (g_cart_id == 0) {\n    printf(\"#\\n# Trying to read gauge field from file %s in %s precision.\\n\",\n\t   conf_filename, (gauge_precision_read_flag == 32 ? \"single\" : \"double\"));\n    fflush(stdout);\n  }\n  if( (j = read_gauge_field(conf_filename,g_gauge_field)) !=0) {\n    fprintf(stderr, \"Error %d while reading gauge field from %s\\n Aborting...\\n\", j, conf_filename);\n    exit(-2);\n  }\n\n  \n  if (g_cart_id == 0) {\n    printf(\"# Finished reading gauge field.\\n\");\n    fflush(stdout);\n  }\n  \n#ifdef MPI\n  \n\n  xchange_gauge(g_gauge_field);\n#endif\n  \n  \n\n  init_jacobi_field(SPACEVOLUME+SPACERAND,3);\n\n#ifdef MPI\n  {\n     \n\n    volatile int i_gdb = 8;\n    char hostname[256];\n    gethostname(hostname, sizeof(hostname));\n    printf(\"PID %d on %s ready for attach\\n\", getpid(), hostname);\n    fflush(stdout);\n    if(g_cart_id == 0){\n      while (0 == i_gdb){\n\tsleep(5);\n      }\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n#endif\n\n  for (k=0 ; k<3 ; k++)\n    random_jacobi_field(g_jacobi_field[k],SPACEVOLUME);\n\n\n  \n\n  \n  for(tslice=0; tslice<T; tslice++){ \n    eigenvalues_Jacobi(&no_eigenvalues,5000, eigenvalue_precision,0,tslice,nstore);\n  }\n  \n#ifdef MPI\n  MPI_Finalize();\n#endif\n  return(0);\n}"}
{"program": "CoryMcCartan_332", "code": "int main(int argc, char *argv[]) \n{ \n    int rank, destrank, nprocs, i;\n    int align_size;\n\n    char * s_buf, *r_buf;\n    MPI_Group comm_group, group;\n    MPI_Win win;\n    int loop;\n    int size;\n    double t_start, t_end;\n        \n\n    if (nprocs != 2) {\n        printf(\"Run this program with 2 processes\\n\");\n    }\n\n    align_size = MESSAGE_ALIGNMENT;\n    loop = LOOP;                                                                                                                                           \n    s_buf =\n        (char *) (((unsigned long) A + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) B + (align_size - 1)) /\n                  align_size * align_size);\n\n    bzero(r_buf, MAX_SIZE);\n    memset(s_buf, 1, MAX_SIZE);\n\n    if (rank == 0) {\n        fprintf(stdout, \"# Size\\t\\tLatency (us) \\n\");\n    }\n    \n     \n    for (size = 0; size <= MAX_SIZE;\n         size = (size ? size * 2 : size + 1)) { \n\n        if (rank == 0) {\n            destrank = 1;\n\n            for (i=0;i<skip+loop;i++)\n            {\n                if (i==skip) {\n                }\n      \t    }\n        }\n        else {  \n\n            destrank = 0;\n    \n    \n            for (i=0;i<skip+loop;i++)\n            {\n\n            }\n        }\n        if ( rank == 0 ) {\n\t        printf(\"%d\\t\\t%f\\n\",size, (t_end-t_start)*1.0e6/loop/2);\n        }\n\n   \n    }\n\n    return 0; \n}", "label": "int main(int argc, char *argv[]) \n{ \n    int rank, destrank, nprocs, i;\n    int align_size;\n\n    char * s_buf, *r_buf;\n    MPI_Group comm_group, group;\n    MPI_Win win;\n    int loop;\n    int size;\n    double t_start, t_end;\n        \n    MPI_Init(&argc,&argv); \n    MPI_Comm_size(MPI_COMM_WORLD,&nprocs); \n    MPI_Comm_rank(MPI_COMM_WORLD,&rank); \n\n    if (nprocs != 2) {\n        printf(\"Run this program with 2 processes\\n\");\n        MPI_Abort(MPI_COMM_WORLD,1);\n    }\n\n    align_size = MESSAGE_ALIGNMENT;\n    loop = LOOP;                                                                                                                                           \n    s_buf =\n        (char *) (((unsigned long) A + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) B + (align_size - 1)) /\n                  align_size * align_size);\n\n    bzero(r_buf, MAX_SIZE);\n    memset(s_buf, 1, MAX_SIZE);\n\n    if (rank == 0) {\n        fprintf(stdout, \"# OSU MPI2 One-sided MPI_Get Latency Test (Version 1.0)\\n\");\n        fprintf(stdout, \"# Size\\t\\tLatency (us) \\n\");\n    }\n    \n    MPI_Comm_group(MPI_COMM_WORLD, &comm_group);\n     \n    for (size = 0; size <= MAX_SIZE;\n         size = (size ? size * 2 : size + 1)) { \n\n        if (rank == 0) {\n            MPI_Win_create(s_buf, size, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win); \n            destrank = 1;\n            MPI_Group_incl(comm_group, 1, &destrank, &group);\n            MPI_Barrier( MPI_COMM_WORLD);\n\n            for (i=0;i<skip+loop;i++)\n            {\n                MPI_Win_start(group, 0, win);\n                if (i==skip) {\n                     t_start=MPI_Wtime();\n                }\n                MPI_Get(r_buf,size, MPI_CHAR, 1, 0, size, MPI_CHAR, win); \n                MPI_Win_complete(win);  \n                MPI_Win_post(group, 0, win);\n                MPI_Win_wait(win);\n      \t    }\n            t_end=MPI_Wtime();\n        }\n        else {  \n\n            MPI_Win_create(s_buf, size, 1, MPI_INFO_NULL,MPI_COMM_WORLD, &win);\n            destrank = 0;\n            MPI_Group_incl(comm_group, 1, &destrank, &group);\n    \n\t        MPI_Barrier( MPI_COMM_WORLD);\n    \n            for (i=0;i<skip+loop;i++)\n            {\n                MPI_Win_post(group, 0, win);\n                MPI_Win_wait(win);\n                MPI_Win_start(group, 0, win);\n                MPI_Get(r_buf,size, MPI_CHAR, 0, 0, size, MPI_CHAR, win);\n                MPI_Win_complete(win);\n\n            }\n        }\n        if ( rank == 0 ) {\n\t        printf(\"%d\\t\\t%f\\n\",size, (t_end-t_start)*1.0e6/loop/2);\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n        MPI_Group_free(&group);\n   \n        MPI_Win_free(&win); \n    }\n\n    MPI_Group_free(&comm_group);\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "JasonNew_333", "code": "int main (int argc, char *argv[])\n{\nint numtasks,              \n\n  taskid,                \n\n  numworkers,            \n\n  source,                \n\n  dest,                  \n\n  mtype,                 \n\n  rows,                  \n\n  averow, extra, offset, \n\n  i, j, k, rc;           \n\n\nMPI_Status status;\n\nMPI_Init(&argc,&argv);\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\nif (numtasks < 2 ) {\n  printf(\"Need at least two MPI tasks. Quitting...\\n\");\n  exit(1);\n}\n\nnumworkers = numtasks-1;\n\nFILE *fp;\nfp = fopen(\"matrix4000\", \"rb\");\n\n\nif (taskid == MASTER)\n{\n  printf(\"mpi_mm has started with %d tasks.\\n\",numtasks);\n\n  for(i=0; i<NRA; i++)\n  {\n    for(j=0; j<NRA; j++)\n        {\n      fscanf(fp, \"%d\", &a[i][j]);\n      fscanf(fp, \"%d\", &b[i][j]);\n    } \n  }\n\n  \n  double start = omp_get_wtime();\n  \n\n  averow = NRA/numworkers;\n  extra = NRA%numworkers;\n  offset = 0;\n  mtype = FROM_MASTER;\n  for (dest=1; dest<=numworkers; dest++)\n  {\n     rows = (dest <= extra) ? averow+1 : averow;    \n     offset = offset + rows;\n  }\n\n  \n\n  mtype = FROM_WORKER;\n  for (i=1; i<=numworkers; i++)\n  {\n     source = i;\n  }\n\n  double end = omp_get_wtime();\n  printf(\"Cost Time %.6g\\n\", end-start);\n  printf (\"Done.\\n\");\n  }\n\n\n\n\n   if (taskid > MASTER)\n   {\n      mtype = FROM_MASTER;\n\n      #pragma omp parallel for private(i,j,k) num_threads(32)\n      for (k=0; k<NCB; k++)\n      {\n         for (i=0; i<rows; i++)\n         {\n            for (j=0; j<NCA; j++)\n            {\n              c[i][k] = c[i][k] + a[i][j] * b[j][k];\n            }\n         }\n      }\n      mtype = FROM_WORKER;\n   }\n}", "label": "int main (int argc, char *argv[])\n{\nint numtasks,              \n\n  taskid,                \n\n  numworkers,            \n\n  source,                \n\n  dest,                  \n\n  mtype,                 \n\n  rows,                  \n\n  averow, extra, offset, \n\n  i, j, k, rc;           \n\n\nMPI_Status status;\n\nMPI_Init(&argc,&argv);\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\nif (numtasks < 2 ) {\n  printf(\"Need at least two MPI tasks. Quitting...\\n\");\n  MPI_Abort(MPI_COMM_WORLD, rc);\n  exit(1);\n}\n\nnumworkers = numtasks-1;\n\nFILE *fp;\nfp = fopen(\"matrix4000\", \"rb\");\n\n\nif (taskid == MASTER)\n{\n  printf(\"mpi_mm has started with %d tasks.\\n\",numtasks);\n\n  for(i=0; i<NRA; i++)\n  {\n    for(j=0; j<NRA; j++)\n        {\n      fscanf(fp, \"%d\", &a[i][j]);\n      fscanf(fp, \"%d\", &b[i][j]);\n    } \n  }\n\n  \n  double start = omp_get_wtime();\n  \n\n  averow = NRA/numworkers;\n  extra = NRA%numworkers;\n  offset = 0;\n  mtype = FROM_MASTER;\n  for (dest=1; dest<=numworkers; dest++)\n  {\n     rows = (dest <= extra) ? averow+1 : averow;    \n     MPI_Send(&offset, 1, MPI_INT, dest, mtype, MPI_COMM_WORLD);\n     MPI_Send(&rows, 1, MPI_INT, dest, mtype, MPI_COMM_WORLD);\n     MPI_Send(&a[offset][0], rows*NCA, MPI_INT, dest, mtype, MPI_COMM_WORLD);\n     MPI_Send(&b, NCA*NCB, MPI_INT, dest, mtype, MPI_COMM_WORLD);\n     offset = offset + rows;\n  }\n\n  \n\n  mtype = FROM_WORKER;\n  for (i=1; i<=numworkers; i++)\n  {\n     source = i;\n     MPI_Recv(&offset, 1, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n     MPI_Recv(&rows, 1, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n     MPI_Recv(&c[offset][0], rows*NCB, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n  }\n\n  double end = omp_get_wtime();\n  printf(\"Cost Time %.6g\\n\", end-start);\n  printf (\"Done.\\n\");\n  }\n\n\n\n\n   if (taskid > MASTER)\n   {\n      mtype = FROM_MASTER;\n      MPI_Recv(&offset, 1, MPI_INT, MASTER, mtype, MPI_COMM_WORLD, &status);\n      MPI_Recv(&rows, 1, MPI_INT, MASTER, mtype, MPI_COMM_WORLD, &status);\n      MPI_Recv(&a, rows*NCA, MPI_INT, MASTER, mtype, MPI_COMM_WORLD, &status);\n      MPI_Recv(&b, NCA*NCB, MPI_INT, MASTER, mtype, MPI_COMM_WORLD, &status);\n\n      #pragma omp parallel for private(i,j,k) num_threads(32)\n      for (k=0; k<NCB; k++)\n      {\n         for (i=0; i<rows; i++)\n         {\n            for (j=0; j<NCA; j++)\n            {\n              c[i][k] = c[i][k] + a[i][j] * b[j][k];\n            }\n         }\n      }\n      mtype = FROM_WORKER;\n      MPI_Send(&offset, 1, MPI_INT, MASTER, mtype, MPI_COMM_WORLD);\n      MPI_Send(&rows, 1, MPI_INT, MASTER, mtype, MPI_COMM_WORLD);\n      MPI_Send(&c, rows*NCB, MPI_INT, MASTER, mtype, MPI_COMM_WORLD);\n   }\n   MPI_Finalize();\n}"}
{"program": "gentryx_334", "code": "int main(int argc, char * argv[])\n{\n#if MPI_VERSION >= 3\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n\n    int rank, size;\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n    MPI_Count c = n/sizeof(double);\n\n    double * baseptr = NULL;\n    MPI_Win win;\n    \n\n\n    if (rank==0) {\n        set_doubles(baseptr, c, 0.0);\n    }\n\n    double * buf = NULL;\n    if ((size==1 && rank==0) || rank==1) {\n        set_doubles(buf, c, 7.0);\n        set_doubles(buf, c, 17.0);\n        set_doubles(buf, c, 0.0);\n\n        double expected = 7.0 + 17.0;\n        size_t errors = verify_doubles(buf, c, expected);\n        if (errors > 0) {\n            printf(\"There were %zu errors!\", errors);\n            for (size_t i=0; i<(size_t)c; i++) {\n                printf(\"buf[%zu] = %lf (expected %lf)\\n\", i, buf[i], expected);\n            }\n        }\n        if (errors==0) {\n            printf(\"SUCCESS\\n\");\n        }\n    }\n\n\n\n#endif\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n#if MPI_VERSION >= 3\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n    MPI_Count c = n/sizeof(double);\n\n    double * baseptr = NULL;\n    MPI_Win win;\n    \n\n    MPI_Win_allocate((MPI_Aint)(rank==0 ? n : 0), sizeof(double), MPI_INFO_NULL, MPI_COMM_WORLD, &baseptr, &win);\n    MPI_Win_lock_all(0, win);\n\n    if (rank==0) {\n        set_doubles(baseptr, c, 0.0);\n        MPI_Win_sync(win);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    double * buf = NULL;\n    if ((size==1 && rank==0) || rank==1) {\n        MPI_Alloc_mem((MPI_Aint)n, MPI_INFO_NULL, &buf);\n        set_doubles(buf, c, 7.0);\n        MPIX_Put_x(buf, c, MPI_DOUBLE, 0 \n, 0 \n, c, MPI_DOUBLE, win);\n        set_doubles(buf, c, 17.0);\n        MPIX_Accumulate_x(buf, c, MPI_DOUBLE, 0 \n, 0 \n, c, MPI_DOUBLE, MPI_SUM, win);\n        set_doubles(buf, c, 0.0);\n        MPIX_Get_x(buf, c, MPI_DOUBLE, 0 \n, 0 \n, c, MPI_DOUBLE, win);\n\n        double expected = 7.0 + 17.0;\n        size_t errors = verify_doubles(buf, c, expected);\n        if (errors > 0) {\n            printf(\"There were %zu errors!\", errors);\n            for (size_t i=0; i<(size_t)c; i++) {\n                printf(\"buf[%zu] = %lf (expected %lf)\\n\", i, buf[i], expected);\n            }\n        }\n        if (errors==0) {\n            printf(\"SUCCESS\\n\");\n        }\n    }\n\n    MPI_Win_unlock_all(win);\n    MPI_Win_free(&win);\n\n    MPI_Finalize();\n\n#endif\n\n    return 0;\n}"}
{"program": "deborasetton_335", "code": "int main(int argc, char** argv) {\n\n  \n\n\n  int my_rank, world_size;\n\n  if (world_size != 4) {\n    fprintf(stderr, \"This program needs exactly 4 processes to run.\\n\");\n  }\n\n  \n\n\n  \n\n  int bufsize;\n\n  \n\n  \n\n  bufsize = ROWS * (bufsize + MPI_BSEND_OVERHEAD);\n\n  if (my_rank == MASTER_RANK) {\n    \n\n    \n\n    bufsize = bufsize * 3;\n  }\n\n  \n\n  int* sendbuf = (int*)malloc(sizeof(int) * bufsize);\n\n  switch (my_rank) {\n    case MASTER_RANK:\n      task4(my_rank); break;\n    case 1:\n      task1(my_rank); break;\n    case 2:\n      task2(my_rank); break;\n    case 3:\n      task3(my_rank); break;\n  }\n\n  \n\n  free(sendbuf);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n\n  \n\n  MPI_Init(NULL, NULL);\n\n  int my_rank, world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  if (world_size != 4) {\n    fprintf(stderr, \"This program needs exactly 4 processes to run.\\n\");\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n\n  \n\n\n  \n\n  int bufsize;\n  MPI_Pack_size(COLS, MPI_INT, MPI_COMM_WORLD, &bufsize);\n\n  \n\n  \n\n  bufsize = ROWS * (bufsize + MPI_BSEND_OVERHEAD);\n\n  if (my_rank == MASTER_RANK) {\n    \n\n    \n\n    bufsize = bufsize * 3;\n  }\n\n  \n\n  int* sendbuf = (int*)malloc(sizeof(int) * bufsize);\n  MPI_Buffer_attach(sendbuf, bufsize);\n\n  switch (my_rank) {\n    case MASTER_RANK:\n      task4(my_rank); break;\n    case 1:\n      task1(my_rank); break;\n    case 2:\n      task2(my_rank); break;\n    case 3:\n      task3(my_rank); break;\n  }\n\n  \n\n  MPI_Buffer_detach(sendbuf, &bufsize);\n  free(sendbuf);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "mpip_337", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t N[3];\n  ptrdiff_t local_N[3], local_N_start[3], local_M;\n  double lower_border[3], upper_border[3];\n  MPI_Comm comm_cart_2d;\n  pnfft_plan pnfft;\n  pnfft_nodes nodes;\n  pnfft_complex *f_hat;\n  double *f, *x;\n  \n  \n\n  pnfft_init();\n  \n  \n\n  np[0] = 2; np[1] = 2;\n  N[0] = 2; N[1] = 2; N[2] = 4;\n  local_M = 1;\n  \n  \n\n  if( pnfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n     pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with 4 processors.\\n\");\n    return 1;\n  }\n  \n  \n\n  pnfft_local_size_3d_c2r(N, comm_cart_2d, PNFFT_TRANSPOSED_NONE,\n      local_N, local_N_start, lower_border, upper_border);\n  \n  \n\n  pnfft = pnfft_init_3d_c2r(N, comm_cart_2d);\n\n  \n\n  nodes = pnfft_init_nodes(local_M, PNFFT_MALLOC_X | PNFFT_MALLOC_F);\n\n  \n\n  f_hat = pnfft_get_f_hat(pnfft);\n  f     = pnfft_get_f_real(nodes);\n  x     = pnfft_get_x(nodes);\n\n  \n\n  pnfft_init_f_hat_3d(N, local_N, local_N_start, PNFFT_TRANSPOSED_NONE,\n      f_hat);\n  \n  \n\n  pnfft_apr_complex_3d(\n      f_hat, local_N, local_N_start, 0, \"PNFFT, f_hat\", MPI_COMM_WORLD);\n  \n  \n\n  pnfft_init_x_3d(lower_border, upper_border, local_M,\n      x);\n\n  \n\n  pnfft_trafo(pnfft, nodes, PNFFT_COMPUTE_F);\n  \n  \n\n  pnfft_vpr_real(x, 3*local_M, \"PNFFT, x\", MPI_COMM_WORLD);\n  pnfft_vpr_real(f, local_M, \"PNFFT, f\", MPI_COMM_WORLD);\n  \n  \n\n  pnfft_adj(pnfft, nodes, PNFFT_COMPUTE_F);\n  \n  \n\n  pnfft_apr_complex_3d(\n      f_hat, local_N, local_N_start, 0, \"PNFFT^H, f_hat\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_finalize(pnfft, PNFFT_FREE_F_HAT);\n  pnfft_free_nodes(nodes, PNFFT_FREE_X | PNFFT_FREE_F);\n  \n  \n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t N[3];\n  ptrdiff_t local_N[3], local_N_start[3], local_M;\n  double lower_border[3], upper_border[3];\n  MPI_Comm comm_cart_2d;\n  pnfft_plan pnfft;\n  pnfft_nodes nodes;\n  pnfft_complex *f_hat;\n  double *f, *x;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  np[0] = 2; np[1] = 2;\n  N[0] = 2; N[1] = 2; N[2] = 4;\n  local_M = 1;\n  \n  \n\n  if( pnfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n     pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with 4 processors.\\n\");\n    return 1;\n  }\n  \n  \n\n  pnfft_local_size_3d_c2r(N, comm_cart_2d, PNFFT_TRANSPOSED_NONE,\n      local_N, local_N_start, lower_border, upper_border);\n  \n  \n\n  pnfft = pnfft_init_3d_c2r(N, comm_cart_2d);\n\n  \n\n  nodes = pnfft_init_nodes(local_M, PNFFT_MALLOC_X | PNFFT_MALLOC_F);\n\n  \n\n  f_hat = pnfft_get_f_hat(pnfft);\n  f     = pnfft_get_f_real(nodes);\n  x     = pnfft_get_x(nodes);\n\n  \n\n  pnfft_init_f_hat_3d(N, local_N, local_N_start, PNFFT_TRANSPOSED_NONE,\n      f_hat);\n  \n  \n\n  pnfft_apr_complex_3d(\n      f_hat, local_N, local_N_start, 0, \"PNFFT, f_hat\", MPI_COMM_WORLD);\n  \n  \n\n  pnfft_init_x_3d(lower_border, upper_border, local_M,\n      x);\n\n  \n\n  pnfft_trafo(pnfft, nodes, PNFFT_COMPUTE_F);\n  \n  \n\n  pnfft_vpr_real(x, 3*local_M, \"PNFFT, x\", MPI_COMM_WORLD);\n  pnfft_vpr_real(f, local_M, \"PNFFT, f\", MPI_COMM_WORLD);\n  \n  \n\n  pnfft_adj(pnfft, nodes, PNFFT_COMPUTE_F);\n  \n  \n\n  pnfft_apr_complex_3d(\n      f_hat, local_N, local_N_start, 0, \"PNFFT^H, f_hat\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_finalize(pnfft, PNFFT_FREE_F_HAT);\n  pnfft_free_nodes(nodes, PNFFT_FREE_X | PNFFT_FREE_F);\n  MPI_Comm_free(&comm_cart_2d);\n  \n  \n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "ghisvail_338", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c(4, n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c(\n      4, n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r(\n      4, n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c(4, n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c(\n      4, n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r(\n      4, n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2r(4, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu3ra_339", "code": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nints, len;\n    char *filename, *tmp;\n    int errs=0, toterrs;\n    MPI_File fh;\n    MPI_Status status[NR_NBOPS];\n    MPI_Request request[NR_NBOPS];\n    int errcode = 0;\n\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: async -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+10);\n    }\n\n\n    buf = (int *) malloc(SIZE);\n    nints = SIZE/sizeof(int);\n    for (i=0; i<nints; i++) buf[i] = rank*100000 + i;\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n    for (i=0; i<NR_NBOPS; i++) { \n\terrcode =\n\tif (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_iwrite\");\n\t}\n    }\n\n\n    \n\n\n    for (i=0; i<nints; i++) buf[i] = 0;\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n\n    for (i=0; i<NR_NBOPS; i++) {\n\terrcode =\n\tif (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n\t}\n    }\n\n\n    \n\n    for (i=0; i<nints; i++) {\n\tif (buf[i] != (rank*100000 + i)) {\n\t    errs++;\n\t    fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n\t}\n    }\n\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(buf);\n    free(filename);\n    free(tmp);\n\n    return 0; \n}", "label": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nints, len;\n    char *filename, *tmp;\n    int errs=0, toterrs;\n    MPI_File fh;\n    MPI_Status status[NR_NBOPS];\n    MPI_Request request[NR_NBOPS];\n    int errcode = 0;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: async -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+10);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n\n    buf = (int *) malloc(SIZE);\n    nints = SIZE/sizeof(int);\n    for (i=0; i<nints; i++) buf[i] = rank*100000 + i;\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    errcode = MPI_File_open(MPI_COMM_SELF, filename, \n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n    MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);\n    for (i=0; i<NR_NBOPS; i++) { \n\terrcode = MPI_File_iwrite_at(fh, nints/NR_NBOPS*i, \n\t\tbuf+(nints/NR_NBOPS*i), nints/NR_NBOPS, MPI_INT, &(request[i]));\n\tif (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_iwrite\");\n\t}\n    }\n    MPI_Waitall(NR_NBOPS, request, status);\n\n    MPI_File_close(&fh);\n\n    \n\n\n    for (i=0; i<nints; i++) buf[i] = 0;\n    errcode = MPI_File_open(MPI_COMM_SELF, filename, \n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n\n    MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);\n    for (i=0; i<NR_NBOPS; i++) {\n\terrcode = MPI_File_iread_at(fh, nints/NR_NBOPS*i, \n\t\tbuf+(nints/NR_NBOPS*i), nints/NR_NBOPS, MPI_INT, &(request[i]));\n\tif (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n\t}\n    }\n    MPI_Waitall(NR_NBOPS, request, status);\n\n    MPI_File_close(&fh);\n\n    \n\n    for (i=0; i<nints; i++) {\n\tif (buf[i] != (rank*100000 + i)) {\n\t    errs++;\n\t    fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n\t}\n    }\n\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(buf);\n    free(filename);\n    free(tmp);\n\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "JulianKunkel_340", "code": "int main( int argc, char * argv[] )\n{\n\topen(\"test.c\", 0);\n\tint rank, size;\n\tMPI_File fh;\n\tMPI_Datatype etype; \n\tMPI_Status status;\n\tchar buf;\n\tMPI_Info info;\n\n\tpid_t pid = getpid();\n\n\tprintf( \"=========\n\tprintf( \"=========\n\tprintf( \"=========\n\tprintf( \"Hello, world, I am %d of %d, pid: %lld .\\n\",\n\t        rank, size, (long long int) pid );\n\n\tetype = MPI_CHAR;\n\tprintf( \"=========\n\n\n\tbuf = 'a';\n\tprintf( \"=========\n\n\tMPI_Datatype darray;\n\tint array_of_gsizes[4] = {20,20,20,20};\n\tint array_of_psizes[4] = {2, 2, 2, 2};\n\tint array_of_distribs[4] = {MPI_DISTRIBUTE_CYCLIC,  MPI_DISTRIBUTE_BLOCK , MPI_DISTRIBUTE_BLOCK, MPI_DISTRIBUTE_BLOCK };\n\tint array_of_dargs[4] = {10, 10, 10, 10};\n\n\tMPI_Aint displs[3];\n\tMPI_Datatype oldtypes[3];\n\tint blocklens[3];\n\tint i;\n\tfor (i=0; i < 3; i++) {\n\t        blocklens[i] = 2*i;\n\t        oldtypes[i] = MPI_INT;\n\t        displs[i] = i * 10;\n\t }\n        oldtypes[2] = darray;\n\tMPI_Datatype structType;\n\n\tint data = 40;\n\n\n\tprintf( \"=========\n\tprintf( \"=========\n\n\treturn 0;\n}", "label": "int main( int argc, char * argv[] )\n{\n\topen(\"test.c\", 0);\n\tint rank, size;\n\tMPI_File fh;\n\tMPI_Datatype etype; \n\tMPI_Status status;\n\tchar buf;\n\tMPI_Info info;\n\n\tpid_t pid = getpid();\n\n\tprintf( \"==========MPI_Init()==========\\n\" );\n\tMPI_Init( &argc, &argv );\n\tprintf( \"==========MPI_Comm_rank()==========\\n\" );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\tprintf( \"==========MPI_Comm_size()==========\\n\" );\n\tMPI_Comm_size( MPI_COMM_WORLD, &size );\n\tprintf( \"Hello, world, I am %d of %d, pid: %lld .\\n\",\n\t        rank, size, (long long int) pid );\n\n\tetype = MPI_CHAR;\n\tprintf( \"==========MPI_File_open()==========\\n\" );\n\n\tMPI_Info_create( &info );\n\tMPI_Info_set( info, \"testhint1\", \"test.txt\" );\n\tMPI_Info_set( info, \"testhint2\", \"2\" );\n\tMPI_Info_set( info, \"noncoll_read_bufsize\", \"4096\" );\n\n\tMPI_File_open( MPI_COMM_WORLD, \"mpi_wrapper_test_file\", MPI_MODE_CREATE | MPI_MODE_RDWR, info, &fh );\n\tbuf = 'a';\n\tprintf( \"==========MPI_File_write()==========\\n\" );\n\tMPI_File_write( fh, &buf, 1, etype, &status );\n\n\tMPI_Datatype darray;\n\tint array_of_gsizes[4] = {20,20,20,20};\n\tint array_of_psizes[4] = {2, 2, 2, 2};\n\tint array_of_distribs[4] = {MPI_DISTRIBUTE_CYCLIC,  MPI_DISTRIBUTE_BLOCK , MPI_DISTRIBUTE_BLOCK, MPI_DISTRIBUTE_BLOCK };\n\tint array_of_dargs[4] = {10, 10, 10, 10};\n\tMPI_Type_create_darray(16, 1, 4, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, MPI_ORDER_C, MPI_INT, & darray);\n\tMPI_Type_commit(& darray);\n\n\tMPI_Aint displs[3];\n\tMPI_Datatype oldtypes[3];\n\tint blocklens[3];\n\tint i;\n\tfor (i=0; i < 3; i++) {\n\t        blocklens[i] = 2*i;\n\t        oldtypes[i] = MPI_INT;\n\t        displs[i] = i * 10;\n\t }\n        oldtypes[2] = darray;\n\tMPI_Datatype structType;\n    \tMPI_Type_struct( 3, blocklens, displs, oldtypes, & structType );\n\tMPI_Type_commit(& structType);\n\n\tMPI_File_set_view( fh, 30, MPI_INT, structType, \"native\", MPI_INFO_NULL );\n\tint data = 40;\n\tMPI_File_seek(fh, 2, MPI_SEEK_CUR);\n\tMPI_File_write( fh, & data, 1, MPI_INT, &status );\n\n\n\tprintf( \"==========MPI_File_close()==========\\n\" );\n\tMPI_File_close( &fh );\n\tprintf( \"==========MPI_Finalize()==========\\n\" );\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "bmi-forum_343", "code": "int main(int argc, char *argv[])\n{\n\tint\t\trank;\n\tint\t\tprocCount;\n\tStream*\t\tstream;\n\t\n\tDictionary*\t\tdictionary;\n\n\tDictionary_Entry_Value* info;\n\tDictionary_Entry_Value* varList;\n\tDictionary_Entry_Value* varValue;\n\t\n\tVariable_Register*\t\tvariable_Register;\n\tConditionFunction_Register*\tconFunc_Register;\n\n\tVariableAllVC* vc;\n\n\tdouble*\t\tdata;\n\tIndex\t\tlength = 3;\n\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\tdata = Memory_Alloc_Array( double, length, \"test\" );\n\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\tdictionary = Dictionary_New();\n\n\tinfo = Dictionary_Entry_Value_NewStruct();\n\tvarList = Dictionary_Entry_Value_NewList();\n\tvarValue = Dictionary_Entry_Value_NewStruct();\n\n\tDictionary_Entry_Value_AddMember( varValue, \"name\", Dictionary_Entry_Value_FromString( \"test\" ) );\n\tDictionary_Entry_Value_AddMember( varValue, \"type\", Dictionary_Entry_Value_FromString( \"double\" ) );\n\tDictionary_Entry_Value_AddMember( varValue, \"value\", Dictionary_Entry_Value_FromDouble( 2.0 ) );\n\n\tDictionary_Entry_Value_AddElement( varList, varValue );\n\n\tDictionary_Entry_Value_AddMember( info, \"variables\", varList );\n\n\tDictionary_Add( dictionary, \"VariableAllVC\", info );\n\t\n\t\n\tconFunc_Register = ConditionFunction_Register_New();\n\t\n\tvariable_Register = Variable_Register_New();\n\t\n\tVariable_NewScalar(\n\t\t\"test\",\n\t\tVariable_DataType_Double,\n\t\t&length,\n\t\t(void*)&data,\n\t\tvariable_Register );\n\t\t\n\tVariable_Register_BuildAll(variable_Register);\n\t\n\tvc = VariableAllVC_New( \"variableAllVC\", \"VariableAllVC\", variable_Register, conFunc_Register, dictionary, NULL );\n\t\n\tBuild( vc, 0, False );\n\t\n\tVariableCondition_Apply(vc, NULL);\n\t\n\tif (rank == 0) {\n\t\tint i;\n\n\t\tfor (i = 0; i < length; ++i )  {\n\t\t\tJournal_Printf( stream, \"%lf\\n\", data[i] );\n\t\t}\n\t}\n\n\tMemory_Free( data );\n\n\tStg_Class_Delete(vc);\n\t\t\n\tStg_Class_Delete(variable_Register);\n\tStg_Class_Delete(conFunc_Register);\n\tStg_Class_Delete(dictionary);\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main(int argc, char *argv[])\n{\n\tint\t\trank;\n\tint\t\tprocCount;\n\tStream*\t\tstream;\n\t\n\tDictionary*\t\tdictionary;\n\n\tDictionary_Entry_Value* info;\n\tDictionary_Entry_Value* varList;\n\tDictionary_Entry_Value* varValue;\n\t\n\tVariable_Register*\t\tvariable_Register;\n\tConditionFunction_Register*\tconFunc_Register;\n\n\tVariableAllVC* vc;\n\n\tdouble*\t\tdata;\n\tIndex\t\tlength = 3;\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\tdata = Memory_Alloc_Array( double, length, \"test\" );\n\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\tdictionary = Dictionary_New();\n\n\tinfo = Dictionary_Entry_Value_NewStruct();\n\tvarList = Dictionary_Entry_Value_NewList();\n\tvarValue = Dictionary_Entry_Value_NewStruct();\n\n\tDictionary_Entry_Value_AddMember( varValue, \"name\", Dictionary_Entry_Value_FromString( \"test\" ) );\n\tDictionary_Entry_Value_AddMember( varValue, \"type\", Dictionary_Entry_Value_FromString( \"double\" ) );\n\tDictionary_Entry_Value_AddMember( varValue, \"value\", Dictionary_Entry_Value_FromDouble( 2.0 ) );\n\n\tDictionary_Entry_Value_AddElement( varList, varValue );\n\n\tDictionary_Entry_Value_AddMember( info, \"variables\", varList );\n\n\tDictionary_Add( dictionary, \"VariableAllVC\", info );\n\t\n\t\n\tconFunc_Register = ConditionFunction_Register_New();\n\t\n\tvariable_Register = Variable_Register_New();\n\t\n\tVariable_NewScalar(\n\t\t\"test\",\n\t\tVariable_DataType_Double,\n\t\t&length,\n\t\t(void*)&data,\n\t\tvariable_Register );\n\t\t\n\tVariable_Register_BuildAll(variable_Register);\n\t\n\tvc = VariableAllVC_New( \"variableAllVC\", \"VariableAllVC\", variable_Register, conFunc_Register, dictionary, NULL );\n\t\n\tBuild( vc, 0, False );\n\t\n\tVariableCondition_Apply(vc, NULL);\n\t\n\tif (rank == 0) {\n\t\tint i;\n\n\t\tfor (i = 0; i < length; ++i )  {\n\t\t\tJournal_Printf( stream, \"%lf\\n\", data[i] );\n\t\t}\n\t}\n\n\tMemory_Free( data );\n\n\tStg_Class_Delete(vc);\n\t\t\n\tStg_Class_Delete(variable_Register);\n\tStg_Class_Delete(conFunc_Register);\n\tStg_Class_Delete(dictionary);\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "lesina_344", "code": "int main(int argc, char **argv)\n{\n\tint *data;\t\t\n\n\tint *resultant_array;\t\n\n\tint *sub;\n\n\tint m, n;\n\tint id, p;\n\tint r;\n\tint s;\n\tint i;\n\tint z;\n\tint move;\n\tMPI_Status status;\n\n\t\n\n\tif (id == 0) {\n\t\tn = 80000;\n\t\ts = n / p;\n\t\tr = n % p;\n\t\tdata = new int[n + s - r];\n\n\t\tofstream file(\"input\");\n\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tdata[i] = rand() % 15000;\n\t\t\tfile << data[i] << \" \";\n\t\t}\n\n\t\tfile.close();\n\n\t\tif (r != 0) {\n\t\t\tfor (i = n; i < n + s - r; i++)\n\t\t\t\tdata[i] = 0;\n\n\t\t\ts = s + 1;\n\t\t}\n\n\t\tstartT =\t\n\n\t\tresultant_array = new int[s];\t\n\n\t\t\n\n\t\tsort(resultant_array, s);\n\t} else {\n\t\t\n\n\t\tresultant_array = new int[s];\n\t\t\n\n\t\tsort(resultant_array, s);\t\n\n\t}\n\n\tmove = 1;\n\n\t\n\n\twhile (move < p) {\n\t\tif (id % (2 * move) == 0) {\n\t\t\tif (id + move < p) {\t\n\n\t\t\t\tsub = new int[m];\t\n\n\t\t\t\t\n\n\t\t\t\tresultant_array =\n\t\t\t\t    mergeArrays(resultant_array, s, sub, m);\n\t\t\t\ts = s + m;\n\t\t\t}\n\t\t} else {\t\n\n\t\t\tint near = id - move;\n\t\t\tbreak;\n\t\t}\n\n\t\tmove = move * 2;\n\t}\n\n\t\n\n\tif (id == 0) {\n\t\tstopT =\n\t\tdouble parallelTime = stopT - startT;\n\t\tprintf(\"\\n\\n\\nTime: %f\", parallelTime);\n\n\t\tofstream file(\"output\");\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tfile << resultant_array[i] << \" \";\n\t\t}\n\n\t\tfile.close();\n\t}\n\n\n}", "label": "int main(int argc, char **argv)\n{\n\tint *data;\t\t\n\n\tint *resultant_array;\t\n\n\tint *sub;\n\n\tint m, n;\n\tint id, p;\n\tint r;\n\tint s;\n\tint i;\n\tint z;\n\tint move;\n\tMPI_Status status;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &id);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\t\n\n\tif (id == 0) {\n\t\tn = 80000;\n\t\ts = n / p;\n\t\tr = n % p;\n\t\tdata = new int[n + s - r];\n\n\t\tofstream file(\"input\");\n\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tdata[i] = rand() % 15000;\n\t\t\tfile << data[i] << \" \";\n\t\t}\n\n\t\tfile.close();\n\n\t\tif (r != 0) {\n\t\t\tfor (i = n; i < n + s - r; i++)\n\t\t\t\tdata[i] = 0;\n\n\t\t\ts = s + 1;\n\t\t}\n\n\t\tstartT = MPI_Wtime();\t\n\n\t\tMPI_Bcast(&s, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\t\tresultant_array = new int[s];\t\n\n\t\t\n\n\t\tMPI_Scatter(data, s, MPI_INT, resultant_array, s, MPI_INT, 0,\n\t\t\t    MPI_COMM_WORLD);\n\t\tsort(resultant_array, s);\n\t} else {\n\t\tMPI_Bcast(&s, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\t\t\n\n\t\tresultant_array = new int[s];\n\t\tMPI_Scatter(data, s, MPI_INT, resultant_array, s, MPI_INT, 0,\n\t\t\t    MPI_COMM_WORLD);\n\t\t\n\n\t\tsort(resultant_array, s);\t\n\n\t}\n\n\tmove = 1;\n\n\t\n\n\twhile (move < p) {\n\t\tif (id % (2 * move) == 0) {\n\t\t\tif (id + move < p) {\t\n\n\t\t\t\tMPI_Recv(&m, 1, MPI_INT, id + move, 0,\n\t\t\t\t\t MPI_COMM_WORLD, &status);\n\t\t\t\tsub = new int[m];\t\n\n\t\t\t\tMPI_Recv(sub, m, MPI_INT, id + move, 0,\n\t\t\t\t\t MPI_COMM_WORLD, &status);\n\t\t\t\t\n\n\t\t\t\tresultant_array =\n\t\t\t\t    mergeArrays(resultant_array, s, sub, m);\n\t\t\t\ts = s + m;\n\t\t\t}\n\t\t} else {\t\n\n\t\t\tint near = id - move;\n\t\t\tMPI_Send(&s, 1, MPI_INT, near, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(resultant_array, s, MPI_INT, near, 0,\n\t\t\t\t MPI_COMM_WORLD);\n\t\t\tbreak;\n\t\t}\n\n\t\tmove = move * 2;\n\t}\n\n\t\n\n\tif (id == 0) {\n\t\tstopT = MPI_Wtime();\n\t\tdouble parallelTime = stopT - startT;\n\t\tprintf(\"\\n\\n\\nTime: %f\", parallelTime);\n\n\t\tofstream file(\"output\");\n\t\tfor (i = 0; i < n; i++) {\n\t\t\tfile << resultant_array[i] << \" \";\n\t\t}\n\n\t\tfile.close();\n\t}\n\n\tMPI_Finalize();\t\t\n\n}"}
{"program": "joeladams_345", "code": "int main(int argc, char** argv) {\n    int id = -1, numProcesses = -1;\n    int sendValue = -1, receivedValue = -1;\n    MPI_Status status;\n\n\n    if (numProcesses > 1) {\n        sendValue = id;\n        if ( odd(id) ) {  \n\n        } else {          \n\n        }\n\n        printf(\"Process %d of %d computed %d and received %d\\n\",\n                id, numProcesses, sendValue, receivedValue);\n    } else if ( !id) {  \n\n        printf(\"\\nPlease run this program using -np N where N is positive and even.\\n\\n\");\n    }\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int id = -1, numProcesses = -1;\n    int sendValue = -1, receivedValue = -1;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    if (numProcesses > 1) {\n        sendValue = id;\n        if ( odd(id) ) {  \n\n            MPI_Send(&sendValue, 1, MPI_INT, id-1, 1, MPI_COMM_WORLD);\n            MPI_Recv(&receivedValue, 1, MPI_INT, id-1, 2,\n                       MPI_COMM_WORLD, &status);\n        } else {          \n\n            MPI_Recv(&receivedValue, 1, MPI_INT, id+1, 1,\n                       MPI_COMM_WORLD, &status);\n            MPI_Send(&sendValue, 1, MPI_INT, id+1, 2, MPI_COMM_WORLD);\n        }\n\n        printf(\"Process %d of %d computed %d and received %d\\n\",\n                id, numProcesses, sendValue, receivedValue);\n    } else if ( !id) {  \n\n        printf(\"\\nPlease run this program using -np N where N is positive and even.\\n\\n\");\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "callmetaste_347", "code": "int main(int argc, char* argv[])\n{\n  int numprocs,myid,fail=-1;\n  int m,p,q,sum,nd_tp;\n  double startwtime, endwtime;\n\n\n  srand(time(NULL));\n  adainit(); \n\n  dimension_broadcast(myid,&m,&p,&q,&nd_tp); \n  input_planes_broadcast(myid,m,p,q);\n  interpolation_points_broadcast(myid,m,p,q);\n\n  if(myid==0)\n  {\n    startwtime =\n    sum = server_distribute(m,p,q,numprocs,nd_tp);\n    endwtime =\n    printf(\"\\nTotal wall time = %lf seconds on %d processors\\n\",\n           endwtime-startwtime, numprocs);\n  }\n  else \n    client_compute(m,p,q,nd_tp);  \n\n  adafinal();      \n\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  int numprocs,myid,fail=-1;\n  int m,p,q,sum,nd_tp;\n  double startwtime, endwtime;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n  srand(time(NULL));\n  adainit(); \n\n  dimension_broadcast(myid,&m,&p,&q,&nd_tp); \n  input_planes_broadcast(myid,m,p,q);\n  interpolation_points_broadcast(myid,m,p,q);\n\n  if(myid==0)\n  {\n    startwtime = MPI_Wtime();\n    sum = server_distribute(m,p,q,numprocs,nd_tp);\n    endwtime = MPI_Wtime();\n    printf(\"\\nTotal wall time = %lf seconds on %d processors\\n\",\n           endwtime-startwtime, numprocs);\n  }\n  else \n    client_compute(m,p,q,nd_tp);  \n\n  adafinal();      \n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "gnu3ra_349", "code": "int main(int argc, char **argv) {\n    int i, j, rank, nranks, peer, bufsize, errors;\n    double **buffer, *src_buf, *dst_buf;\n    int count[2], src_stride, trg_stride, stride_level;\n\n    ARMCI_Init();\n\n\n    buffer = (double **) malloc(sizeof(double *) * nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    ARMCI_Malloc((void **) buffer, bufsize);\n    src_buf = ARMCI_Malloc_local(bufsize);\n    dst_buf = ARMCI_Malloc_local(bufsize);\n\n    if (rank == 0)\n        printf(\"ARMCI Strided Put Test:\\n\");\n\n    src_stride = XDIM * sizeof(double);\n    trg_stride = XDIM * sizeof(double);\n    stride_level = 1;\n\n    count[1] = YDIM;\n    count[0] = XDIM * sizeof(double);\n\n    ARMCI_Barrier();\n\n    peer = (rank+1) % nranks;\n\n    for (i = 0; i < ITERATIONS; i++) {\n\n      for (j = 0; j < XDIM*YDIM; j++) {\n        *(src_buf + j) = rank + i;\n      }\n\n      ARMCI_PutS(\n          src_buf,\n          &src_stride,\n          (void *) buffer[peer],\n          &trg_stride,\n          count,\n          stride_level,\n          peer);\n\n      ARMCI_GetS(\n          (void *) buffer[peer],\n          &trg_stride,\n          dst_buf,\n          &src_stride,\n          count,\n          stride_level,\n          peer);\n    }\n\n    ARMCI_Barrier();\n\n    ARMCI_Access_begin(buffer[rank]);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer[rank] + i + j*XDIM);\n        const double expected = (1.0 + rank) + (1.0 + ((rank+nranks-1)%nranks)) + (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    ARMCI_Access_end(buffer[rank]);\n\n    ARMCI_Free((void *) buffer[rank]);\n    ARMCI_Free_local(src_buf);\n    ARMCI_Free_local(dst_buf);\n\n    ARMCI_Finalize();\n\n    if (errors == 0) {\n      printf(\"%d: Success\\n\", rank);\n      return 0;\n    } else {\n      printf(\"%d: Fail\\n\", rank);\n      return 1;\n    }\n}", "label": "int main(int argc, char **argv) {\n    int i, j, rank, nranks, peer, bufsize, errors;\n    double **buffer, *src_buf, *dst_buf;\n    int count[2], src_stride, trg_stride, stride_level;\n\n    MPI_Init(&argc, &argv);\n    ARMCI_Init();\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    buffer = (double **) malloc(sizeof(double *) * nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    ARMCI_Malloc((void **) buffer, bufsize);\n    src_buf = ARMCI_Malloc_local(bufsize);\n    dst_buf = ARMCI_Malloc_local(bufsize);\n\n    if (rank == 0)\n        printf(\"ARMCI Strided Put Test:\\n\");\n\n    src_stride = XDIM * sizeof(double);\n    trg_stride = XDIM * sizeof(double);\n    stride_level = 1;\n\n    count[1] = YDIM;\n    count[0] = XDIM * sizeof(double);\n\n    ARMCI_Barrier();\n\n    peer = (rank+1) % nranks;\n\n    for (i = 0; i < ITERATIONS; i++) {\n\n      for (j = 0; j < XDIM*YDIM; j++) {\n        *(src_buf + j) = rank + i;\n      }\n\n      ARMCI_PutS(\n          src_buf,\n          &src_stride,\n          (void *) buffer[peer],\n          &trg_stride,\n          count,\n          stride_level,\n          peer);\n\n      ARMCI_GetS(\n          (void *) buffer[peer],\n          &trg_stride,\n          dst_buf,\n          &src_stride,\n          count,\n          stride_level,\n          peer);\n    }\n\n    ARMCI_Barrier();\n\n    ARMCI_Access_begin(buffer[rank]);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer[rank] + i + j*XDIM);\n        const double expected = (1.0 + rank) + (1.0 + ((rank+nranks-1)%nranks)) + (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    ARMCI_Access_end(buffer[rank]);\n\n    ARMCI_Free((void *) buffer[rank]);\n    ARMCI_Free_local(src_buf);\n    ARMCI_Free_local(dst_buf);\n\n    ARMCI_Finalize();\n    MPI_Finalize();\n\n    if (errors == 0) {\n      printf(\"%d: Success\\n\", rank);\n      return 0;\n    } else {\n      printf(\"%d: Fail\\n\", rank);\n      return 1;\n    }\n}"}
{"program": "JulianKunkel_350", "code": "int main( int argc, char ** argv )\n{\n\tint rank;\n\tint loop1;\n\tint loop2;\n\tint signal;\n\tFILE * pFile;\n\tchar filename[50];\n\tconst char content[] = \"This is an apple.\\n\";\n\n\tint b[4] = {67, 68, 69, 70};\n\tmemset( filename, 0, 50 );\n\tsnprintf( filename, 49, \"example%d.txt\", rank );\n\tfor( loop1 = 0; loop1 < 4; loop1++ ) {\n\t\tif( rank == 1 ) {\n\t\t} else if( rank == 2 ) {\n\t\t\tMPI_Status stat;\n\t\t}\n\t\tpFile = fopen( filename , \"w\" );\n\t\tfor( loop2 = 0; loop2 < 4; loop2++ ) {\n\t\t\tfwrite( content, sizeof( content ), 1, pFile );\n\t\t}\n\t\tfwrite( ( void * ) b, sizeof( int ), 4, pFile );\n\t\tfclose( pFile );\n\t}\n}", "label": "int main( int argc, char ** argv )\n{\n\tint rank;\n\tint loop1;\n\tint loop2;\n\tint signal;\n\tFILE * pFile;\n\tchar filename[50];\n\tconst char content[] = \"This is an apple.\\n\";\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\tint b[4] = {67, 68, 69, 70};\n\tmemset( filename, 0, 50 );\n\tsnprintf( filename, 49, \"example%d.txt\", rank );\n\tfor( loop1 = 0; loop1 < 4; loop1++ ) {\n\t\tif( rank == 1 ) {\n\t\t\tMPI_Send( &signal, 1, MPI_INT, 2, 0, MPI_COMM_WORLD );\n\t\t} else if( rank == 2 ) {\n\t\t\tMPI_Status stat;\n\t\t\tMPI_Recv( &signal, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &stat );\n\t\t}\n\t\tpFile = fopen( filename , \"w\" );\n\t\tfor( loop2 = 0; loop2 < 4; loop2++ ) {\n\t\t\tfwrite( content, sizeof( content ), 1, pFile );\n\t\t}\n\t\tfwrite( ( void * ) b, sizeof( int ), 4, pFile );\n\t\tfclose( pFile );\n\t}\n\tMPI_Finalize();\n}"}
{"program": "cot_351", "code": "int main(int argc, char **argv) {\n  int numtasks, rank;\n  int rank_dst, ping_side;\n\n  \n\n\n  if (numtasks != 2) {\n    printf(\"Need 2 processes\\n\");\n    exit(1);\n  }\n\n  ping_side = !(rank & 1);\n  rank_dst = ping_side?(rank | 1) : (rank & ~1);\n\n  if (ping_side) {\n    int x=42, y;\n    MPI_Request send_request;\n    MPI_Request recv_request;\n\n\n\n\n    if (y == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n\n\n    if (y == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n  }\n  else {\n    int x, y;\n\n\n    if (x == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n    if (y == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n  }\n\n  exit(0);\n}", "label": "int main(int argc, char **argv) {\n  int numtasks, rank;\n  int rank_dst, ping_side;\n\n  \n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n  if (numtasks != 2) {\n    printf(\"Need 2 processes\\n\");\n    MPI_Abort(MPI_COMM_WORLD, 1);\n    exit(1);\n  }\n\n  ping_side = !(rank & 1);\n  rank_dst = ping_side?(rank | 1) : (rank & ~1);\n\n  if (ping_side) {\n    int x=42, y;\n    MPI_Request send_request;\n    MPI_Request recv_request;\n\n    MPI_Send_init(&x, 1, MPI_INT, rank_dst, 1, MPI_COMM_WORLD, &send_request);\n    MPI_Start(&send_request);\n    MPI_Wait(&send_request, MPI_STATUS_IGNORE);\n\n    MPI_Start(&send_request);\n    MPI_Wait(&send_request, MPI_STATUS_IGNORE);\n\n    MPI_Recv_init(&y, 1, MPI_INT, rank_dst, 1, MPI_COMM_WORLD, &recv_request);\n    MPI_Start(&recv_request);\n    MPI_Wait(&recv_request, MPI_STATUS_IGNORE);\n\n    if (y == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n\n    MPI_Start(&recv_request);\n    MPI_Wait(&recv_request, MPI_STATUS_IGNORE);\n\n    if (y == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n  }\n  else {\n    int x, y;\n    MPI_Recv(&x, 1, MPI_INT, rank_dst, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(&y, 1, MPI_INT, rank_dst, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    MPI_Send(&y, 1, MPI_INT, rank_dst, 1, MPI_COMM_WORLD);\n    MPI_Send(&y, 1, MPI_INT, rank_dst, 1, MPI_COMM_WORLD);\n\n    if (x == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n    if (y == 42) \n      printf(\"success\\n\"); \n    else\n      printf(\"failure\\n\");\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n  exit(0);\n}"}
{"program": "wilseypa_352", "code": "int main(int argc,char *argv[])\n{\n    int    n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int    namelen;\n    char   processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n    fprintf(stdout,\"Process %d of %d is on %s\\n\",\n            myid, numprocs, processor_name);\n    fflush(stdout);\n\n    n = 10000;                  \n\n    if (myid == 0)\n        startwtime =\n\n\n    h   = 1.0 / (double) n;\n    sum = 0.0;\n    \n\n    for (i = myid + 1; i <= n; i += numprocs)\n    {\n        x = h * ((double)i - 0.5);\n        sum += f(x);\n    }\n    mypi = h * sum;\n\n\n    if (myid == 0) {\n        endwtime =\n        printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n               pi, fabs(pi - PI25DT));\n        printf(\"wall clock time = %f\\n\", endwtime-startwtime);         \n        fflush(stdout);\n    }\n\n    return 0;\n}", "label": "int main(int argc,char *argv[])\n{\n    int    n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int    namelen;\n    char   processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    MPI_Get_processor_name(processor_name,&namelen);\n\n    fprintf(stdout,\"Process %d of %d is on %s\\n\",\n            myid, numprocs, processor_name);\n    fflush(stdout);\n\n    n = 10000;                  \n\n    if (myid == 0)\n        startwtime = MPI_Wtime();\n\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    h   = 1.0 / (double) n;\n    sum = 0.0;\n    \n\n    for (i = myid + 1; i <= n; i += numprocs)\n    {\n        x = h * ((double)i - 0.5);\n        sum += f(x);\n    }\n    mypi = h * sum;\n\n    MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (myid == 0) {\n        endwtime = MPI_Wtime();\n        printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n               pi, fabs(pi - PI25DT));\n        printf(\"wall clock time = %f\\n\", endwtime-startwtime);         \n        fflush(stdout);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "nschloe_353", "code": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p, i;\n  char message1[15];\n  char message2[15];\n  int source, dest, tag; \n  MPI_Status status;\n\n\n  for(i = 0 ; i < 10 ; i++) {\n      message1[i] = i;\n  }\n  for(i=0; i<10; i++) {\n     printf(\"%d \", message1[i]);\n  }\n  printf(\"\\n--\\n\");\n  source = tag = dest = 0;\n\n  for(i=0; i<10; i++)\n  {\n    printf(\"%d \", message2[i]);\n  }\n  return 0;\n}", "label": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p, i;\n  char message1[15];\n  char message2[15];\n  int source, dest, tag; \n  MPI_Status status;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  for(i = 0 ; i < 10 ; i++) {\n      message1[i] = i;\n  }\n  for(i=0; i<10; i++) {\n     printf(\"%d \", message1[i]);\n  }\n  printf(\"\\n--\\n\");\n  source = tag = dest = 0;\n  MPI_Send(message1, 15, MPI_CHAR, dest, tag, MPI_COMM_WORLD);\n  MPI_Recv(message2, 15, MPI_CHAR, source, tag, MPI_COMM_WORLD, &status);\n\n  for(i=0; i<10; i++)\n  {\n    printf(\"%d \", message2[i]);\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu3ra_355", "code": "int main( int argc, char ** argv ) {\n    MPI_Comm tmp, comm, startComm;\n    char * fname;\n    char * actualFname = NULL;\n    char * globalFname = NULL;\n    int totalSize, expectedRank, size, cachedRank;\n    char portName[MPI_MAX_PORT_NAME];\n    int rankToAccept = 1;\n\n    \n\n    msg( \"MPICH library taken from: %s\\n\", MPICHLIBSTR );\n\n    if( argc != 4 ) {\n        printf( \"Usage: %s <fname> <totalSize> <idx-1-based>\\n\", argv[0] );\n        exit( 1 );\n    }\n\n    \n\n    fname = argv[1];\n    \n\n    totalSize = atoi( argv[2] );\n    \n\n    expectedRank = atoi( argv[3] )-1;\n\n    \n\n    startWatchdog( 120 );\n\n    \n\n    msg( \"Waiting for: %d - my rank is %d\\n\", totalSize, expectedRank );\n\n    \n\n\n    \n\n\n    \n    if( expectedRank == 0 ) {\n        \n\n        \n        \n\n        actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n\n        \n\n        globalFname = writePortToFile( portName, fname );\n        installExitHandler( globalFname );\n\n        comm = startComm;\n    } else {\n        char * readPort;\n        readPort = getPortFromFile( \"%s.%d\", fname, expectedRank );\n        strncpy( portName, readPort, MPI_MAX_PORT_NAME );\n        free( readPort );\n        msg( \"Read port <%s>\\n\", portName );\n        \n        comm = tmp;\n        msg( \"After my first merge, size is now: %d\\n\", size );\n    }\n    while( size < totalSize ) {\n        \n\n        strokeWatchdog();\n\n        \n\n\n        \n\n\n        \n\n\n        \n\n\n        if( expectedRank == 0 ) {\n            msg( \"Up to size: %d\\n\", size );\n\n            \n\n            unlink( actualFname );\n            free( actualFname );\n\n            \n\n            actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n        }\n    }\n\n    msg( \"All done - I got rank: %d.\\n\", cachedRank );\n\n\n    if( expectedRank == 0 ) {\n\n        \n\n        sleep( 4 );\n        unlink( actualFname );\n        free( actualFname );\n        unlink( globalFname );\n        free( globalFname );\n\n        \n\n        indicateConnectSucceeded();\n    }\n\n    return 0;\n}", "label": "int main( int argc, char ** argv ) {\n    MPI_Comm tmp, comm, startComm;\n    char * fname;\n    char * actualFname = NULL;\n    char * globalFname = NULL;\n    int totalSize, expectedRank, size, cachedRank;\n    char portName[MPI_MAX_PORT_NAME];\n    int rankToAccept = 1;\n\n    \n\n    msg( \"MPICH library taken from: %s\\n\", MPICHLIBSTR );\n\n    if( argc != 4 ) {\n        printf( \"Usage: %s <fname> <totalSize> <idx-1-based>\\n\", argv[0] );\n        exit( 1 );\n    }\n\n    \n\n    fname = argv[1];\n    \n\n    totalSize = atoi( argv[2] );\n    \n\n    expectedRank = atoi( argv[3] )-1;\n\n    \n\n    startWatchdog( 120 );\n\n    \n\n    msg( \"Waiting for: %d - my rank is %d\\n\", totalSize, expectedRank );\n\n    \n\n    MPI_Init( 0, 0 );\n\n    \n\n    MPI_Comm_dup( MPI_COMM_SELF, &startComm );\n\n    \n    if( expectedRank == 0 ) {\n        \n\n        MPI_Open_port( MPI_INFO_NULL, portName );\n        \n        \n\n        actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n\n        \n\n        globalFname = writePortToFile( portName, fname );\n        installExitHandler( globalFname );\n\n        comm = startComm;\n    } else {\n        char * readPort;\n        readPort = getPortFromFile( \"%s.%d\", fname, expectedRank );\n        strncpy( portName, readPort, MPI_MAX_PORT_NAME );\n        free( readPort );\n        msg( \"Read port <%s>\\n\", portName );\n        \n        MPI_Comm_connect( portName, MPI_INFO_NULL, 0, startComm, &comm );\n        MPI_Intercomm_merge( comm, 1, &tmp );\n        comm = tmp;\n        MPI_Comm_size( comm, &size );\n        msg( \"After my first merge, size is now: %d\\n\", size );\n    }\n    while( size < totalSize ) {\n        \n\n        strokeWatchdog();\n\n        \n\n        MPI_Comm_accept( portName, MPI_INFO_NULL, 0, comm, &tmp );\n\n        \n\n        MPI_Intercomm_merge( tmp, 0, &comm );\n\n        \n\n        MPI_Comm_free( &tmp );\n\n        \n\n        MPI_Comm_rank( comm, &cachedRank );\n        MPI_Comm_size( comm, &size );\n\n        if( expectedRank == 0 ) {\n            msg( \"Up to size: %d\\n\", size );\n\n            \n\n            unlink( actualFname );\n            free( actualFname );\n\n            \n\n            actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n        }\n    }\n    MPI_Comm_rank( comm, &cachedRank );\n\n    msg( \"All done - I got rank: %d.\\n\", cachedRank );\n\n    MPI_Barrier( comm );\n\n    if( expectedRank == 0 ) {\n\n        \n\n        sleep( 4 );\n        unlink( actualFname );\n        free( actualFname );\n        unlink( globalFname );\n        free( globalFname );\n\n        \n\n        indicateConnectSucceeded();\n    }\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ShadenSmith_356", "code": "int main(\n  int argc,\n  char **argv)\n{\n  setvbuf(stdout, NULL, _IOLBF, 0);\n\n  int rank = 0;\n#ifdef SPLATT_USE_MPI\n\n  int size;\n#endif\n\n  srand(time(NULL) * (rank+1));\n\n  \n\n  init_timers();\n  timer_start(&timers[TIMER_ALL]);\n\n  \n\n  cmd_struct args;\n  int nargs = argc > 1 ? 2 : 1;\n  argp_parse(&cmd_argp, nargs, argv, ARGP_IN_ORDER, 0, &args);\n\n  \n\n  int ret = args.func(argc-1, argv+1);\n\n#ifdef SPLATT_USE_MPI\n  \n\n#endif\n\n  timer_stop(&timers[TIMER_ALL]);\n  if(rank == 0) {\n    report_times();\n    printf(\"****************************************************************\\n\");\n  }\n\n#ifdef SPLATT_USE_MPI\n#endif\n\n  return ret;\n}", "label": "int main(\n  int argc,\n  char **argv)\n{\n  setvbuf(stdout, NULL, _IOLBF, 0);\n\n  int rank = 0;\n#ifdef SPLATT_USE_MPI\n  MPI_Init(&argc, &argv);\n\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n#endif\n\n  srand(time(NULL) * (rank+1));\n\n  \n\n  init_timers();\n  timer_start(&timers[TIMER_ALL]);\n\n  \n\n  cmd_struct args;\n  int nargs = argc > 1 ? 2 : 1;\n  argp_parse(&cmd_argp, nargs, argv, ARGP_IN_ORDER, 0, &args);\n\n  \n\n  int ret = args.func(argc-1, argv+1);\n\n#ifdef SPLATT_USE_MPI\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n#endif\n\n  timer_stop(&timers[TIMER_ALL]);\n  if(rank == 0) {\n    report_times();\n    printf(\"****************************************************************\\n\");\n  }\n\n#ifdef SPLATT_USE_MPI\n  MPI_Finalize();\n#endif\n\n  return ret;\n}"}
{"program": "mkurnosov_358", "code": "nt main(int argc, char **argv)\n{    \n\n    \n\n    double tserial = 0;\n    if (get_comm_rank() == 0)\n        tserial = run_serial();\n\n    \n\n    double tparallel = run_parallel();\n        \n    if (get_comm_rank() == 0) {\n        printf(\"Count prime numbers on [%d, %d]\\n\", a, b);\n        printf(\"Execution time (serial): %.6f\\n\", tserial);\n        printf(\"Execution time (parallel): %.6f\\n\", tparallel);\n        printf(\"Speedup (processes %d): %.2f\\n\", get_comm_size(), tserial / tparallel);\n    }\n    \n    return 0;\n}\n\n", "label": "nt main(int argc, char **argv)\n{    \n    MPI_Init(&argc, &argv);\n\n    \n\n    double tserial = 0;\n    if (get_comm_rank() == 0)\n        tserial = run_serial();\n\n    \n\n    double tparallel = run_parallel();\n        \n    if (get_comm_rank() == 0) {\n        printf(\"Count prime numbers on [%d, %d]\\n\", a, b);\n        printf(\"Execution time (serial): %.6f\\n\", tserial);\n        printf(\"Execution time (parallel): %.6f\\n\", tparallel);\n        printf(\"Speedup (processes %d): %.2f\\n\", get_comm_size(), tserial / tparallel);\n    }\n    \n    MPI_Finalize();\n    return 0;\n}\n\n"}
{"program": "joao-lima_359", "code": "int main(int argc, char **argv)\n{\n\tint size, n, x=789;\n\tint color;\n\tMPI_Comm newcomm;\n\tint rank, newrank;\n\tint ret;\n\tstarpu_data_handle_t data[2];\n\n\n        if (size < 4)\n        {\n\t\tFPRINTF(stderr, \"We need at least 4 processes.\\n\");\n                return STARPU_TEST_SKIPPED;\n        }\n\n\tcolor = rank%2;\n\tFPRINTF(stderr, \"[%d][%d] color %d\\n\", rank, newrank, color);\n\n\tif (newrank == 0)\n\t{\n\t\tFPRINTF(stderr, \"[%d][%d] sending %d\\n\", rank, newrank, rank);\n\t}\n\telse if (newrank == 1)\n\t{\n\t\tFPRINTF(stderr, \"[%d][%d] received %d\\n\", rank, newrank, x);\n\t}\n\n        ret = starpu_init(NULL);\n        STARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n        ret = starpu_mpi_init_comm(NULL, NULL, 0, newcomm);\n        STARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tif (newrank == 0)\n\t{\n\t\tstarpu_variable_data_register(&data[0], STARPU_MAIN_RAM, (uintptr_t)&rank, sizeof(int));\n\t\tstarpu_variable_data_register(&data[1], STARPU_MAIN_RAM, (uintptr_t)&rank, sizeof(int));\n\t\tstarpu_mpi_data_register_comm(data[1], 22, 0, newcomm);\n\t}\n\telse\n\t\tstarpu_variable_data_register(&data[0], -1, (uintptr_t)NULL, sizeof(int));\n\tstarpu_mpi_data_register_comm(data[0], 12, 0, newcomm);\n\n\tif (newrank == 0)\n\t{\n\t\tstarpu_mpi_req req[2];\n\t\tstarpu_mpi_issend(data[1], &req[0], 1, 22, newcomm);\n\t\tstarpu_mpi_isend(data[0], &req[1], 1, 12, newcomm);\n\t\tstarpu_mpi_wait(&req[0], MPI_STATUS_IGNORE);\n\t\tstarpu_mpi_wait(&req[1], MPI_STATUS_IGNORE);\n\t}\n\telse if (newrank == 1)\n\t{\n\t\tint *xx;\n\n\t\tstarpu_mpi_recv(data[0], 0, 12, newcomm, MPI_STATUS_IGNORE);\n\t\tstarpu_data_acquire(data[0], STARPU_RW);\n\t\txx = (int *)starpu_variable_get_local_ptr(data[0]);\n\t\tstarpu_data_release(data[0]);\n\t\tFPRINTF(stderr, \"[%d][%d] received %d\\n\", rank, newrank, *xx);\n\t\tSTARPU_ASSERT_MSG(x==*xx, \"Received value %d is incorrect (should be %d)\\n\", *xx, x);\n\n\t\tstarpu_variable_data_register(&data[1], -1, (uintptr_t)NULL, sizeof(int));\n\t\tstarpu_mpi_data_register_comm(data[1], 22, 0, newcomm);\n\t\tstarpu_mpi_recv(data[0], 0, 22, newcomm, MPI_STATUS_IGNORE);\n\t\tstarpu_data_acquire(data[0], STARPU_RW);\n\t\txx = (int *)starpu_variable_get_local_ptr(data[0]);\n\t\tstarpu_data_release(data[0]);\n\t\tFPRINTF(stderr, \"[%d][%d] received %d\\n\", rank, newrank, *xx);\n\t\tSTARPU_ASSERT_MSG(x==*xx, \"Received value %d is incorrect (should be %d)\\n\", *xx, x);\n\t}\n\n\tif (newrank == 0 || newrank == 1)\n\t{\n\t\tstarpu_mpi_insert_task(newcomm, &mycodelet,\n\t\t\t\t       STARPU_RW, data[0],\n\t\t\t\t       STARPU_VALUE, &x, sizeof(x),\n\t\t\t\t       STARPU_EXECUTE_ON_NODE, 1,\n\t\t\t\t       0);\n\n\t\tstarpu_task_wait_for_all();\n\t\tstarpu_data_unregister(data[0]);\n\t\tstarpu_data_unregister(data[1]);\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tint size, n, x=789;\n\tint color;\n\tMPI_Comm newcomm;\n\tint rank, newrank;\n\tint ret;\n\tstarpu_data_handle_t data[2];\n\n        MPI_Init(&argc, &argv);\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        if (size < 4)\n        {\n\t\tFPRINTF(stderr, \"We need at least 4 processes.\\n\");\n                MPI_Finalize();\n                return STARPU_TEST_SKIPPED;\n        }\n\n\tcolor = rank%2;\n\tMPI_Comm_split(MPI_COMM_WORLD, color, rank, &newcomm);\n\tMPI_Comm_rank(newcomm, &newrank);\n\tFPRINTF(stderr, \"[%d][%d] color %d\\n\", rank, newrank, color);\n\n\tif (newrank == 0)\n\t{\n\t\tFPRINTF(stderr, \"[%d][%d] sending %d\\n\", rank, newrank, rank);\n\t\tMPI_Send(&rank, 1, MPI_INT, 1, 10, newcomm);\n\t}\n\telse if (newrank == 1)\n\t{\n\t\tMPI_Recv(&x, 1, MPI_INT, 0, 10, newcomm, MPI_STATUS_IGNORE);\n\t\tFPRINTF(stderr, \"[%d][%d] received %d\\n\", rank, newrank, x);\n\t}\n\n        ret = starpu_init(NULL);\n        STARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n        ret = starpu_mpi_init_comm(NULL, NULL, 0, newcomm);\n        STARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tif (newrank == 0)\n\t{\n\t\tstarpu_variable_data_register(&data[0], STARPU_MAIN_RAM, (uintptr_t)&rank, sizeof(int));\n\t\tstarpu_variable_data_register(&data[1], STARPU_MAIN_RAM, (uintptr_t)&rank, sizeof(int));\n\t\tstarpu_mpi_data_register_comm(data[1], 22, 0, newcomm);\n\t}\n\telse\n\t\tstarpu_variable_data_register(&data[0], -1, (uintptr_t)NULL, sizeof(int));\n\tstarpu_mpi_data_register_comm(data[0], 12, 0, newcomm);\n\n\tif (newrank == 0)\n\t{\n\t\tstarpu_mpi_req req[2];\n\t\tstarpu_mpi_issend(data[1], &req[0], 1, 22, newcomm);\n\t\tstarpu_mpi_isend(data[0], &req[1], 1, 12, newcomm);\n\t\tstarpu_mpi_wait(&req[0], MPI_STATUS_IGNORE);\n\t\tstarpu_mpi_wait(&req[1], MPI_STATUS_IGNORE);\n\t}\n\telse if (newrank == 1)\n\t{\n\t\tint *xx;\n\n\t\tstarpu_mpi_recv(data[0], 0, 12, newcomm, MPI_STATUS_IGNORE);\n\t\tstarpu_data_acquire(data[0], STARPU_RW);\n\t\txx = (int *)starpu_variable_get_local_ptr(data[0]);\n\t\tstarpu_data_release(data[0]);\n\t\tFPRINTF(stderr, \"[%d][%d] received %d\\n\", rank, newrank, *xx);\n\t\tSTARPU_ASSERT_MSG(x==*xx, \"Received value %d is incorrect (should be %d)\\n\", *xx, x);\n\n\t\tstarpu_variable_data_register(&data[1], -1, (uintptr_t)NULL, sizeof(int));\n\t\tstarpu_mpi_data_register_comm(data[1], 22, 0, newcomm);\n\t\tstarpu_mpi_recv(data[0], 0, 22, newcomm, MPI_STATUS_IGNORE);\n\t\tstarpu_data_acquire(data[0], STARPU_RW);\n\t\txx = (int *)starpu_variable_get_local_ptr(data[0]);\n\t\tstarpu_data_release(data[0]);\n\t\tFPRINTF(stderr, \"[%d][%d] received %d\\n\", rank, newrank, *xx);\n\t\tSTARPU_ASSERT_MSG(x==*xx, \"Received value %d is incorrect (should be %d)\\n\", *xx, x);\n\t}\n\n\tif (newrank == 0 || newrank == 1)\n\t{\n\t\tstarpu_mpi_insert_task(newcomm, &mycodelet,\n\t\t\t\t       STARPU_RW, data[0],\n\t\t\t\t       STARPU_VALUE, &x, sizeof(x),\n\t\t\t\t       STARPU_EXECUTE_ON_NODE, 1,\n\t\t\t\t       0);\n\n\t\tstarpu_task_wait_for_all();\n\t\tstarpu_data_unregister(data[0]);\n\t\tstarpu_data_unregister(data[1]);\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\tMPI_Comm_free(&newcomm);\n        MPI_Finalize();\n\treturn 0;\n}"}
{"program": "peoronoob_361", "code": "int main( int argc, char **argv )\n{\n\tchar *mem, *mem_recv;;\n\tlong size, count;\n\tlong i;\n\t\n\t\n\tif( argc != 3 ) {\n\t\tTESTS_ERROR( 1, \"Usage: %s total_memory num_sends\", argv[0] );\n\t}\n\tsize = atoi( argv[1] );\n\tcount = atoi( argv[2] );\n\tif( size <= 0 ) {\n\t\tTESTS_ERROR( 1, \"total_memory must be positive\" );\n\t}\n\tif( count <= 0 || size%count != 0 ) {\n\t\tTESTS_ERROR( 1, \"num_sends must be positive and integer divisor of size\" );\n\t}\n\t\n\tif( GET_N() < 2 ) {\n\t\tTESTS_ERROR( 1, \"you must use 2 processes\" );\n\t}\n\t\n\t\n\n\tif( GET_ID() == 0 ) {\n\t\tTESTS_MPI_SEND( &size, 1, MPI_LONG, 1 );\n\t\tTESTS_MPI_SEND( &count, 1, MPI_LONG, 1 );\n\t}\n\telse if( GET_ID() == 1 ) {\n\t\tTESTS_MPI_RECEIVE( &size, 1, MPI_LONG, 0 );\n\t\tTESTS_MPI_RECEIVE( &count, 1, MPI_LONG, 0 );\n\t}\n\t\n\t\n\n\tmem = (char*) malloc( size );\n\tif( ! mem ) {\n\t\tTESTS_ERROR( 1, \"malloc(%ld) failed\", size );\n\t}\n\tmem_recv = (char*) malloc( size );\n\tif( ! mem_recv ) {\n\t\tTESTS_ERROR( 1, \"malloc(%ld) failed\", size );\n\t}\n\t\n\t\n\tlong blockSize = size / count;\n\tlong block_disp = ( blockSize / GET_N() ); \n\n\tlong current = 0;\n\tTime start, end;\n\tTimeDiff res;\n\t\n\t\n\n\t{\n\t\tif( GET_ID() == 0 ) {\n\t\t\t\n\n\t\t\tstart = now( );\n\t\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\t\tTESTS_MPI_SEND( mem + current, blockSize, MPI_CHAR, 1 );\n\t\t\t\tcurrent += blockSize;\n\t\t\t}\n\t\t\tend = now( );\n\t\t\tres = timeDiff( start, end );\n\t\t\t\n\n\t\t\t\n\n\t\t\tprintf ( \"%ld.%.3ld\\t\", res.time, res.mtime );\n\t\t}\n\t\telse if( GET_ID() == 1 ) {\n\t\t\t\n\n\t\t\tstart = now( );\n\t\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\t\tTESTS_MPI_RECEIVE( mem + current, blockSize, MPI_CHAR, 0 );\n\t\t\t\tcurrent += blockSize;\n\t\t\t}\n\t\t\tend = now( );\n\t\t\tres = timeDiff( start, end );\n\t\t\t\n\n\t\t}\n\t}\n\t\n\t\n\t\n\n\t{\t\n\t\t\n\n\t\t\t\n\n\t\t\n\t\tSPD_ASSERT ( blockSize % GET_N() == 0, \"Scattered block must be of equal size, now blockSize mod N = %ld\", blockSize % GET_N() );\n\t\t\n\t\tcurrent = 0;\n\t\t\n\t\tstart = now( );\n\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\tTESTS_MPI_SCATTER( mem + current, block_disp, MPI_CHAR, mem_recv + i*block_disp, 0 );\n\t\t\tcurrent += blockSize;\n\t\t}\n\t\tend = now( );\n\t\tres = timeDiff( start, end );\n\t\t\n\t\tif ( GET_ID() == 0 )\n\t\t\tprintf ( \"%ld.%.3ld\\t\", res.time, res.mtime );\n\t}\n\t\n\t\n\n\t{\t\n\t\t\n\n\t\t\t\n\n\t\t\n\t\tSPD_ASSERT ( blockSize % GET_N() == 0, \"Gathered block must be of equal size, now blockSize mod N = %ld\", blockSize % GET_N() );\n\t\t\n\t\tcurrent = 0;\n\t\t\n\t\tstart = now( );\n\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\tTESTS_MPI_GATHER( mem + current, size, block_disp, MPI_CHAR, mem_recv + i*block_disp, 0 );\n\t\t\tcurrent += blockSize;\n\t\t}\n\t\tend = now( );\n\t\tres = timeDiff( start, end );\n\t\t\n\t\tif ( GET_ID() == 0 )\n\t\t\tprintf ( \"%ld.%.3ld\\t\", res.time, res.mtime );\n\t}\n\t\n\t\n\n\t{\n\t\t\n\n\t\t\t\n\n\t\t\n\t\tSPD_ASSERT ( blockSize % GET_N() == 0, \"Scattered block must be of equal size, now blockSize mod N = %ld\", blockSize % GET_N() );\n\t\t\n\t\tcurrent = 0;\n\t\t\n\t\tstart = now( );\n\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\tTESTS_MPI_ALLTOALL( mem + current, block_disp, MPI_CHAR, mem_recv + current );\n\t\t\tcurrent += blockSize;\n\t\t}\n\t\tend = now( );\n\t\tres = timeDiff( start, end );\n\t\t\n\t\tif ( GET_ID() == 0 )\n\t\t\tprintf ( \"%ld.%.3ld\\n\", res.time, res.mtime );\n\t}\n\t\n\tfree ( mem );\n\tfree ( mem_recv );\n\t\n\treturn 0;\n}", "label": "int main( int argc, char **argv )\n{\n\tchar *mem, *mem_recv;;\n\tlong size, count;\n\tlong i;\n\t\n\tMPI_Init( &argc, &argv );\n\t\n\tif( argc != 3 ) {\n\t\tTESTS_ERROR( 1, \"Usage: %s total_memory num_sends\", argv[0] );\n\t}\n\tsize = atoi( argv[1] );\n\tcount = atoi( argv[2] );\n\tif( size <= 0 ) {\n\t\tTESTS_ERROR( 1, \"total_memory must be positive\" );\n\t}\n\tif( count <= 0 || size%count != 0 ) {\n\t\tTESTS_ERROR( 1, \"num_sends must be positive and integer divisor of size\" );\n\t}\n\t\n\tif( GET_N() < 2 ) {\n\t\tTESTS_ERROR( 1, \"you must use 2 processes\" );\n\t}\n\t\n\t\n\n\tif( GET_ID() == 0 ) {\n\t\tTESTS_MPI_SEND( &size, 1, MPI_LONG, 1 );\n\t\tTESTS_MPI_SEND( &count, 1, MPI_LONG, 1 );\n\t}\n\telse if( GET_ID() == 1 ) {\n\t\tTESTS_MPI_RECEIVE( &size, 1, MPI_LONG, 0 );\n\t\tTESTS_MPI_RECEIVE( &count, 1, MPI_LONG, 0 );\n\t}\n\t\n\t\n\n\tmem = (char*) malloc( size );\n\tif( ! mem ) {\n\t\tTESTS_ERROR( 1, \"malloc(%ld) failed\", size );\n\t}\n\tmem_recv = (char*) malloc( size );\n\tif( ! mem_recv ) {\n\t\tTESTS_ERROR( 1, \"malloc(%ld) failed\", size );\n\t}\n\t\n\t\n\tlong blockSize = size / count;\n\tlong block_disp = ( blockSize / GET_N() ); \n\n\tlong current = 0;\n\tTime start, end;\n\tTimeDiff res;\n\t\n\t\n\n\t{\n\t\tif( GET_ID() == 0 ) {\n\t\t\t\n\n\t\t\tstart = now( );\n\t\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\t\tTESTS_MPI_SEND( mem + current, blockSize, MPI_CHAR, 1 );\n\t\t\t\tcurrent += blockSize;\n\t\t\t}\n\t\t\tend = now( );\n\t\t\tres = timeDiff( start, end );\n\t\t\t\n\n\t\t\t\n\n\t\t\tprintf ( \"%ld.%.3ld\\t\", res.time, res.mtime );\n\t\t}\n\t\telse if( GET_ID() == 1 ) {\n\t\t\t\n\n\t\t\tstart = now( );\n\t\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\t\tTESTS_MPI_RECEIVE( mem + current, blockSize, MPI_CHAR, 0 );\n\t\t\t\tcurrent += blockSize;\n\t\t\t}\n\t\t\tend = now( );\n\t\t\tres = timeDiff( start, end );\n\t\t\t\n\n\t\t}\n\t}\n\t\n\t\n\t\n\n\t{\t\n\t\t\n\n\t\t\t\n\n\t\t\n\t\tSPD_ASSERT ( blockSize % GET_N() == 0, \"Scattered block must be of equal size, now blockSize mod N = %ld\", blockSize % GET_N() );\n\t\t\n\t\tcurrent = 0;\n\t\t\n\t\tstart = now( );\n\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\tTESTS_MPI_SCATTER( mem + current, block_disp, MPI_CHAR, mem_recv + i*block_disp, 0 );\n\t\t\tcurrent += blockSize;\n\t\t}\n\t\tend = now( );\n\t\tres = timeDiff( start, end );\n\t\t\n\t\tif ( GET_ID() == 0 )\n\t\t\tprintf ( \"%ld.%.3ld\\t\", res.time, res.mtime );\n\t}\n\t\n\t\n\n\t{\t\n\t\t\n\n\t\t\t\n\n\t\t\n\t\tSPD_ASSERT ( blockSize % GET_N() == 0, \"Gathered block must be of equal size, now blockSize mod N = %ld\", blockSize % GET_N() );\n\t\t\n\t\tcurrent = 0;\n\t\t\n\t\tstart = now( );\n\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\tTESTS_MPI_GATHER( mem + current, size, block_disp, MPI_CHAR, mem_recv + i*block_disp, 0 );\n\t\t\tcurrent += blockSize;\n\t\t}\n\t\tend = now( );\n\t\tres = timeDiff( start, end );\n\t\t\n\t\tif ( GET_ID() == 0 )\n\t\t\tprintf ( \"%ld.%.3ld\\t\", res.time, res.mtime );\n\t}\n\t\n\t\n\n\t{\n\t\t\n\n\t\t\t\n\n\t\t\n\t\tSPD_ASSERT ( blockSize % GET_N() == 0, \"Scattered block must be of equal size, now blockSize mod N = %ld\", blockSize % GET_N() );\n\t\t\n\t\tcurrent = 0;\n\t\t\n\t\tstart = now( );\n\t\tfor( i = 0; i < count; ++ i ) {\n\t\t\tTESTS_MPI_ALLTOALL( mem + current, block_disp, MPI_CHAR, mem_recv + current );\n\t\t\tcurrent += blockSize;\n\t\t}\n\t\tend = now( );\n\t\tres = timeDiff( start, end );\n\t\t\n\t\tif ( GET_ID() == 0 )\n\t\t\tprintf ( \"%ld.%.3ld\\n\", res.time, res.mtime );\n\t}\n\t\n\tfree ( mem );\n\tfree ( mem_recv );\n\t\n\tMPI_Finalize( );\n\treturn 0;\n}"}
{"program": "germasch_362", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  mrc_class_register_subclass(&mrc_class_mrc_ts_monitor,\n\t\t\t      &mrc_ts_monitor_output_phi_ops);\n\n  struct rmhd *rmhd = rmhd_create(MPI_COMM_WORLD);\n  rmhd_set_from_options(rmhd);\n  rmhd_setup(rmhd);\n  rmhd_view(rmhd);\n\n  \n\n  struct mrc_crds *crds = mrc_domain_get_crds(rmhd->domain);\n  struct mrc_fld *By0 = rmhd->By0;\n  struct mrc_fld *x = rmhd_get_fld(rmhd, NR_FLDS, \"x\");\n  mrc_fld_set_comp_name(x, OM_I, \"om_i\");\n  mrc_fld_set_comp_name(x, PSI_R, \"psi_r\");\n  mrc_fld_set_comp_name(x, BZ_I, \"bz_i\");\n  mrc_fld_set_comp_name(x, VZ_R, \"vz_r\");\n\n  \n\n  mrc_f1_foreach(x, ix, 1, 1) {\n    float By;\n    float xx = CRDX(ix);\n    float lambda = rmhd->lambda;\n#if 0\n    By = tanh(lambda * xx);\n#else\n    const float x0 = .92 / lambda, alpha = 1.85;\n    if (xx < -x0) {\n      By = -1.;\n    } else if (xx > x0) {\n      By = 1.;\n    } else {\n      By = alpha * exp(-sqr(xx*lambda)) * sqrt(M_PI) / 2. * mrc_erfi(xx*lambda);\n    }\n#endif\n    MRC_F1(By0, 0, ix) = By;\n    MRC_F1(x, PSI_R, ix) = exp(-sqr(xx));\n  } mrc_f1_foreach_end;\n\n  \n\n  mrc_fld_dump(rmhd->By0, \"By0\", 0);\n\n  \n\n  int gdims[3];\n  mrc_domain_get_global_dims(rmhd->domain, gdims);\n  float dx = rmhd->Lx / gdims[0]; \n\n  float dt = rmhd->cfl * fminf(dx, rmhd->S * sqr(dx));\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, rmhd_diag, rmhd);\n\n  struct mrc_ts_monitor *mon_output_phi =\n    mrc_ts_monitor_create(mrc_ts_comm(ts));\n  mrc_ts_monitor_set_type(mon_output_phi, \"output_phi\");\n  mrc_ts_monitor_set_name(mon_output_phi, \"mrc_ts_output_phi\");\n  mrc_ts_add_monitor(ts, mon_output_phi);\n\n  mrc_ts_set_context(ts, rmhd_to_mrc_obj(rmhd));\n\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, rmhd_calc_rhs, rmhd);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_set_dt(ts, dt);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  rmhd_destroy(rmhd);\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  mrc_class_register_subclass(&mrc_class_mrc_ts_monitor,\n\t\t\t      &mrc_ts_monitor_output_phi_ops);\n\n  struct rmhd *rmhd = rmhd_create(MPI_COMM_WORLD);\n  rmhd_set_from_options(rmhd);\n  rmhd_setup(rmhd);\n  rmhd_view(rmhd);\n\n  \n\n  struct mrc_crds *crds = mrc_domain_get_crds(rmhd->domain);\n  struct mrc_fld *By0 = rmhd->By0;\n  struct mrc_fld *x = rmhd_get_fld(rmhd, NR_FLDS, \"x\");\n  mrc_fld_set_comp_name(x, OM_I, \"om_i\");\n  mrc_fld_set_comp_name(x, PSI_R, \"psi_r\");\n  mrc_fld_set_comp_name(x, BZ_I, \"bz_i\");\n  mrc_fld_set_comp_name(x, VZ_R, \"vz_r\");\n\n  \n\n  mrc_f1_foreach(x, ix, 1, 1) {\n    float By;\n    float xx = CRDX(ix);\n    float lambda = rmhd->lambda;\n#if 0\n    By = tanh(lambda * xx);\n#else\n    const float x0 = .92 / lambda, alpha = 1.85;\n    if (xx < -x0) {\n      By = -1.;\n    } else if (xx > x0) {\n      By = 1.;\n    } else {\n      By = alpha * exp(-sqr(xx*lambda)) * sqrt(M_PI) / 2. * mrc_erfi(xx*lambda);\n    }\n#endif\n    MRC_F1(By0, 0, ix) = By;\n    MRC_F1(x, PSI_R, ix) = exp(-sqr(xx));\n  } mrc_f1_foreach_end;\n\n  \n\n  mrc_fld_dump(rmhd->By0, \"By0\", 0);\n\n  \n\n  int gdims[3];\n  mrc_domain_get_global_dims(rmhd->domain, gdims);\n  float dx = rmhd->Lx / gdims[0]; \n\n  float dt = rmhd->cfl * fminf(dx, rmhd->S * sqr(dx));\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, rmhd_diag, rmhd);\n\n  struct mrc_ts_monitor *mon_output_phi =\n    mrc_ts_monitor_create(mrc_ts_comm(ts));\n  mrc_ts_monitor_set_type(mon_output_phi, \"output_phi\");\n  mrc_ts_monitor_set_name(mon_output_phi, \"mrc_ts_output_phi\");\n  mrc_ts_add_monitor(ts, mon_output_phi);\n\n  mrc_ts_set_context(ts, rmhd_to_mrc_obj(rmhd));\n\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, rmhd_calc_rhs, rmhd);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_set_dt(ts, dt);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  rmhd_destroy(rmhd);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "radarsat1_364", "code": "int main()\n{\n#ifdef SICONOS_HAS_MPI\n#endif\n  int info = add_test();\n\n  info += test_CSparseMatrix_alloc();\n  info += test_CSparseMatrix_spsolve();\n  printf(\"info : %i\\n\", info);\n  info += test_CSparseMatrix_chol_spsolve();\n  info += test_CSparseMatrix_ldlt_solve();\n  printf(\"info : %i\\n\", info);\n\n#ifdef SICONOS_HAS_MPI\n#endif\n  return info;\n}", "label": "int main()\n{\n#ifdef SICONOS_HAS_MPI\n  MPI_Init(NULL, NULL);\n#endif\n  int info = add_test();\n\n  info += test_CSparseMatrix_alloc();\n  info += test_CSparseMatrix_spsolve();\n  printf(\"info : %i\\n\", info);\n  info += test_CSparseMatrix_chol_spsolve();\n  info += test_CSparseMatrix_ldlt_solve();\n  printf(\"info : %i\\n\", info);\n\n#ifdef SICONOS_HAS_MPI\n  MPI_Finalize();\n#endif\n  return info;\n}"}
{"program": "bmi-forum_366", "code": "int main(int argc, char *argv[])\n{\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tStream*\t\t\tstream;\n\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\n\tRegressionTest_Init( \"Base/Foundation/PrimitiveObject\" );\n\n\tstream = Journal_Register( \"info\", \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\n\t{\n\t\tStg_ObjectList* list;\n\n\t\tlist = Stg_ObjectList_New();\n\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedChar( 'a', \"char item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedShort( 123, \"short item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedInt( 456, \"int item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedLong( 789, \"long item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Char( 'a', \"char item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Short( -123, \"short item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Int( -456, \"int item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Long( -789, \"long item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Float( 1.2f, \"float item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Double( 2.4, \"double item\" ) );\n\n\t\tStg_ObjectList_PrintAllObjects( list, stream );\n\n\t\tStg_Class_Delete( list );\n\t}\n\t\n\tRegressionTest_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main(int argc, char *argv[])\n{\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tStream*\t\t\tstream;\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tBaseFoundation_Init( &argc, &argv );\n\n\tRegressionTest_Init( \"Base/Foundation/PrimitiveObject\" );\n\n\tstream = Journal_Register( \"info\", \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\n\t{\n\t\tStg_ObjectList* list;\n\n\t\tlist = Stg_ObjectList_New();\n\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedChar( 'a', \"char item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedShort( 123, \"short item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedInt( 456, \"int item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_UnsignedLong( 789, \"long item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Char( 'a', \"char item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Short( -123, \"short item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Int( -456, \"int item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Long( -789, \"long item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Float( 1.2f, \"float item\" ) );\n\t\tStg_ObjectList_Append( list, Stg_PrimitiveObject_New_Double( 2.4, \"double item\" ) );\n\n\t\tStg_ObjectList_PrintAllObjects( list, stream );\n\n\t\tStg_Class_Delete( list );\n\t}\n\t\n\tRegressionTest_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "bmi-forum_367", "code": "int main( int argc, char* argv[] ) {\n\tXML_IO_Handler* io_handler;\n\tDictionary* dictionary;\n\tStream* stream;\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\tio_handler = XML_IO_Handler_New();\n\tdictionary = Dictionary_New();\n\n\tIO_Handler_ReadAllFilesFromCommandLine( io_handler, argc, argv, dictionary );\n\tStg_Class_Print( dictionary, stream );\n\t\n\tStg_Class_Delete( io_handler );\n\tStg_Class_Delete( dictionary );\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tXML_IO_Handler* io_handler;\n\tDictionary* dictionary;\n\tStream* stream;\n\t\n\tMPI_Init( &argc, &argv );\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\tio_handler = XML_IO_Handler_New();\n\tdictionary = Dictionary_New();\n\n\tIO_Handler_ReadAllFilesFromCommandLine( io_handler, argc, argv, dictionary );\n\tStg_Class_Print( dictionary, stream );\n\t\n\tStg_Class_Delete( io_handler );\n\tStg_Class_Delete( dictionary );\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "dionesf_368", "code": "main(int argc, char *argv[])\n{\n    int i, numtasks, rank, rc;\n    int global_primo = 0, local_primo = 0;\n    int n,m;\n\n    MPI_Status Stat;\n\n\n    if(rank == 0)\n    {\n        n = N;\n        m = M;\n    }\n\n    \n\n    rc =\n    rc =\n\n    int numeros_por_task = (m-n)/numtasks;\n\n    int n0 = rank*numeros_por_task;\n    int nf = (rank+1)*numeros_por_task - 1;\n\n\n    \n\n    for(i=n0;i<=nf;i++)\n    {\n        local_primo += num_primo(i);\n    }\n\n\n    \n\n    rc =\n\n    if(rank == 0)\n    {\n        printf(\"No intervalo entre N = %d e M = %d temos %d numeros primos\\n\",n,m,global_primo);\n    }\n\n}", "label": "main(int argc, char *argv[])\n{\n    int i, numtasks, rank, rc;\n    int global_primo = 0, local_primo = 0;\n    int n,m;\n\n    MPI_Status Stat;\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n    if(rank == 0)\n    {\n        n = N;\n        m = M;\n    }\n\n    \n\n    rc = MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);\n    rc = MPI_Bcast(&m,1,MPI_INT,0,MPI_COMM_WORLD);\n\n    int numeros_por_task = (m-n)/numtasks;\n\n    int n0 = rank*numeros_por_task;\n    int nf = (rank+1)*numeros_por_task - 1;\n\n\n    \n\n    for(i=n0;i<=nf;i++)\n    {\n        local_primo += num_primo(i);\n    }\n\n\n    \n\n    rc = MPI_Reduce(&local_primo, &global_primo, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0)\n    {\n        printf(\"No intervalo entre N = %d e M = %d temos %d numeros primos\\n\",n,m,global_primo);\n    }\n\n    MPI_Finalize();\n}"}
{"program": "GustavoKatel_369", "code": "int main( int argc, char *argv[] )\n{\n\tint myid, numprocs;\n\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n\n    double starttime, endtime;\n    starttime =\n\n\tint n = atoi(argv[1]);\n    int range = n / numprocs;\n    int intComp = 0;\n    if(myid==numprocs-1)\n    {\n        intComp += n - numprocs*range; \n\n    }\n\n    \n\n    int count = 0;\n    int start =  range * myid + 1;\n\n    if(start==1) start++; \n\n    if(start+range>=n) range = n - start;  \n\n\n    int i;\n    for(i=start;i<start+range+intComp;i++)\n    {\n        if( ! (n%i) )\n            count++;\n    }\n    \n\n\n\tprintf (\"[%s] %d of %d: range:%d start:%d\\n\", processor_name, myid, numprocs, range+intComp, start);\n\n    if(myid!=0) \n\n    {\n    } else {\n        int sum = count, i;\n        for(i=1;i<numprocs;i++)\n        {\n            sum += count;\n        }\n        printf(\"Total: %d\\n\", sum);\n    }\n\n    endtime   =\n    if(!myid)\n        printf(\"That took %f seconds\\n\",endtime-starttime);\n\n\treturn 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n\tint myid, numprocs;\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    double starttime, endtime;\n    starttime = MPI_Wtime();\n\n\tint n = atoi(argv[1]);\n    int range = n / numprocs;\n    int intComp = 0;\n    if(myid==numprocs-1)\n    {\n        intComp += n - numprocs*range; \n\n    }\n\n    \n\n    int count = 0;\n    int start =  range * myid + 1;\n\n    if(start==1) start++; \n\n    if(start+range>=n) range = n - start;  \n\n\n    int i;\n    for(i=start;i<start+range+intComp;i++)\n    {\n        if( ! (n%i) )\n            count++;\n    }\n    \n\n\n\tprintf (\"[%s] %d of %d: range:%d start:%d\\n\", processor_name, myid, numprocs, range+intComp, start);\n\n    if(myid!=0) \n\n    {\n        MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        int sum = count, i;\n        for(i=1;i<numprocs;i++)\n        {\n            MPI_Recv(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            sum += count;\n        }\n        printf(\"Total: %d\\n\", sum);\n    }\n\n    endtime   = MPI_Wtime();\n    if(!myid)\n        printf(\"That took %f seconds\\n\",endtime-starttime);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "gustfrontar_371", "code": "int main(int argc, char **argv){\n\n  int maxspawn = 100000;\n  int nprocs, myrank, new_nprocs[maxspawn], nspawn, i;\n  char path[maxspawn][SPAWNPATHLEN], wdir[maxspawn][SPAWNPATHLEN], cwd[SPAWNPATHLEN];\n  MPI_Comm intercomm;\n  MPI_Info info;\n  int *ary_ierr;\n  FILE *fp;\n\n  printf(\"nprocs=%d, myrank=%d\\n\", nprocs, myrank);\n\n  nspawn = 0;\n  if(myrank == 0){\n    fp = fopen(argv[1], \"r\");\n    while(feof(fp) == 0){\n      fscanf(fp, \"%d %\" SPAWNPATHLENS \"s %\" SPAWNPATHLENS \"s\", &(new_nprocs[nspawn]), path[nspawn], wdir[nspawn]);\n      printf(\"%d %s %s\\n\", new_nprocs[nspawn], path[nspawn], wdir[nspawn]);\n      if(wdir[nspawn][0] == 0) break;\n      nspawn += 1;\n    }\n    printf(\"nspawn = %d\\n\", nspawn);\n    fclose(fp);\n    \n\n    \n\n    \n\n    \n\n  }\n  \n  \n\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n  return 0;\n}", "label": "int main(int argc, char **argv){\n\n  int maxspawn = 100000;\n  int nprocs, myrank, new_nprocs[maxspawn], nspawn, i;\n  char path[maxspawn][SPAWNPATHLEN], wdir[maxspawn][SPAWNPATHLEN], cwd[SPAWNPATHLEN];\n  MPI_Comm intercomm;\n  MPI_Info info;\n  int *ary_ierr;\n  FILE *fp;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  printf(\"nprocs=%d, myrank=%d\\n\", nprocs, myrank);\n\n  nspawn = 0;\n  if(myrank == 0){\n    fp = fopen(argv[1], \"r\");\n    while(feof(fp) == 0){\n      fscanf(fp, \"%d %\" SPAWNPATHLENS \"s %\" SPAWNPATHLENS \"s\", &(new_nprocs[nspawn]), path[nspawn], wdir[nspawn]);\n      printf(\"%d %s %s\\n\", new_nprocs[nspawn], path[nspawn], wdir[nspawn]);\n      if(wdir[nspawn][0] == 0) break;\n      nspawn += 1;\n    }\n    printf(\"nspawn = %d\\n\", nspawn);\n    fclose(fp);\n    \n\n    \n\n    \n\n    \n\n  }\n  MPI_Bcast(&nspawn, 1, MPI_INTEGER, 0, MPI_COMM_WORLD);\n  MPI_Bcast(new_nprocs, maxspawn, MPI_INTEGER, 0, MPI_COMM_WORLD);\n  MPI_Bcast(path, SPAWNPATHLEN * maxspawn, MPI_CHARACTER, 0, MPI_COMM_WORLD);\n  MPI_Bcast(wdir, SPAWNPATHLEN * maxspawn, MPI_CHARACTER, 0, MPI_COMM_WORLD);\n  \n  \n\n  MPI_Finalize();\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n  return 0;\n}"}
{"program": "tcsiwula_372", "code": "int main(int argc, char* argv[]) {\n   int *loc_arr;\n   int n, loc_n, p, my_rank;\n   MPI_Comm comm;\n\n   comm = MPI_COMM_WORLD;\n\n   n = Read_n(my_rank, comm);\n   loc_n = n/p;\n   loc_arr = malloc(loc_n*sizeof(int));\n\n#  ifdef DEBUG\n   printf(\"Proc %d > p = %d, n = %d, loc_n = %d\\n\",\n         my_rank, p, n, loc_n);\n#  endif\n\n   Get_array(loc_arr, n, loc_n, my_rank, comm);\n   Print_loc_array(loc_arr, loc_n, my_rank);\n   Print_array(loc_arr, n, loc_n, my_rank, comm);\n\n   free(loc_arr);\n\n   return 0;\n}", "label": "int main(int argc, char* argv[]) {\n   int *loc_arr;\n   int n, loc_n, p, my_rank;\n   MPI_Comm comm;\n\n   MPI_Init(&argc, &argv);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &p);\n   MPI_Comm_rank(comm, &my_rank);\n\n   n = Read_n(my_rank, comm);\n   loc_n = n/p;\n   loc_arr = malloc(loc_n*sizeof(int));\n\n#  ifdef DEBUG\n   printf(\"Proc %d > p = %d, n = %d, loc_n = %d\\n\",\n         my_rank, p, n, loc_n);\n#  endif\n\n   Get_array(loc_arr, n, loc_n, my_rank, comm);\n   Print_loc_array(loc_arr, loc_n, my_rank);\n   Print_array(loc_arr, n, loc_n, my_rank, comm);\n\n   free(loc_arr);\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "Wuteyan_373", "code": "int main(int narg, char **args)\n{\n  int me,nprocs;\n\n\n  \n\n\n  if (narg != 9 && narg != 10) {\n    if (me == 0) printf(\"Syntax: rmat N Nz a b c d frac seed {outfile}\\n\");\n  }\n\n  RMAT rmat;\n  rmat.nlevels = atoi(args[1]); \n  rmat.nnonzero = atoi(args[2]); \n  rmat.a = atof(args[3]); \n  rmat.b = atof(args[4]); \n  rmat.c = atof(args[5]); \n  rmat.d = atof(args[6]); \n  rmat.fraction = atof(args[7]); \n  int seed = atoi(args[8]);\n  if (narg == 10) {\n    int n = strlen(args[9]) + 1;\n    rmat.outfile = (char *) malloc(n*sizeof(char));\n    strcpy(rmat.outfile,args[9]);\n  } else rmat.outfile = NULL;\n\n  if (rmat.a + rmat.b + rmat.c + rmat.d != 1.0) {\n    if (me == 0) printf(\"ERROR: a,b,c,d must sum to 1\\n\");\n  }\n\n  if (rmat.fraction >= 1.0) {\n    if (me == 0) printf(\"ERROR: fraction must be < 1\\n\");\n  }\n\n  srand48(seed+me);\n  rmat.order = 1 << rmat.nlevels;\n\n  void *mr = MR_create(MPI_COMM_WORLD);\n\n  \n\n\n  double tstart =\n\n  int niterate = 0;\n  int ntotal = (1 << rmat.nlevels) * rmat.nnonzero;\n  int nremain = ntotal;\n  while (nremain) {\n    niterate++;\n    rmat.ngenerate = nremain/nprocs;\n    if (me < nremain % nprocs) rmat.ngenerate++;\n    MR_map_add(mr,nprocs,&generate,&rmat,1);\n    int nunique = MR_collate(mr,NULL);\n    if (nunique == ntotal) break;\n    MR_reduce(mr,&cull,&rmat);\n    nremain = ntotal - nunique;\n  }\n\n  double tstop =\n\n  \n\n\n  if (rmat.outfile) {\n    char fname[128];\n    sprintf(fname,\"%s.%d\",rmat.outfile,me);\n    rmat.fp = fopen(fname,\"w\");\n    if (rmat.fp == NULL) {\n      printf(\"ERROR: Could not open output file\\n\");\n    }\n    void *mr2 = MR_copy(mr);\n    MR_reduce(mr2,&output,&rmat);\n    fclose(rmat.fp);\n    MR_destroy(mr2);\n  }\n\n  \n\n  \n\n\n  if (me == 0) {\n    printf(\"%d rows in matrix\\n\",rmat.order);\n    printf(\"%d nonzeroes in matrix\\n\",ntotal);\n  }\n\n  MR_reduce(mr,&nonzero,NULL);\n  MR_collate(mr,NULL);\n  MR_reduce(mr,&degree,NULL);\n  MR_collate(mr,NULL);\n  MR_reduce(mr,&histo,NULL);\n  MR_gather(mr,1);\n  MR_sort_keys(mr,&ncompare);\n  int total = 0;\n  MR_map_kv(mr,mr,&stats,&total);\n  if (me == 0) printf(\"%d rows with 0 nonzeroes\\n\",rmat.order-total);\n\n  if (me == 0)\n    printf(\"%g secs to generate matrix on %d procs in %d iterations\\n\",\n     tstop-tstart,nprocs,niterate);\n\n  \n\n\n  MR_destroy(mr);\n  free(rmat.outfile);\n}", "label": "int main(int narg, char **args)\n{\n  int me,nprocs;\n\n  MPI_Init(&narg,&args);\n  MPI_Comm_rank(MPI_COMM_WORLD,&me);\n  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n\n  \n\n\n  if (narg != 9 && narg != 10) {\n    if (me == 0) printf(\"Syntax: rmat N Nz a b c d frac seed {outfile}\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  RMAT rmat;\n  rmat.nlevels = atoi(args[1]); \n  rmat.nnonzero = atoi(args[2]); \n  rmat.a = atof(args[3]); \n  rmat.b = atof(args[4]); \n  rmat.c = atof(args[5]); \n  rmat.d = atof(args[6]); \n  rmat.fraction = atof(args[7]); \n  int seed = atoi(args[8]);\n  if (narg == 10) {\n    int n = strlen(args[9]) + 1;\n    rmat.outfile = (char *) malloc(n*sizeof(char));\n    strcpy(rmat.outfile,args[9]);\n  } else rmat.outfile = NULL;\n\n  if (rmat.a + rmat.b + rmat.c + rmat.d != 1.0) {\n    if (me == 0) printf(\"ERROR: a,b,c,d must sum to 1\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  if (rmat.fraction >= 1.0) {\n    if (me == 0) printf(\"ERROR: fraction must be < 1\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  srand48(seed+me);\n  rmat.order = 1 << rmat.nlevels;\n\n  void *mr = MR_create(MPI_COMM_WORLD);\n\n  \n\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  double tstart = MPI_Wtime();\n\n  int niterate = 0;\n  int ntotal = (1 << rmat.nlevels) * rmat.nnonzero;\n  int nremain = ntotal;\n  while (nremain) {\n    niterate++;\n    rmat.ngenerate = nremain/nprocs;\n    if (me < nremain % nprocs) rmat.ngenerate++;\n    MR_map_add(mr,nprocs,&generate,&rmat,1);\n    int nunique = MR_collate(mr,NULL);\n    if (nunique == ntotal) break;\n    MR_reduce(mr,&cull,&rmat);\n    nremain = ntotal - nunique;\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  double tstop = MPI_Wtime();\n\n  \n\n\n  if (rmat.outfile) {\n    char fname[128];\n    sprintf(fname,\"%s.%d\",rmat.outfile,me);\n    rmat.fp = fopen(fname,\"w\");\n    if (rmat.fp == NULL) {\n      printf(\"ERROR: Could not open output file\\n\");\n      MPI_Abort(MPI_COMM_WORLD,1);\n    }\n    void *mr2 = MR_copy(mr);\n    MR_reduce(mr2,&output,&rmat);\n    fclose(rmat.fp);\n    MR_destroy(mr2);\n  }\n\n  \n\n  \n\n\n  if (me == 0) {\n    printf(\"%d rows in matrix\\n\",rmat.order);\n    printf(\"%d nonzeroes in matrix\\n\",ntotal);\n  }\n\n  MR_reduce(mr,&nonzero,NULL);\n  MR_collate(mr,NULL);\n  MR_reduce(mr,&degree,NULL);\n  MR_collate(mr,NULL);\n  MR_reduce(mr,&histo,NULL);\n  MR_gather(mr,1);\n  MR_sort_keys(mr,&ncompare);\n  int total = 0;\n  MR_map_kv(mr,mr,&stats,&total);\n  if (me == 0) printf(\"%d rows with 0 nonzeroes\\n\",rmat.order-total);\n\n  if (me == 0)\n    printf(\"%g secs to generate matrix on %d procs in %d iterations\\n\",\n     tstop-tstart,nprocs,niterate);\n\n  \n\n\n  MR_destroy(mr);\n  free(rmat.outfile);\n  MPI_Finalize();\n}"}
{"program": "bmi-forum_376", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\tStream_SetFileBranch( Journal_GetTypedStream( ErrorStream_Type ), stJournal->stdOut );\n\tstJournal->firewallProducesAssert = False;\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tIndexSet*\t\t\tis;\n\t\t\n\t\tprintf( \"Watching rank: %i\\n\", rank );\n\t\t\n\t\tprintf( \"* Test Construction *\\n\" );\n\t\tis = IndexSet_New( 24 );\n\n\t\tprintf( \"* Test Access(IsMember) over limit *\\n\" );\n\t\tIndexSet_IsMember( is, 24 );\n\t\tprintf( \"* Shouldn't get here\\n\" );\n\n\t\tStg_Class_Delete( is );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\tStream_SetFileBranch( Journal_GetTypedStream( ErrorStream_Type ), stJournal->stdOut );\n\tstJournal->firewallProducesAssert = False;\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tIndexSet*\t\t\tis;\n\t\t\n\t\tprintf( \"Watching rank: %i\\n\", rank );\n\t\t\n\t\tprintf( \"* Test Construction *\\n\" );\n\t\tis = IndexSet_New( 24 );\n\n\t\tprintf( \"* Test Access(IsMember) over limit *\\n\" );\n\t\tIndexSet_IsMember( is, 24 );\n\t\tprintf( \"* Shouldn't get here\\n\" );\n\n\t\tStg_Class_Delete( is );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "mpip_377", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(4, ni, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_real(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(4, ni, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_real(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "yinyanlong_378", "code": "int main (int argc, char *args[]) {\n\n    int ret;\n    int i;\n    int myrank;\n    int req_size = 256;\n    int range = 32*1024;\n    MPI_File datafile;\n    MPI_Status status;\n    char * buf;\n\n\n    printf(\"%s.\\n\", args[1]);\n    ret =\n    if (ret != MPI_SUCCESS) {\n        printf(\"Failed to open data file. \\n\");\n        return -1;\n    }\n    \n\n\n    if(myrank==0) {\n\n\n        buf = malloc(req_size);\n        if (!buf) {\n            perror(\"Failed to malloc. \\n\");\n            return;\n        }\n\n        int size_remain = range;\n\n        while (size_remain > req_size) {\n            size_remain -= req_size;\n        }\n        free(buf);\n\n    }\n    else {\n\n        buf = malloc(req_size);\n        if (!buf) {\n            perror(\"Failed to malloc. \\n\");\n            return;\n        }\n\n        int size_remain = range;\n        \n        while (size_remain > req_size) {\n            size_remain -= req_size;\n        }\n        free(buf);\n\n    }\n}", "label": "int main (int argc, char *args[]) {\n\n    int ret;\n    int i;\n    int myrank;\n    int req_size = 256;\n    int range = 32*1024;\n    MPI_File datafile;\n    MPI_Status status;\n    char * buf;\n\n    MPI_Init( &argc, &args );\n    MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n\n    printf(\"%s.\\n\", args[1]);\n    ret = MPI_File_open(MPI_COMM_WORLD, args[1], MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &datafile );\n    if (ret != MPI_SUCCESS) {\n        printf(\"Failed to open data file. \\n\");\n        MPI_Finalize();\n        return -1;\n    }\n    \n\n\n    if(myrank==0) {\n\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        buf = malloc(req_size);\n        if (!buf) {\n            perror(\"Failed to malloc. \\n\");\n            MPI_File_close(&datafile);\n            MPI_Finalize();\n            return;\n        }\n\n        int size_remain = range;\n        MPI_File_seek(datafile, range*myrank, MPI_SEEK_SET);\n\n        while (size_remain > req_size) {\n            MPI_File_write(datafile, buf, req_size, MPI_CHAR, &status);\n            size_remain -= req_size;\n        }\n        free(buf);\n\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        buf = malloc(req_size);\n        if (!buf) {\n            perror(\"Failed to malloc. \\n\");\n            MPI_File_close(&datafile);\n            MPI_Finalize();\n            return;\n        }\n\n        int size_remain = range;\n        MPI_File_seek(datafile, range*myrank, MPI_SEEK_SET);\n        \n        while (size_remain > req_size) {\n            MPI_File_write(datafile, buf, req_size, MPI_CHAR, &status);\n            size_remain -= req_size;\n        }\n        free(buf);\n\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n    MPI_File_close(&datafile);\n    MPI_Finalize();\n}"}
{"program": "gentryx_379", "code": "int main(int argc, char * argv[])\n{\n\n    int rank, size;\n\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    if (size<2) {\n        printf(\"Use 2 or more processes. \\n\");\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf = NULL;\n\n\n    memset(buf, rank, (size_t)n);\n\n    size_t errors = 0;\n    for (int r = 1; r < size; r++) {\n\n        \n\n        if (rank==r) {\n        }\n        else if (rank==0) {\n\n            errors += verify_buffer(buf, n, r);\n            if (errors > 0) {\n                printf(\"There were %zu errors!\", errors);\n            }\n        }\n    }\n\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    if (size<2) {\n        printf(\"Use 2 or more processes. \\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf = NULL;\n\n    MPI_Alloc_mem((MPI_Aint)n, MPI_INFO_NULL, &buf);\n\n    memset(buf, rank, (size_t)n);\n\n    size_t errors = 0;\n    for (int r = 1; r < size; r++) {\n\n        \n\n        if (rank==r) {\n            MPIX_Send_x(buf, n, MPI_CHAR, 0 \n, r \n, MPI_COMM_WORLD);\n        }\n        else if (rank==0) {\n            MPIX_Recv_x(buf, n, MPI_CHAR, r \n, r \n, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            errors += verify_buffer(buf, n, r);\n            if (errors > 0) {\n                printf(\"There were %zu errors!\", errors);\n            }\n        }\n    }\n\n    MPI_Free_mem(buf);\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "qingu_380", "code": "int main(int argc, char ** argv) {\n  int    rank, nproc, i;\n  int   *buf;\n#ifdef SHARED_BUF\n  void **base_ptrs;\n#endif\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCI GOP test with %d processes\\n\", nproc);\n\n#ifdef SHARED_BUF\n  base_ptrs = malloc(nproc*sizeof(void*));\n  ARMCI_Malloc(base_ptrs, DATA_SZ*sizeof(int));\n  buf = base_ptrs[rank];\n#else\n  buf = malloc(DATA_SZ*sizeof(int));\n#endif\n\n  if (rank == 0) printf(\" - Testing ABSMIN\\n\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    buf[i] = (rank+1) * ((i % 2) ? -1 : 1);\n\n  armci_msg_igop(buf, DATA_SZ, \"absmin\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    if (buf[i] != 1) {\n      printf(\"Err: buf[%d] = %d expected 1\\n\", i, buf[i]);\n      ARMCI_Error(\"Fail\", 1);\n    }\n\n  if (rank == 0) printf(\" - Testing ABSMAX\\n\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    buf[i] = (rank+1) * ((i % 2) ? -1 : 1);\n\n  armci_msg_igop(buf, DATA_SZ, \"absmax\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    if (buf[i] != nproc) {\n      printf(\"Err: buf[%d] = %d expected %d\\n\", i, buf[i], nproc);\n      ARMCI_Error(\"Fail\", 1);\n    }\n\n#ifdef SHARED_BUF\n  ARMCI_Free(base_ptrs[rank]);\n  free(base_ptrs);\n#else\n  free(buf);\n#endif\n\n  if (rank == 0) printf(\"Pass.\\n\");\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int    rank, nproc, i;\n  int   *buf;\n#ifdef SHARED_BUF\n  void **base_ptrs;\n#endif\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCI GOP test with %d processes\\n\", nproc);\n\n#ifdef SHARED_BUF\n  base_ptrs = malloc(nproc*sizeof(void*));\n  ARMCI_Malloc(base_ptrs, DATA_SZ*sizeof(int));\n  buf = base_ptrs[rank];\n#else\n  buf = malloc(DATA_SZ*sizeof(int));\n#endif\n\n  if (rank == 0) printf(\" - Testing ABSMIN\\n\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    buf[i] = (rank+1) * ((i % 2) ? -1 : 1);\n\n  armci_msg_igop(buf, DATA_SZ, \"absmin\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    if (buf[i] != 1) {\n      printf(\"Err: buf[%d] = %d expected 1\\n\", i, buf[i]);\n      ARMCI_Error(\"Fail\", 1);\n    }\n\n  if (rank == 0) printf(\" - Testing ABSMAX\\n\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    buf[i] = (rank+1) * ((i % 2) ? -1 : 1);\n\n  armci_msg_igop(buf, DATA_SZ, \"absmax\");\n\n  for (i = 0; i < DATA_SZ; i++)\n    if (buf[i] != nproc) {\n      printf(\"Err: buf[%d] = %d expected %d\\n\", i, buf[i], nproc);\n      ARMCI_Error(\"Fail\", 1);\n    }\n\n#ifdef SHARED_BUF\n  ARMCI_Free(base_ptrs[rank]);\n  free(base_ptrs);\n#else\n  free(buf);\n#endif\n\n  if (rank == 0) printf(\"Pass.\\n\");\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "xyuan_382", "code": "int\nmain (int argc, char **argv)\n{\n  MPI_Comm            mpicomm;\n  int                 mpiret;\n  int                 found_total;\n  p4est_locidx_t      jt, Al, Bl;\n  p4est_connectivity_t *conn;\n  p4est_quadrant_t   *A, *B;\n  p4est_geometry_t   *geom;\n  p4est_t            *p4est;\n  sc_array_t         *points;\n  test_point_t       *p;\n  const char         *vtkname;\n\n  \n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n\n  \n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n#ifndef P4_TO_P8\n  conn = p4est_connectivity_new_star ();\n  geom = NULL;\n  vtkname = \"test_search2\";\n#else\n  conn = p8est_connectivity_new_sphere ();\n  geom = p8est_geometry_new_sphere (1., 0.191728, 0.039856);\n  vtkname = \"test_search3\";\n#endif\n  p4est = p4est_new_ext (mpicomm, conn, 0, 0, 0, 0, NULL, NULL);\n  p4est_refine (p4est, 1, refine_fn, NULL);\n  p4est_partition (p4est, NULL);\n  p4est_vtk_write_file (p4est, geom, vtkname);\n\n  \n\n  points = sc_array_new_size (sizeof (test_point_t), 2);\n\n  \n\n  p = (test_point_t *) sc_array_index (points, 0);\n  p->name = \"A\";\n  A = &p->quad;\n  P4EST_QUADRANT_INIT (A);\n  p4est_quadrant_set_morton (A, 3, 23);\n  A->p.piggy3.which_tree = 0;\n  A->p.piggy3.local_num = -1;\n  Al = -1;\n\n  \n\n  p = (test_point_t *) sc_array_index (points, 1);\n  p->name = \"B\";\n  B = &p->quad;\n  P4EST_QUADRANT_INIT (B);\n  p4est_quadrant_set_morton (B, 2, 13);\n  B->p.piggy3.which_tree = conn->num_trees / 2;\n  B->p.piggy3.local_num = -1;\n  Bl = -1;\n\n  \n\n  for (jt = p4est->first_local_tree; jt <= p4est->last_local_tree; ++jt) {\n    size_t              zz;\n    p4est_tree_t       *tree = p4est_tree_array_index (p4est->trees, jt);\n    p4est_quadrant_t   *quad;\n    sc_array_t         *tquadrants = &tree->quadrants;\n\n    for (zz = 0; zz < tquadrants->elem_count; ++zz) {\n      quad = p4est_quadrant_array_index (tquadrants, zz);\n      if (A->p.piggy3.which_tree == jt && !p4est_quadrant_compare (quad, A)) {\n        Al = tree->quadrants_offset + (p4est_locidx_t) zz;\n        P4EST_VERBOSEF (\"Searching for A at %lld\\n\", (long long) Al);\n      }\n      if (B->p.piggy3.which_tree == jt && !p4est_quadrant_compare (quad, B)) {\n        Bl = tree->quadrants_offset + (p4est_locidx_t) zz;\n        P4EST_VERBOSEF (\"Searching for B at %lld\\n\", (long long) Bl);\n      }\n    }\n  }\n\n  \n\n  found_count = 0;\n  p4est_search (p4est, search_callback, points);\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  SC_CHECK_ABORT (found_total == (int) points->elem_count, \"Point search\");\n  SC_CHECK_ABORT (A->p.piggy3.local_num == Al, \"Search A\");\n  SC_CHECK_ABORT (B->p.piggy3.local_num == Bl, \"Search B\");\n\n  \n\n  sc_array_destroy (points);\n  p4est_destroy (p4est);\n  P4EST_FREE (geom);\n  p4est_connectivity_destroy (conn);\n\n  \n\n  sc_finalize ();\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  MPI_Comm            mpicomm;\n  int                 mpiret;\n  int                 found_total;\n  p4est_locidx_t      jt, Al, Bl;\n  p4est_connectivity_t *conn;\n  p4est_quadrant_t   *A, *B;\n  p4est_geometry_t   *geom;\n  p4est_t            *p4est;\n  sc_array_t         *points;\n  test_point_t       *p;\n  const char         *vtkname;\n\n  \n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n\n  \n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n#ifndef P4_TO_P8\n  conn = p4est_connectivity_new_star ();\n  geom = NULL;\n  vtkname = \"test_search2\";\n#else\n  conn = p8est_connectivity_new_sphere ();\n  geom = p8est_geometry_new_sphere (1., 0.191728, 0.039856);\n  vtkname = \"test_search3\";\n#endif\n  p4est = p4est_new_ext (mpicomm, conn, 0, 0, 0, 0, NULL, NULL);\n  p4est_refine (p4est, 1, refine_fn, NULL);\n  p4est_partition (p4est, NULL);\n  p4est_vtk_write_file (p4est, geom, vtkname);\n\n  \n\n  points = sc_array_new_size (sizeof (test_point_t), 2);\n\n  \n\n  p = (test_point_t *) sc_array_index (points, 0);\n  p->name = \"A\";\n  A = &p->quad;\n  P4EST_QUADRANT_INIT (A);\n  p4est_quadrant_set_morton (A, 3, 23);\n  A->p.piggy3.which_tree = 0;\n  A->p.piggy3.local_num = -1;\n  Al = -1;\n\n  \n\n  p = (test_point_t *) sc_array_index (points, 1);\n  p->name = \"B\";\n  B = &p->quad;\n  P4EST_QUADRANT_INIT (B);\n  p4est_quadrant_set_morton (B, 2, 13);\n  B->p.piggy3.which_tree = conn->num_trees / 2;\n  B->p.piggy3.local_num = -1;\n  Bl = -1;\n\n  \n\n  for (jt = p4est->first_local_tree; jt <= p4est->last_local_tree; ++jt) {\n    size_t              zz;\n    p4est_tree_t       *tree = p4est_tree_array_index (p4est->trees, jt);\n    p4est_quadrant_t   *quad;\n    sc_array_t         *tquadrants = &tree->quadrants;\n\n    for (zz = 0; zz < tquadrants->elem_count; ++zz) {\n      quad = p4est_quadrant_array_index (tquadrants, zz);\n      if (A->p.piggy3.which_tree == jt && !p4est_quadrant_compare (quad, A)) {\n        Al = tree->quadrants_offset + (p4est_locidx_t) zz;\n        P4EST_VERBOSEF (\"Searching for A at %lld\\n\", (long long) Al);\n      }\n      if (B->p.piggy3.which_tree == jt && !p4est_quadrant_compare (quad, B)) {\n        Bl = tree->quadrants_offset + (p4est_locidx_t) zz;\n        P4EST_VERBOSEF (\"Searching for B at %lld\\n\", (long long) Bl);\n      }\n    }\n  }\n\n  \n\n  found_count = 0;\n  p4est_search (p4est, search_callback, points);\n  mpiret = MPI_Allreduce (&found_count, &found_total,\n                          1, MPI_INT, MPI_SUM, mpicomm);\n  SC_CHECK_MPI (mpiret);\n  SC_CHECK_ABORT (found_total == (int) points->elem_count, \"Point search\");\n  SC_CHECK_ABORT (A->p.piggy3.local_num == Al, \"Search A\");\n  SC_CHECK_ABORT (B->p.piggy3.local_num == Bl, \"Search B\");\n\n  \n\n  sc_array_destroy (points);\n  p4est_destroy (p4est);\n  P4EST_FREE (geom);\n  p4est_connectivity_destroy (conn);\n\n  \n\n  sc_finalize ();\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "mingpz2010_383", "code": "int main(int argc, char *argv[]) {\n  int mypid;\n  PETRA_COMM petra_comm;\n  \n\n  petra_comm = petra_comm_create( MPI_COMM_WORLD );\n  \n\n  mypid = petra_comm_getmypid(petra_comm);\n  printf(\"MyPID = %d\\n\",mypid);\n\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  int mypid;\n  PETRA_COMM petra_comm;\n  \n\n  MPI_Init(&argc,&argv);\n  petra_comm = petra_comm_create( MPI_COMM_WORLD );\n  \n\n  mypid = petra_comm_getmypid(petra_comm);\n  printf(\"MyPID = %d\\n\",mypid);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "cbries_384", "code": "main (int argc, char **argv)\n{\n\tint i;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\tdouble messungTime = 0.0;\n\tdouble messungTimeSumme = 0.0;\n\tint datalen = 0;\n\tchar *data = NULL;\n\n\tMPI_Status status;\n\t\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tif(argc<2) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Usage: hw10b bytes\\n\");\t\n\t\t}\n\t\treturn 1;\n\t}\n\n\tsrand(time(NULL));\n\n\t\n\n\tdatalen = atoi(argv[1]);\n\tdata = (char*) malloc(datalen*sizeof(char));\n\tif(myrank==0) {\n\t\tfor(i=0; i<datalen; i++) {\n\t\t\tdata[i] = (char) rand()%'A'+'Z';\n\t\t}\n\t}\n\n\tmessungTime =\n\n\n\tmessungTime = MPI_Wtime() - messungTime;\n\n\n\tif(myrank==0) {\n\t}\n\n\t\n\n\n\treturn 0;\n}\n/* ", "label": "main (int argc, char **argv)\n{\n\tint i;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\tdouble messungTime = 0.0;\n\tdouble messungTimeSumme = 0.0;\n\tint datalen = 0;\n\tchar *data = NULL;\n\n\tMPI_Status status;\n\t\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tif(argc<2) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Usage: hw10b bytes\\n\");\t\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\tsrand(time(NULL));\n\n\t\n\n\tdatalen = atoi(argv[1]);\n\tdata = (char*) malloc(datalen*sizeof(char));\n\tif(myrank==0) {\n\t\tfor(i=0; i<datalen; i++) {\n\t\t\tdata[i] = (char) rand()%'A'+'Z';\n\t\t}\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tmessungTime = MPI_Wtime();\n\n\tMPI_Bcast(data, datalen, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n\tmessungTime = MPI_Wtime() - messungTime;\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tMPI_Reduce(&messungTime, &messungTimeSumme, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif(myrank==0) {\n\t\tprintf(\"Dauer mit MPI_Bcast(...): %.10lfsec\\n\", messungTimeSumme);\n\t}\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n/* "}
{"program": "m45t3r_385", "code": "int main(int argc, char **argv) {\n\n  double mpiip(int p, int s, int n, double *x, double *y);\n  int nloc(int p, int s, int n);\n  double *x, alpha, time0, time1;\n  int p, s, n, nl, i, iglob;\n\n  \n\n\n  \n\n\n\n\n\n  if (s == 0) {\n    printf(\"Please enter n:\\n\");\n    fflush(stdout);\n    scanf(\"%d\", &n);\n    if (n < 0)\n  }\n\n\n  nl = nloc(p, s, n);\n  x = vecallocd(nl);\n  for (i = 0; i < nl; i++) {\n    iglob = i * p + s;\n    x[i] = iglob + 1;\n  }\n  time0 =\n\n  alpha = mpiip(p, s, n, x, x);\n  time1 =\n\n  printf(\"Processor %d: sum of squares up to %d*%d is %.lf\\n\", s, n, n, alpha);\n  fflush(stdout);\n  if (s == 0) {\n    printf(\"This took only %.6lf seconds.\\n\", time1 - time0);\n    fflush(stdout);\n  }\n\n  vecfreed(x);\n\n  \n\n  exit(0);\n\n}", "label": "int main(int argc, char **argv) {\n\n  double mpiip(int p, int s, int n, double *x, double *y);\n  int nloc(int p, int s, int n);\n  double *x, alpha, time0, time1;\n  int p, s, n, nl, i, iglob;\n\n  \n\n\n  \n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &p); \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &s); \n\n\n  if (s == 0) {\n    printf(\"Please enter n:\\n\");\n    fflush(stdout);\n    scanf(\"%d\", &n);\n    if (n < 0)\n      MPI_Abort(MPI_COMM_WORLD, -1);\n  }\n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  nl = nloc(p, s, n);\n  x = vecallocd(nl);\n  for (i = 0; i < nl; i++) {\n    iglob = i * p + s;\n    x[i] = iglob + 1;\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  time0 = MPI_Wtime();\n\n  alpha = mpiip(p, s, n, x, x);\n  MPI_Barrier(MPI_COMM_WORLD);\n  time1 = MPI_Wtime();\n\n  printf(\"Processor %d: sum of squares up to %d*%d is %.lf\\n\", s, n, n, alpha);\n  fflush(stdout);\n  if (s == 0) {\n    printf(\"This took only %.6lf seconds.\\n\", time1 - time0);\n    fflush(stdout);\n  }\n\n  vecfreed(x);\n  MPI_Finalize();\n\n  \n\n  exit(0);\n\n}"}
{"program": "matthagy_386", "code": "int\nmain(int argc, char **argv)\n{\n        if (IS_MASTER()) {\n                main_master(argc, argv);\n        } else {\n                main_slave();\n        }\n        return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n        MPI_Init(&argc, &argv);\n        MPI_Comm_rank(MPI_COMM_WORLD, &CEX_rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &CEX_size);\n        if (IS_MASTER()) {\n                main_master(argc, argv);\n        } else {\n                main_slave();\n        }\n        MPI_Finalize();\n        return 0;\n}"}
{"program": "rainwoodman_387", "code": "int main(int argc, char * argv[]) {\n    \n    \n    int i;\n    int ch;\n    char * filename = alloca(1500);\n    \n    while(-1 != (ch = getopt(argc, argv, getoptstr))) {\n        switch(ch) {\n            case 'A':\n                aggregated = 1;\n                break;\n            case 'p':\n                purge = 1;\n                break;\n            case 'w':\n                if(1 != sscanf(optarg, \"%d\", &nmemb)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 'f':\n                if(1 != sscanf(optarg, \"%d\", &Nfile)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 'n':\n                if(1 != sscanf(optarg, \"%d\", &Nwriter)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 's':\n                if(1 != sscanf(optarg, \"%td\", &size)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 'h':\n            default:\n                usage();\n                goto byebye;\n        }    \n    }\n    if(optind == argc) {\n        usage();\n        goto byebye;\n    }\n\n    char * smode = strdup(argv[optind]);\n    for(mode = 0; mode < 4; mode ++) {\n        if(0 == strcmp(MODES[mode], smode)) break;\n    }\n    if(mode == 4) {\n        usage(); goto byebye;\n    }\n\n    optind++;\n\n    sprintf(filename, \"%s\", argv[optind]);\n\n    if (Nwriter > NTask || Nwriter == 0) {\n        info(\"############## CAUTION: you chose %d ranks and %d writers! ##############\\n\"\n             \" #  If you want %d writers, allocate at least %d ranks with <mpirun -n %d> #\\n\"\n             \" ################### Can only use %d writers instead! ###################\\n\",\n             NTask, Nwriter, Nwriter, Nwriter, Nwriter, NTask);\n        Nwriter = NTask;\n    }\n    if (Nfile == 0) {\n        Nfile = Nwriter;\n    }\n\n    iosim(filename);\n\n    if (purge) {\n        if(ThisTask == 0) {\n            char buffer[1500];\n            sprintf(buffer, \"rm -rf %s/TestBlock\", filename);\n            system(buffer);\n        }\n    }\n\n\nbyebye:\n    return 0;\n}", "label": "int main(int argc, char * argv[]) {\n    MPI_Init(&argc, &argv);\n    \n    MPI_Comm_rank(MPI_COMM_WORLD, &ThisTask);\n    MPI_Comm_size(MPI_COMM_WORLD, &NTask);\n    \n    int i;\n    int ch;\n    char * filename = alloca(1500);\n    \n    while(-1 != (ch = getopt(argc, argv, getoptstr))) {\n        switch(ch) {\n            case 'A':\n                aggregated = 1;\n                break;\n            case 'p':\n                purge = 1;\n                break;\n            case 'w':\n                if(1 != sscanf(optarg, \"%d\", &nmemb)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 'f':\n                if(1 != sscanf(optarg, \"%d\", &Nfile)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 'n':\n                if(1 != sscanf(optarg, \"%d\", &Nwriter)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 's':\n                if(1 != sscanf(optarg, \"%td\", &size)) {\n                    usage();\n                    goto byebye;\n                }\n                break;\n            case 'h':\n            default:\n                usage();\n                goto byebye;\n        }    \n    }\n    if(optind == argc) {\n        usage();\n        goto byebye;\n    }\n\n    char * smode = strdup(argv[optind]);\n    for(mode = 0; mode < 4; mode ++) {\n        if(0 == strcmp(MODES[mode], smode)) break;\n    }\n    if(mode == 4) {\n        usage(); goto byebye;\n    }\n\n    optind++;\n\n    sprintf(filename, \"%s\", argv[optind]);\n\n    if (Nwriter > NTask || Nwriter == 0) {\n        info(\"############## CAUTION: you chose %d ranks and %d writers! ##############\\n\"\n             \" #  If you want %d writers, allocate at least %d ranks with <mpirun -n %d> #\\n\"\n             \" ################### Can only use %d writers instead! ###################\\n\",\n             NTask, Nwriter, Nwriter, Nwriter, Nwriter, NTask);\n        Nwriter = NTask;\n    }\n    if (Nfile == 0) {\n        Nfile = Nwriter;\n    }\n\n    iosim(filename);\n\n    if (purge) {\n        if(ThisTask == 0) {\n            char buffer[1500];\n            sprintf(buffer, \"rm -rf %s/TestBlock\", filename);\n            system(buffer);\n        }\n    }\n\n\nbyebye:\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "bmi-forum_388", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tIndexMap*\t\tmap;\n\t\tIndex\t\t\tidx;\n\t\tStream*\t\t\tstream;\n\t\t\n\t\tstream = Journal_Register( Info_Type, \"myStream\" );\n\t\t\n\t\tmap = IndexMap_New();\n\t\tfor( idx = 0; idx < 100; idx++ )\n\t\t\tIndexMap_Append( map, idx + 1, 100 - idx );\n\t\t\n\t\tprintf( \"List Data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ )\n\t\t\tprintf( \"\\tlooking for %d, found %d\\n\", idx + 1, IndexMap_Find( map, idx + 1 ) );\n\t\t\n\t\tStg_Class_Delete( map );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tIndexMap*\t\tmap;\n\t\tIndex\t\t\tidx;\n\t\tStream*\t\t\tstream;\n\t\t\n\t\tstream = Journal_Register( Info_Type, \"myStream\" );\n\t\t\n\t\tmap = IndexMap_New();\n\t\tfor( idx = 0; idx < 100; idx++ )\n\t\t\tIndexMap_Append( map, idx + 1, 100 - idx );\n\t\t\n\t\tprintf( \"List Data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ )\n\t\t\tprintf( \"\\tlooking for %d, found %d\\n\", idx + 1, IndexMap_Find( map, idx + 1 ) );\n\t\t\n\t\tStg_Class_Delete( map );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "likesmusik_389", "code": "int main(int argc, char* argv[])\n{\n\t\n\n\tint rank, size;\n\t\n\t\n\n\tchar receive[BUF_SIZE];\n\t\n\n\t\n\n\t\n\n\tlong int usec=999999, lowest_usec;\n\t\n\tif(rank == 0) {\n\t\t\n\n\t\tint i;\n\t\tfor(i=1; i<size; ++i) {\n\t\t\t\n\n\t\t\t\n\n\t\t\tprintf(\"%s\\n\", receive);\n\t\t}\n\t} else {\n\t\t\n\n\t\tchar message[BUF_SIZE];\n\t\t\n\t\t\n\n\t\tif(gethostname(message, BUF_SIZE) != 0) {\n\t\t\tprintf(\"Error in gethostname\\n\");\n\t\t}\n\t\tstrcat(message, \": \");\n\t\t\n\t\t\n\n\t\t\n\n\t\ttime_t date = time(NULL);\n\t\tstruct tm *tmptr = localtime(&date);\n\t\tif(tmptr == NULL) {\n\t\t\tprintf(\"Error in localtime\\n\");\n\t\t}\n\t\tstruct timeval time;\n\t\tif(gettimeofday(&time, NULL) != 0) {\n\t\t\tprintf(\"Error in gettimeofday\\n\");\n\t\t}\n\t\t\n\n\t\tstrftime(message+strlen(message), BUF_SIZE-strlen(message), \"%Y-%m-%d %H:%M:%S.\", tmptr);\n\t\t\n\n\t\tsnprintf(message+strlen(message), BUF_SIZE-strlen(message), \"%06ld\", time.tv_usec);\n\t\t\n\t\t\n\n\t\t\n\t\t\n\n\t\tusec = time.tv_usec;\n\t}\n\t\n\t\n\n\tif(rank == 0) {\n\t\t\n\n\t\tprintf(\"%06ld\\n\", lowest_usec);\n\t}\n\t\n\t\n\n\t\n\n\tif(MPI_Barrier(MPI_COMM_WORLD) != MPI_SUCCESS) {\n\t\tprintf(\"Error in MPI_Barrier\\n\");\n\t}\n\t\n\t\n\n\tprintf(\"Rank %i beendet jetzt!\\n\", rank);\n\t\n\treturn 0;\n}", "label": "int main(int argc, char* argv[])\n{\n\t\n\n\tint rank, size;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\t\n\t\n\n\tchar receive[BUF_SIZE];\n\t\n\n\t\n\n\t\n\n\tlong int usec=999999, lowest_usec;\n\t\n\tif(rank == 0) {\n\t\t\n\n\t\tint i;\n\t\tfor(i=1; i<size; ++i) {\n\t\t\t\n\n\t\t\tMPI_Recv(receive, BUF_SIZE, MPI_CHAR, i, 0, MPI_COMM_WORLD, NULL);\n\t\t\t\n\n\t\t\tprintf(\"%s\\n\", receive);\n\t\t}\n\t} else {\n\t\t\n\n\t\tchar message[BUF_SIZE];\n\t\t\n\t\t\n\n\t\tif(gethostname(message, BUF_SIZE) != 0) {\n\t\t\tprintf(\"Error in gethostname\\n\");\n\t\t}\n\t\tstrcat(message, \": \");\n\t\t\n\t\t\n\n\t\t\n\n\t\ttime_t date = time(NULL);\n\t\tstruct tm *tmptr = localtime(&date);\n\t\tif(tmptr == NULL) {\n\t\t\tprintf(\"Error in localtime\\n\");\n\t\t}\n\t\tstruct timeval time;\n\t\tif(gettimeofday(&time, NULL) != 0) {\n\t\t\tprintf(\"Error in gettimeofday\\n\");\n\t\t}\n\t\t\n\n\t\tstrftime(message+strlen(message), BUF_SIZE-strlen(message), \"%Y-%m-%d %H:%M:%S.\", tmptr);\n\t\t\n\n\t\tsnprintf(message+strlen(message), BUF_SIZE-strlen(message), \"%06ld\", time.tv_usec);\n\t\t\n\t\t\n\n\t\tMPI_Send(message, BUF_SIZE, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n\t\t\n\t\t\n\n\t\tusec = time.tv_usec;\n\t}\n\t\n\t\n\n\tMPI_Reduce(&usec, &lowest_usec, 1, MPI_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\tif(rank == 0) {\n\t\t\n\n\t\tprintf(\"%06ld\\n\", lowest_usec);\n\t}\n\t\n\t\n\n\t\n\n\tif(MPI_Barrier(MPI_COMM_WORLD) != MPI_SUCCESS) {\n\t\tprintf(\"Error in MPI_Barrier\\n\");\n\t}\n\t\n\t\n\n\tprintf(\"Rank %i beendet jetzt!\\n\", rank);\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "airqinc_390", "code": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n\n  int world_rank, world_size;\n\n  \n\n  float *rands = NULL;\n  if (world_rank == 0) {\n    rands = create_rands(ELEMENTS_PER_PROC * world_size, MAX_RANDOM_VAL, world_rank);\n  }\n\n  \n\n  float *buff_rands = (float *)malloc(sizeof(float) * ELEMENTS_PER_PROC);\n\n  \n\n  dumpData ( world_rank , world_size , ELEMENTS_PER_PROC , buff_rands , \" Data \" , 1);\n\n  \n\n  float sub_avg = calc_avg(buff_rands, ELEMENTS_PER_PROC);\n  printf(\" Process %d: Avg of subset is %f\\n\", world_rank, sub_avg);\n\n  \n\n  float *sub_avgs = NULL;\n  if (world_rank == 0) {\n    sub_avgs = (float *)malloc(sizeof(float) * world_size);\n  }\n\n  \n\n  if (world_rank == 0) {\n    float avg = calc_avg(sub_avgs, world_size);\n    printf(\" Process %d: Avg of all elements is %f\\n\", world_rank, avg);\n    \n\n    float original_data_avg = calc_avg(rands, ELEMENTS_PER_PROC * world_size);\n    printf(\" Process %d: Avg of original data is %f\\n\", world_rank, original_data_avg);\n  }\n\n  if (world_rank == 0) {\n    free(rands);\n    free(sub_avgs);\n  }\n  free(buff_rands);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n\n  int world_rank, world_size;\n  MPI_Init(NULL, NULL);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  float *rands = NULL;\n  if (world_rank == 0) {\n    rands = create_rands(ELEMENTS_PER_PROC * world_size, MAX_RANDOM_VAL, world_rank);\n  }\n\n  \n\n  float *buff_rands = (float *)malloc(sizeof(float) * ELEMENTS_PER_PROC);\n\n  \n\n  MPI_Scatter(rands, ELEMENTS_PER_PROC, MPI_FLOAT, buff_rands,\n              ELEMENTS_PER_PROC, MPI_FLOAT, 0, MPI_COMM_WORLD);\n  dumpData ( world_rank , world_size , ELEMENTS_PER_PROC , buff_rands , \" Data \" , 1);\n\n  \n\n  float sub_avg = calc_avg(buff_rands, ELEMENTS_PER_PROC);\n  printf(\" Process %d: Avg of subset is %f\\n\", world_rank, sub_avg);\n\n  \n\n  float *sub_avgs = NULL;\n  if (world_rank == 0) {\n    sub_avgs = (float *)malloc(sizeof(float) * world_size);\n  }\n  MPI_Gather(&sub_avg, 1, MPI_FLOAT, sub_avgs, 1, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n  \n\n  if (world_rank == 0) {\n    float avg = calc_avg(sub_avgs, world_size);\n    printf(\" Process %d: Avg of all elements is %f\\n\", world_rank, avg);\n    \n\n    float original_data_avg = calc_avg(rands, ELEMENTS_PER_PROC * world_size);\n    printf(\" Process %d: Avg of original data is %f\\n\", world_rank, original_data_avg);\n  }\n\n  if (world_rank == 0) {\n    free(rands);\n    free(sub_avgs);\n  }\n  free(buff_rands);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingl97_391", "code": "nprocs, myrank;\n    double a = 0.0, b = 1.0;\n    double eps, res0, res, a0, b0, cpu0, cpu1, wall0, wall1, wall2;\n\n\n    if (myrank == 0) {\n\tif (argc != 2) {\n\t    fprintf(stderr, \"Usage:   %s epsilon\\n\", argv[0]);\n\t    fprintf(stderr, \"Example: %s 1e-4\\n\", argv[0]);\n\t    return 1;\n\t}\n\teps = atof(argv[1]);\n    }\n\n    \n\n\n    \n\n    a0 = a + (myrank + 0) * (b - a) / nprocs;\n    b0 = a + (myrank + 1) * (b - a) / nprocs;\n\n    \n\n    gettime(&cpu0, &wall0);\n    res0 = integration(a0, F(a0), b0, F(b0), eps / (b - a), F);\n    gettime(&cpu1, &wall1);\n    printf(\"\\tRank=%d, # of evaluations=%u, cputime=%0.4lf, wtime=%0.4lf\\n\",\n\t   myrank, evaluation_count, cpu1 - cpu0, wall1 - wall0);\n    gettime(NULL, &wall2);\n    if (myrank == 0) {\n\tprintf(\"result=%0.16lf, error=%0.4le, wtime=%0.4lf\\n\",\n\t\tres, res - RESULT, wall2 - wall0);\n    }\n\n    return 0;\n}\n", "label": "nprocs, myrank;\n    double a = 0.0, b = 1.0;\n    double eps, res0, res, a0, b0, cpu0, cpu1, wall0, wall1, wall2;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n    if (myrank == 0) {\n\tif (argc != 2) {\n\t    fprintf(stderr, \"Usage:   %s epsilon\\n\", argv[0]);\n\t    fprintf(stderr, \"Example: %s 1e-4\\n\", argv[0]);\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t    return 1;\n\t}\n\teps = atof(argv[1]);\n    }\n\n    \n\n    MPI_Bcast(&eps, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    \n\n    a0 = a + (myrank + 0) * (b - a) / nprocs;\n    b0 = a + (myrank + 1) * (b - a) / nprocs;\n\n    \n\n    gettime(&cpu0, &wall0);\n    res0 = integration(a0, F(a0), b0, F(b0), eps / (b - a), F);\n    gettime(&cpu1, &wall1);\n    printf(\"\\tRank=%d, # of evaluations=%u, cputime=%0.4lf, wtime=%0.4lf\\n\",\n\t   myrank, evaluation_count, cpu1 - cpu0, wall1 - wall0);\n    MPI_Reduce(&res0, &res, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    gettime(NULL, &wall2);\n    if (myrank == 0) {\n\tprintf(\"result=%0.16lf, error=%0.4le, wtime=%0.4lf\\n\",\n\t\tres, res - RESULT, wall2 - wall0);\n    }\n\n    MPI_Finalize();\n    return 0;\n}\n"}
{"program": "tancheng_392", "code": "int main(int argc, char *argv[])\n{\n\tint rank;\n\tint n_ranks, start_rank;\n\tint i,j;\n\tfloat gamma = 0.25, rho = -0.495266;\n\tfloat GLOB_SUM = 0, sum = 0;\n\n\n\tprintf(\"before get data in id %d\\n\", rank);\n\n\tget_data(rank%4);\n\tstart_rank = 6;\n\tn_ranks = 4;\n\n\tprintf(\"done getting dat rank %d\\n\", rank);\n\n\n\n\t\n\tfor (j = 0; j < INPUT_SIZE; ++j)\n\t{\n\t\tget_input(rank, start_rank, n_ranks);\n\t\tsum = compute_svm_sum(rank%4, gamma);\n\t\tif(rank == start_rank)\n\t\t{\n\t\t\tfloat tempBuff;\n\t\t\tGLOB_SUM = sum;\n\t\t\tfor (i = start_rank+1; i < start_rank + n_ranks; ++i) {\n\t\t\t\tGLOB_SUM = GLOB_SUM + tempBuff;\n\t\t\t}\n\t\t\tGLOB_SUM -= rho;\n\t\t}\n\t\telse {\n\t\t}\n\t}\n\n\t\n\n\t\n\n\n\tif(rank == 6)\n\t{\n\t\t#ifdef DUMP\n\t\t\tm5_dump_stats(0, 0);\n\t\t\tm5_reset_stats(0, 0);\n\t\t#endif\n\t}\n\n\t\n\n\n\tif(rank == 6)\t\n\t\tprintf(\"global sum = %f\\n\", GLOB_SUM);\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint rank;\n\tint n_ranks, start_rank;\n\tint i,j;\n\tfloat gamma = 0.25, rho = -0.495266;\n\tfloat GLOB_SUM = 0, sum = 0;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tprintf(\"before get data in id %d\\n\", rank);\n\n\tget_data(rank%4);\n\tstart_rank = 6;\n\tn_ranks = 4;\n\n\tprintf(\"done getting dat rank %d\\n\", rank);\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\n\t\n\tfor (j = 0; j < INPUT_SIZE; ++j)\n\t{\n\t\tget_input(rank, start_rank, n_ranks);\n\t\tsum = compute_svm_sum(rank%4, gamma);\n\t\tif(rank == start_rank)\n\t\t{\n\t\t\tfloat tempBuff;\n\t\t\tGLOB_SUM = sum;\n\t\t\tfor (i = start_rank+1; i < start_rank + n_ranks; ++i) {\n\t\t\t\tMPI_Recv(&tempBuff, 1, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\t\tGLOB_SUM = GLOB_SUM + tempBuff;\n\t\t\t}\n\t\t\tGLOB_SUM -= rho;\n\t\t}\n\t\telse {\n\t\t\tMPI_Send((float*)&sum, 1, MPI_FLOAT, start_rank, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n\n\t\n\n\t\n\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif(rank == 6)\n\t{\n\t\t#ifdef DUMP\n\t\t\tm5_dump_stats(0, 0);\n\t\t\tm5_reset_stats(0, 0);\n\t\t#endif\n\t}\n\n\t\n\n\n\tif(rank == 6)\t\n\t\tprintf(\"global sum = %f\\n\", GLOB_SUM);\n\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "bmi-forum_394", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tDictionary*\t\tdictionary;\n\tGeometry*\t\tgeometry;\n\tElementLayout*\t\teLayout;\n\tTopology*\t\tnTopology;\n\tNodeLayout*\t\tnLayout;\n\tHexaMD*\t\t\tmeshDecomp;\n\tIndex\t\t\ti;\n\tProcessor_Index\t\tprocToWatch;\n\t\n\t\n\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\t\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 13 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tDictionary_Add( dictionary, \"maxX\", Dictionary_Entry_Value_FromUnsignedInt( 6 ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"isPeriodicI\", Dictionary_Entry_Value_FromBool( True ) );\n\n\t\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tmeshDecomp = HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\t\n\tElementLayout_Build( eLayout, meshDecomp );\n\t\n\tif (rank == procToWatch) {\n\t\tprintf( \"Element with point:\\n\" );\n\t}\n\tgeometry = eLayout->geometry;\n\tfor( i = 0; i < geometry->pointCount; i++ ) {\n\t\tCoord point;\n\t\tint excEl, incEl;\n\t\t\n\t\tgeometry->pointAt( geometry, i, point );\n\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tNode %u (%0.2f,%0.2f,%0.2f):\\n\", i, point[0], point[1], point[2] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\t\t\n\t\t}\n\n\t\tpoint[0] += 0.1;\n\t\tpoint[1] += 0.1;\n\t\tpoint[2] += 0.1;\n\t\t\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tTest point %u (%0.2f,%0.2f,%0.2f):\\n\", i, point[0], point[1], point[2] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\t\t\n\t\t}\n\t}\n\tif (rank == procToWatch) {\n\t\tprintf( \"\\n\" );\n\t}\t\n\t\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( meshDecomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( eLayout );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tDictionary*\t\tdictionary;\n\tGeometry*\t\tgeometry;\n\tElementLayout*\t\teLayout;\n\tTopology*\t\tnTopology;\n\tNodeLayout*\t\tnLayout;\n\tHexaMD*\t\t\tmeshDecomp;\n\tIndex\t\t\ti;\n\tProcessor_Index\t\tprocToWatch;\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size(CommWorld, &procCount);\n\tMPI_Comm_rank(CommWorld, &rank);\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\t\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 13 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tDictionary_Add( dictionary, \"maxX\", Dictionary_Entry_Value_FromUnsignedInt( 6 ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"isPeriodicI\", Dictionary_Entry_Value_FromBool( True ) );\n\n\t\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tmeshDecomp = HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\t\n\tElementLayout_Build( eLayout, meshDecomp );\n\t\n\tif (rank == procToWatch) {\n\t\tprintf( \"Element with point:\\n\" );\n\t}\n\tgeometry = eLayout->geometry;\n\tfor( i = 0; i < geometry->pointCount; i++ ) {\n\t\tCoord point;\n\t\tint excEl, incEl;\n\t\t\n\t\tgeometry->pointAt( geometry, i, point );\n\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tNode %u (%0.2f,%0.2f,%0.2f):\\n\", i, point[0], point[1], point[2] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\t\t\n\t\t}\n\n\t\tpoint[0] += 0.1;\n\t\tpoint[1] += 0.1;\n\t\tpoint[2] += 0.1;\n\t\t\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tTest point %u (%0.2f,%0.2f,%0.2f):\\n\", i, point[0], point[1], point[2] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\t\t\n\t\t}\n\t}\n\tif (rank == procToWatch) {\n\t\tprintf( \"\\n\" );\n\t}\t\n\t\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( meshDecomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( eLayout );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "cot_396", "code": "nt\nmain(int    argc,\n     char **argv)\n{\n        int ping_side;\n        int rank_dst;\n        int start_len      = LEN;\n        int iterations     = LOOPS;\n\tint i, j;\n\n\n\n        if (gethostname(host_name, 1023) < 0) {\n                perror(\"gethostname\");\n                exit(1);\n        }\n\n        if (comm_size < 2) {\n                fprintf(stderr, \"This program requires at least 2 MPI processes, aborting...\\n\");\n                goto out;\n        }\n\n        fprintf(stdout, \"(%s): My rank is %d\\n\", host_name, comm_rank);\n\n        ping_side\t= !(comm_rank & 1);\n\n\n\tmain_buffer = malloc(sizeof(unsigned char*) * NB_MSG);\n\tfor(i=0; i<NB_MSG; i++)\n\t  main_buffer[i] = malloc(start_len);\n\tfill_buffer(main_buffer, start_len);\n\n\tint size = 1024;\n\t\n\n\tfor(i = 0; i< iterations; i++) {\n\t  for(j=0;j<comm_rank; j++)\n\t    compute(100);\n\n\t\tif(! comm_rank) {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t} else {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t}\n\n\n\t\tcompute(100);\n\t\tif(! comm_rank) {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t} else {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t}\n\n\t}\n\n out:\n\tfor(i=0; i<NB_MSG; i++)\n\t  free(main_buffer[i]);\n\n        return 0;\n}\n", "label": "nt\nmain(int    argc,\n     char **argv)\n{\n        int ping_side;\n        int rank_dst;\n        int start_len      = LEN;\n        int iterations     = LOOPS;\n\tint i, j;\n\n\tMPI_Init(&argc,&argv);\n\n        MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n        MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n        if (gethostname(host_name, 1023) < 0) {\n                perror(\"gethostname\");\n                exit(1);\n        }\n\n        if (comm_size < 2) {\n                fprintf(stderr, \"This program requires at least 2 MPI processes, aborting...\\n\");\n                goto out;\n        }\n\n        fprintf(stdout, \"(%s): My rank is %d\\n\", host_name, comm_rank);\n\n        ping_side\t= !(comm_rank & 1);\n\n\n\tmain_buffer = malloc(sizeof(unsigned char*) * NB_MSG);\n\tfor(i=0; i<NB_MSG; i++)\n\t  main_buffer[i] = malloc(start_len);\n\tfill_buffer(main_buffer, start_len);\n\n\tint size = 1024;\n\t\n\n\tfor(i = 0; i< iterations; i++) {\n\t  for(j=0;j<comm_rank; j++)\n\t    compute(100);\n\n\t\tif(! comm_rank) {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t    MPI_Send(main_buffer[j], size, MPI_CHAR, (comm_rank+1)%comm_size, 0, MPI_COMM_WORLD);\n\t\t} else {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t    MPI_Recv(main_buffer[j], size, MPI_CHAR, (comm_rank+comm_size-1)%comm_size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\n\n\t\tcompute(100);\n\t\tif(! comm_rank) {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t    MPI_Recv(main_buffer[j], size, MPI_CHAR, (comm_rank+comm_size-1)%comm_size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t} else {\n\t\t  for(j=0; j<NB_MSG; j++)\n\t\t    MPI_Send(main_buffer[j], size, MPI_CHAR, (comm_rank+1)%comm_size, 0, MPI_COMM_WORLD);\n\t\t}\n\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n out:\n\tfor(i=0; i<NB_MSG; i++)\n\t  free(main_buffer[i]);\n        MPI_Finalize();\n\n        return 0;\n}\n"}
{"program": "chu11_397", "code": "int main(int argc, char *argv[])\n{\n    int myid, numprocs, next, namelen;\n    char buffer[BUFLEN], processor_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Status status;\n\n    fprintf(stderr, \"Process %d of %d is alive on %s\\n\", myid, numprocs, processor_name);\n\n    strcpy(buffer, \"hello there\");\n    if (myid == numprocs - 1)\n        next = 0;\n    else\n        next = myid + 1;\n\n    if (myid == 0) {\n        printf(\"%d sending '%s' \\n\", myid, buffer);\n        printf(\"%d receiving \\n\", myid);\n        printf(\"%d received '%s' \\n\", myid, buffer);\n        \n\n    } else {\n        printf(\"%d receiving  \\n\", myid);\n        printf(\"%d received '%s' \\n\", myid, buffer);\n        \n\n        printf(\"%d sent '%s' \\n\", myid, buffer);\n    }\n    return (0);\n}", "label": "int main(int argc, char *argv[])\n{\n    int myid, numprocs, next, namelen;\n    char buffer[BUFLEN], processor_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n    MPI_Get_processor_name(processor_name, &namelen);\n    fprintf(stderr, \"Process %d of %d is alive on %s\\n\", myid, numprocs, processor_name);\n\n    strcpy(buffer, \"hello there\");\n    if (myid == numprocs - 1)\n        next = 0;\n    else\n        next = myid + 1;\n\n    if (myid == 0) {\n        printf(\"%d sending '%s' \\n\", myid, buffer);\n        MPI_Send(buffer, (int) strlen(buffer) + 1, MPI_CHAR, next, 99, MPI_COMM_WORLD);\n        printf(\"%d receiving \\n\", myid);\n        MPI_Recv(buffer, BUFLEN, MPI_CHAR, MPI_ANY_SOURCE, 99, MPI_COMM_WORLD, &status);\n        printf(\"%d received '%s' \\n\", myid, buffer);\n        \n\n    } else {\n        printf(\"%d receiving  \\n\", myid);\n        MPI_Recv(buffer, BUFLEN, MPI_CHAR, MPI_ANY_SOURCE, 99, MPI_COMM_WORLD, &status);\n        printf(\"%d received '%s' \\n\", myid, buffer);\n        \n\n        MPI_Send(buffer, (int) strlen(buffer) + 1, MPI_CHAR, next, 99, MPI_COMM_WORLD);\n        printf(\"%d sent '%s' \\n\", myid, buffer);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Finalize();\n    return (0);\n}"}
{"program": "parthsl_398", "code": "int main(int argc, char** argv) \n\n{\n\tint num_procs, num_local;\n\tnd total_cities = 0;\n\tnd* min_circuit = NULL;\n\tnd total_threads_available=1;\n\n\n\tif(num_local==master)\n\t\tprintf(\"Parallel ranks=%d\\n\",num_procs);\n\n\tsrand(0);\n\n\tif(num_local == master && argc<2) {\n\t\tfprintf(stderr,\"Provide input file\\n\");\n\t\treturn 0;\n\t}\n\tif(num_local == master) {\n\t\tif( readGraph(argv[1],&total_cities)==EAGAIN) return EINVAL;\n\t}\n\n\t\n\n\n\tif(num_local != master) {\n\t\tG = (double**)malloc(2*sizeof(double*));\n\t\tfor(nd i=0; i<2; i++)\n\t\t\tG[i] = (double*)malloc(sizeof(double)*(total_cities+1));\n\t}\n\n\n\tif(argc>2)omp_set_num_threads(atoi(argv[2]));\n\n\tdouble stime = 0;\n\tif(num_local==master) stime = omp_get_wtime();\n\n\n\t#pragma omp parallel\n\t{\n\t\ttotal_threads_available = omp_get_num_threads();\n\t}\n\n#ifdef DEBUG\n\tprintf(\"%d: Threads spawn: %ld\\n\",num_local, total_threads_available);\n#endif\n\n\t\n\n\tif(num_local==master) {\n\t\tomp_set_num_threads(8);\n\t\tmin_circuit = VNN(G,total_cities,0);\n\t\tprintf(\"Tour length after VNN : %lf\\n\",find_tour_length(G,min_circuit,total_cities));\n\t\tomp_set_num_threads(total_threads_available);\n\t}\n\telse {\n\t\tmin_circuit = (nd*)malloc(sizeof(nd)*(total_cities+1));\n\t}\n\n\tchar mach_name[100];\n\tint mach_len;\n\tprintf(\"%d:%s\\n\",num_local, mach_name);\n\t\n\n\n#ifdef DEBUG\n\tif(num_local==master)\n\t\tprintf(\"Broadcasted Min_circuit id=%d\\n\",num_local);\n#endif\n\n\t\n\n\n\tif(num_local != master) {\n\t\ttwo_opt_max_swap(G, min_circuit,total_cities, num_procs, num_local);\n\t}\n\telse {\n\t\tnd counter = 0;\n\t\tdouble tour_length;\n\t\tcounter += two_opt_max_swap(G, min_circuit,total_cities, num_procs, num_local);\n\t\ttour_length = find_tour_length(G,min_circuit,total_cities);\n\t\tprintf(\"Hills climbed = %ld\\n\",counter);\n\t\tprintf(\"Min distance = %lf\\n\",tour_length);\n\t\tprintf(\"Time taken = %lf\\n\",omp_get_wtime()-stime);\n\n#ifdef DEBUG\n\t\tprint_tour(G,min_circuit,total_cities);\n#endif\n\t}\n\tfree(min_circuit);\n\tfree(G[0]);\n\tfree(G[1]);\n\treturn 0;\n}", "label": "int main(int argc, char** argv) \n\n{\n\tint num_procs, num_local;\n\tnd total_cities = 0;\n\tnd* min_circuit = NULL;\n\tnd total_threads_available=1;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\tMPI_Comm_rank (MPI_COMM_WORLD, &num_local);\n\n\tif(num_local==master)\n\t\tprintf(\"Parallel ranks=%d\\n\",num_procs);\n\n\tsrand(0);\n\n\tif(num_local == master && argc<2) {\n\t\tfprintf(stderr,\"Provide input file\\n\");\n\t\treturn 0;\n\t}\n\tif(num_local == master) {\n\t\tif( readGraph(argv[1],&total_cities)==EAGAIN) return EINVAL;\n\t}\n\n\t\n\n\tMPI_Bcast(&total_cities, 1, MPI_LONG_LONG, master, MPI_COMM_WORLD);\n\n\tif(num_local != master) {\n\t\tG = (double**)malloc(2*sizeof(double*));\n\t\tfor(nd i=0; i<2; i++)\n\t\t\tG[i] = (double*)malloc(sizeof(double)*(total_cities+1));\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tMPI_Bcast(&G[0][0],total_cities+1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(&G[1][0],total_cities+1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tif(argc>2)omp_set_num_threads(atoi(argv[2]));\n\n\tdouble stime = 0;\n\tif(num_local==master) stime = omp_get_wtime();\n\n\n\t#pragma omp parallel\n\t{\n\t\ttotal_threads_available = omp_get_num_threads();\n\t}\n\n#ifdef DEBUG\n\tprintf(\"%d: Threads spawn: %ld\\n\",num_local, total_threads_available);\n#endif\n\n\t\n\n\tif(num_local==master) {\n\t\tomp_set_num_threads(8);\n\t\tmin_circuit = VNN(G,total_cities,0);\n\t\tprintf(\"Tour length after VNN : %lf\\n\",find_tour_length(G,min_circuit,total_cities));\n\t\tomp_set_num_threads(total_threads_available);\n\t}\n\telse {\n\t\tmin_circuit = (nd*)malloc(sizeof(nd)*(total_cities+1));\n\t}\n\n\tchar mach_name[100];\n\tint mach_len;\n\tMPI_Get_processor_name(mach_name,&mach_len);\n\tprintf(\"%d:%s\\n\",num_local, mach_name);\n\t\n\n\tMPI_Bcast(min_circuit,total_cities+1, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n#ifdef DEBUG\n\tif(num_local==master)\n\t\tprintf(\"Broadcasted Min_circuit id=%d\\n\",num_local);\n#endif\n\n\t\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tif(num_local != master) {\n\t\ttwo_opt_max_swap(G, min_circuit,total_cities, num_procs, num_local);\n\t}\n\telse {\n\t\tnd counter = 0;\n\t\tdouble tour_length;\n\t\tcounter += two_opt_max_swap(G, min_circuit,total_cities, num_procs, num_local);\n\t\ttour_length = find_tour_length(G,min_circuit,total_cities);\n\t\tprintf(\"Hills climbed = %ld\\n\",counter);\n\t\tprintf(\"Min distance = %lf\\n\",tour_length);\n\t\tprintf(\"Time taken = %lf\\n\",omp_get_wtime()-stime);\n\n#ifdef DEBUG\n\t\tprint_tour(G,min_circuit,total_cities);\n#endif\n\t}\n\tMPI_Finalize();\n\tfree(min_circuit);\n\tfree(G[0]);\n\tfree(G[1]);\n\treturn 0;\n}"}
{"program": "michel2323_399", "code": "int main(int narg, char *arg[])\n{\n  comm_ext world; int np;\n  struct comm comm;\n  \n#ifdef MPI\n  world = MPI_COMM_WORLD;\n#else\n  world=0, np=1;\n#endif\n\n  comm_init(&comm,world);\n\n  test(&comm);\n  \n  comm_free(&comm);\n\n#ifdef MPI\n#endif\n\n  return 0;\n}", "label": "int main(int narg, char *arg[])\n{\n  comm_ext world; int np;\n  struct comm comm;\n  \n#ifdef MPI\n  MPI_Init(&narg,&arg);\n  world = MPI_COMM_WORLD;\n  MPI_Comm_size(world,&np);\n#else\n  world=0, np=1;\n#endif\n\n  comm_init(&comm,world);\n\n  test(&comm);\n  \n  comm_free(&comm);\n\n#ifdef MPI\n  MPI_Finalize();\n#endif\n\n  return 0;\n}"}
{"program": "yuwen41200_400", "code": "int main(int argc, char *argv[]) {\n\tint rank, size;\n\tchar msg[100];\n\tMPI_Status stat;\n\n\n\tif (rank != 0) {\n\t\tsprintf(msg, \"process %d\\n\", rank);\n\t}\n\telse {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tprintf(\"%s\", msg);\n\t\t}\n\t\tprintf(\"process %d\\n\", rank);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tint rank, size;\n\tchar msg[100];\n\tMPI_Status stat;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (rank != 0) {\n\t\tsprintf(msg, \"process %d\\n\", rank);\n\t\tMPI_Send(msg, strlen(msg) + 1, MPI_CHAR, \n 0, \n 0, MPI_COMM_WORLD);\n\t}\n\telse {\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Recv(msg, 100, MPI_CHAR, i, \n 0, MPI_COMM_WORLD, &stat);\n\t\t\tprintf(\"%s\", msg);\n\t\t}\n\t\tprintf(\"process %d\\n\", rank);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "abanaiyan_403", "code": "int main( int argc, char *argv[] )\n{\n   int n, myid, numprocs, i;\n   double PI25DT = 3.141592653589793238462643;\n   double mypi, pi, h, sum, x;\n\n\n   n = 100000;\n\n   SimRoiStart();\n\n   h   = 1.0 / (double) n;\n   sum = 0.0;\n   for (i = myid + 1; i <= n; i += numprocs)\n   {\n      x = h * ((double)i - 0.5);\n      sum += (4.0 / (1.0 + x*x));\n   }\n   mypi = h * sum;\n\n   printf(\"local sum for thread %d is %f\\n\", myid, mypi);\n\n   if (myid == 0)\n      printf(\"pi is approximately %.16f, Error is %.16f\\n\", pi, fabs(pi - PI25DT));\n\n   SimRoiEnd();\n\n   return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n   int n, myid, numprocs, i;\n   double PI25DT = 3.141592653589793238462643;\n   double mypi, pi, h, sum, x;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n   n = 100000;\n\n   SimRoiStart();\n\n   MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   h   = 1.0 / (double) n;\n   sum = 0.0;\n   for (i = myid + 1; i <= n; i += numprocs)\n   {\n      x = h * ((double)i - 0.5);\n      sum += (4.0 / (1.0 + x*x));\n   }\n   mypi = h * sum;\n\n   printf(\"local sum for thread %d is %f\\n\", myid, mypi);\n   MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (myid == 0)\n      printf(\"pi is approximately %.16f, Error is %.16f\\n\", pi, fabs(pi - PI25DT));\n\n   SimRoiEnd();\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "rahlk_404", "code": "int\nmain (int argc, char **argv)\n{\n  int nprocs = -1;\n  int rank = -1;\n  float data = 0.0;\n  int tag = 30;\n  char processor_name[128];\n  int namelen = 128;\n  int j;\n  const int loopLimit = 10;\n  int markerID;\n\n  \n\n\n  printf (\"Initializing (%d of %d)\\n\", rank, nprocs);\n  printf (\"(%d) is alive on %s\\n\", rank, processor_name);\n\n  \n\n  if (rank == 0)\n    {\n      printf (\"Some Sends and Receives...\\n\");\n    }\n  for (j = 0; j < loopLimit; j++)\n    {\n      if ((rank % 2) == 0)\n\t{\n\t  int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n\t  int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n\t  MPI_Status status;\n\t  data = rank;\n\t  printf (\"(%d) sent data %f\\n\", rank, data);\n\t  fflush (stdout);\n\n\t  printf (\"(%d) got data %f\\n\", rank, data);\n\t  fflush (stdout);\n\t}\n      else\n\t{\n\t  int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n\t  int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n\t  MPI_Status status;\n\t  printf (\"(%d) got data %f\\n\", rank, data);\n\t  fflush (stdout);\n\n\t  data = rank;\n\t  printf (\"(%d) sent data %f\\n\", rank, data);\n\t  fflush (stdout);\n\t}\n    }\n\n  \n\n  \n\n  if (rank == 0)\n    {\n      printf (\"Some broadcasts...\\n\");\n    }\n  {\n    for (j = 0; j < loopLimit; j++)\n      {\n\tint root = 0;\n\tif (rank == 0)\t\t\n\n\t  {\n\t    data = j;\n\t    printf (\"(%d) bcast data %f\\n\", rank, data);\n\t    fflush (stdout);\n\t  }\n\t\n\n      }\n  }\n\n  \n\n  \n\n  if (rank == 0)\n    {\n      printf (\"Some reductions...\\n\");\n    }\n  {\n    float iData = 0.0;\n    float oData = 0.0;\n\n    \n\n    for (j = 0; j < loopLimit; j++)\n      {\n\tiData = rank;\n\tif (rank == 0)\n\t  {\n\t    printf (\"(%d) reduction total = %f\\n\", rank, oData);\n\t  }\n\t\n\n      }\n  }\n\n  \n\n  printf (\"(%d) Finished normally\\n\", rank);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int nprocs = -1;\n  int rank = -1;\n  float data = 0.0;\n  int tag = 30;\n  char processor_name[128];\n  int namelen = 128;\n  int j;\n  const int loopLimit = 10;\n  int markerID;\n\n  \n\n  MPI_Init (&argc, &argv);\n\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  printf (\"Initializing (%d of %d)\\n\", rank, nprocs);\n  MPI_Get_processor_name (processor_name, &namelen);\n  printf (\"(%d) is alive on %s\\n\", rank, processor_name);\n\n  \n\n  if (rank == 0)\n    {\n      printf (\"Some Sends and Receives...\\n\");\n    }\n  for (j = 0; j < loopLimit; j++)\n    {\n      MPI_Barrier (MPI_COMM_WORLD);\n      if ((rank % 2) == 0)\n\t{\n\t  int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n\t  int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n\t  MPI_Status status;\n\t  data = rank;\n\t  MPI_Send (&data, 1, MPI_FLOAT, dest, tag, MPI_COMM_WORLD);\n\t  printf (\"(%d) sent data %f\\n\", rank, data);\n\t  fflush (stdout);\n\n\t  MPI_Recv (&data, 1, MPI_FLOAT, src, tag, MPI_COMM_WORLD, &status);\n\t  printf (\"(%d) got data %f\\n\", rank, data);\n\t  fflush (stdout);\n\t}\n      else\n\t{\n\t  int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n\t  int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n\t  MPI_Status status;\n\t  MPI_Recv (&data, 1, MPI_FLOAT, src, tag, MPI_COMM_WORLD, &status);\n\t  printf (\"(%d) got data %f\\n\", rank, data);\n\t  fflush (stdout);\n\n\t  data = rank;\n\t  MPI_Send (&data, 1, MPI_FLOAT, dest, tag, MPI_COMM_WORLD);\n\t  printf (\"(%d) sent data %f\\n\", rank, data);\n\t  fflush (stdout);\n\t}\n    }\n\n  MPI_Barrier (MPI_COMM_WORLD);\n  \n\n  \n\n  if (rank == 0)\n    {\n      printf (\"Some broadcasts...\\n\");\n    }\n  {\n    for (j = 0; j < loopLimit; j++)\n      {\n\tint root = 0;\n\tif (rank == 0)\t\t\n\n\t  {\n\t    data = j;\n\t    printf (\"(%d) bcast data %f\\n\", rank, data);\n\t    fflush (stdout);\n\t  }\n\tMPI_Bcast (&data, 1, MPI_FLOAT, root, MPI_COMM_WORLD);\n\t\n\n      }\n  }\n\n  MPI_Barrier (MPI_COMM_WORLD);\n  \n\n  \n\n  if (rank == 0)\n    {\n      printf (\"Some reductions...\\n\");\n    }\n  {\n    float iData = 0.0;\n    float oData = 0.0;\n\n    \n\n    for (j = 0; j < loopLimit; j++)\n      {\n\tiData = rank;\n\tMPI_Reduce ((void *) &iData, (void *) &oData, 1, MPI_FLOAT, MPI_SUM,\n\t\t    0, MPI_COMM_WORLD);\n\tif (rank == 0)\n\t  {\n\t    printf (\"(%d) reduction total = %f\\n\", rank, oData);\n\t  }\n\t\n\n      }\n  }\n\n  \n\n  MPI_Finalize ();\n  printf (\"(%d) Finished normally\\n\", rank);\n\n  return 0;\n}"}
{"program": "Kitware_405", "code": "int main(int argc, char* argv[])\n{\n  Grid grid = (Grid) { .NumberOfPoints = 0, .Points = 0, .NumberOfCells = 0, .Cells = 0};\n  unsigned int numPoints[3] = {70, 60, 44};\n  double spacing[3] = {1, 1.1, 1.3};\n  InitializeGrid(&grid, numPoints, spacing);\n  Attributes attributes;\n  InitializeAttributes(&attributes, &grid);\n\n#ifdef USE_CATALYST\n  CatalystInitialize(argc, argv);\n#endif\n  unsigned int numberOfTimeSteps = 100;\n  unsigned int timeStep;\n  for(timeStep=0;timeStep<numberOfTimeSteps;timeStep++)\n    {\n    \n\n    double time = timeStep * 0.1;\n    UpdateFields(&attributes, time);\n#ifdef USE_CATALYST\n    int lastTimeStep = 0;\n    if(timeStep == numberOfTimeSteps-1)\n      {\n      lastTimeStep = 1;\n      }\n    CatalystCoProcess(grid.NumberOfPoints, grid.Points, grid.NumberOfCells, grid.Cells,\n                      attributes.Velocity, attributes.Pressure, time, timeStep, lastTimeStep);\n#endif\n    }\n\n#ifdef USE_CATALYST\n  CatalystFinalize();\n#endif\n\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  MPI_Init(&argc, &argv);\n  Grid grid = (Grid) { .NumberOfPoints = 0, .Points = 0, .NumberOfCells = 0, .Cells = 0};\n  unsigned int numPoints[3] = {70, 60, 44};\n  double spacing[3] = {1, 1.1, 1.3};\n  InitializeGrid(&grid, numPoints, spacing);\n  Attributes attributes;\n  InitializeAttributes(&attributes, &grid);\n\n#ifdef USE_CATALYST\n  CatalystInitialize(argc, argv);\n#endif\n  unsigned int numberOfTimeSteps = 100;\n  unsigned int timeStep;\n  for(timeStep=0;timeStep<numberOfTimeSteps;timeStep++)\n    {\n    \n\n    double time = timeStep * 0.1;\n    UpdateFields(&attributes, time);\n#ifdef USE_CATALYST\n    int lastTimeStep = 0;\n    if(timeStep == numberOfTimeSteps-1)\n      {\n      lastTimeStep = 1;\n      }\n    CatalystCoProcess(grid.NumberOfPoints, grid.Points, grid.NumberOfCells, grid.Cells,\n                      attributes.Velocity, attributes.Pressure, time, timeStep, lastTimeStep);\n#endif\n    }\n\n#ifdef USE_CATALYST\n  CatalystFinalize();\n#endif\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "jiangxincode_407", "code": "int main(int argc, char **argv)\n{\n\tdtype **a; \n\n\tdtype *b; \n\n\tdtype *c_block; \n\n\tdtype *c; \n\n\tdtype *storage; \n\n\tint i,j;\n\tint id; \n\n\tint p; \n\n\tint m,n; \n\n\tint nprime; \n\n\tint rows; \n\n\n\tdouble elapsed_time;\n\n\n\n\tread_row_striped_matrix(argv[1], (void*)&a, (void*)&storage, mpitype, &m ,&n, MPI_COMM_WORLD);\n\trows = BLOCK_SIZE(id, p, m);\n\t\n\n\tread_replicated_vector(argv[2], (void*)&b, mpitype, &nprime, MPI_COMM_WORLD);\n\t\n\n\n\telapsed_time = -\n\n\tc_block = (dtype*)malloc(rows*sizeof(dtype));\n\tc = (dtype*)malloc(n*sizeof(dtype));\n\tfor(i=0; i<rows; i++)\n\t{\n\t\tc_block[i] = 0.0;\n\t\tfor(j=0; j<n; j++)\n\t\t{\n\t\t\tc_block[i] += a[i][j]*b[j];\n\t\t}\n\t}\n\treplicate_block_vector(c_block, n, (void*)c, mpitype, MPI_COMM_WORLD);\n\t\n\n\n\t\n\n\n\telapsed_time +=\n\tif(id == 0)\n\t{\n\t\tprintf(\"This program cost %f ms\\n\", elapsed_time);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tdtype **a; \n\n\tdtype *b; \n\n\tdtype *c_block; \n\n\tdtype *c; \n\n\tdtype *storage; \n\n\tint i,j;\n\tint id; \n\n\tint p; \n\n\tint m,n; \n\n\tint nprime; \n\n\tint rows; \n\n\n\tdouble elapsed_time;\n\n\tMPI_Init(&argc, &argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &id);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\n\tread_row_striped_matrix(argv[1], (void*)&a, (void*)&storage, mpitype, &m ,&n, MPI_COMM_WORLD);\n\trows = BLOCK_SIZE(id, p, m);\n\t\n\n\tread_replicated_vector(argv[2], (void*)&b, mpitype, &nprime, MPI_COMM_WORLD);\n\t\n\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\telapsed_time = - MPI_Wtime();\n\n\tc_block = (dtype*)malloc(rows*sizeof(dtype));\n\tc = (dtype*)malloc(n*sizeof(dtype));\n\tfor(i=0; i<rows; i++)\n\t{\n\t\tc_block[i] = 0.0;\n\t\tfor(j=0; j<n; j++)\n\t\t{\n\t\t\tc_block[i] += a[i][j]*b[j];\n\t\t}\n\t}\n\treplicate_block_vector(c_block, n, (void*)c, mpitype, MPI_COMM_WORLD);\n\t\n\n\n\t\n\n\n\telapsed_time += MPI_Wtime();\n\tif(id == 0)\n\t{\n\t\tprintf(\"This program cost %f ms\\n\", elapsed_time);\n\t}\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "linhbngo_409", "code": "int main(int argc, char **argv)\n{\n  int *buf, i, rank, nints, len;\n  char *filename, *tmp;\n  int  errs = 0, toterrs;\n  MPI_File fh;\n  MPI_Status status;\n\n\n  \n\n  if (!rank) {\n    i = 1;\n    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n      i++;\n      argv++;\n    }\n    if (i >= argc) {\n      fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n    }\n    argv++;\n    len = strlen(*argv);\n    filename = (char *) malloc(len+10);\n    strcpy(filename, *argv);\n  } else {\n    filename = (char *) malloc(len+10);\n  }\n\n  buf = (int *) malloc(SIZE);\n  nints = SIZE/sizeof(int);\n  for (i=0; i<nints; i++){\n    buf[i] = rank*100000 + i;\n  }\n\n  \n\n  tmp = (char *) malloc(len+10);\n  strcpy(tmp, filename);\n  sprintf(filename, \"%s.%d\", tmp, rank);\n\n\n  \n\n  for (i=0; i<nints; i++){\n    buf[i] = 0;\n  }\n    \n\n  \n\n  for (i=0; i<nints; i++) {\n    if (buf[i] != (rank*100000 + i)) {\n      errs++;\n      fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n    }\n  }\n\n  if (rank == 0) {\n    if( toterrs > 0) {\n      fprintf( stderr, \"Found %d errors\\n\", toterrs );\n    }\n    else {\n      fprintf( stdout, \" No Errors\\n\" );\n    }\n  }\n\n  free(buf);\n  free(filename);\n  free(tmp);\n\n  return 0; \n}", "label": "int main(int argc, char **argv)\n{\n  int *buf, i, rank, nints, len;\n  char *filename, *tmp;\n  int  errs = 0, toterrs;\n  MPI_File fh;\n  MPI_Status status;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  if (!rank) {\n    i = 1;\n    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n      i++;\n      argv++;\n    }\n    if (i >= argc) {\n      fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n      MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    argv++;\n    len = strlen(*argv);\n    filename = (char *) malloc(len+10);\n    strcpy(filename, *argv);\n    MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    filename = (char *) malloc(len+10);\n    MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n  }\n\n  buf = (int *) malloc(SIZE);\n  nints = SIZE/sizeof(int);\n  for (i=0; i<nints; i++){\n    buf[i] = rank*100000 + i;\n  }\n\n  \n\n  tmp = (char *) malloc(len+10);\n  strcpy(tmp, filename);\n  sprintf(filename, \"%s.%d\", tmp, rank);\n\n  MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR,MPI_INFO_NULL, &fh);\n  MPI_File_write(fh, buf, nints, MPI_INT, &status);\n  MPI_File_close(&fh);\n\n  \n\n  for (i=0; i<nints; i++){\n    buf[i] = 0;\n  }\n    \n  MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n  MPI_File_read(fh, buf, nints, MPI_INT, &status);\n  MPI_File_close(&fh);\n\n  \n\n  for (i=0; i<nints; i++) {\n    if (buf[i] != (rank*100000 + i)) {\n      errs++;\n      fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n    }\n  }\n\n  MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n  if (rank == 0) {\n    if( toterrs > 0) {\n      fprintf( stderr, \"Found %d errors\\n\", toterrs );\n    }\n    else {\n      fprintf( stdout, \" No Errors\\n\" );\n    }\n  }\n\n  free(buf);\n  free(filename);\n  free(tmp);\n\n  MPI_Finalize();\n  return 0; \n}"}
{"program": "fmihpc_411", "code": "int main(int argc,char **argv){\n\n   int rank,i;\n   int nIterations=1000000;\n\n   if(rank==0)\n      printf( \"Measuring performance of start-stop calls\\n\");\n\n   phiprof_initialize();\n   \n\n   phiprof_start(\"Benchmarking phiprof\"); \n   phiprof_start(\"Initalized timers using ID\");\n\n   if(rank==0)\n      printf( \"  1/3\\n\" );\n\n   int id_a=phiprof_initializeTimerWithGroups1(\"a\",\"A with ID\");\n   for(i=0;i<nIterations;i++){\n      phiprof_startId(id_a);\n      phiprof_stopId(id_a);\n   }\n   phiprof_stopUnits(\"Initalized timers using ID\",nIterations,\"start-stop\");\n\n   if(rank==0)\n      printf( \"  2/3\\n\" );\n   phiprof_start(\"Re-initialized timers using ID\");\n   for(i=0;i<nIterations;i++){\n     int id_a=phiprof_initializeTimerWithGroups1(\"a\",\"A with ID\");\n     phiprof_startId(id_a);\n     phiprof_stopId(id_a);\n   }\n   phiprof_stopUnits(\"Re-initialized timers using ID\",nIterations,\"start-stop\");\n\n   if(rank==0)\n      printf( \"  3/3\\n\" );\n   phiprof_start(\"Timers using labels\");\n   for(i=0;i<nIterations;i++){\n      phiprof_start(\"a\");\n      phiprof_stop(\"a\");\n   }\n   phiprof_stopUnits(\"Timers using labels\",nIterations*2,\"start-stop\");\n   phiprof_stop(\"Benchmarking phiprof\"); \n\n\n   if(rank==0)\n      printf( \"Measuring accuracy\\n\" );\n   phiprof_start(\"Test accuracy\");\n   if(rank==0)\n      printf( \"  1/2\\n\");\n   phiprof_start(\"100 computations x 0.1s\"); \n   for(i=0;i<100;i++){\n      phiprof_start(\"compute\");\n      compute(0.1);\n      phiprof_stop(\"compute\");\n   }\n   phiprof_stop(\"100 computations x 0.1s\");\n\n\n   phiprof_stop(\"Test accuracy\");\n   \n   phiprof_print(MPI_COMM_WORLD,\"profile\");\n\n   if(rank==0)   \n\n\n}", "label": "int main(int argc,char **argv){\n\n   int rank,i;\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n   int nIterations=1000000;\n\n   if(rank==0)\n      printf( \"Measuring performance of start-stop calls\\n\");\n\n   phiprof_initialize();\n   \n\n   phiprof_start(\"Benchmarking phiprof\"); \n   phiprof_start(\"Initalized timers using ID\");\n\n   if(rank==0)\n      printf( \"  1/3\\n\" );\n\n   int id_a=phiprof_initializeTimerWithGroups1(\"a\",\"A with ID\");\n   for(i=0;i<nIterations;i++){\n      phiprof_startId(id_a);\n      phiprof_stopId(id_a);\n   }\n   phiprof_stopUnits(\"Initalized timers using ID\",nIterations,\"start-stop\");\n\n   if(rank==0)\n      printf( \"  2/3\\n\" );\n   phiprof_start(\"Re-initialized timers using ID\");\n   for(i=0;i<nIterations;i++){\n     int id_a=phiprof_initializeTimerWithGroups1(\"a\",\"A with ID\");\n     phiprof_startId(id_a);\n     phiprof_stopId(id_a);\n   }\n   phiprof_stopUnits(\"Re-initialized timers using ID\",nIterations,\"start-stop\");\n\n   if(rank==0)\n      printf( \"  3/3\\n\" );\n   phiprof_start(\"Timers using labels\");\n   for(i=0;i<nIterations;i++){\n      phiprof_start(\"a\");\n      phiprof_stop(\"a\");\n   }\n   phiprof_stopUnits(\"Timers using labels\",nIterations*2,\"start-stop\");\n   phiprof_stop(\"Benchmarking phiprof\"); \n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   if(rank==0)\n      printf( \"Measuring accuracy\\n\" );\n   phiprof_start(\"Test accuracy\");\n   if(rank==0)\n      printf( \"  1/2\\n\");\n   phiprof_start(\"100 computations x 0.1s\"); \n   for(i=0;i<100;i++){\n      phiprof_start(\"compute\");\n      compute(0.1);\n      phiprof_stop(\"compute\");\n   }\n   phiprof_stop(\"100 computations x 0.1s\");\n\n\n   phiprof_stop(\"Test accuracy\");\n   \n   MPI_Barrier(MPI_COMM_WORLD);\n   double t1=MPI_Wtime();\n   phiprof_print(MPI_COMM_WORLD,\"profile\");\n\n   if(rank==0)   \n      printf( \"Print time is %g\\n\",MPI_Wtime()-t1);\n\n\n   MPI_Finalize();\n}"}
{"program": "bmi-forum_413", "code": "int main(int argc, char *argv[])\n{\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tStream*\t\t\tstream;\n\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\n\tRegressionTest_Init( \"Base/Foundation/CommonRoutines\" );\n\n\tstream = Journal_Register( \"info\", \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\n\tif ( rank == procToWatch ) {\n\t\tconst char* fiftyBytes = \"01234567890123456789012345678901234567890123456789\";\n\n\t\tchar* testString;\n\n\t\tJournal_Printf( stream, \"Stress testing Stg_asprintf beyond the default alloc number of bytes\\n\" );\n\n\t\tStg_asprintf( &testString, \"%s%s%s%s\", fiftyBytes, fiftyBytes, fiftyBytes, fiftyBytes );\n\n\t\tJournal_Printf( stream, \"%s\\n\", testString );\n\n\t\tMemory_Free( testString );\n\t}\n\n\tRegressionTest_Finalise();\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main(int argc, char *argv[])\n{\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tStream*\t\t\tstream;\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tBaseFoundation_Init( &argc, &argv );\n\n\tRegressionTest_Init( \"Base/Foundation/CommonRoutines\" );\n\n\tstream = Journal_Register( \"info\", \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\n\tif ( rank == procToWatch ) {\n\t\tconst char* fiftyBytes = \"01234567890123456789012345678901234567890123456789\";\n\n\t\tchar* testString;\n\n\t\tJournal_Printf( stream, \"Stress testing Stg_asprintf beyond the default alloc number of bytes\\n\" );\n\n\t\tStg_asprintf( &testString, \"%s%s%s%s\", fiftyBytes, fiftyBytes, fiftyBytes, fiftyBytes );\n\n\t\tJournal_Printf( stream, \"%s\\n\", testString );\n\n\t\tMemory_Free( testString );\n\t}\n\n\tRegressionTest_Finalise();\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "joeladams_414", "code": "int main(int argc, char** argv) {\n    int answer = 0, length = 0;\n    int myRank = 0;\n\n    char myHostName[MPI_MAX_PROCESSOR_NAME];\n\n\n    printf(\"BEFORE the broadcast, process %d on host '%s' has answer = %d\\n\",\n             myRank, myHostName, answer);\n\n    getInput(argc, argv, myRank, &answer);\n\n    printf(\"AFTER the broadcast, process %d on host '%s' has answer = %d\\n\",\n             myRank, myHostName, answer);\n\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int answer = 0, length = 0;\n    int myRank = 0;\n\n    char myHostName[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Get_processor_name (myHostName, &length);\n\n    printf(\"BEFORE the broadcast, process %d on host '%s' has answer = %d\\n\",\n             myRank, myHostName, answer);\n\n    getInput(argc, argv, myRank, &answer);\n\n    printf(\"AFTER the broadcast, process %d on host '%s' has answer = %d\\n\",\n             myRank, myHostName, answer);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "sdsc_415", "code": "int main(int argc, char *argv[])\n{\n        int i, iterations = 1, c;\n\n        setbuf(stdout, 0);\n        setbuf(stderr, 0);\n\n        \n\n        for (i = 1; i < argc; ++i) {\n                if (!strcmp(argv[i], \"-h\") || !strcmp(argv[i], \"--help\"))\n                        usage(argv[0]);\n        }\n\n\n        \n\n        while (1) {\n                c = getopt(argc, argv, \"d:ghn:t:vV:\");\n                if (c == -1)\n                        break;\n\n                switch (c) {\n                case 'd':\n                        testdir = optarg;\n                        break;\n                case 'g':\n                        debug = 1;\n                        break;\n                case 'h':\n                        usage(argv[0]);\n                        break;\n                case 'n':\n                        iterations = atoi(optarg);\n                        break;\n                case 't':\n                        only_test = atoi(optarg);\n                        break;\n                case 'v':\n                        verbose += 1;\n                        break;\n                case 'V':\n                        verbose = atoi(optarg);\n                        break;\n                }\n        }\n\n        if (rank == 0)\n                printf(\"%s is running with %d task(es) %s\\n\",\n                       argv[0], size, debug ? \"in DEBUG mode\" : \"\\b\\b\");\n\n        if (size < MIN_GLHOST) {\n                fprintf(stderr, \"Error: \"\n                        \"%d tasks run, but should be at least %d tasks to run \"\n                        \"the test!\\n\", size, MIN_GLHOST);\n        }\n\n        if (testdir == NULL && rank == 0) {\n                fprintf(stderr, \"Please specify a test directory! \"\n                        \"(\\\"%s -h\\\" for help)\\n\",\n                       argv[0]);\n        }\n\n        lp_gethostname();\n\n        for (i = 0; i < iterations; ++i) {\n                if (rank == 0)\n                        printf(\"%s: Running test #%s(iter %d)\\n\",\n                               timestamp(), argv[0], i);\n\n                parallel_grouplock();\n        }\n\n        if (rank == 0) {\n                printf(\"%s: All tests passed!\\n\", timestamp());\n        }\n        return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n        int i, iterations = 1, c;\n\n        setbuf(stdout, 0);\n        setbuf(stderr, 0);\n\n        \n\n        for (i = 1; i < argc; ++i) {\n                if (!strcmp(argv[i], \"-h\") || !strcmp(argv[i], \"--help\"))\n                        usage(argv[0]);\n        }\n\n        MPI_Init(&argc, &argv);\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        \n\n        while (1) {\n                c = getopt(argc, argv, \"d:ghn:t:vV:\");\n                if (c == -1)\n                        break;\n\n                switch (c) {\n                case 'd':\n                        testdir = optarg;\n                        break;\n                case 'g':\n                        debug = 1;\n                        break;\n                case 'h':\n                        usage(argv[0]);\n                        break;\n                case 'n':\n                        iterations = atoi(optarg);\n                        break;\n                case 't':\n                        only_test = atoi(optarg);\n                        break;\n                case 'v':\n                        verbose += 1;\n                        break;\n                case 'V':\n                        verbose = atoi(optarg);\n                        break;\n                }\n        }\n\n        if (rank == 0)\n                printf(\"%s is running with %d task(es) %s\\n\",\n                       argv[0], size, debug ? \"in DEBUG mode\" : \"\\b\\b\");\n\n        if (size < MIN_GLHOST) {\n                fprintf(stderr, \"Error: \"\n                        \"%d tasks run, but should be at least %d tasks to run \"\n                        \"the test!\\n\", size, MIN_GLHOST);\n                MPI_Abort(MPI_COMM_WORLD, 2);\n        }\n\n        if (testdir == NULL && rank == 0) {\n                fprintf(stderr, \"Please specify a test directory! \"\n                        \"(\\\"%s -h\\\" for help)\\n\",\n                       argv[0]);\n                MPI_Abort(MPI_COMM_WORLD, 2);\n        }\n\n        lp_gethostname();\n\n        for (i = 0; i < iterations; ++i) {\n                if (rank == 0)\n                        printf(\"%s: Running test #%s(iter %d)\\n\",\n                               timestamp(), argv[0], i);\n\n                parallel_grouplock();\n                MPI_Barrier(MPI_COMM_WORLD);\n        }\n\n        if (rank == 0) {\n                printf(\"%s: All tests passed!\\n\", timestamp());\n        }\n        MPI_Finalize();\n        return 0;\n}"}
{"program": "c-PRIMED_416", "code": "int main(int argc, char **argv)\n{\n  double sum, p_sum, pi, pi_exact;\n  int opt, n=2;\n  int tid, nthreads;\n  char cpu_name[80];\n  long i, loop_max, loop_min, iterations = 0;\n\n  sum = 0.0;\n  p_sum\t= 0.0;\n  pi_exact = 4.0*atan(1.0);\n  iterations = 1e9;\n\n  while ((opt = getopt(argc, argv, \"n:\")) != -1) {\n    switch (opt) {\n    case 'n':\n      n = atoi(optarg);\n      break;\n    default:\n      usage(argv[0]);\n    }\n  }\n\n  if (n < 0)\n    usage(argv[0]);\n\n\n  gethostname(cpu_name, sizeof(cpu_name));\n  printf(\"tid=%d of %d: running on machine = %s\\n\",tid, nthreads, cpu_name);\n\n\n  loop_min \t= 1 +  (long)((tid + 0)  *  (iterations-1)/nthreads);\n  loop_max\t=      (long)((tid + 1)  *  (iterations-1)/nthreads);\n\n  for(i=loop_max-1; i>=loop_min; i--)\n    p_sum += 1.0/pow(i,(double)n);\n\n\n  if (tid == 0) {\n    printf(\"zeta(%d)  = %-.15f \\n\",n,sum);\n    dump_hdf5_d(\"z\", sum, \"zeta\");\n  }\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  double sum, p_sum, pi, pi_exact;\n  int opt, n=2;\n  int tid, nthreads;\n  char cpu_name[80];\n  long i, loop_max, loop_min, iterations = 0;\n\n  sum = 0.0;\n  p_sum\t= 0.0;\n  pi_exact = 4.0*atan(1.0);\n  iterations = 1e9;\n\n  while ((opt = getopt(argc, argv, \"n:\")) != -1) {\n    switch (opt) {\n    case 'n':\n      n = atoi(optarg);\n      break;\n    default:\n      usage(argv[0]);\n    }\n  }\n\n  if (n < 0)\n    usage(argv[0]);\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &tid);\n  MPI_Comm_size(MPI_COMM_WORLD, &nthreads);\n\n  gethostname(cpu_name, sizeof(cpu_name));\n  printf(\"tid=%d of %d: running on machine = %s\\n\",tid, nthreads, cpu_name);\n\n\n  loop_min \t= 1 +  (long)((tid + 0)  *  (iterations-1)/nthreads);\n  loop_max\t=      (long)((tid + 1)  *  (iterations-1)/nthreads);\n\n  for(i=loop_max-1; i>=loop_min; i--)\n    p_sum += 1.0/pow(i,(double)n);\n\n  MPI_Reduce(&p_sum,&sum,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n  MPI_Finalize();\n\n  if (tid == 0) {\n    printf(\"zeta(%d)  = %-.15f \\n\",n,sum);\n    dump_hdf5_d(\"z\", sum, \"zeta\");\n  }\n\n  return 0;\n}"}
{"program": "joao-lima_422", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tstarpu_vector_data_register(&token_handle, STARPU_MAIN_RAM, (uintptr_t)&token, 1, sizeof(token));\n\n\tint nloops = NITER;\n\tint loop;\n\n\tint last_loop = nloops - 1;\n\tint last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (loop == 0 && rank == 0)\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tFPRINTF(stdout, \"Start with token value %u\\n\", token);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(token_handle, &req, (rank+size-1)%size, tag, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\n\t\tincrement_token();\n\n\t\tif (loop == last_loop && rank == last_rank)\n\t\t{\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tFPRINTF(stdout, \"Finished : token value %u\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t\telse {\n\t\t\tstarpu_mpi_req req;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_isend(token_handle, &req, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\n\tstarpu_data_unregister(token_handle);\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tstarpu_vector_data_register(&token_handle, STARPU_MAIN_RAM, (uintptr_t)&token, 1, sizeof(token));\n\n\tint nloops = NITER;\n\tint loop;\n\n\tint last_loop = nloops - 1;\n\tint last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (loop == 0 && rank == 0)\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tFPRINTF(stdout, \"Start with token value %u\\n\", token);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(token_handle, &req, (rank+size-1)%size, tag, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\n\t\tincrement_token();\n\n\t\tif (loop == last_loop && rank == last_rank)\n\t\t{\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tFPRINTF(stdout, \"Finished : token value %u\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t\telse {\n\t\t\tstarpu_mpi_req req;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_isend(token_handle, &req, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\n\tstarpu_data_unregister(token_handle);\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n"}
{"program": "qingl97_424", "code": "MPI_Init(&argc, &argv);\n    if (argc != 2) {\n\tfprintf(stderr, \"Usage:   %s epsilon\\n\", argv[0]);\n\tfprintf(stderr, \"Example: %s 1e-4\\n\", argv[0]);\n    }\n    if (nprocs < 2) {\n\tfprintf(stderr, \"This program needs at least two processes.\\n\");\n    }\n    if (myrank == 0) {\t\t\t\n\n\tdouble res, wall0, wall1;\n\tgettime(NULL, &wall0);\n\tres = integration_nr(0.0, 1.0, atof(argv[1]), F1);\n\tgettime(NULL, &wall1);\n\tprintf(\"result=%0.16lf, error=%0.4le, wtime=%0.4lf\\n\",\n\t\tres, res - RESULT, wall1 - wall0);\n\tprintf(\"%u function evaluations.\\n\", count);\n\n\t\n\n\tfor (i = 0; i < nprocs; i++) cnts[i] = -1;\n    }\n    else {\t\t\t\t\n\n\twhile (1) {\n\t    static double *xsave = NULL;\n\t    static int size = 0;\n\t    if (n < 0) break;\n\t    if (n > size) {\n\t\tif (xsave != NULL) free(xsave);\n\t\txsave = malloc((size = n) * sizeof(*xsave));\n\t    }\n\t    for (i = 0; i < n; i++) xsave[i] = F(xsave[i]);\n\t}\n    }\n    return 0;\n}\n", "label": "MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    if (argc != 2) {\n\tfprintf(stderr, \"Usage:   %s epsilon\\n\", argv[0]);\n\tfprintf(stderr, \"Example: %s 1e-4\\n\", argv[0]);\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    if (nprocs < 2) {\n\tfprintf(stderr, \"This program needs at least two processes.\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    if (myrank == 0) {\t\t\t\n\n\tdouble res, wall0, wall1;\n\tgettime(NULL, &wall0);\n\tres = integration_nr(0.0, 1.0, atof(argv[1]), F1);\n\tgettime(NULL, &wall1);\n\tprintf(\"result=%0.16lf, error=%0.4le, wtime=%0.4lf\\n\",\n\t\tres, res - RESULT, wall1 - wall0);\n\tprintf(\"%u function evaluations.\\n\", count);\n\n\t\n\n\tfor (i = 0; i < nprocs; i++) cnts[i] = -1;\n\tMPI_Scatter(cnts, 1, MPI_INT, &i, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n    else {\t\t\t\t\n\n\twhile (1) {\n\t    static double *xsave = NULL;\n\t    static int size = 0;\n\t    MPI_Scatter(NULL, 0, MPI_INT, &n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\t    if (n < 0) break;\n\t    if (n > size) {\n\t\tif (xsave != NULL) free(xsave);\n\t\txsave = malloc((size = n) * sizeof(*xsave));\n\t    }\n\t    MPI_Scatterv(NULL, 0, 0, MPI_DOUBLE, xsave, n, MPI_DOUBLE, 0, \n\t\t     MPI_COMM_WORLD);\n\t    for (i = 0; i < n; i++) xsave[i] = F(xsave[i]);\n\t    MPI_Gatherv(xsave, n, MPI_DOUBLE, NULL, 0, 0, MPI_DOUBLE, 0,\n\t\t\tMPI_COMM_WORLD);\n\t}\n    }\n    MPI_Finalize();\n    return 0;\n}\n"}
{"program": "syftalent_427", "code": "int main(int argc, char **argv)\n{\n    int rank, size, err;\n    char buf[10];\n    MPI_Request request;\n\n    if (size < 4) {\n        fprintf( stderr, \"Must run with at least 4 processes\\n\" );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =\n        err +=\n        if (err) {\n            fprintf(stderr, \"An error occurred during the send operation\\n\");\n        }\n    }\n\n    if (rank == 3) {\n        err =\n        err +=\n        if (err) {\n            fprintf(stderr, \"An error occurred during the recv operation\\n\");\n        }\n    }\n\n    if (rank == 3) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =\n        err +=\n        if (err) {\n            fprintf(stderr, \"An error occurred during the send operation\\n\");\n        }\n    }\n\n    if (rank == 2) {\n        err =\n        err +=\n        if (err) {\n            fprintf(stderr, \"An error occurred during the recv operation\\n\");\n        } else {\n            printf(\" %s\\n\", buf);\n            fflush(stdout);\n        }\n    }\n\n    if (rank == 2) {\n        exit(EXIT_FAILURE);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, err;\n    char buf[10];\n    MPI_Request request;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size < 4) {\n        fprintf( stderr, \"Must run with at least 4 processes\\n\" );\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =  MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);\n        err += MPI_Wait(&request, MPI_STATUS_IGNORE);\n        if (err) {\n            fprintf(stderr, \"An error occurred during the send operation\\n\");\n        }\n    }\n\n    if (rank == 3) {\n        err =  MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);\n        err += MPI_Wait(&request, MPI_STATUS_IGNORE);\n        if (err) {\n            fprintf(stderr, \"An error occurred during the recv operation\\n\");\n        }\n    }\n\n    if (rank == 3) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =  MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);\n        err += MPI_Wait(&request, MPI_STATUS_IGNORE);\n        if (err) {\n            fprintf(stderr, \"An error occurred during the send operation\\n\");\n        }\n    }\n\n    if (rank == 2) {\n        err =  MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);\n        err += MPI_Wait(&request, MPI_STATUS_IGNORE);\n        if (err) {\n            fprintf(stderr, \"An error occurred during the recv operation\\n\");\n        } else {\n            printf(\" %s\\n\", buf);\n            fflush(stdout);\n        }\n    }\n\n    if (rank == 2) {\n        exit(EXIT_FAILURE);\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "damonge_430", "code": "int main(int argc,char* argv[])\n{\n  mpi_init(&argc,&argv);\n  msg_init();\n\n  if(argc<2)\n    msg_abort(1,\"Error: Parameter file not specified. cola_halo param.init\\n\");\n  read_parameters(argv[1]);\n\n  msg_printf(\"Reading particles\\n\");\n  Particles* particles=read_input_snapshot();\n\n  msg_printf(\"Getting halos\\n\");\n  lint n_halos;\n  FoFHalo *fh=fof_get_halos(&n_halos,particles);\n\n  msg_printf(\"Writing output\\n\");\n  write_halos(n_halos,fh);\n\n  free(particles->p);\n  free(particles->p_back);\n  free(particles);\n\n  return 0;\n}", "label": "int main(int argc,char* argv[])\n{\n  mpi_init(&argc,&argv);\n  msg_init();\n\n  if(argc<2)\n    msg_abort(1,\"Error: Parameter file not specified. cola_halo param.init\\n\");\n  read_parameters(argv[1]);\n\n  msg_printf(\"Reading particles\\n\");\n  Particles* particles=read_input_snapshot();\n\n  msg_printf(\"Getting halos\\n\");\n  lint n_halos;\n  FoFHalo *fh=fof_get_halos(&n_halos,particles);\n\n  msg_printf(\"Writing output\\n\");\n  write_halos(n_halos,fh);\n\n  free(particles->p);\n  free(particles->p_back);\n  free(particles);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "daidong_431", "code": "int main(int argc, char **argv) \n{\n    int nprocs;\n    int mynod;\n    int i;\n    int ret;\n    int iters;\n    double time1, time2;\n    char* path;\n\n\n    \n\n    if(nprocs > 1)\n    {\n        if(mynod == 0)\n        {\n            fprintf(stderr, \"Error: this benchmark should be run with exactly one process.\\n\");\n        }\n        return(-1);\n    }\n\n    \n\n    if(argc != 3)\n    {\n        fprintf(stderr, \"Usage: %s <path> <number of iterations>\\n\", argv[0]);\n        return(-1);\n    }\n\n    ret = sscanf(argv[2], \"%d\", &iters);\n    if(ret != 1)\n    {\n        fprintf(stderr, \"Usage: %s <number of iterations>\\n\", argv[0]);\n        return(-1);\n    }\n\n    time1 =\n    for(i=0; i<iters; i++)\n    {\n        path = realpath(argv[1], NULL);    \n        free(path);\n    }\n    time2 =\n\n    sleep(1);\n\n    \n\n    printf(\"#<op>\\t<iters>\\t<total (s)>\\t<per op (s)>\\n\");\n    printf(\"wtime\\t%d\\t%.9f\\t%.9f\\n\", iters, time2-time1, (time2-time1)/(double)iters);\n\n    return(0);\n}", "label": "int main(int argc, char **argv) \n{\n    int nprocs;\n    int mynod;\n    int i;\n    int ret;\n    int iters;\n    double time1, time2;\n    char* path;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    \n\n    if(nprocs > 1)\n    {\n        if(mynod == 0)\n        {\n            fprintf(stderr, \"Error: this benchmark should be run with exactly one process.\\n\");\n        }\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    if(argc != 3)\n    {\n        fprintf(stderr, \"Usage: %s <path> <number of iterations>\\n\", argv[0]);\n        MPI_Finalize();\n        return(-1);\n    }\n\n    ret = sscanf(argv[2], \"%d\", &iters);\n    if(ret != 1)\n    {\n        fprintf(stderr, \"Usage: %s <number of iterations>\\n\", argv[0]);\n        MPI_Finalize();\n        return(-1);\n    }\n\n    time1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        path = realpath(argv[1], NULL);    \n        free(path);\n    }\n    time2 = MPI_Wtime();\n\n    sleep(1);\n\n    \n\n    printf(\"#<op>\\t<iters>\\t<total (s)>\\t<per op (s)>\\n\");\n    printf(\"wtime\\t%d\\t%.9f\\t%.9f\\n\", iters, time2-time1, (time2-time1)/(double)iters);\n\n    MPI_Finalize();\n    return(0);\n}"}
{"program": "qingu_432", "code": "int main( int argc, char *argv[] )\n{\n    int i, rank, nproc;\n    int errors = 0, all_errors = 0;\n    MPI_Win win;\n    MPI_Request put_req[M] = { MPI_REQUEST_NULL };\n    MPI_Request get_req;\n    double *baseptr;\n    double data[M][N]; \n\n\n\n    assert(M < NSTEPS);\n\n\n\n    for (i = 0; i < NSTEPS; i++) {\n        int target = (rank+1) % nproc;\n        int j;\n\n        \n\n        if (i < M) {\n            j = i;\n        } else {\n        }\n\n\n        compute(i, data[j]);\n\n    }\n\n\n\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int i, rank, nproc;\n    int errors = 0, all_errors = 0;\n    MPI_Win win;\n    MPI_Request put_req[M] = { MPI_REQUEST_NULL };\n    MPI_Request get_req;\n    double *baseptr;\n    double data[M][N]; \n\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    assert(M < NSTEPS);\n\n    MPI_Win_allocate(NSTEPS*N*sizeof(double), sizeof(double), MPI_INFO_NULL,\n                     MPI_COMM_WORLD, &baseptr, &win);\n\n    MPI_Win_lock_all(0, win);\n\n    for (i = 0; i < NSTEPS; i++) {\n        int target = (rank+1) % nproc;\n        int j;\n\n        \n\n        if (i < M) {\n            j = i;\n        } else {\n            MPI_Waitany(M, put_req, &j, MPI_STATUS_IGNORE);\n        }\n\n        MPI_Rget(data[j], N, MPI_DOUBLE, target, i*N, N, MPI_DOUBLE, win,\n                 &get_req);\n        MPI_Wait(&get_req,MPI_STATUS_IGNORE);\n\n        compute(i, data[j]);\n        MPI_Rput(data[j], N, MPI_DOUBLE, target, i*N, N, MPI_DOUBLE, win,\n                 &put_req[j]);\n\n    }\n\n    MPI_Waitall(M, put_req, MPI_STATUSES_IGNORE);\n    MPI_Win_unlock_all(win);\n\n    MPI_Win_free(&win);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bjoern-leder_434", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,bs[4];\n   int nb,isw,n,ie;\n   block_t *b;\n   FILE *flog=NULL,*fin=NULL;\n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check3.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n\n      printf(\"\\n\");\n      printf(\"Check of assign_ud2ubgr() and assign_ud2udblk()\\n\");\n      printf(\"-----------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"bs\",\"%d %d %d %d\",&bs[0],&bs[1],&bs[2],&bs[3]);\n      fclose(fin);\n\n      printf(\"bs = %d %d %d %d\\n\\n\",bs[0],bs[1],bs[2],bs[3]);\n   }\n\n\n   start_ranlux(0,1234);\n   geometry();\n   set_sap_parms(bs,0,1,1);\n   set_dfl_parms(bs,2);\n   alloc_bgr(SAP_BLOCKS);\n   alloc_bgr(DFL_BLOCKS);\n\n   random_ud();\n   assign_ud2ubgr(SAP_BLOCKS);\n   set_ud();\n   assign_ud2ubgr(SAP_BLOCKS);\n   assign_ud2u();\n   print_flags();\n   print_grid_flags(SAP_BLOCKS);\n   \n   error(check_ubgr(SAP_BLOCKS),1,\"main [check3.c]\",\n         \"assign_ud2ubgr() is incorrect\");      \n\n   b=blk_list(DFL_BLOCKS,&nb,&isw);\n   random_ud();\n   assign_ud2udblk(DFL_BLOCKS,0);\n   set_ud();\n   ie=0;\n\n   for (n=0;n<nb;n++)\n   {\n      assign_ud2udblk(DFL_BLOCKS,n);\n      ie|=check_udblk(b+n);\n   }\n\n   print_flags();   \n   print_grid_flags(DFL_BLOCKS);\n   \n   error(ie,1,\"main [check3.c]\",\n         \"assign_ud2udblk() is incorrect\");\n   error_chk();\n\n   if (my_rank==0)\n   {\n      printf(\"No errors detected\\n\\n\");\n      fclose(flog);\n   }\n\n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,bs[4];\n   int nb,isw,n,ie;\n   block_t *b;\n   FILE *flog=NULL,*fin=NULL;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check3.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n\n      printf(\"\\n\");\n      printf(\"Check of assign_ud2ubgr() and assign_ud2udblk()\\n\");\n      printf(\"-----------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"bs\",\"%d %d %d %d\",&bs[0],&bs[1],&bs[2],&bs[3]);\n      fclose(fin);\n\n      printf(\"bs = %d %d %d %d\\n\\n\",bs[0],bs[1],bs[2],bs[3]);\n   }\n\n   MPI_Bcast(bs,4,MPI_INT,0,MPI_COMM_WORLD);\n\n   start_ranlux(0,1234);\n   geometry();\n   set_sap_parms(bs,0,1,1);\n   set_dfl_parms(bs,2);\n   alloc_bgr(SAP_BLOCKS);\n   alloc_bgr(DFL_BLOCKS);\n\n   random_ud();\n   assign_ud2ubgr(SAP_BLOCKS);\n   set_ud();\n   assign_ud2ubgr(SAP_BLOCKS);\n   assign_ud2u();\n   print_flags();\n   print_grid_flags(SAP_BLOCKS);\n   \n   error(check_ubgr(SAP_BLOCKS),1,\"main [check3.c]\",\n         \"assign_ud2ubgr() is incorrect\");      \n\n   b=blk_list(DFL_BLOCKS,&nb,&isw);\n   random_ud();\n   assign_ud2udblk(DFL_BLOCKS,0);\n   set_ud();\n   ie=0;\n\n   for (n=0;n<nb;n++)\n   {\n      assign_ud2udblk(DFL_BLOCKS,n);\n      ie|=check_udblk(b+n);\n   }\n\n   print_flags();   \n   print_grid_flags(DFL_BLOCKS);\n   \n   error(ie,1,\"main [check3.c]\",\n         \"assign_ud2udblk() is incorrect\");\n   error_chk();\n\n   if (my_rank==0)\n   {\n      printf(\"No errors detected\\n\\n\");\n      fclose(flog);\n   }\n\n   MPI_Finalize();\n   exit(0);\n}"}
{"program": "TuftsBCB_436", "code": "int\nmain(int argc, char **argv)\n{\n  int              status   = eslOK;\n\n  ESL_GETOPTS     *go  = NULL;\t\n\n  struct cfg_s     cfg;         \n\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.hmmfile    = NULL;\n  cfg.dbfile     = NULL;\n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.hmmfile, &cfg.dbfile);    \n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  int              status   = eslOK;\n\n  ESL_GETOPTS     *go  = NULL;\t\n\n  struct cfg_s     cfg;         \n\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.hmmfile    = NULL;\n  cfg.dbfile     = NULL;\n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.hmmfile, &cfg.dbfile);    \n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n      MPI_Init(&argc, &argv);\n      MPI_Comm_rank(MPI_COMM_WORLD, &(cfg.my_rank));\n      MPI_Comm_size(MPI_COMM_WORLD, &(cfg.nproc));\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n      MPI_Finalize();\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}"}
{"program": "JiangXL_437", "code": "int\nmain(int argc, char **argv)\n{\n    int mpi_namelen;\n    char mpi_name[MPI_MAX_PROCESSOR_NAME];\n    int i, n;\n\n    \n\n    if ((SPACE1_DIM1 % mpi_size) || (SPACE1_DIM2 % mpi_size)){\n\tprintf(\"DIM1(%d) and DIM2(%d) must be multiples of processes (%d)\\n\",\n\t    SPACE1_DIM1, SPACE1_DIM2, mpi_size);\n\tnerrors++;\n\tgoto finish;\n    }\n\n    if (parse_options(argc, argv) != 0)\n\tgoto finish;\n\n    \n\n    if (mpi_rank == 0){\n\tn = sizeof(testfiles)/sizeof(testfiles[0]);\n\tprintf(\"Parallel test files are:\\n\");\n\tfor (i=0; i<n; i++){\n\t    printf(\"   %s\\n\", testfiles[i]);\n\t}\n    }\n\n    if (dowrite){\n\ttest_split_comm_access(testfiles);\n\tphdf5writeInd(testfiles[0]);\n\tphdf5writeAll(testfiles[1]);\n    }\n    if (doread){\n\tphdf5readInd(testfiles[0]);\n\tphdf5readAll(testfiles[1]);\n    }\n\n    if (!(dowrite || doread)){\n\tusage();\n\tnerrors++;\n    }\n\nfinish:\n    if (mpi_rank == 0){\t\t\n\n\tif (nerrors)\n\t    printf(\"***PHDF5 tests detected %d errors***\\n\", nerrors);\n\telse{\n\t    printf(\"===================================\\n\");\n\t    printf(\"PHDF5 tests finished with no errors\\n\");\n\t    printf(\"===================================\\n\");\n\t}\n    }\n    if (docleanup)\n\tcleanup();\n\n    return(nerrors);\n}", "label": "int\nmain(int argc, char **argv)\n{\n    int mpi_namelen;\n    char mpi_name[MPI_MAX_PROCESSOR_NAME];\n    int i, n;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);\n    MPI_Get_processor_name(mpi_name,&mpi_namelen);\n    \n\n    if ((SPACE1_DIM1 % mpi_size) || (SPACE1_DIM2 % mpi_size)){\n\tprintf(\"DIM1(%d) and DIM2(%d) must be multiples of processes (%d)\\n\",\n\t    SPACE1_DIM1, SPACE1_DIM2, mpi_size);\n\tnerrors++;\n\tgoto finish;\n    }\n\n    if (parse_options(argc, argv) != 0)\n\tgoto finish;\n\n    \n\n    if (mpi_rank == 0){\n\tn = sizeof(testfiles)/sizeof(testfiles[0]);\n\tprintf(\"Parallel test files are:\\n\");\n\tfor (i=0; i<n; i++){\n\t    printf(\"   %s\\n\", testfiles[i]);\n\t}\n    }\n\n    if (dowrite){\n\tMPI_BANNER(\"testing PHDF5 dataset using split communicators...\");\n\ttest_split_comm_access(testfiles);\n\tMPI_BANNER(\"testing PHDF5 dataset independent write...\");\n\tphdf5writeInd(testfiles[0]);\n\tMPI_BANNER(\"testing PHDF5 dataset collective write...\");\n\tphdf5writeAll(testfiles[1]);\n    }\n    if (doread){\n\tMPI_BANNER(\"testing PHDF5 dataset independent read...\");\n\tphdf5readInd(testfiles[0]);\n\tMPI_BANNER(\"testing PHDF5 dataset collective read...\");\n\tphdf5readAll(testfiles[1]);\n    }\n\n    if (!(dowrite || doread)){\n\tusage();\n\tnerrors++;\n    }\n\nfinish:\n    if (mpi_rank == 0){\t\t\n\n\tif (nerrors)\n\t    printf(\"***PHDF5 tests detected %d errors***\\n\", nerrors);\n\telse{\n\t    printf(\"===================================\\n\");\n\t    printf(\"PHDF5 tests finished with no errors\\n\");\n\t    printf(\"===================================\\n\");\n\t}\n    }\n    if (docleanup)\n\tcleanup();\n    MPI_Finalize();\n\n    return(nerrors);\n}"}
{"program": "alucas_439", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif (rank == 0)\n\t\t{\n\t\t\tMPI_Status status;\n\n\t\t\tint sent = 0;\n\t\t\tstarpu_mpi_isend_detached(tab_handle, other_rank, loop, MPI_COMM_WORLD, callback, &sent);\n\n\t\t\tpthread_mutex_lock(&mutex);\n\t\t\twhile (!sent)\n\t\t\t\tpthread_cond_wait(&cond, &mutex);\n\t\t\tpthread_mutex_unlock(&mutex);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(tab_handle, other_rank, loop, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif (rank == 0)\n\t\t{\n\t\t\tMPI_Status status;\n\n\t\t\tint sent = 0;\n\t\t\tstarpu_mpi_isend_detached(tab_handle, other_rank, loop, MPI_COMM_WORLD, callback, &sent);\n\n\t\t\tpthread_mutex_lock(&mutex);\n\t\t\twhile (!sent)\n\t\t\t\tpthread_cond_wait(&cond, &mutex);\n\t\t\tpthread_mutex_unlock(&mutex);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(tab_handle, other_rank, loop, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "cbries_442", "code": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble *A = NULL;\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tsrand(time(NULL));\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tB[i] = rand()%100/100.0f + myrank;\n\t}\n\n\tprintf(\"On processor of rank %d B=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f   \", B[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tif(myrank==0) {\n\t\tA = (double*) malloc(DIMENSION*nprocs*sizeof(double));\n\t\tif(A==NULL) {\n\t\t\tperror(\"malloc for A failed\");\n\t\t}\t\t\n\t}\n\n\n\tif(myrank==0) {\n\t\tfor(i=0; i<nprocs; i++) {\n\t\t\tprintf(\"Column %d of A=\", i);\n\t\t\tfor(j=0; j<DIMENSION; j++) {\n\t\t\t\tprintf(\"%.2f   \", A[i*DIMENSION+j]);\n\t\t\t}\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\t\n\t\tif(A!=NULL) {\n\t\t\tfree(A); A = NULL;\n\t\t}\n\t}\n\n\t\n\n\n\treturn 0;\n}\n", "label": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble *A = NULL;\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tsrand(time(NULL));\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tB[i] = rand()%100/100.0f + myrank;\n\t}\n\n\tprintf(\"On processor of rank %d B=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f   \", B[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tif(myrank==0) {\n\t\tA = (double*) malloc(DIMENSION*nprocs*sizeof(double));\n\t\tif(A==NULL) {\n\t\t\tperror(\"malloc for A failed\");\n\t\t}\t\t\n\t}\n\n\tMPI_Gather(B, DIMENSION, MPI_DOUBLE, A, DIMENSION, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tif(myrank==0) {\n\t\tfor(i=0; i<nprocs; i++) {\n\t\t\tprintf(\"Column %d of A=\", i);\n\t\t\tfor(j=0; j<DIMENSION; j++) {\n\t\t\t\tprintf(\"%.2f   \", A[i*DIMENSION+j]);\n\t\t\t}\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\t\n\t\tif(A!=NULL) {\n\t\t\tfree(A); A = NULL;\n\t\t}\n\t}\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "gnu3ra_443", "code": "int main( int argc, char *argv[], char *envp[] )\n{\n    int  i, myid, numprocs;\n    int  namelen;\n    char *p;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    char curr_wd[MAX_DIRNAME_SIZE];\n\n\n    fprintf( stdout, \"[%d] Process %d of %d (%s) is on %s\\n\",\n\t     myid, myid, numprocs, argv[0], processor_name );\n    fflush( stdout );\n\n    for ( i = 1; i < argc; i++ ) {\n\tfprintf( stdout, \"[%d] argv[%d]=\\\"%s\\\"\\n\", myid, i, argv[i] ); \n\tfflush( stdout );\n    }\n\n    getcwd( curr_wd, MAX_DIRNAME_SIZE ); \n    fprintf( stdout, \"[%d] current working directory=%s\\n\", myid, curr_wd );\n\n    p = getenv(\"PATH\");\n    if ( p )\n\tfprintf( stdout, \"[%d] PATH=%s\\n\", myid, p );\n    else\n\tfprintf( stdout, \"[%d] PATH not set in environment\\n\", myid );\n\n#ifdef PRINTENV\n    \n\n    for ( i = 0; envp[i]; i++ )\n\tfprintf( stdout, \"[%d] envp[%d]=%s\\n\", myid, i, envp[i] );\n#endif\n\n}", "label": "int main( int argc, char *argv[], char *envp[] )\n{\n    int  i, myid, numprocs;\n    int  namelen;\n    char *p;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    char curr_wd[MAX_DIRNAME_SIZE];\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_size( MPI_COMM_WORLD, &numprocs );\n    MPI_Comm_rank( MPI_COMM_WORLD, &myid );\n    MPI_Get_processor_name( processor_name, &namelen );\n\n    fprintf( stdout, \"[%d] Process %d of %d (%s) is on %s\\n\",\n\t     myid, myid, numprocs, argv[0], processor_name );\n    fflush( stdout );\n\n    for ( i = 1; i < argc; i++ ) {\n\tfprintf( stdout, \"[%d] argv[%d]=\\\"%s\\\"\\n\", myid, i, argv[i] ); \n\tfflush( stdout );\n    }\n\n    getcwd( curr_wd, MAX_DIRNAME_SIZE ); \n    fprintf( stdout, \"[%d] current working directory=%s\\n\", myid, curr_wd );\n\n    p = getenv(\"PATH\");\n    if ( p )\n\tfprintf( stdout, \"[%d] PATH=%s\\n\", myid, p );\n    else\n\tfprintf( stdout, \"[%d] PATH not set in environment\\n\", myid );\n\n#ifdef PRINTENV\n    \n\n    for ( i = 0; envp[i]; i++ )\n\tfprintf( stdout, \"[%d] envp[%d]=%s\\n\", myid, i, envp[i] );\n#endif\n\n    MPI_Finalize( );\n}"}
{"program": "j2sg_444", "code": "int main(int argc, char *argv[])\n{\n   int idp;              \n\n   int np;               \n\n\n   \n\n\n   \n\n\n   \n\n   MPI_Datatype MPI_VECTOR;\n   int vectorBlockLength[VECTOR_BLOCKS] = {3};\n   MPI_Aint vectorOffset[VECTOR_BLOCKS] = {0};\n   MPI_Datatype vectorOldType[VECTOR_BLOCKS] = {MPI_DOUBLE};\n\n   \n\n   MPI_Datatype MPI_PARTICLE;\n   int particleBlockLength[PARTICLE_BLOCKS] = {1, 3};\n   MPI_Aint doubleExtent;\n   MPI_Aint particleOffset[PARTICLE_BLOCKS] = {0, doubleExtent};\n   MPI_Datatype particleOldType[PARTICLE_BLOCKS] = {MPI_DOUBLE, MPI_VECTOR};\n\n   MPI_Status status;\n\n   \n\n   if(idp == 0) {\n      while(--np) {\n         Particle particleReceived;\n\n         printf(\"Particle from %d [m=%.3lf p=%.2lfi+%.2lfj+%.2lfk v=%.2lfi+%.2lfj+%.2lfk a=%.2lfi+%.2lfj+%.2lfk]\\n\",\n                status.MPI_SOURCE, particleReceived.mass,\n                particleReceived.position.x, particleReceived.position.y, particleReceived.position.z,\n                particleReceived.speed.x, particleReceived.speed.y, particleReceived.speed.z,\n                particleReceived.acceleration.x, particleReceived.acceleration.y, particleReceived.acceleration.z);\n      }\n   } else {\n      Particle particleSent;\n\n      particleSent.mass = idp / 10.0;\n      particleSent.position.x = particleSent.position.y = particleSent.position.z = idp;\n      particleSent.speed.x = particleSent.speed.y = particleSent.speed.z = idp * 10;\n      particleSent.acceleration.x = particleSent.acceleration.y = particleSent.acceleration.z = idp * 100;\n\n   }\n\n\n\n   \n\n\n\n   return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n   int idp;              \n\n   int np;               \n\n\n   \n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &idp);\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n   \n\n\n   \n\n   MPI_Datatype MPI_VECTOR;\n   int vectorBlockLength[VECTOR_BLOCKS] = {3};\n   MPI_Aint vectorOffset[VECTOR_BLOCKS] = {0};\n   MPI_Datatype vectorOldType[VECTOR_BLOCKS] = {MPI_DOUBLE};\n   MPI_Type_struct(VECTOR_BLOCKS, vectorBlockLength, vectorOffset, vectorOldType, &MPI_VECTOR);\n   MPI_Type_commit(&MPI_VECTOR);\n\n   \n\n   MPI_Datatype MPI_PARTICLE;\n   int particleBlockLength[PARTICLE_BLOCKS] = {1, 3};\n   MPI_Aint doubleExtent;\n   MPI_Type_extent(MPI_DOUBLE, &doubleExtent);\n   MPI_Aint particleOffset[PARTICLE_BLOCKS] = {0, doubleExtent};\n   MPI_Datatype particleOldType[PARTICLE_BLOCKS] = {MPI_DOUBLE, MPI_VECTOR};\n   MPI_Type_struct(PARTICLE_BLOCKS, particleBlockLength, particleOffset, particleOldType, &MPI_PARTICLE);\n   MPI_Type_commit(&MPI_PARTICLE);\n\n   MPI_Status status;\n\n   \n\n   if(idp == 0) {\n      while(--np) {\n         Particle particleReceived;\n\n         MPI_Recv(&particleReceived, 1, MPI_PARTICLE, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n         printf(\"Particle from %d [m=%.3lf p=%.2lfi+%.2lfj+%.2lfk v=%.2lfi+%.2lfj+%.2lfk a=%.2lfi+%.2lfj+%.2lfk]\\n\",\n                status.MPI_SOURCE, particleReceived.mass,\n                particleReceived.position.x, particleReceived.position.y, particleReceived.position.z,\n                particleReceived.speed.x, particleReceived.speed.y, particleReceived.speed.z,\n                particleReceived.acceleration.x, particleReceived.acceleration.y, particleReceived.acceleration.z);\n      }\n   } else {\n      Particle particleSent;\n\n      particleSent.mass = idp / 10.0;\n      particleSent.position.x = particleSent.position.y = particleSent.position.z = idp;\n      particleSent.speed.x = particleSent.speed.y = particleSent.speed.z = idp * 10;\n      particleSent.acceleration.x = particleSent.acceleration.y = particleSent.acceleration.z = idp * 100;\n\n      MPI_Send(&particleSent, 1, MPI_PARTICLE, 0, 0, MPI_COMM_WORLD);\n   }\n\n\n\n   \n\n\n   MPI_Type_free(&MPI_PARTICLE);\n   MPI_Type_free(&MPI_VECTOR);\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "gentryx_445", "code": "int main(int argc, char * argv[])\n{\n\n    int rank, size;\n\n    int n = (argc > 1) ? atoi(argv[1]) : 1000;\n\n    double * sbuf1 = NULL;\n    double * rbuf1 = NULL;\n    double * sbuf2 = NULL;\n    double * rbuf2 = NULL;\n\n    MPI_Aint bytes = n*sizeof(double);\n\n    for (int i=0; i<n; i++) {\n        sbuf1[i] = (double)rank;\n    }\n    for (int i=0; i<n; i++) {\n        rbuf1[i] = 0.0;\n    }\n    for (int i=0; i<n; i++) {\n        sbuf2[i] = (double)rank;\n    }\n    for (int i=0; i<n; i++) {\n        rbuf2[i] = 0.0;\n    }\n\n    \n\n\n    BigMPI_Allreduce(sbuf1, rbuf2, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    const double val = (double)size*(size-1)/2.;\n    int error1 = verify_doubles(rbuf1, n, val);\n    if (error1>0) {\n        printf(\"There were %d errors out of %d elements!\\n\", error1, n);\n        for (int i=0; i<n; i++) {\n            printf(\"rbuf1[%d] = %lf (expected %lf - %s)\\n\",\n                    i, rbuf1[i], val, rbuf1[i]==val ? \"RIGHT\" : \"WRONG\");\n        }\n        fflush(stdout);\n    }\n    int error2 = verify_doubles(rbuf2, n, val);\n    if (error2>0) {\n        printf(\"There were %d errors out of %d elements!\\n\", error2, n);\n        for (int i=0; i<n; i++) {\n            printf(\"rbuf2[%d] = %lf (expected %lf - %s)\\n\",\n                    i, rbuf2[i], val, rbuf2[i]==val ? \"RIGHT\" : \"WRONG\");\n        }\n        fflush(stdout);\n    }\n\n    \n\n    double t0, t1, dtmpi, dtusr;\n    \n    t0 =\n    t1 =\n    dtmpi = t1-t0;\n    \n    t0 =\n    BigMPI_Allreduce(sbuf2, rbuf2, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    t1 =\n    dtusr = t1-t0;\n    \n\n\n    \n\n    if (rank==0 && error1==0 && error2==0) {\n        printf(\"n = %d tmpi = %lf dtusr = %lf\\n\", n, dtmpi, dtusr);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = (argc > 1) ? atoi(argv[1]) : 1000;\n\n    double * sbuf1 = NULL;\n    double * rbuf1 = NULL;\n    double * sbuf2 = NULL;\n    double * rbuf2 = NULL;\n\n    MPI_Aint bytes = n*sizeof(double);\n    MPI_Alloc_mem(bytes, MPI_INFO_NULL, &sbuf1);\n    MPI_Alloc_mem(bytes, MPI_INFO_NULL, &rbuf1);\n    MPI_Alloc_mem(bytes, MPI_INFO_NULL, &sbuf2);\n    MPI_Alloc_mem(bytes, MPI_INFO_NULL, &rbuf2);\n\n    for (int i=0; i<n; i++) {\n        sbuf1[i] = (double)rank;\n    }\n    for (int i=0; i<n; i++) {\n        rbuf1[i] = 0.0;\n    }\n    for (int i=0; i<n; i++) {\n        sbuf2[i] = (double)rank;\n    }\n    for (int i=0; i<n; i++) {\n        rbuf2[i] = 0.0;\n    }\n\n    \n\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Allreduce(sbuf1, rbuf1, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n    BigMPI_Allreduce(sbuf1, rbuf2, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    const double val = (double)size*(size-1)/2.;\n    int error1 = verify_doubles(rbuf1, n, val);\n    if (error1>0) {\n        printf(\"There were %d errors out of %d elements!\\n\", error1, n);\n        for (int i=0; i<n; i++) {\n            printf(\"rbuf1[%d] = %lf (expected %lf - %s)\\n\",\n                    i, rbuf1[i], val, rbuf1[i]==val ? \"RIGHT\" : \"WRONG\");\n        }\n        fflush(stdout);\n    }\n    int error2 = verify_doubles(rbuf2, n, val);\n    if (error2>0) {\n        printf(\"There were %d errors out of %d elements!\\n\", error2, n);\n        for (int i=0; i<n; i++) {\n            printf(\"rbuf2[%d] = %lf (expected %lf - %s)\\n\",\n                    i, rbuf2[i], val, rbuf2[i]==val ? \"RIGHT\" : \"WRONG\");\n        }\n        fflush(stdout);\n    }\n\n    \n\n    double t0, t1, dtmpi, dtusr;\n    MPI_Barrier(MPI_COMM_WORLD);\n    \n    t0 = MPI_Wtime();\n    MPI_Allreduce(sbuf1, rbuf1, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    t1 = MPI_Wtime();\n    dtmpi = t1-t0;\n    MPI_Barrier(MPI_COMM_WORLD);\n    \n    t0 = MPI_Wtime();\n    BigMPI_Allreduce(sbuf2, rbuf2, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    t1 = MPI_Wtime();\n    dtusr = t1-t0;\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Free_mem(sbuf2);\n    MPI_Free_mem(rbuf2);\n    MPI_Free_mem(sbuf1);\n    MPI_Free_mem(rbuf1);\n\n    \n\n    if (rank==0 && error1==0 && error2==0) {\n        printf(\"n = %d tmpi = %lf dtusr = %lf\\n\", n, dtmpi, dtusr);\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "auag92_446", "code": "int main(int argc, char *argv[]){\n  int my_rank;          \n\n  int p;                \n\n  int source;           \n\n  int dest;             \n\n  int tag = 0;          \n\n  char message[100];    \n\n  MPI_Status status;    \n\n\n  \n\n  \n\n\n  if(my_rank != 0){\n    \n\n    sprintf(message,\"Greetings from process %d\",my_rank);\n    dest = 0;\n    \n\n  }\n  else{\n    for(source=1; source < p; source++){\n      printf(\"%s\\n\",message);\n    }\n  }\n  \n\n}", "label": "int main(int argc, char *argv[]){\n  int my_rank;          \n\n  int p;                \n\n  int source;           \n\n  int dest;             \n\n  int tag = 0;          \n\n  char message[100];    \n\n  MPI_Status status;    \n\n\n  \n\n  MPI_Init(&argc, &argv);\n  \n\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  if(my_rank != 0){\n    \n\n    sprintf(message,\"Greetings from process %d\",my_rank);\n    dest = 0;\n    \n\n    MPI_Send(message, strlen(message)+1, MPI_CHAR,dest,tag, MPI_COMM_WORLD);\n  }\n  else{\n    for(source=1; source < p; source++){\n      MPI_Recv(message,100,MPI_CHAR,source,tag,MPI_COMM_WORLD,&status);\n      printf(\"%s\\n\",message);\n    }\n  }\n  \n\n  MPI_Finalize();\n}"}
{"program": "syftalent_447", "code": "int main( int argc, char *argv[] ) {\n  int np = NUM_SPAWNS;\n  int my_rank, size;\n  int errcodes[NUM_SPAWNS];\n  MPI_Comm allcomm;\n  MPI_Comm intercomm;\n\n\n\n\n  if ( intercomm == MPI_COMM_NULL ) {\n      fprintf(stdout, \"intercomm is null\\n\");\n  }\n\n\n\n  \n\n  \n\n\n  fprintf(stdout, \"%s:%d: MTestSleep starting; children should exit\\n\",\n          __FILE__, __LINE__ );fflush(stdout);\n  MTestSleep(30);\n  fprintf(stdout, \n          \"%s:%d: MTestSleep done; all children should have already exited\\n\",\n          __FILE__, __LINE__ );fflush(stdout);\n\n  return 0;\n}", "label": "int main( int argc, char *argv[] ) {\n  int np = NUM_SPAWNS;\n  int my_rank, size;\n  int errcodes[NUM_SPAWNS];\n  MPI_Comm allcomm;\n  MPI_Comm intercomm;\n\n  MPI_Init( &argc, &argv );\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\n  MPI_Comm_spawn( (char*)\"./spawntest_child\", MPI_ARGV_NULL, np,\n                  MPI_INFO_NULL, 0, MPI_COMM_WORLD, &intercomm, errcodes );\n\n  if ( intercomm == MPI_COMM_NULL ) {\n      fprintf(stdout, \"intercomm is null\\n\");\n  }\n\n  MPI_Intercomm_merge(intercomm, 0, &allcomm);\n\n  MPI_Comm_rank(allcomm, &my_rank);\n  MPI_Comm_size(allcomm, &size);\n\n  \n\n  MPI_Barrier( allcomm );\n  \n\n  MPI_Comm_free( &allcomm );\n  MPI_Comm_disconnect( &intercomm );\n\n  fprintf(stdout, \"%s:%d: MTestSleep starting; children should exit\\n\",\n          __FILE__, __LINE__ );fflush(stdout);\n  MTestSleep(30);\n  fprintf(stdout, \n          \"%s:%d: MTestSleep done; all children should have already exited\\n\",\n          __FILE__, __LINE__ );fflush(stdout);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "Unidata_448", "code": "int main(int argc, char* argv[])\n{\n    int ecode = 0;\n    int ncid;\n    int cmode, format;\n    int nprocs, rank;\n    MPI_Comm comm=MPI_COMM_SELF;\n    MPI_Info info=MPI_INFO_NULL;\n\n    printf(\"\\n*** Testing nc_inq_format_extended for PnetCDF...\");\n\n\n    if (nprocs > 1 && rank == 0)\n        printf(\"This test program is intended to run on ONE process\\n\");\n    if (rank > 0) goto fn_exit;\n\n    \n\n#ifdef DISABLE_PNETCDF_ALIGNMENT\n#endif\n\n    \n\n    cmode = NC_CLOBBER;\n    if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n\n    if (nc_enddef(ncid)) ERR;\n\n    if(nc_inq_format_extended(ncid,&format,&cmode)) ERR;\n    if(format != NC_FORMATX_PNETCDF) {\n\tprintf(\"***FAIL at line %d: format was %d ; expected %d\\n\",__LINE__,format,NC_FORMATX_PNETCDF);\n\tecode = 1;\n\tERR;\n    }\n    if (nc_close(ncid)) ERR;\n\n    \n\n    cmode = NC_CLOBBER | NC_64BIT_OFFSET;\n    if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n\n    if (nc_enddef(ncid)) ERR;\n\n    if(nc_inq_format_extended(ncid,&format,&cmode)) ERR;\n    if((cmode & NC_64BIT_OFFSET) != NC_64BIT_OFFSET) {\n\tprintf(\"***FAIL at line %d: mode was %08x ; expected %08x\\n\",__LINE__,cmode,NC_64BIT_OFFSET);\n\tecode = 1;\n\tERR;\n    }\n    if(format != NC_FORMATX_PNETCDF) {\n\tprintf(\"***FAIL at line %d: format was %d ; expected %d\\n\",__LINE__,format,NC_FORMATX_PNETCDF);\n\tecode = 1;\n\tERR;\n    }\n    if (nc_close(ncid)) ERR;\n\n    \n\n    cmode = NC_CLOBBER | NC_64BIT_DATA;\n    if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n\n    if (nc_enddef(ncid)) ERR;\n\n    if(nc_inq_format_extended(ncid,&format,&cmode)) ERR;\n    if((cmode & NC_64BIT_DATA) != NC_64BIT_DATA) {\n\tprintf(\"***FAIL at line %d: mode was %08x ; expected %08x\\n\",__LINE__,cmode,NC_64BIT_DATA);\n\tecode = 1;\n\tERR;\n    }\n    if(format != NC_FORMATX_PNETCDF) {\n\tprintf(\"***FAIL at line %d: format was %d ; expected %d\\n\",__LINE__,format,NC_FORMATX_PNETCDF);\n\tecode = 1;\n\tERR;\n    }\n    if (nc_close(ncid)) ERR;\n\nfn_exit:\n    SUMMARIZE_ERR;\n    FINAL_RESULTS;\n    return ecode;\n}", "label": "int main(int argc, char* argv[])\n{\n    int ecode = 0;\n    int ncid;\n    int cmode, format;\n    int nprocs, rank;\n    MPI_Comm comm=MPI_COMM_SELF;\n    MPI_Info info=MPI_INFO_NULL;\n\n    printf(\"\\n*** Testing nc_inq_format_extended for PnetCDF...\");\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (nprocs > 1 && rank == 0)\n        printf(\"This test program is intended to run on ONE process\\n\");\n    if (rank > 0) goto fn_exit;\n\n    \n\n#ifdef DISABLE_PNETCDF_ALIGNMENT\n    MPI_Info_create(&info);\n    MPI_Info_set(info, \"nc_header_align_size\", \"1\");\n    MPI_Info_set(info, \"nc_var_align_size\",    \"1\");\n#endif\n\n    \n\n    cmode = NC_CLOBBER;\n    if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n\n    if (nc_enddef(ncid)) ERR;\n\n    if(nc_inq_format_extended(ncid,&format,&cmode)) ERR;\n    if(format != NC_FORMATX_PNETCDF) {\n\tprintf(\"***FAIL at line %d: format was %d ; expected %d\\n\",__LINE__,format,NC_FORMATX_PNETCDF);\n\tecode = 1;\n\tERR;\n    }\n    if (nc_close(ncid)) ERR;\n\n    \n\n    cmode = NC_CLOBBER | NC_64BIT_OFFSET;\n    if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n\n    if (nc_enddef(ncid)) ERR;\n\n    if(nc_inq_format_extended(ncid,&format,&cmode)) ERR;\n    if((cmode & NC_64BIT_OFFSET) != NC_64BIT_OFFSET) {\n\tprintf(\"***FAIL at line %d: mode was %08x ; expected %08x\\n\",__LINE__,cmode,NC_64BIT_OFFSET);\n\tecode = 1;\n\tERR;\n    }\n    if(format != NC_FORMATX_PNETCDF) {\n\tprintf(\"***FAIL at line %d: format was %d ; expected %d\\n\",__LINE__,format,NC_FORMATX_PNETCDF);\n\tecode = 1;\n\tERR;\n    }\n    if (nc_close(ncid)) ERR;\n\n    \n\n    cmode = NC_CLOBBER | NC_64BIT_DATA;\n    if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n\n    if (nc_enddef(ncid)) ERR;\n\n    if(nc_inq_format_extended(ncid,&format,&cmode)) ERR;\n    if((cmode & NC_64BIT_DATA) != NC_64BIT_DATA) {\n\tprintf(\"***FAIL at line %d: mode was %08x ; expected %08x\\n\",__LINE__,cmode,NC_64BIT_DATA);\n\tecode = 1;\n\tERR;\n    }\n    if(format != NC_FORMATX_PNETCDF) {\n\tprintf(\"***FAIL at line %d: format was %d ; expected %d\\n\",__LINE__,format,NC_FORMATX_PNETCDF);\n\tecode = 1;\n\tERR;\n    }\n    if (nc_close(ncid)) ERR;\n\nfn_exit:\n    MPI_Finalize();\n    SUMMARIZE_ERR;\n    FINAL_RESULTS;\n    return ecode;\n}"}
{"program": "eliask_449", "code": "main(int argc, char *argv[])\n{\n  int streamnum, nstreams, *stream;\n  float rn;\n  int i, myid, nprocs;\n\n\n  \n\n            \n\n\n\n\n  \n\n            \n  streamnum = myid;\n  nstreams = nprocs;\t\t\n\n\n  stream = init_sprng(streamnum,nstreams,SEED,SPRNG_DEFAULT);\t\n\n  printf(\"Process %d: Print information about stream:\\n\",myid);\n  print_sprng(stream);\n\n  \n\n            \n  for (i=0;i<3;i++)\n  {\n    rn = sprng(stream);\t\t\n\n    printf(\"Process %d, random number %d: %f\\n\", myid, i+1, rn);\n  }\n\n  \n\n            \n  free_sprng(stream);           \n\n\n\n}", "label": "main(int argc, char *argv[])\n{\n  int streamnum, nstreams, *stream;\n  float rn;\n  int i, myid, nprocs;\n\n\n  \n\n            \n  MPI_Init(&argc, &argv);       \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\t\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs); \n\n\n  \n\n            \n  streamnum = myid;\n  nstreams = nprocs;\t\t\n\n\n  stream = init_sprng(streamnum,nstreams,SEED,SPRNG_DEFAULT);\t\n\n  printf(\"Process %d: Print information about stream:\\n\",myid);\n  print_sprng(stream);\n\n  \n\n            \n  for (i=0;i<3;i++)\n  {\n    rn = sprng(stream);\t\t\n\n    printf(\"Process %d, random number %d: %f\\n\", myid, i+1, rn);\n  }\n\n  \n\n            \n  free_sprng(stream);           \n\n\n  MPI_Finalize();\t\t\n\n}"}
{"program": "atchekho_450", "code": "int mpi_init(int argc,char *argv[])\n{\n  int dim;\n#ifdef MPI\n  int ntiles=1;\n  int error = 0;\n#endif\n  \n  \n\n  \n\n  \n\n  \n\n  \n\n  \n  \n\n  mpi_periods[1] = 0;\n  mpi_periods[2] = 0;\n  mpi_periods[3] = 0;\n#if PERIODIC && !BL\n  mpi_periods[1] = 1;\n  mpi_periods[2] = 1;\n  mpi_periods[3] = 1;\n#endif\n#if BL\n  mpi_periods[3] = 1;\n#endif\n  \n\n#ifdef MPI\n  ntiles=1;\n  error = 0;\n\n  \n\n  mpi_reorder = 0; \n\n  \n  \n\n  if (argc < MPI_NDIM+1) {\n    printf(\"Usage: %s ntiles1 ntiles2 ntiles3\\n\", argv[0]);\n    exit(1);\n  }\n\n  \n\n\n  \n\n  \n\n  for( dim = 1; dim < NDIM; dim++){\n    mpi_dims[dim] = atoi(argv[dim]);\n    ntiles *= mpi_dims[dim];\n  }\n\n  mpi_ntile[1] = N1;\n  mpi_ntile[2] = N2;\n  mpi_ntile[3] = N3;\n  \n  for(dim=1;dim<NDIM;dim++) {\n    mpi_ntot[dim]=mpi_ntile[dim]*mpi_dims[dim];\n    if(mpi_dims[dim]>1 && mpi_ntile[dim]==1){\n      fprintf(stderr,\"Cannot have N%d = 1 for nonunity Ntile%d = %d\\n\", dim, dim, mpi_ntile[dim]);\n      exit(1);\n    }\n  }\n  \n  if (ntiles != mpi_numtasks) {\n    \n\n    printf(\"The number of tiles, %d, does not match the number of tasks, %d\\n\",\n           ntiles, mpi_numtasks);\n    fflush(stdout);\n    exit(1);\n  }\n\n  \n\n  \n\n  \n\n  i_am_the_master = (MASTER == mpi_rank);\n  \n\n  \n\n\n  \n\n  if( 0 == mpi_rank ) {\n    printf(\"CPU geometry: %d %d %d (total number of tasks: %d)\\n\", mpi_dims[1], mpi_dims[2], mpi_dims[3], mpi_numtasks);\n    printf(\"Tile size: %d %d %d\\n\", N1, N2, N3);\n    printf(\"Total resolution: %d %d %d\\n\", mpi_ntot[1], mpi_ntot[2], mpi_ntot[3]);\n  }\n  \n  printf( \"MPI process %4d (%4d,%4d,%4d) has PID = %d\\n\", mpi_rank, mpi_coords[1], mpi_coords[2], mpi_coords[3], getpid() );\n  fflush(stdout);\n  \n\n\n  \n\n  for( dim = 1; dim < NDIM; dim++) {\n    \n\n    mpi_startn[dim] = mpi_coords[dim]*mpi_ntile[dim];\n  }\n  \n#else\n  \n\n  for(dim=1;dim<NDIM;dim++){\n    mpi_coords[dim]=0;\n    mpi_startn[dim]=0;\n    mpi_dims[dim]=1;\n  }\n  mpi_ntot[1]=N1;\n  mpi_ntot[2]=N2;\n  mpi_ntot[3]=N3;\n  mpi_rank = MASTER;\n  i_am_the_master = 1;\n#endif  \n\n\n  return(0);\n}", "label": "int mpi_init(int argc,char *argv[])\n{\n  int dim;\n#ifdef MPI\n  int ntiles=1;\n  int error = 0;\n#endif\n  \n  \n\n  \n\n  \n\n  \n\n  \n\n  \n  \n\n  mpi_periods[1] = 0;\n  mpi_periods[2] = 0;\n  mpi_periods[3] = 0;\n#if PERIODIC && !BL\n  mpi_periods[1] = 1;\n  mpi_periods[2] = 1;\n  mpi_periods[3] = 1;\n#endif\n#if BL\n  mpi_periods[3] = 1;\n#endif\n  \n\n#ifdef MPI\n  ntiles=1;\n  error = 0;\n\n  \n\n  mpi_reorder = 0; \n\n  \n  \n\n  if (argc < MPI_NDIM+1) {\n    printf(\"Usage: %s ntiles1 ntiles2 ntiles3\\n\", argv[0]);\n    exit(1);\n  }\n\n  \n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_numtasks);\n\n  \n\n  \n\n  for( dim = 1; dim < NDIM; dim++){\n    mpi_dims[dim] = atoi(argv[dim]);\n    ntiles *= mpi_dims[dim];\n  }\n\n  mpi_ntile[1] = N1;\n  mpi_ntile[2] = N2;\n  mpi_ntile[3] = N3;\n  \n  for(dim=1;dim<NDIM;dim++) {\n    mpi_ntot[dim]=mpi_ntile[dim]*mpi_dims[dim];\n    if(mpi_dims[dim]>1 && mpi_ntile[dim]==1){\n      MPI_Finalize();\n      fprintf(stderr,\"Cannot have N%d = 1 for nonunity Ntile%d = %d\\n\", dim, dim, mpi_ntile[dim]);\n      exit(1);\n    }\n  }\n  \n  if (ntiles != mpi_numtasks) {\n    MPI_Finalize();\n    \n\n    printf(\"The number of tiles, %d, does not match the number of tasks, %d\\n\",\n           ntiles, mpi_numtasks);\n    fflush(stdout);\n    exit(1);\n  }\n\n  \n\n  \n\n  MPI_Cart_create(MPI_COMM_WORLD, MPI_NDIM, mpi_dims+1, mpi_periods+1, mpi_reorder, &mpi_cartcomm);\n  \n\n  MPI_Comm_rank(mpi_cartcomm, &mpi_rank);\n  i_am_the_master = (MASTER == mpi_rank);\n  \n\n  \n\n  MPI_Cart_coords(mpi_cartcomm, mpi_rank, MPI_NDIM, mpi_coords+1);\n\n  \n\n  if( 0 == mpi_rank ) {\n    printf(\"CPU geometry: %d %d %d (total number of tasks: %d)\\n\", mpi_dims[1], mpi_dims[2], mpi_dims[3], mpi_numtasks);\n    printf(\"Tile size: %d %d %d\\n\", N1, N2, N3);\n    printf(\"Total resolution: %d %d %d\\n\", mpi_ntot[1], mpi_ntot[2], mpi_ntot[3]);\n  }\n  \n  printf( \"MPI process %4d (%4d,%4d,%4d) has PID = %d\\n\", mpi_rank, mpi_coords[1], mpi_coords[2], mpi_coords[3], getpid() );\n  fflush(stdout);\n  \n\n\n  \n\n  for( dim = 1; dim < NDIM; dim++) {\n    MPI_Cart_shift(mpi_cartcomm, dim-1, 1, &mpi_nbrs[dim][0], &mpi_nbrs[dim][1]);\n    \n\n    mpi_startn[dim] = mpi_coords[dim]*mpi_ntile[dim];\n  }\n  \n#else\n  \n\n  for(dim=1;dim<NDIM;dim++){\n    mpi_coords[dim]=0;\n    mpi_startn[dim]=0;\n    mpi_dims[dim]=1;\n  }\n  mpi_ntot[1]=N1;\n  mpi_ntot[2]=N2;\n  mpi_ntot[3]=N3;\n  mpi_rank = MASTER;\n  i_am_the_master = 1;\n#endif  \n\n\n  return(0);\n}"}
{"program": "gnu3ra_451", "code": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    MPI_Status status;\n    MPI_Offset size;\n    long long *buf, i;\n    char *filename;\n    int j, mynod, nprocs, len, flag, err;\n\n\n\n    if (nprocs != 1) {\n\tfprintf(stderr, \"Run this program on one process only\\n\");\n    }\n\n    i = 1;\n    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n\ti++;\n\targv++;\n    }\n    if (i >= argc) {\n\tfprintf(stderr, \"\\n*#  Usage: large -fname filename\\n\\n\");\n    }\n    argv++;\n    len = strlen(*argv);\n    filename = (char *) malloc(len+1);\n    strcpy(filename, *argv);\n    fprintf(stderr, \"This program creates an 4 Gbyte file. Don't run it if you don't have that much disk space!\\n\");\n\n    buf = (long long *) malloc(SIZE * sizeof(long long));\n    if (!buf) {\n\tfprintf(stderr, \"not enough memory to allocate buffer\\n\");\n    }\n\n\n    for (i=0; i<NTIMES; i++) {\n\tfor (j=0; j<SIZE; j++)\n\t    buf[j] = i*SIZE + j;\n\t\n\terr =\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t}\n    }\n\n    fprintf(stderr, \"file size = %lld bytes\\n\", size);\n\n\n    for (j=0; j<SIZE; j++) buf[j] = -1;\n\n    flag = 0;\n    for (i=0; i<NTIMES; i++) {\n\terr =\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t}\n\tfor (j=0; j<SIZE; j++) \n\t    if (buf[j] != i*SIZE + j) {\n\t\tfprintf(stderr, \"error: buf %d is %lld, should be %lld \\n\", j, buf[j], \n                                 i*SIZE + j);\n\t\tflag = 1;\n\t    }\n    }\n\n    if (!flag) fprintf(stderr, \"Data read back is correct\\n\");\n\n    free(buf);\n    free(filename);\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    MPI_Status status;\n    MPI_Offset size;\n    long long *buf, i;\n    char *filename;\n    int j, mynod, nprocs, len, flag, err;\n\n    MPI_Init(&argc,&argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    if (nprocs != 1) {\n\tfprintf(stderr, \"Run this program on one process only\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    i = 1;\n    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n\ti++;\n\targv++;\n    }\n    if (i >= argc) {\n\tfprintf(stderr, \"\\n*#  Usage: large -fname filename\\n\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    argv++;\n    len = strlen(*argv);\n    filename = (char *) malloc(len+1);\n    strcpy(filename, *argv);\n    fprintf(stderr, \"This program creates an 4 Gbyte file. Don't run it if you don't have that much disk space!\\n\");\n\n    buf = (long long *) malloc(SIZE * sizeof(long long));\n    if (!buf) {\n\tfprintf(stderr, \"not enough memory to allocate buffer\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR,\n                  MPI_INFO_NULL, &fh);\n\n    for (i=0; i<NTIMES; i++) {\n\tfor (j=0; j<SIZE; j++)\n\t    buf[j] = i*SIZE + j;\n\t\n\terr = MPI_File_write(fh, buf, SIZE, MPI_DOUBLE, &status);\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n    }\n\n    MPI_File_get_size(fh, &size);\n    fprintf(stderr, \"file size = %lld bytes\\n\", size);\n\n    MPI_File_seek(fh, 0, MPI_SEEK_SET);\n\n    for (j=0; j<SIZE; j++) buf[j] = -1;\n\n    flag = 0;\n    for (i=0; i<NTIMES; i++) {\n\terr = MPI_File_read(fh, buf, SIZE, MPI_DOUBLE, &status);\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\tfor (j=0; j<SIZE; j++) \n\t    if (buf[j] != i*SIZE + j) {\n\t\tfprintf(stderr, \"error: buf %d is %lld, should be %lld \\n\", j, buf[j], \n                                 i*SIZE + j);\n\t\tflag = 1;\n\t    }\n    }\n\n    if (!flag) fprintf(stderr, \"Data read back is correct\\n\");\n    MPI_File_close(&fh);\n\n    free(buf);\n    free(filename);\n    MPI_Finalize(); \n    return 0;\n}"}
{"program": "michel2323_452", "code": "int main(int narg, char *arg[])\n{\n  comm_ext world; int np;\n  struct comm comm;\n  struct crystal crystal;\n  struct array A, A0=null_array; r_work *row, *row_0;\n  uint i;\n#ifdef MPI\n  world = MPI_COMM_WORLD;\n#else\n  world=0, np=1;\n#endif\n\n  comm_init(&comm,world);\n  crystal_init(&crystal,&comm);\n\n  array_init(r_work,&A,np*3), A.n=np*3, row=A.ptr;\n  for(i=0;i<A.n;++i) {\n    row[i].i = rand();\n    row[i].l = row[i].l2 = rand();\n    row[i].p = rand()%np;\n    row[i].d = rand()/(double)rand();\n  }\n  \n  sarray_sort_3(r_work,row,A.n, i,0, l,1, p,0, &crystal.data);\n  \n  for(i=0;i<A.n;++i)\n    printf(\"%02d send -> %02d: %08x %08x %d %g\\n\",\n      (int)comm.id,(int)row[i].p,(int)row[i].i,\n      (int)row[i].l,(int)row[i].p,row[i].d);\n  \n  array_cat(r_work,&A0, row,A.n);\n  \n  sarray_transfer(r_work,&A, p,1, &crystal);\n\n  row=A.ptr;\n  for(i=0;i<A.n;++i)\n    printf(\"%02d recv <- %02d: %08x %08x %d %g\\n\",\n      (int)comm.id,(int)row[i].p,(int)row[i].i,\n      (int)row[i].l,(int)row[i].p,row[i].d);\n\n  sarray_transfer(r_work,&A, p,1, &crystal);\n  sarray_sort_3(r_work,row,A.n, i,0, l,1, p,0, &crystal.data);\n  if(A.n!=A0.n)\n    fail(1,__FILE__,__LINE__,\"final array has different length than original\");\n  row=A.ptr, row_0=A0.ptr;\n  for(i=0;i<A.n;++i)\n    if(   row[i].d != row_0[i].d\n       || row[i].l != row_0[i].l\n       || row[i].l2!= row_0[i].l2\n       || row[i].i != row_0[i].i\n       || row[i].p != row_0[i].p)\n      fail(1,__FILE__,__LINE__,\"final array differs from original\");\n      \n  array_free(&A0);\n  array_free(&A);\n  crystal_free(&crystal);\n\n  fflush(stdout); comm_barrier(&comm);\n  if(comm.id==0) printf(\"tests passed\\n\"), fflush(stdout);\n  \n  comm_free(&comm);\n  \n#ifdef MPI\n#endif\n\n  return 0;\n}", "label": "int main(int narg, char *arg[])\n{\n  comm_ext world; int np;\n  struct comm comm;\n  struct crystal crystal;\n  struct array A, A0=null_array; r_work *row, *row_0;\n  uint i;\n#ifdef MPI\n  MPI_Init(&narg,&arg);\n  world = MPI_COMM_WORLD;\n  MPI_Comm_size(world,&np);\n#else\n  world=0, np=1;\n#endif\n\n  comm_init(&comm,world);\n  crystal_init(&crystal,&comm);\n\n  array_init(r_work,&A,np*3), A.n=np*3, row=A.ptr;\n  for(i=0;i<A.n;++i) {\n    row[i].i = rand();\n    row[i].l = row[i].l2 = rand();\n    row[i].p = rand()%np;\n    row[i].d = rand()/(double)rand();\n  }\n  \n  sarray_sort_3(r_work,row,A.n, i,0, l,1, p,0, &crystal.data);\n  \n  for(i=0;i<A.n;++i)\n    printf(\"%02d send -> %02d: %08x %08x %d %g\\n\",\n      (int)comm.id,(int)row[i].p,(int)row[i].i,\n      (int)row[i].l,(int)row[i].p,row[i].d);\n  \n  array_cat(r_work,&A0, row,A.n);\n  \n  sarray_transfer(r_work,&A, p,1, &crystal);\n\n  row=A.ptr;\n  for(i=0;i<A.n;++i)\n    printf(\"%02d recv <- %02d: %08x %08x %d %g\\n\",\n      (int)comm.id,(int)row[i].p,(int)row[i].i,\n      (int)row[i].l,(int)row[i].p,row[i].d);\n\n  sarray_transfer(r_work,&A, p,1, &crystal);\n  sarray_sort_3(r_work,row,A.n, i,0, l,1, p,0, &crystal.data);\n  if(A.n!=A0.n)\n    fail(1,__FILE__,__LINE__,\"final array has different length than original\");\n  row=A.ptr, row_0=A0.ptr;\n  for(i=0;i<A.n;++i)\n    if(   row[i].d != row_0[i].d\n       || row[i].l != row_0[i].l\n       || row[i].l2!= row_0[i].l2\n       || row[i].i != row_0[i].i\n       || row[i].p != row_0[i].p)\n      fail(1,__FILE__,__LINE__,\"final array differs from original\");\n      \n  array_free(&A0);\n  array_free(&A);\n  crystal_free(&crystal);\n\n  fflush(stdout); comm_barrier(&comm);\n  if(comm.id==0) printf(\"tests passed\\n\"), fflush(stdout);\n  \n  comm_free(&comm);\n  \n#ifdef MPI\n  MPI_Finalize();\n#endif\n\n  return 0;\n}"}
{"program": "ebaty_454", "code": "int main(int argc, char **argv) {\n\n\t\n\n\tFILE *fp = fopen(kFileName, \"w\");\n\tif ( fp == NULL ) {\n\t\tprintf(\"can't open %s.\\n\", kFileName);\n\t\treturn 1;\n\t}\n\n\tlong long i;\n\tfor(i = 1LL << 25 ; i >= 1; i >>= 1) {\n\t\tlong long size;\n\t\tdouble ave_time = 0;\n\t\tif ( i == 2147483648 ) i = 2147483647;\n\n\t\tprintf(\"%s, %d\\n\", __PRETTY_FUNCTION__, i);\n\t\tint j;\n\t\tfor(j = 0; j < 10; ++j) {\n\t\t\tmpi_result result = sendData(i);\n\t\t\tave_time += result.time;\n\t\t\tsize = result.size;\n\t\t}\n\t\tave_time /= 10.0f;\n\n\t\tfprintf(fp, \"%lld\\t%lf\\n\", size, ave_time);\n\t}\n\n\tfclose(fp);\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv) {\n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tFILE *fp = fopen(kFileName, \"w\");\n\tif ( fp == NULL ) {\n\t\tprintf(\"can't open %s.\\n\", kFileName);\n\t\treturn 1;\n\t}\n\n\tlong long i;\n\tfor(i = 1LL << 25 ; i >= 1; i >>= 1) {\n\t\tlong long size;\n\t\tdouble ave_time = 0;\n\t\tif ( i == 2147483648 ) i = 2147483647;\n\n\t\tprintf(\"%s, %d\\n\", __PRETTY_FUNCTION__, i);\n\t\tint j;\n\t\tfor(j = 0; j < 10; ++j) {\n\t\t\tmpi_result result = sendData(i);\n\t\t\tave_time += result.time;\n\t\t\tsize = result.size;\n\t\t}\n\t\tave_time /= 10.0f;\n\n\t\tfprintf(fp, \"%lld\\t%lf\\n\", size, ave_time);\n\t}\n\n\tfclose(fp);\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "qingu_455", "code": "int main(int argc, char ** argv) {\n  int    rank, nproc, val, i;\n  void **base_ptrs;\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCI mutex read-modify-write test with %d processes\\n\", nproc);\n\n  base_ptrs = malloc(nproc*sizeof(void*));\n\n  ARMCI_Create_mutexes(rank == 0 ? 1 : 0);\n  ARMCI_Malloc(base_ptrs, (rank == 0) ? sizeof(int) : 0); \n\n\n  if (rank == 0) {\n    val = 0;\n    ARMCI_Put(&val, base_ptrs[0], sizeof(int), 0);\n  }\n\n  ARMCI_Barrier();\n\n  for (i = 0; i < NITER; i++) {\n    ARMCI_Lock(0, 0);\n\n    ARMCI_Get(base_ptrs[0], &val, sizeof(int), 0);\n    val += ADDIN;\n    ARMCI_Put(&val, base_ptrs[0], sizeof(int), 0);\n\n    ARMCI_Unlock(0, 0);\n  }\n\n  printf(\" + %3d done\\n\", rank);\n  fflush(NULL);\n\n  ARMCI_Barrier();\n\n  if (rank == 0) {\n    ARMCI_Get(base_ptrs[0], &val, sizeof(int), 0);\n\n    if (val == ADDIN*nproc*NITER)\n      printf(\"Test complete: PASS.\\n\");\n    else\n      printf(\"Test complete: FAIL.  Got %d, expected %d.\\n\", val, ADDIN*nproc*NITER);\n  }\n\n  ARMCI_Free(base_ptrs[rank]);\n  ARMCI_Destroy_mutexes();\n  free(base_ptrs);\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int    rank, nproc, val, i;\n  void **base_ptrs;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCI mutex read-modify-write test with %d processes\\n\", nproc);\n\n  base_ptrs = malloc(nproc*sizeof(void*));\n\n  ARMCI_Create_mutexes(rank == 0 ? 1 : 0);\n  ARMCI_Malloc(base_ptrs, (rank == 0) ? sizeof(int) : 0); \n\n\n  if (rank == 0) {\n    val = 0;\n    ARMCI_Put(&val, base_ptrs[0], sizeof(int), 0);\n  }\n\n  ARMCI_Barrier();\n\n  for (i = 0; i < NITER; i++) {\n    ARMCI_Lock(0, 0);\n\n    ARMCI_Get(base_ptrs[0], &val, sizeof(int), 0);\n    val += ADDIN;\n    ARMCI_Put(&val, base_ptrs[0], sizeof(int), 0);\n\n    ARMCI_Unlock(0, 0);\n  }\n\n  printf(\" + %3d done\\n\", rank);\n  fflush(NULL);\n\n  ARMCI_Barrier();\n\n  if (rank == 0) {\n    ARMCI_Get(base_ptrs[0], &val, sizeof(int), 0);\n\n    if (val == ADDIN*nproc*NITER)\n      printf(\"Test complete: PASS.\\n\");\n    else\n      printf(\"Test complete: FAIL.  Got %d, expected %d.\\n\", val, ADDIN*nproc*NITER);\n  }\n\n  ARMCI_Free(base_ptrs[rank]);\n  ARMCI_Destroy_mutexes();\n  free(base_ptrs);\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "raulperula_456", "code": "int main(int argc, char** argv)\n{\n\tint myrank, size;\n\tchar cad1[N], cad2[N];\n\tMPI_Status estado;\n\t\n\n\t\n\n\n\t\n\tsprintf(cad1, \"Hola mundo!!\\n\", myrank);\n\t\n\tif(myrank == 0){\n\t\tbzero(cad2, sizeof(cad2));\n\t\tprintf(\"Padre %d: %s\\n\", myrank, cad2);\n\t}\n\telse{\n\t\tbzero(cad2, sizeof(cad2));\n\t\tprintf(\"Hijo %d: %s\\n\", myrank, cad2);\n\t}\n\t\n\n\t\n\treturn 0;\n}", "label": "int main(int argc, char** argv)\n{\n\tint myrank, size;\n\tchar cad1[N], cad2[N];\n\tMPI_Status estado;\n\t\n\tMPI_Init(&argc, &argv); \n\n\t\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n\n\tMPI_Comm_size(MPI_COMM_WORLD, &size); \n\n\t\n\tsprintf(cad1, \"Hola mundo!!\\n\", myrank);\n\t\n\tif(myrank == 0){\n\t\tbzero(cad2, sizeof(cad2));\n\t\tMPI_Scatter(cad1, 2, MPI_CHAR, cad2, 2, MPI_CHAR, 0, MPI_COMM_WORLD);\n\t\tprintf(\"Padre %d: %s\\n\", myrank, cad2);\n\t}\n\telse{\n\t\tbzero(cad2, sizeof(cad2));\n\t\tMPI_Scatter(cad1, 2, MPI_CHAR, cad2, 2, MPI_CHAR, 0, MPI_COMM_WORLD);\n\t\tprintf(\"Hijo %d: %s\\n\", myrank, cad2);\n\t}\n\t\n\tMPI_Finalize(); \n\n\t\n\treturn 0;\n}"}
{"program": "rahlk_457", "code": "int main (int argc, char *argv[])\n{\n\n  \n\n  int   numproc, rank, len;\n\n  \n\n  char  hostname[MPI_MAX_PROCESSOR_NAME];\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n  printf (\"Hello from task %d on %s!\\n\", rank, hostname);\n\n  \n\n  if (rank == ROOT)\n   printf(\"MASTER: Number of MPI tasks is: %d\\n\", numproc);\n\n  \n\n\n}", "label": "int main (int argc, char *argv[])\n{\n\n  \n\n  int   numproc, rank, len;\n\n  \n\n  char  hostname[MPI_MAX_PROCESSOR_NAME];\n\n  \n\n  MPI_Init(&argc, &argv);\n\n  \n\n  MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n\n  \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  MPI_Get_processor_name(hostname, &len);\n\n  \n\n  printf (\"Hello from task %d on %s!\\n\", rank, hostname);\n\n  \n\n  if (rank == ROOT)\n   printf(\"MASTER: Number of MPI tasks is: %d\\n\", numproc);\n\n  \n\n  MPI_Finalize();\n\n}"}
{"program": "germasch_459", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n\n  \n\n  mrc_domain_set_param_int3(domain, \"m\", (int [3]) { 160, 1, 1 });\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  mrc_crds_set_param_int(crds, \"sw\", BND);\n  mrc_crds_set_param_double3(crds, \"l\", (double[3]) { -8., 0., 0. });\n  mrc_crds_set_param_double3(crds, \"h\", (double[3]) {  8., 0., 0. });\n\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n\n  struct mrc_fld *x = mrc_domain_f1_create(domain);\n  mrc_fld_set_name(x, \"x\");\n  mrc_fld_set_param_int(x, \"nr_ghosts\", BND);\n  mrc_fld_set_param_int(x, \"nr_comps\", NR_FLDS);\n  mrc_fld_setup(x);\n  mrc_fld_set_comp_name(x, U, \"u\");\n\n  \n\n  mrc_f1_foreach(x, ix, 0, 0) {\n    \n\n    MRC_F1(x, U, ix) = -12. * 1./sqr(cosh(CRDX(ix))); \n\n  } mrc_f1_foreach_end;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, NULL, NULL);\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, calc_rhs, domain);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n\n  \n\n  mrc_domain_set_param_int3(domain, \"m\", (int [3]) { 160, 1, 1 });\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  mrc_crds_set_param_int(crds, \"sw\", BND);\n  mrc_crds_set_param_double3(crds, \"l\", (double[3]) { -8., 0., 0. });\n  mrc_crds_set_param_double3(crds, \"h\", (double[3]) {  8., 0., 0. });\n\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n\n  struct mrc_fld *x = mrc_domain_f1_create(domain);\n  mrc_fld_set_name(x, \"x\");\n  mrc_fld_set_param_int(x, \"nr_ghosts\", BND);\n  mrc_fld_set_param_int(x, \"nr_comps\", NR_FLDS);\n  mrc_fld_setup(x);\n  mrc_fld_set_comp_name(x, U, \"u\");\n\n  \n\n  mrc_f1_foreach(x, ix, 0, 0) {\n    \n\n    MRC_F1(x, U, ix) = -12. * 1./sqr(cosh(CRDX(ix))); \n\n  } mrc_f1_foreach_end;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, NULL, NULL);\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, calc_rhs, domain);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "aababilov_460", "code": "int main(int argc, char **argv)\n{\n\n\t\n\n\tparg(argc, argv);\n\tparg(argc, argv);\n\t\n\n\n\tif (myrank == 0) {\n\t\tmaster();\n\t} else {\n\t\tslave();\n\t}\n\n\t\n\n\tprintf(\"finr....\\n\");\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n\t\n\n\tparg(argc, argv);\n\tMPI_Init(&argc, &argv);\n\tparg(argc, argv);\n\t\n\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\tif (myrank == 0) {\n\t\tmaster();\n\t} else {\n\t\tslave();\n\t}\n\n\t\n\n\tprintf(\"finr....\\n\");\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "gnu3ra_462", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = indexed_contig_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_contig_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_first_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_first_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_middle_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_middle_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_last_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_last_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = indexed_contig_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_contig_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_first_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_first_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_middle_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_middle_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_last_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_last_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "TuftsBCB_463", "code": "int\nmain(int argc, char **argv)\n{\n  int              status   = eslOK;\n\n  ESL_GETOPTS     *go  = NULL;\t\n\n  struct cfg_s     cfg;         \n\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.qfile      = NULL;\n  cfg.dbfile     = NULL;\n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.qfile, &cfg.dbfile);    \n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  int              status   = eslOK;\n\n  ESL_GETOPTS     *go  = NULL;\t\n\n  struct cfg_s     cfg;         \n\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.qfile      = NULL;\n  cfg.dbfile     = NULL;\n\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.qfile, &cfg.dbfile);    \n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n      MPI_Init(&argc, &argv);\n      MPI_Comm_rank(MPI_COMM_WORLD, &(cfg.my_rank));\n      MPI_Comm_size(MPI_COMM_WORLD, &(cfg.nproc));\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n      MPI_Finalize();\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}"}
{"program": "arjona00_466", "code": "int main(int argc, char** argv) {\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: bin numbers_per_proc\\n\");\n    exit(1);\n  }\n\n  \n\n  int numbers_per_proc = atoi(argv[1]);\n\n\n  int world_rank;\n  int world_size;\n\n  \n\n  srand(time(NULL) * world_rank);\n\n  \n\n  \n\n  float *rand_nums = create_random_numbers(numbers_per_proc);\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  int *send_amounts_per_proc = get_send_amounts_per_proc(rand_nums,\n                                                         numbers_per_proc,\n                                                         world_size); \n\n  \n\n  \n\n  int *recv_amounts_per_proc = get_recv_amounts_per_proc(send_amounts_per_proc,\n                                                         world_size);\n\n  \n\n  \n\n  int *send_offsets_per_proc = prefix_sum(send_amounts_per_proc, world_size);\n  int *recv_offsets_per_proc = prefix_sum(recv_amounts_per_proc, world_size);\n\n  \n\n  \n\n  int total_recv_amount = sum(recv_amounts_per_proc, world_size);\n  float *binned_nums = (float *)malloc(sizeof(float) * total_recv_amount);\n\n  \n\n  \n\n  \n\n  \n\n  qsort(rand_nums, numbers_per_proc, sizeof(float), &compare_float); \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  printf(\"Process %d received %d numbers in bin [%f - %f)\\n\", world_rank, total_recv_amount,\n         get_bin_start(world_rank, world_size), get_bin_end(world_rank, world_size));\n\n  \n\n  verify_bin_nums(binned_nums, total_recv_amount, world_rank, world_size);\n\n\n  \n\n  free(rand_nums);\n  free(send_amounts_per_proc);\n  free(recv_amounts_per_proc);\n  free(send_offsets_per_proc);\n  free(recv_offsets_per_proc);\n  free(binned_nums);\n}", "label": "int main(int argc, char** argv) {\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: bin numbers_per_proc\\n\");\n    exit(1);\n  }\n\n  \n\n  int numbers_per_proc = atoi(argv[1]);\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  srand(time(NULL) * world_rank);\n\n  \n\n  \n\n  float *rand_nums = create_random_numbers(numbers_per_proc);\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  int *send_amounts_per_proc = get_send_amounts_per_proc(rand_nums,\n                                                         numbers_per_proc,\n                                                         world_size); \n\n  \n\n  \n\n  int *recv_amounts_per_proc = get_recv_amounts_per_proc(send_amounts_per_proc,\n                                                         world_size);\n\n  \n\n  \n\n  int *send_offsets_per_proc = prefix_sum(send_amounts_per_proc, world_size);\n  int *recv_offsets_per_proc = prefix_sum(recv_amounts_per_proc, world_size);\n\n  \n\n  \n\n  int total_recv_amount = sum(recv_amounts_per_proc, world_size);\n  float *binned_nums = (float *)malloc(sizeof(float) * total_recv_amount);\n\n  \n\n  \n\n  \n\n  \n\n  qsort(rand_nums, numbers_per_proc, sizeof(float), &compare_float); \n\n  \n\n  \n\n  \n\n  \n\n  MPI_Alltoallv(rand_nums, send_amounts_per_proc, send_offsets_per_proc, MPI_FLOAT,\n                binned_nums, recv_amounts_per_proc, recv_offsets_per_proc, MPI_FLOAT,\n                MPI_COMM_WORLD);\n\n  \n\n  printf(\"Process %d received %d numbers in bin [%f - %f)\\n\", world_rank, total_recv_amount,\n         get_bin_start(world_rank, world_size), get_bin_end(world_rank, world_size));\n\n  \n\n  verify_bin_nums(binned_nums, total_recv_amount, world_rank, world_size);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n\n  \n\n  free(rand_nums);\n  free(send_amounts_per_proc);\n  free(recv_amounts_per_proc);\n  free(send_offsets_per_proc);\n  free(recv_offsets_per_proc);\n  free(binned_nums);\n}"}
{"program": "arturocastro_468", "code": "int main (int argc, char *argv[])\n{\n    int ncol,nrow;             \n\n    double Gnew,r,omega,       \n\n\tstopdiff,maxdiff,diff;  \n\n    int i,j,phase,iteration;   \n\n    int print = 0;\n    double inittime, totaltime;\n     \n    \n\n     \n    \n\n    N = 20;\n     \n    for(i=1; i<argc; i++) {\n\tif(!strcmp(argv[i], \"-print\")) {\n\t    print = 1;\n\t} else {\n\t    N = atoi(argv[i]);\n\t}\n    }\n     \n    if(mynode == 0) {\n\tfprintf(stderr, \"Running %d x %d SOR\\n\", N, N);\n    }\n     \n    N += 2; \n\n            \n\n     \n            \n\n    ncol = nrow = N;\n    r        = 0.5 * ( cos( M_PI / ncol ) + cos( M_PI / nrow ) );\n    omega    = 2.0 / ( 1 + sqrt( 1 - r * r ) );\n    stopdiff = TOLERANCE / ( 2.0 - omega );\n    omega   *= MAGIC;\n     \n    alloc_grid(&G, N);\n    alloc_grid(&buff, N);\n    init_grid(G, N);\n     \n    \n\n    bufs1 = (double *) malloc(N * sizeof (double));\n    bufr1 = (double *) malloc(N * sizeof (double));\n     \n    \n\n    bufs2 = (double *) malloc(N * sizeof (double));\n    bufr2 = (double *) malloc(N * sizeof (double));\n     \n    \n\n    range(1, N - 1, totalnodes, mynode, &jsta, &jend);\n    \n    printf(\"numtasks = %d\\nrank %d: from %d to %d\\n\", totalnodes, mynode, jsta, jend);\n     \n    inext = mynode + 1;\n    iprev = mynode - 1;\n     \n    if (inext == totalnodes)\n\tinext = MPI_PROC_NULL;\n     \n    if (iprev == -1)\n\tiprev = MPI_PROC_NULL;\n     \n    inittime =\n     \n    \n\n    iteration = 0;\n    do {\n\tmaxdiff = 0.0;\n\tfor ( phase = 0; phase < 2 ; phase++){\n     \n\t    shift(phase);\n     \n\t    for ( i = 1 ; i < N-1 ; i++ ){\n\t\tfor ( j = jsta + (even(i) ^ phase); j <= jend ; j += 2 ){\n\t\t    Gnew = stencil(G,i,j);\n\t\t    diff = fabs(Gnew - G[i][j]);\n\t\t    if ( diff > maxdiff )\n\t\t\tmaxdiff = diff;\n\t\t    G[i][j] = G[i][j] + omega * (Gnew-G[i][j]);\n\t\t}\n\t    }\n\t}\n     \n\tmaxdiff = diff;\n     \n\titeration++;\n    } while (maxdiff > stopdiff);\n     \n    totaltime = MPI_Wtime() - inittime;\n     \n     \n    if(print == 1) {\n\tprintf(\"Node: %d\\n\", mynode);\n\tprint_grid(G, N);\n\tprintf(\"\\n\");\n    }\n     \n     \nprintf(\"%f\", totaltime);\n    return 0;\n}", "label": "int main (int argc, char *argv[])\n{\n    int ncol,nrow;             \n\n    double Gnew,r,omega,       \n\n\tstopdiff,maxdiff,diff;  \n\n    int i,j,phase,iteration;   \n\n    int print = 0;\n    double inittime, totaltime;\n     \n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &totalnodes);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynode);\n     \n    \n\n    N = 20;\n     \n    for(i=1; i<argc; i++) {\n\tif(!strcmp(argv[i], \"-print\")) {\n\t    print = 1;\n\t} else {\n\t    N = atoi(argv[i]);\n\t}\n    }\n     \n    if(mynode == 0) {\n\tfprintf(stderr, \"Running %d x %d SOR\\n\", N, N);\n    }\n     \n    N += 2; \n\n            \n\n     \n            \n\n    ncol = nrow = N;\n    r        = 0.5 * ( cos( M_PI / ncol ) + cos( M_PI / nrow ) );\n    omega    = 2.0 / ( 1 + sqrt( 1 - r * r ) );\n    stopdiff = TOLERANCE / ( 2.0 - omega );\n    omega   *= MAGIC;\n     \n    alloc_grid(&G, N);\n    alloc_grid(&buff, N);\n    init_grid(G, N);\n     \n    \n\n    bufs1 = (double *) malloc(N * sizeof (double));\n    bufr1 = (double *) malloc(N * sizeof (double));\n     \n    \n\n    bufs2 = (double *) malloc(N * sizeof (double));\n    bufr2 = (double *) malloc(N * sizeof (double));\n     \n    \n\n    range(1, N - 1, totalnodes, mynode, &jsta, &jend);\n    \n    printf(\"numtasks = %d\\nrank %d: from %d to %d\\n\", totalnodes, mynode, jsta, jend);\n     \n    inext = mynode + 1;\n    iprev = mynode - 1;\n     \n    if (inext == totalnodes)\n\tinext = MPI_PROC_NULL;\n     \n    if (iprev == -1)\n\tiprev = MPI_PROC_NULL;\n     \n    inittime = MPI_Wtime();\n     \n    \n\n    iteration = 0;\n    do {\n\tmaxdiff = 0.0;\n\tfor ( phase = 0; phase < 2 ; phase++){\n     \n\t    shift(phase);\n     \n\t    for ( i = 1 ; i < N-1 ; i++ ){\n\t\tfor ( j = jsta + (even(i) ^ phase); j <= jend ; j += 2 ){\n\t\t    Gnew = stencil(G,i,j);\n\t\t    diff = fabs(Gnew - G[i][j]);\n\t\t    if ( diff > maxdiff )\n\t\t\tmaxdiff = diff;\n\t\t    G[i][j] = G[i][j] + omega * (Gnew-G[i][j]);\n\t\t}\n\t    }\n\t}\n     \n\tMPI_Allreduce(&maxdiff, &diff, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\tmaxdiff = diff;\n     \n\titeration++;\n    } while (maxdiff > stopdiff);\n     \n    totaltime = MPI_Wtime() - inittime;\n     \n    MPI_Barrier(MPI_COMM_WORLD);\n     \n    if(print == 1) {\n\tprintf(\"Node: %d\\n\", mynode);\n\tprint_grid(G, N);\n\tprintf(\"\\n\");\n    }\n     \n    MPI_Finalize();\n     \nprintf(\"%f\", totaltime);\n    return 0;\n}"}
{"program": "TobbeTripitaka_469", "code": "int main(int argc, char *argv[])\n{\n\n    init();\n\n    if (forward) {\n        if (tau) update_tau(0);\n        else     update(0);\n    }\n    if (backward) {\n        if (tau) update_tau(1);\n        else     update(1);\n    }\n\n    exit(EXIT_SUCCESS);\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    init();\n\n    if (forward) {\n        if (tau) update_tau(0);\n        else     update(0);\n    }\n    if (backward) {\n        if (tau) update_tau(1);\n        else     update(1);\n    }\n    MPI_Finalize();\n\n    exit(EXIT_SUCCESS);\n}"}
{"program": "gnu3ra_470", "code": "int main( int argc, char *argv[] )\n{\n    MPI_Request rreq, sreq, rreq2;\n    int wrank, wsize;\n    int buf = -1, sbuf = 2, rbuf=-1;;\n    int vbuf[10];\n    MPI_Comm dupworld;\n\n\n    \n\n    init_dbr();\n\n    \n\n    \n\n    \n\n\n    \n\n    printf( \"Should see pending recv with tag 17, 19 on dupworld and send with tag 18 on world\\n\" );\n    showQueues();\n\n    \n\n\n    \n\n    printf( \"\\nAfter a few send/receives\\n\" );\n    printf( \"Should see recv with tag 19 on dupworld\\n\" );\n    showQueues();\n\n\n    \n\n    printf( \"\\nAfter a few send/receives\\n\" );\n    showQueues();\n\n    \n\n\n    \n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    MPI_Request rreq, sreq, rreq2;\n    int wrank, wsize;\n    int buf = -1, sbuf = 2, rbuf=-1;;\n    int vbuf[10];\n    MPI_Comm dupworld;\n\n    MPI_Init( &argc, &argv );\n\n    \n\n    init_dbr();\n\n    \n\n    MPI_Comm_dup( MPI_COMM_WORLD, &dupworld );\n    MPI_Comm_set_name( dupworld, \"Dup of comm world\" );\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n    MPI_Comm_size( MPI_COMM_WORLD, &wsize );\n    MPI_Irecv( &buf, 1, MPI_INT, (wrank + 1) % wsize, 17, dupworld, &rreq );\n    MPI_Irecv( vbuf, 10, MPI_INT, (wrank + 1) % wsize, 19, dupworld, &rreq2 );\n    MPI_Isend( &sbuf, 1, MPI_INT, (wrank + wsize - 1) % wsize, 18, \n\t       MPI_COMM_WORLD, &sreq );\n    \n\n    \n\n\n    \n\n    printf( \"Should see pending recv with tag 17, 19 on dupworld and send with tag 18 on world\\n\" );\n    showQueues();\n    MPI_Barrier( MPI_COMM_WORLD );\n\n    \n\n    MPI_Send( &sbuf, 1, MPI_INT, (wrank + wsize - 1) % wsize, 17, dupworld );\n    MPI_Recv( &rbuf, 1, MPI_INT, (wrank + 1) % wsize, 18, MPI_COMM_WORLD, \n    \t      MPI_STATUS_IGNORE );\n    MPI_Wait( &rreq, MPI_STATUS_IGNORE );\n    MPI_Wait( &sreq, MPI_STATUS_IGNORE );\n\n    \n\n    printf( \"\\nAfter a few send/receives\\n\" );\n    printf( \"Should see recv with tag 19 on dupworld\\n\" );\n    showQueues();\n\n    MPI_Barrier( MPI_COMM_WORLD );\n    MPI_Send( &sbuf, 1, MPI_INT, (wrank + wsize - 1) % wsize, 19, dupworld );\n\n    \n\n    printf( \"\\nAfter a few send/receives\\n\" );\n    showQueues();\n\n    \n\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "joao-lima_471", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\t\n\n\n\tfloat *block;\n\tstarpu_data_handle_t block_handle;\n\n\tif (rank == 0)\n\t{\n\t\tstarpu_malloc((void **)&block,\n\t\t\t\tBIGSIZE*BIGSIZE*BIGSIZE*sizeof(float));\n\t\tmemset(block, 0, BIGSIZE*BIGSIZE*BIGSIZE*sizeof(float));\n\n\t\t\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tblock[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] = 1.0f;\n\t\t}\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, BIGSIZE, BIGSIZE*BIGSIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\telse if (rank == 1)\n\t{\n\t\tstarpu_malloc((void **)&block,\n\t\t\tSIZE*SIZE*SIZE*sizeof(float));\n\t\tmemset(block, 0, SIZE*SIZE*SIZE*sizeof(float));\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, SIZE, SIZE*SIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\n\tif (rank == 0)\n\t{\n\t\tMPI_Status status;\n\n\t\tret = starpu_mpi_send(block_handle, 1, 0x42, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\n\t\tret = starpu_mpi_recv(block_handle, 1, 0x1337, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tstarpu_data_acquire(block_handle, STARPU_R);\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] == 33.0f);\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t}\n\telse if (rank == 1)\n\t{\n\t\tMPI_Status status;\n\n\t\tret = starpu_mpi_recv(block_handle, 0, 0x42, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tret = starpu_data_acquire(block_handle, STARPU_RW);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_data_acquire\");\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*SIZE + k*SIZE*SIZE] == 1.0f);\n\t\t\tblock[i + j*SIZE + k*SIZE*SIZE] = 33.0f;\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t\tret = starpu_mpi_send(block_handle, 0, 0x1337, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\n\t}\n\n\tif (rank == 0 || rank == 1)\n\t{\n\t\tstarpu_data_unregister(block_handle);\n\t\tstarpu_free(block);\n\t}\n\n\tFPRINTF(stdout, \"Rank %d is done\\n\", rank);\n\tfflush(stdout);\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\t\n\n\n\tfloat *block;\n\tstarpu_data_handle_t block_handle;\n\n\tif (rank == 0)\n\t{\n\t\tstarpu_malloc((void **)&block,\n\t\t\t\tBIGSIZE*BIGSIZE*BIGSIZE*sizeof(float));\n\t\tmemset(block, 0, BIGSIZE*BIGSIZE*BIGSIZE*sizeof(float));\n\n\t\t\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tblock[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] = 1.0f;\n\t\t}\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, BIGSIZE, BIGSIZE*BIGSIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\telse if (rank == 1)\n\t{\n\t\tstarpu_malloc((void **)&block,\n\t\t\tSIZE*SIZE*SIZE*sizeof(float));\n\t\tmemset(block, 0, SIZE*SIZE*SIZE*sizeof(float));\n\n\t\tstarpu_block_data_register(&block_handle, STARPU_MAIN_RAM,\n\t\t\t(uintptr_t)block, SIZE, SIZE*SIZE,\n\t\t\tSIZE, SIZE, SIZE, sizeof(float));\n\t}\n\n\tif (rank == 0)\n\t{\n\t\tMPI_Status status;\n\n\t\tret = starpu_mpi_send(block_handle, 1, 0x42, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\n\t\tret = starpu_mpi_recv(block_handle, 1, 0x1337, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tstarpu_data_acquire(block_handle, STARPU_R);\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*BIGSIZE + k*BIGSIZE*BIGSIZE] == 33.0f);\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t}\n\telse if (rank == 1)\n\t{\n\t\tMPI_Status status;\n\n\t\tret = starpu_mpi_recv(block_handle, 0, 0x42, MPI_COMM_WORLD, &status);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_recv\");\n\n\t\t\n\n\t\tret = starpu_data_acquire(block_handle, STARPU_RW);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_data_acquire\");\n\n\t\tunsigned i, j, k;\n\t\tfor (k = 0; k < SIZE; k++)\n\t\tfor (j = 0; j < SIZE; j++)\n\t\tfor (i = 0; i < SIZE; i++)\n\t\t{\n\t\t\tassert(block[i + j*SIZE + k*SIZE*SIZE] == 1.0f);\n\t\t\tblock[i + j*SIZE + k*SIZE*SIZE] = 33.0f;\n\t\t}\n\t\tstarpu_data_release(block_handle);\n\n\t\tret = starpu_mpi_send(block_handle, 0, 0x1337, MPI_COMM_WORLD);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_send\");\n\n\t}\n\n\tif (rank == 0 || rank == 1)\n\t{\n\t\tstarpu_data_unregister(block_handle);\n\t\tstarpu_free(block);\n\t}\n\n\tFPRINTF(stdout, \"Rank %d is done\\n\", rank);\n\tfflush(stdout);\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "nschloe_472", "code": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag; \n  MPI_Status status;\n\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  \n  printf(\"%s\\n\", message2);\n\n  return 0;\n}", "label": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag; \n  MPI_Status status;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  MPI_Send(message1, strlen(message1)+1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);\n  MPI_Recv(message2, 50, MPI_CHAR, source, tag, MPI_COMM_WORLD, &status);\n  \n  printf(\"%s\\n\", message2);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "TobbeTripitaka_477", "code": "int main(int argc, char *argv[])\n{\n\n    init(argc,argv);\n\n    if (forward)  {\n        if (tau) update_tau(0);\n        else     update(0);\n    }\n    if (backward) {\n        if (tau) update_tau(1);\n        else     update(1);\n    }\n\n\n    exit(EXIT_SUCCESS);\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    init(argc,argv);\n\n    if (forward)  {\n        if (tau) update_tau(0);\n        else     update(0);\n    }\n    if (backward) {\n        if (tau) update_tau(1);\n        else     update(1);\n    }\n\n    MPI_Finalize();\n\n    exit(EXIT_SUCCESS);\n}"}
{"program": "eddelbuettel_478", "code": "int main( int argc, char **argv )\n{\n     PGAContext *ctx;\n     int         len, maxiter;\n\n\n\n     len = GetIntegerParameter(\"String length?\\n\");\n     maxiter = GetIntegerParameter(\"How many iterations?\\n\");\n\n     ctx = PGACreate(&argc, argv, PGA_DATATYPE_CHARACTER, len, PGA_MAXIMIZE);\n\n     PGASetRandomSeed(ctx, 1);\n     PGASetMaxGAIterValue(ctx, maxiter);\n     PGASetUserFunction(ctx, PGA_USERFUNCTION_MUTATION, (void *)myMutation);\n\n     PGASetUp(ctx);\n     PGARun(ctx, NumberOfZs);\n     PGADestroy(ctx);\n\n\n     return(0);\n}", "label": "int main( int argc, char **argv )\n{\n     PGAContext *ctx;\n     int         len, maxiter;\n\n\n     MPI_Init(&argc, &argv);\n\n     len = GetIntegerParameter(\"String length?\\n\");\n     maxiter = GetIntegerParameter(\"How many iterations?\\n\");\n\n     ctx = PGACreate(&argc, argv, PGA_DATATYPE_CHARACTER, len, PGA_MAXIMIZE);\n\n     PGASetRandomSeed(ctx, 1);\n     PGASetMaxGAIterValue(ctx, maxiter);\n     PGASetUserFunction(ctx, PGA_USERFUNCTION_MUTATION, (void *)myMutation);\n\n     PGASetUp(ctx);\n     PGARun(ctx, NumberOfZs);\n     PGADestroy(ctx);\n\n     MPI_Finalize();\n\n     return(0);\n}"}
{"program": "gnu3ra_479", "code": "int main(int argc, char ** argv) {\n  int     rank, nproc, test_iter, i;\n  void ***base_ptrs;\n  int    *buf;\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCI irregular memory allocation test with %d processes\\n\", nproc);\n\n  buf       = malloc(DATA_SZ);\n  base_ptrs = malloc(sizeof(void**)*NUM_ITERATIONS);\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + allocation %d\\n\", test_iter);\n\n    base_ptrs[test_iter] = malloc(sizeof(void*)*nproc);\n    ARMCI_Malloc((void**)base_ptrs[test_iter], (test_iter % nproc == rank) ? DATA_SZ : 0);\n  }\n\n  ARMCI_Barrier();\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (test_iter % nproc == rank) {\n      ARMCI_Access_begin(base_ptrs[test_iter][rank]);\n      for (i = 0; i < DATA_NELTS; i++)\n        ((int*)base_ptrs[test_iter][rank])[i] = rank;\n      ARMCI_Access_end(base_ptrs[test_iter][rank]);\n    }\n  }\n\n  ARMCI_Barrier();\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    ARMCI_Get(base_ptrs[test_iter][test_iter%nproc], buf, DATA_SZ, test_iter%nproc);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (buf[i] != test_iter % nproc)\n        printf(\"Error: got %d expected %d\\n\", buf[i], test_iter%nproc);\n    }\n  }\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + free %d\\n\", test_iter);\n\n    ARMCI_Free(((void**)base_ptrs[test_iter])[rank]);\n    free(base_ptrs[test_iter]);\n  }\n\n  ARMCI_Barrier();\n\n  free(base_ptrs);\n  free(buf);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int     rank, nproc, test_iter, i;\n  void ***base_ptrs;\n  int    *buf;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCI irregular memory allocation test with %d processes\\n\", nproc);\n\n  buf       = malloc(DATA_SZ);\n  base_ptrs = malloc(sizeof(void**)*NUM_ITERATIONS);\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + allocation %d\\n\", test_iter);\n\n    base_ptrs[test_iter] = malloc(sizeof(void*)*nproc);\n    ARMCI_Malloc((void**)base_ptrs[test_iter], (test_iter % nproc == rank) ? DATA_SZ : 0);\n  }\n\n  ARMCI_Barrier();\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (test_iter % nproc == rank) {\n      ARMCI_Access_begin(base_ptrs[test_iter][rank]);\n      for (i = 0; i < DATA_NELTS; i++)\n        ((int*)base_ptrs[test_iter][rank])[i] = rank;\n      ARMCI_Access_end(base_ptrs[test_iter][rank]);\n    }\n  }\n\n  ARMCI_Barrier();\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    ARMCI_Get(base_ptrs[test_iter][test_iter%nproc], buf, DATA_SZ, test_iter%nproc);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (buf[i] != test_iter % nproc)\n        printf(\"Error: got %d expected %d\\n\", buf[i], test_iter%nproc);\n    }\n  }\n\n  \n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + free %d\\n\", test_iter);\n\n    ARMCI_Free(((void**)base_ptrs[test_iter])[rank]);\n    free(base_ptrs[test_iter]);\n  }\n\n  ARMCI_Barrier();\n\n  free(base_ptrs);\n  free(buf);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "gnu3ra_486", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = blockindexed_contig_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in blockindexed test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = blockindexed_vector_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in blockindexed vector test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = blockindexed_contig_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in blockindexed test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = blockindexed_vector_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in blockindexed vector test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "hansfilipelo_488", "code": "int main (int argc, char ** argv) {\n  int n_tasks, my_rank, colmax, total_pixels;\n\n  struct timespec stime, etime;\n\n  size_data_t size_data;\n  unsigned int partitioned_pixels;\n  unsigned int remainder_pixels;\n  pixel_t* src;\n  double* all_partitioned_means;\n  unsigned int total_mean;\n\n  \n\n  MPI_Comm com = MPI_COMM_WORLD;\n\n  \n\n  if (argc != 3) {\n    if(my_rank == 0) {\n      fprintf(stderr, \"Usage: %s infile outfile\\n\", argv[0]);\n    }\n    exit(1);\n  }\n\n  \n\n  if(my_rank == 0)\n  {\n    src = malloc(sizeof(pixel_t) * MAX_PIXELS);\n    all_partitioned_means = malloc(sizeof(double) * (n_tasks+1));\n\n    \n\n    if(read_ppm (argv[1], &size_data.width, &size_data.height, &colmax, (char *) src) != 0) {\n      exit(1); \n\n    }\n\n    if (colmax > 255) {\n      fprintf(stderr, \"Too large maximum color-component value\\n\");\n      exit(1); \n\n    }\n\n    printf(\"Has read the image.\\n\");\n  }\n\n  \n\n  MPI_Datatype mpi_size_data;\n  MPI_Datatype mpi_pixel;\n  pixel_t dummy_pixel;\n  create_mpi_size_data(&size_data, &mpi_size_data);\n  create_mpi_pixel(&dummy_pixel, &mpi_pixel);\n\n  \n\n\n  \n\n  total_pixels = size_data.height*size_data.width;\n  partitioned_pixels = total_pixels/n_tasks;\n  remainder_pixels = total_pixels-(n_tasks*partitioned_pixels);\n  double partitioned_mean;\n\n  if(my_rank != 0) {\n      src = malloc(sizeof(pixel_t) * partitioned_pixels);\n  }\n\n  if(my_rank == 0) {\n    printf(\"Scatter picture to filter it\\n\");\n    clock_gettime(CLOCK_REALTIME, &stime);\n  }\n\n  \n\n  if (my_rank == 0) {\n  }\n  else{\n  }\n\n  partitioned_mean = calculate_partitioned_mean(partitioned_pixels, src);\n\n\n  if(my_rank == 0)\n  {\n    \n\n    all_partitioned_means[n_tasks] = calculate_partitioned_mean(remainder_pixels, &src[total_pixels - remainder_pixels]);\n    total_mean = calculate_total_mean(all_partitioned_means, n_tasks+1, partitioned_pixels, remainder_pixels, total_pixels);\n  }\n\n\n  thresfilter(partitioned_pixels, total_mean, src);\n\n  if (my_rank == 0) {\n    thresfilter(remainder_pixels, total_mean, &src[total_pixels-remainder_pixels]);\n  }\n\n  if (my_rank == 0) {\n  }\n  else{\n  }\n\n  if(my_rank == 0) {\n    clock_gettime(CLOCK_REALTIME, &etime);\n    printf(\"Gathered picture after filtering\\n\");\n\n    printf(\"Filtering took: %g secs\\n\", (etime.tv_sec  - stime.tv_sec) +\n    1e-9*(etime.tv_nsec  - stime.tv_nsec)) ;\n\n    \n\n    printf(\"Writing output file\\n\");\n\n    if(write_ppm (argv[2], size_data.width, size_data.height, (char *)src) != 0){\n      exit(1);\n    }\n  }\n\n  return(0);\n}", "label": "int main (int argc, char ** argv) {\n  int n_tasks, my_rank, colmax, total_pixels;\n\n  struct timespec stime, etime;\n\n  size_data_t size_data;\n  unsigned int partitioned_pixels;\n  unsigned int remainder_pixels;\n  pixel_t* src;\n  double* all_partitioned_means;\n  unsigned int total_mean;\n\n  \n\n  MPI_Comm com = MPI_COMM_WORLD;\n  MPI_Init( &argc, &argv );\n  MPI_Comm_size( com, &n_tasks );\n  MPI_Comm_rank( com, &my_rank );\n\n  \n\n  if (argc != 3) {\n    if(my_rank == 0) {\n      fprintf(stderr, \"Usage: %s infile outfile\\n\", argv[0]);\n    }\n    exit(1);\n  }\n\n  \n\n  if(my_rank == 0)\n  {\n    src = malloc(sizeof(pixel_t) * MAX_PIXELS);\n    all_partitioned_means = malloc(sizeof(double) * (n_tasks+1));\n\n    \n\n    if(read_ppm (argv[1], &size_data.width, &size_data.height, &colmax, (char *) src) != 0) {\n      exit(1); \n\n    }\n\n    if (colmax > 255) {\n      fprintf(stderr, \"Too large maximum color-component value\\n\");\n      exit(1); \n\n    }\n\n    printf(\"Has read the image.\\n\");\n  }\n\n  \n\n  MPI_Datatype mpi_size_data;\n  MPI_Datatype mpi_pixel;\n  pixel_t dummy_pixel;\n  create_mpi_size_data(&size_data, &mpi_size_data);\n  create_mpi_pixel(&dummy_pixel, &mpi_pixel);\n\n  \n\n  MPI_Bcast(&size_data, 1, mpi_size_data, 0, com);\n\n  \n\n  total_pixels = size_data.height*size_data.width;\n  partitioned_pixels = total_pixels/n_tasks;\n  remainder_pixels = total_pixels-(n_tasks*partitioned_pixels);\n  double partitioned_mean;\n\n  if(my_rank != 0) {\n      src = malloc(sizeof(pixel_t) * partitioned_pixels);\n  }\n\n  if(my_rank == 0) {\n    printf(\"Scatter picture to filter it\\n\");\n    clock_gettime(CLOCK_REALTIME, &stime);\n  }\n\n  \n\n  if (my_rank == 0) {\n    MPI_Scatter(src, partitioned_pixels, mpi_pixel, MPI_IN_PLACE, partitioned_pixels, mpi_pixel, 0, com);\n  }\n  else{\n    MPI_Scatter(src, partitioned_pixels, mpi_pixel, src, partitioned_pixels, mpi_pixel, 0, com);\n  }\n\n  partitioned_mean = calculate_partitioned_mean(partitioned_pixels, src);\n\n  MPI_Gather(&partitioned_mean, 1, MPI_DOUBLE, all_partitioned_means, 1, MPI_DOUBLE, 0, com);\n\n  if(my_rank == 0)\n  {\n    \n\n    all_partitioned_means[n_tasks] = calculate_partitioned_mean(remainder_pixels, &src[total_pixels - remainder_pixels]);\n    total_mean = calculate_total_mean(all_partitioned_means, n_tasks+1, partitioned_pixels, remainder_pixels, total_pixels);\n  }\n\n  MPI_Bcast(&total_mean, 1, MPI_DOUBLE, 0, com);\n\n  thresfilter(partitioned_pixels, total_mean, src);\n\n  if (my_rank == 0) {\n    thresfilter(remainder_pixels, total_mean, &src[total_pixels-remainder_pixels]);\n  }\n\n  if (my_rank == 0) {\n    MPI_Gather(MPI_IN_PLACE, partitioned_pixels, mpi_pixel, src, partitioned_pixels, mpi_pixel, 0, com);\n  }\n  else{\n    MPI_Gather(src, partitioned_pixels, mpi_pixel, src, partitioned_pixels, mpi_pixel, 0, com);\n  }\n\n  if(my_rank == 0) {\n    clock_gettime(CLOCK_REALTIME, &etime);\n    printf(\"Gathered picture after filtering\\n\");\n\n    printf(\"Filtering took: %g secs\\n\", (etime.tv_sec  - stime.tv_sec) +\n    1e-9*(etime.tv_nsec  - stime.tv_nsec)) ;\n\n    \n\n    printf(\"Writing output file\\n\");\n\n    if(write_ppm (argv[2], size_data.width, size_data.height, (char *)src) != 0){\n      exit(1);\n    }\n  }\n\n  MPI_Finalize();\n  return(0);\n}"}
{"program": "rmonat_489", "code": "int main(int argc, char *argv[]) {\n    int rank, size;\n\n    if (argc != 3) {\n        fprintf(stderr, \"Usage: %s p q\", argv[0]);\n    }\n    int p = atoi(argv[1]);\n    int q = atoi(argv[2]);\n    assert(size == p*q);\n    \n    MPI_Comm comm;\n    int dim[2] = {p, q};\n    int period[2] = {1, 1};\n    int reorder = 1;\n    int coord[2];\n\n    size_t height = 4;\n    size_t width = 4;\n    cell *cells = malloc(height/p*width/q*sizeof(cell));\n\n    for(int i = 0; i < height*width/(p*q); i++)\n    {\n\tcells[i].type = 1;\n\tcells[i].u = rank + 0.5;\n\tcells[i].v = width*height - rank;\n    }\n\n    MPI_Datatype d_type;\n\n    MPI_Datatype mem_matrix;\n\n    int sizes[2] = {height, width};\n    int subsizes[2] = {height/p, width/q};\n    int starts[2] = {0, 0};\n   \n    MPI_Datatype d_matrix;\n\n\n\n    MPI_File last_file;\n    char *lastdump = \"bla.dump\";\n\n\n\n   \n    free(cells);\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    MPI_Init(&argc, &argv);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (argc != 3) {\n        fprintf(stderr, \"Usage: %s p q\", argv[0]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    int p = atoi(argv[1]);\n    int q = atoi(argv[2]);\n    assert(size == p*q);\n    \n    MPI_Comm comm;\n    int dim[2] = {p, q};\n    int period[2] = {1, 1};\n    int reorder = 1;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, dim, period, reorder, &comm);\n    int coord[2];\n    MPI_Cart_coords(comm, rank, 2, coord);\n\n    size_t height = 4;\n    size_t width = 4;\n    cell *cells = malloc(height/p*width/q*sizeof(cell));\n\n    for(int i = 0; i < height*width/(p*q); i++)\n    {\n\tcells[i].type = 1;\n\tcells[i].u = rank + 0.5;\n\tcells[i].v = width*height - rank;\n    }\n\n    MPI_Datatype d_type;\n    MPI_Type_create_resized(MPI_DOUBLE, 0, sizeof(cell), &d_type);\n    MPI_Type_commit(&d_type);\n\n    MPI_Datatype mem_matrix;\n    MPI_Type_contiguous(width*height/(p*q), d_type, &mem_matrix);\n    MPI_Type_commit(&mem_matrix);\n\n    int sizes[2] = {height, width};\n    int subsizes[2] = {height/p, width/q};\n    int starts[2] = {0, 0};\n   \n    MPI_Datatype d_matrix;\n    MPI_Type_create_subarray(2, sizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &d_matrix);\n    MPI_Type_commit(&d_matrix);\n\n\n\n    MPI_File last_file;\n    char *lastdump = \"bla.dump\";\n    MPI_File_open(comm, lastdump, MPI_MODE_WRONLY | MPI_MODE_CREATE , MPI_INFO_NULL, &last_file); \n\n    MPI_File_set_view(last_file, width*height/p*sizeof(double)*coord[0]+width/q*sizeof(double)*coord[1], MPI_DOUBLE, d_matrix, \"native\", MPI_INFO_NULL);\n\n    MPI_File_write_all(last_file, &(cells[0]), 1, mem_matrix, MPI_STATUS_IGNORE);\n\n   \n    free(cells);\n    MPI_Type_free(&mem_matrix);\n    MPI_Type_free(&d_matrix);\n    MPI_Type_free(&d_type);\n    MPI_File_close(&last_file);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "mpip_490", "code": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_real(3, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real(3, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "mpip_491", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 32;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 32;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "dellswor_492", "code": "int main(int argc, char **argv)\n{\n    struct rapl_data *rd = NULL;\n    uint64_t *rapl_flags = NULL;\n    uint64_t cores = 0;\n    uint64_t threads = 0;\n    uint64_t sockets = 0;\n    int ri_stat = 0;\n    unsigned i;\n\n    if (!sockets)\n    {\n        core_config(&cores, &threads, &sockets, NULL);\n    }\n#ifdef MPI\n    fprintf(stdout, \"===== MPI Init Done =====\\n\");\n#endif\n\n#ifdef SKIPMSR\n    goto csrpart;\n#endif\n\n    if (init_msr())\n    {\n        libmsr_error_handler(\"Unable to initialize libmsr\", LIBMSR_ERROR_MSR_INIT, getenv(\"HOSTNAME\"), __FILE__, __LINE__);\n        return -1;\n    }\n    fprintf(stdout, \"\\n===== MSR Init Done =====\\n\");\n\n    ri_stat = rapl_init(&rd, &rapl_flags);\n    if (ri_stat < 0)\n    {\n        libmsr_error_handler(\"Unable to initialize rapl\", LIBMSR_ERROR_RAPL_INIT, getenv(\"HOSTNAME\"), __FILE__, __LINE__);\n        return -1;\n    }\n    fprintf(stdout, \"\\n===== RAPL Init Done =====\\n\");\n\n    fprintf(stdout, \"\\n===== Available RAPL Registers =====\\n\");\n    print_available_rapl();\n\n    fprintf(stdout, \"\\n===== Available Performance Counters =====\\n\");\n    print_available_counters();\n\n    fprintf(stdout, \"\\n===== POWER INFO =====\\n\");\n    dump_rapl_power_info(stdout);\n\n    fprintf(stdout, \"\\n===== POWER UNIT =====\\n\");\n    dump_rapl_power_unit(stdout);\n\n    fprintf(stdout, \"\\n===== Enabling Fixed-Function Performance Counters =====\\n\");\n    enable_fixed_counters();\n\n    fprintf(stdout, \"\\n===== Get Initial RAPL Power Limits =====\\n\");\n    get_limits();\n\n    for (i = 0; i < sockets; i++)\n    {\n        fprintf(stdout, \"\\n===== Start Socket %u RAPL Power Limit Test =====\\n\", i);\n        fprintf(stdout, \"\\n--- Testing Pkg Domain Lower Limit ---\\n\");\n        test_pkg_lower_limit(i);\n        fprintf(stdout, \"\\n--- Testing Pkg Domain Upper Limit ---\\n\");\n        test_pkg_upper_limit(i);\n        fprintf(stdout, \"\\n===== End Socket %u RAPL Power Limit Test =====\\n\", i);\n    }\n\n    fprintf(stdout, \"\\n--- Testing Socket 0 All RAPL Power Limits ---\\n\");\n    test_socket_0_limits(0);\n\n    fprintf(stdout, \"\\n--- Testing Socket 1 All RAPL Limits ---\\n\");\n    test_socket_1_limits(1);\n\n    fprintf(stdout, \"\\n===== Testing All RAPL Power Limits All Sockets =====\\n\");\n    test_all_limits();\n    fprintf(stdout, \"===== End Testing All RAPL Power Limits All Sockets =====\\n\");\n\n    fprintf(stdout, \"\\n===== Poll RAPL Data 2X =====\\n\");\n    rapl_r_test(&rd);\n\n    fprintf(stdout, \"\\n===== Thermal Test =====\\n\");\n    thermal_test();\n\n    fprintf(stdout, \"\\n===== Clocks Test =====\\n\");\n    clocks_test();\n\n    fprintf(stdout, \"\\n===== Counters Test =====\\n\");\n    counters_test();\n\n    fprintf(stdout, \"\\n===== Turbo Test =====\\n\");\n    turbo_test();\n\n    fprintf(stdout, \"\\n===== Read IA32_MISC_ENABLE =====\\n\");\n    misc_test();\n\n    fprintf(stdout, \"\\n===== Repeated RAPL Polling Test =====\\n\");\n    repeated_poll_test();\n\n    fprintf(stdout, \"\\n===== Setting Defaults =====\\n\");\n    set_to_defaults();\n\n    finalize_msr();\n    fprintf(stdout, \"===== MSR Finalized =====\\n\");\n\n#ifdef MPI\ncsrpart:\n#endif\n\n    fprintf(stdout, \"\\n===== Test Finished Successfully =====\\n\");\n    if (ri_stat)\n    {\n        fprintf(stdout, \"\\nFound %d locked rapl register(s)\\n\", ri_stat);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    struct rapl_data *rd = NULL;\n    uint64_t *rapl_flags = NULL;\n    uint64_t cores = 0;\n    uint64_t threads = 0;\n    uint64_t sockets = 0;\n    int ri_stat = 0;\n    unsigned i;\n\n    if (!sockets)\n    {\n        core_config(&cores, &threads, &sockets, NULL);\n    }\n#ifdef MPI\n    MPI_Init(&argc, &argv);\n    fprintf(stdout, \"===== MPI Init Done =====\\n\");\n#endif\n\n#ifdef SKIPMSR\n    goto csrpart;\n#endif\n\n    if (init_msr())\n    {\n        libmsr_error_handler(\"Unable to initialize libmsr\", LIBMSR_ERROR_MSR_INIT, getenv(\"HOSTNAME\"), __FILE__, __LINE__);\n        return -1;\n    }\n    fprintf(stdout, \"\\n===== MSR Init Done =====\\n\");\n\n    ri_stat = rapl_init(&rd, &rapl_flags);\n    if (ri_stat < 0)\n    {\n        libmsr_error_handler(\"Unable to initialize rapl\", LIBMSR_ERROR_RAPL_INIT, getenv(\"HOSTNAME\"), __FILE__, __LINE__);\n        return -1;\n    }\n    fprintf(stdout, \"\\n===== RAPL Init Done =====\\n\");\n\n    fprintf(stdout, \"\\n===== Available RAPL Registers =====\\n\");\n    print_available_rapl();\n\n    fprintf(stdout, \"\\n===== Available Performance Counters =====\\n\");\n    print_available_counters();\n\n    fprintf(stdout, \"\\n===== POWER INFO =====\\n\");\n    dump_rapl_power_info(stdout);\n\n    fprintf(stdout, \"\\n===== POWER UNIT =====\\n\");\n    dump_rapl_power_unit(stdout);\n\n    fprintf(stdout, \"\\n===== Enabling Fixed-Function Performance Counters =====\\n\");\n    enable_fixed_counters();\n\n    fprintf(stdout, \"\\n===== Get Initial RAPL Power Limits =====\\n\");\n    get_limits();\n\n    for (i = 0; i < sockets; i++)\n    {\n        fprintf(stdout, \"\\n===== Start Socket %u RAPL Power Limit Test =====\\n\", i);\n        fprintf(stdout, \"\\n--- Testing Pkg Domain Lower Limit ---\\n\");\n        test_pkg_lower_limit(i);\n        fprintf(stdout, \"\\n--- Testing Pkg Domain Upper Limit ---\\n\");\n        test_pkg_upper_limit(i);\n        fprintf(stdout, \"\\n===== End Socket %u RAPL Power Limit Test =====\\n\", i);\n    }\n\n    fprintf(stdout, \"\\n--- Testing Socket 0 All RAPL Power Limits ---\\n\");\n    test_socket_0_limits(0);\n\n    fprintf(stdout, \"\\n--- Testing Socket 1 All RAPL Limits ---\\n\");\n    test_socket_1_limits(1);\n\n    fprintf(stdout, \"\\n===== Testing All RAPL Power Limits All Sockets =====\\n\");\n    test_all_limits();\n    fprintf(stdout, \"===== End Testing All RAPL Power Limits All Sockets =====\\n\");\n\n    fprintf(stdout, \"\\n===== Poll RAPL Data 2X =====\\n\");\n    rapl_r_test(&rd);\n\n    fprintf(stdout, \"\\n===== Thermal Test =====\\n\");\n    thermal_test();\n\n    fprintf(stdout, \"\\n===== Clocks Test =====\\n\");\n    clocks_test();\n\n    fprintf(stdout, \"\\n===== Counters Test =====\\n\");\n    counters_test();\n\n    fprintf(stdout, \"\\n===== Turbo Test =====\\n\");\n    turbo_test();\n\n    fprintf(stdout, \"\\n===== Read IA32_MISC_ENABLE =====\\n\");\n    misc_test();\n\n    fprintf(stdout, \"\\n===== Repeated RAPL Polling Test =====\\n\");\n    repeated_poll_test();\n\n    fprintf(stdout, \"\\n===== Setting Defaults =====\\n\");\n    set_to_defaults();\n\n    finalize_msr();\n    fprintf(stdout, \"===== MSR Finalized =====\\n\");\n\n#ifdef MPI\ncsrpart:\n    MPI_Finalize();\n#endif\n\n    fprintf(stdout, \"\\n===== Test Finished Successfully =====\\n\");\n    if (ri_stat)\n    {\n        fprintf(stdout, \"\\nFound %d locked rapl register(s)\\n\", ri_stat);\n    }\n\n    return 0;\n}"}
{"program": "abhi9bakshi_493", "code": "int main ( int argc, char *argv[] )\r\n{\r\nint p,myid,i,f,*x;\r\ndouble start, end;\r\nMPI_Status status;\r\nMPI_Init(&argc,&argv);\r\nMPI_Comm_size(MPI_COMM_WORLD,&p);\r\nMPI_Comm_rank(MPI_COMM_WORLD,&myid);\r\nif(myid == 0) \n\r\n{\r\nx = (int*)calloc(p,sizeof(int));\r\nx[0] = 50;\r\nfor (i=1; i<p; i++) x[i] = 2*x[i-1];\r\nif(v>0)\r\n{\r\nprintf(\"The data to square : \");\r\nfor (i=0; i<p; i++)\r\n printf(\" %d\",x[i]);\r\n printf(\"\\n\");\r\n}\r\n}\r\nif(myid == 0) \n\r\n{             \n\r\nf = x[0];\r\nfor(i=1; i<p; i++) \r\nMPI_Send(&x[i],1,MPI_INT,i,tag,MPI_COMM_WORLD);\r\n}\r\nelse \n\r\n{\r\nMPI_Recv(&f,1,MPI_INT,0,tag,MPI_COMM_WORLD,&status);\r\nif(v>0) \r\nprintf(\"Node %d will square %d\\n\",myid,f);\r\n}\r\nstart =\r\nf *= f;       \n\r\nif(myid == 0) \n\r\nfor(i=1; i<p; i++)\r\nMPI_Recv(&x[i],1,MPI_INT,i,tag,MPI_COMM_WORLD,&status);\r\nelse     \n\r\nMPI_Send(&f,1,MPI_INT,0,tag,MPI_COMM_WORLD);\r\nif(myid == 0) \n\r\n{\r\nx[0] = f;\r\nprintf(\"The squared numbers : \");\r\nfor(i=0; i<p; i++) \r\nprintf(\" %d\",x[i]);\r\n printf(\"\\n\");\r\nend =\r\nprintf(\"Runtime = %f\\n\", end-start);\r\n}\r\n\r\nMPI_Finalize();\r\nreturn 0;\r\n}", "label": "int main ( int argc, char *argv[] )\r\n{\r\nint p,myid,i,f,*x;\r\ndouble start, end;\r\nMPI_Status status;\r\nMPI_Init(&argc,&argv);\r\nMPI_Comm_size(MPI_COMM_WORLD,&p);\r\nMPI_Comm_rank(MPI_COMM_WORLD,&myid);\r\nif(myid == 0) \n\r\n{\r\nx = (int*)calloc(p,sizeof(int));\r\nx[0] = 50;\r\nfor (i=1; i<p; i++) x[i] = 2*x[i-1];\r\nif(v>0)\r\n{\r\nprintf(\"The data to square : \");\r\nfor (i=0; i<p; i++)\r\n printf(\" %d\",x[i]);\r\n printf(\"\\n\");\r\n}\r\n}\r\nif(myid == 0) \n\r\n{             \n\r\nf = x[0];\r\nfor(i=1; i<p; i++) \r\nMPI_Send(&x[i],1,MPI_INT,i,tag,MPI_COMM_WORLD);\r\n}\r\nelse \n\r\n{\r\nMPI_Recv(&f,1,MPI_INT,0,tag,MPI_COMM_WORLD,&status);\r\nif(v>0) \r\nprintf(\"Node %d will square %d\\n\",myid,f);\r\n}\r\nstart = MPI_Wtime();\r\nf *= f;       \n\r\nif(myid == 0) \n\r\nfor(i=1; i<p; i++)\r\nMPI_Recv(&x[i],1,MPI_INT,i,tag,MPI_COMM_WORLD,&status);\r\nelse     \n\r\nMPI_Send(&f,1,MPI_INT,0,tag,MPI_COMM_WORLD);\r\nif(myid == 0) \n\r\n{\r\nx[0] = f;\r\nprintf(\"The squared numbers : \");\r\nfor(i=0; i<p; i++) \r\nprintf(\" %d\",x[i]);\r\n printf(\"\\n\");\r\nend = MPI_Wtime();\r\nprintf(\"Runtime = %f\\n\", end-start);\r\n}\r\n\r\nMPI_Finalize();\r\nreturn 0;\r\n}"}
{"program": "gnu3ra_494", "code": "int main(int argc, char **argv) {\n    int itr, i, j, rank, nranks, peer, bufsize, errors;\n    double *buffer, *src_buf;\n    MPI_Win buf_win;\n\n\n\n    bufsize = XDIM * YDIM * sizeof(double);\n\n    if (rank == 0)\n        printf(\"MPI RMA Strided Accumulate Test:\\n\");\n\n    for (i = 0; i < XDIM*YDIM; i++) {\n        *(buffer  + i) = 1.0 + rank;\n        *(src_buf + i) = 1.0 + rank;\n    }\n\n\n    peer = 0; \n\n\n    for (itr = 0; itr < ITERATIONS; itr++) {\n\n\n      for (j = 0; j < YDIM; j++) {\n      }\n\n    }\n\n#if 0\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer + i + j*XDIM);\n        const double expected = (1.0 + rank) + (1.0 + ((rank+nranks-1)%nranks)) * (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n#endif\n\n\n\n    if (errors == 0) {\n      printf(\"%d: Success\\n\", rank);\n      return 0;\n    } else {\n      printf(\"%d: Fail\\n\", rank);\n      return 1;\n    }\n}", "label": "int main(int argc, char **argv) {\n    int itr, i, j, rank, nranks, peer, bufsize, errors;\n    double *buffer, *src_buf;\n    MPI_Win buf_win;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    MPI_Alloc_mem(bufsize, MPI_INFO_NULL, &buffer);\n    MPI_Alloc_mem(bufsize, MPI_INFO_NULL, &src_buf);\n\n    if (rank == 0)\n        printf(\"MPI RMA Strided Accumulate Test:\\n\");\n\n    for (i = 0; i < XDIM*YDIM; i++) {\n        *(buffer  + i) = 1.0 + rank;\n        *(src_buf + i) = 1.0 + rank;\n    }\n\n    MPI_Win_create(buffer, bufsize, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &buf_win);\n\n    peer = 0; \n\n\n    for (itr = 0; itr < ITERATIONS; itr++) {\n\n      MPI_Win_lock(MPI_LOCK_EXCLUSIVE, peer, 0, buf_win);\n\n      for (j = 0; j < YDIM; j++) {\n        MPI_Accumulate(src_buf + j*XDIM, XDIM, MPI_DOUBLE, peer,\n                       j*XDIM*sizeof(double), XDIM, MPI_DOUBLE, MPI_SUM, buf_win);\n      }\n\n      MPI_Win_unlock(peer, buf_win);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n#if 0\n    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, rank, 0, buf_win);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer + i + j*XDIM);\n        const double expected = (1.0 + rank) + (1.0 + ((rank+nranks-1)%nranks)) * (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    MPI_Win_unlock(rank, buf_win);\n#endif\n\n    MPI_Win_free(&buf_win);\n    MPI_Free_mem(buffer);\n    MPI_Free_mem(src_buf);\n\n    MPI_Finalize();\n\n    if (errors == 0) {\n      printf(\"%d: Success\\n\", rank);\n      return 0;\n    } else {\n      printf(\"%d: Fail\\n\", rank);\n      return 1;\n    }\n}"}
{"program": "miky-kr5_495", "code": "int main(int argc, char **argv) {\n  int size, rank, elems, i, sum = 0;\n  \n  \n\n\n  \n\n  if(rank == 0) {\n    int array[BUFSIZE];\n    int temp;\n    \n    \n\n    printf(\"MASTER: %d processes available.\\n\", size);\n\n    \n\n    for(i = 0; i < BUFSIZE; i++) {\n      array[i] = 9;\n    }\n\n    \n\n    elems = BUFSIZE / (size - 1);\n    \n    \n\n    for(i = 1; i < size; i++) {\n    }\n\n    \n\n    for(i = 1; i < size; i++) {\n      sum += temp;\n    }\n\n    printf(\"MASTER: The sum of the array is: %d.\\n\", sum);\n    \n    \n\n  } else {\n    int * buffer;\n    \n    \n\n    elems = BUFSIZE / (size - 1);\n    buffer = (int *)malloc(sizeof(int) * elems);\n    \n    \n\n\n    \n\n    for(i = 0; i < elems; i++)\n      sum += buffer[i];\n\n    \n\n\n    \n\n    free(buffer);\n  }\n \n  \n\n  \n  return EXIT_SUCCESS;\n}", "label": "int main(int argc, char **argv) {\n  int size, rank, elems, i, sum = 0;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  if(rank == 0) {\n    int array[BUFSIZE];\n    int temp;\n    \n    \n\n    printf(\"MASTER: %d processes available.\\n\", size);\n\n    \n\n    for(i = 0; i < BUFSIZE; i++) {\n      array[i] = 9;\n    }\n\n    \n\n    elems = BUFSIZE / (size - 1);\n    \n    \n\n    for(i = 1; i < size; i++) {\n      MPI_Send(&array[(i - 1) * elems], elems, MPI_INT, i, TAG, MPI_COMM_WORLD);\n    }\n\n    \n\n    for(i = 1; i < size; i++) {\n      MPI_Recv(&temp, 1, MPI_INT, i, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      sum += temp;\n    }\n\n    printf(\"MASTER: The sum of the array is: %d.\\n\", sum);\n    \n    \n\n  } else {\n    int * buffer;\n    \n    \n\n    elems = BUFSIZE / (size - 1);\n    buffer = (int *)malloc(sizeof(int) * elems);\n    \n    \n\n    MPI_Recv(buffer, elems, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    \n\n    for(i = 0; i < elems; i++)\n      sum += buffer[i];\n\n    \n\n    MPI_Send(&sum, 1, MPI_INT, 0, TAG, MPI_COMM_WORLD);\n\n    \n\n    free(buffer);\n  }\n \n  \n\n  MPI_Finalize();\n  \n  return EXIT_SUCCESS;\n}"}
{"program": "airqinc_496", "code": "int main (int argc,  char *argv[]){\n  int   world_size,\n        world_rank,\n        i,\n        pc,                       \n\n        pcsum,                    \n\n        found_prime,              \n\n        max_prime,                \n\n        start,                    \n\n        stride;                   \n\n\n  double start_time,  end_time;\n\n  if (((world_size % 2) != 0) || ((LIMIT % world_size) != 0)) {\n    printf(\"Se requiere un n\u00famero par de procesos que sea divisible entre %d.\\n\", LIMIT);\n    exit(0);\n  }\n\n  start_time =\n  start = (world_rank * 2) + 1;   \n\n  stride = world_size * 2;        \n\n  pc = 0;\n  found_prime = 0;\n\n  \n\n  if (world_rank == ROOT) {\n    printf(\"%d procesos para buscar entre %d n\u00fameros\\n\", world_size, LIMIT);\n    \n\n    for (i = start; i <= LIMIT; i = i + stride) {\n      if (\n) {\n        \n\n        \n\n      }\n    }\n    \n\n    \n\n    end_time =\n    printf(\"Hecho. El primo m\u00e1s grande es %d. N\u00famero total de primos encontrados: %d\\n\", max_prime, pcsum);\n    printf(\"Tiempo consumido: %.2lf segundos\\n\", end_time - start_time);\n  }\n\n  \n\n  if (world_rank > ROOT) {\n    for (i = start; i <= LIMIT; i = i + stride) {\n      if (\n) {\n        \n\n        \n\n      }\n    }\n    \n\n    \n\n  }\n\n}\n", "label": "int main (int argc,  char *argv[]){\n  int   world_size,\n        world_rank,\n        i,\n        pc,                       \n\n        pcsum,                    \n\n        found_prime,              \n\n        max_prime,                \n\n        start,                    \n\n        stride;                   \n\n\n  double start_time,  end_time;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  if (((world_size % 2) != 0) || ((LIMIT % world_size) != 0)) {\n    printf(\"Se requiere un n\u00famero par de procesos que sea divisible entre %d.\\n\", LIMIT);\n    MPI_Finalize();\n    exit(0);\n  }\n\n  start_time = MPI_Wtime();\n  start = (world_rank * 2) + 1;   \n\n  stride = world_size * 2;        \n\n  pc = 0;\n  found_prime = 0;\n\n  \n\n  if (world_rank == ROOT) {\n    printf(\"%d procesos para buscar entre %d n\u00fameros\\n\", world_size, LIMIT);\n    \n\n    for (i = start; i <= LIMIT; i = i + stride) {\n      if (\n) {\n        \n\n        \n\n      }\n    }\n    \n\n    \n\n    end_time = MPI_Wtime();\n    printf(\"Hecho. El primo m\u00e1s grande es %d. N\u00famero total de primos encontrados: %d\\n\", max_prime, pcsum);\n    printf(\"Tiempo consumido: %.2lf segundos\\n\", end_time - start_time);\n  }\n\n  \n\n  if (world_rank > ROOT) {\n    for (i = start; i <= LIMIT; i = i + stride) {\n      if (\n) {\n        \n\n        \n\n      }\n    }\n    \n\n    \n\n  }\n\n  MPI_Finalize();\n}\n"}
{"program": "bjoern-leder_497", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,nsize;\n   su3_dble **usv;\n   su3_alg_dble **fsv;\n   char loc_dir[NAME_SIZE],cnfg[NAME_SIZE],mfld[NAME_SIZE];\n   FILE *flog=NULL,*fin=NULL;\n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check1.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Writing and reading gauge and momentum configurations\\n\");\n      printf(\"-----------------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"loc_dir\",\"%s\\n\",loc_dir);\n      fclose(fin);\n      \n      fflush(flog);\n   }\n\n\n   start_ranlux(0,123456);\n   geometry();\n   alloc_wud(1);\n   alloc_wfd(1);\n\n   check_dir(loc_dir);   \n   nsize=name_size(\"%s/testcnfg_%d\",loc_dir,NPROC);\n   error_root(nsize>=NAME_SIZE,1,\"main [check1.c]\",\"loc_dir name is too long\");\n   sprintf(cnfg,\"%s/testcnfg_%d\",loc_dir,my_rank);   \n   sprintf(mfld,\"%s/testmfld_%d\",loc_dir,my_rank);   \n   \n   if (my_rank==0)\n   {\n      printf(\"Write random field configurations to the files %s/testcnfg_*\\n\"\n             \"and %s/testmfld_* on the local disks.\\n\\n\",loc_dir,loc_dir);\n      printf(\"Then read the fields from there, compare with the saved fields\\n\"\n             \"and remove all files.\\n\\n\");\n   }\n\n   usv=reserve_wud(1);\n   fsv=reserve_wfd(1);\n\n   random_ud();\n   random_mom();\n   save_flds(usv[0],fsv[0]);\n\n   write_cnfg(cnfg);\n   write_mfld(mfld);\n\n   random_ud();\n   random_mom();\n   \n   read_cnfg(cnfg);\n   read_mfld(mfld);\n   \n   error_chk();\n   remove(cnfg);\n   remove(mfld);\n   \n   error(check_ud(usv[0])!=0,1,\"main [check1.c]\",\n         \"The gauge field is not properly restored\");\n   error(check_fd(fsv[0])!=0,1,\"main [check1.c]\",\n         \"The momentum field is not properly restored\");\n\n   print_flags();\n   \n   if (my_rank==0)\n   {   \n      printf(\"No errors detected --- the fields are correctly written\\n\\n\");\n      fclose(flog);\n   }\n\n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,nsize;\n   su3_dble **usv;\n   su3_alg_dble **fsv;\n   char loc_dir[NAME_SIZE],cnfg[NAME_SIZE],mfld[NAME_SIZE];\n   FILE *flog=NULL,*fin=NULL;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check1.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Writing and reading gauge and momentum configurations\\n\");\n      printf(\"-----------------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"loc_dir\",\"%s\\n\",loc_dir);\n      fclose(fin);\n      \n      fflush(flog);\n   }\n\n   MPI_Bcast(loc_dir,NAME_SIZE,MPI_CHAR,0,MPI_COMM_WORLD);   \n\n   start_ranlux(0,123456);\n   geometry();\n   alloc_wud(1);\n   alloc_wfd(1);\n\n   check_dir(loc_dir);   \n   nsize=name_size(\"%s/testcnfg_%d\",loc_dir,NPROC);\n   error_root(nsize>=NAME_SIZE,1,\"main [check1.c]\",\"loc_dir name is too long\");\n   sprintf(cnfg,\"%s/testcnfg_%d\",loc_dir,my_rank);   \n   sprintf(mfld,\"%s/testmfld_%d\",loc_dir,my_rank);   \n   \n   if (my_rank==0)\n   {\n      printf(\"Write random field configurations to the files %s/testcnfg_*\\n\"\n             \"and %s/testmfld_* on the local disks.\\n\\n\",loc_dir,loc_dir);\n      printf(\"Then read the fields from there, compare with the saved fields\\n\"\n             \"and remove all files.\\n\\n\");\n   }\n\n   usv=reserve_wud(1);\n   fsv=reserve_wfd(1);\n\n   random_ud();\n   random_mom();\n   save_flds(usv[0],fsv[0]);\n\n   write_cnfg(cnfg);\n   write_mfld(mfld);\n\n   random_ud();\n   random_mom();\n   \n   read_cnfg(cnfg);\n   read_mfld(mfld);\n   \n   error_chk();\n   remove(cnfg);\n   remove(mfld);\n   \n   error(check_ud(usv[0])!=0,1,\"main [check1.c]\",\n         \"The gauge field is not properly restored\");\n   error(check_fd(fsv[0])!=0,1,\"main [check1.c]\",\n         \"The momentum field is not properly restored\");\n\n   print_flags();\n   \n   if (my_rank==0)\n   {   \n      printf(\"No errors detected --- the fields are correctly written\\n\\n\");\n      fclose(flog);\n   }\n\n   MPI_Finalize();\n   exit(0);\n}"}
{"program": "arjona00_498", "code": "int main(int argc, char **argv) {\n\n\t\n\n\tint world_rank, world_size;\n\n\tint color = world_rank / 4; \n\n\n\t\n\n\tMPI_Comm row_comm;\n\n\tint row_rank, row_size;\n\n\tprintf(\"WORLD RANK/SIZE: %d/%d \\t ROW RANK/SIZE: %d/%d\\n\",\n\t\tworld_rank, world_size, row_rank, row_size);\n\n\n}", "label": "int main(int argc, char **argv) {\n\tMPI_Init(NULL, NULL);\n\n\t\n\n\tint world_rank, world_size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n\tint color = world_rank / 4; \n\n\n\t\n\n\tMPI_Comm row_comm;\n\tMPI_Comm_split(MPI_COMM_WORLD, color, world_rank, &row_comm);\n\n\tint row_rank, row_size;\n\tMPI_Comm_rank(row_comm, &row_rank);\n\tMPI_Comm_size(row_comm, &row_size);\n\n\tprintf(\"WORLD RANK/SIZE: %d/%d \\t ROW RANK/SIZE: %d/%d\\n\",\n\t\tworld_rank, world_size, row_rank, row_size);\n\n\tMPI_Comm_free(&row_comm);\n\n\tMPI_Finalize();\n}"}
{"program": "pyrovski_501", "code": "int main(int argc, char* argv[]) {\n  int id=0, numprocs;\n  int color;\n  MPI_Comm local;\n  double t1, t2;\n\n  #pragma pomp inst init\n  #pragma pomp inst begin(pomp$main)\n\n  printf(\"%03d: ctest-pomp start\\n\", id);\n  color = (id >= numprocs/2);\n\n  t1 =\n  parallel(MPI_COMM_WORLD);\n  parallel(local);\n  parallel(MPI_COMM_WORLD);\n  t2 =\n\n  printf(\"%03d: ctest-pomp end (%12.9f)\\n\", id, (t2-t1));\n\n  #pragma pomp inst end(pomp$main)\n  return 0;\n}", "label": "int main(int argc, char* argv[]) {\n  int id=0, numprocs;\n  int color;\n  MPI_Comm local;\n  double t1, t2;\n\n  #pragma pomp inst init\n  #pragma pomp inst begin(pomp$main)\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n  printf(\"%03d: ctest-pomp start\\n\", id);\n  color = (id >= numprocs/2);\n  MPI_Comm_split(MPI_COMM_WORLD, color, id, &local);\n\n  t1 = MPI_Wtime();\n  parallel(MPI_COMM_WORLD);\n  parallel(local);\n  parallel(MPI_COMM_WORLD);\n  t2 = MPI_Wtime();\n\n  MPI_Comm_free(&local);\n  MPI_Finalize();\n  printf(\"%03d: ctest-pomp end (%12.9f)\\n\", id, (t2-t1));\n\n  #pragma pomp inst end(pomp$main)\n  return 0;\n}"}
{"program": "to-ko_504", "code": "int main(int argc,char *argv[])\n{\n   int nc,iend,status[4];\n   int nws,nwsd,nwv,nwvd;\n   double wt1,wt2,wtavg;\n   dfl_parms_t dfl;\n\n\n   read_infile(argc,argv);\n   alloc_data();\n   check_files();\n   print_info();\n   dfl=dfl_parms();\n\n   geometry();\n   init_rng();\n\n   make_proplist();\n   wsize(&nws,&nwsd,&nwv,&nwvd);\n   alloc_ws(nws);\n   alloc_wsd(nwsd);\n   alloc_wv(nwv);\n   alloc_wvd(nwvd);\n\n   iend=0;\n   wtavg=0.0;\n\n   for (nc=first;(iend==0)&&(nc<=last);nc+=step)\n   {\n\n      if (my_rank==0)\n         printf(\"Configuration no %d\\n\",nc);\n\n      if (noexp)\n      {\n         save_ranlux();\n         sprintf(cnfg_file,\"%s/%sn%d_%d\",loc_dir,nbase,nc,my_rank);\n         read_cnfg(cnfg_file);\n         restore_ranlux();\n      }\n      else\n      {\n         sprintf(cnfg_file,\"%s/%sn%d\",cnfg_dir,nbase,nc);\n         import_cnfg(cnfg_file);\n      }\n\n      if (dfl.Ns)\n      {\n         dfl_modes(status);\n         error_root(status[0]<0,1,\"main [mesons.c]\",\n                    \"Deflation subspace generation failed (status = %d)\",\n                    status[0]);\n\n         if (my_rank==0)\n            printf(\"Deflation subspace generation: status = %d\\n\",status[0]);\n      }\n\n      set_data(nc);\n      write_data();\n\n      export_ranlux(nc,rng_file);\n      error_chk();\n      \n      wtavg+=(wt2-wt1);\n      \n\n      if (my_rank==0)\n      {\n         printf(\"Configuration no %d fully processed in %.2e sec \",\n                nc,wt2-wt1);\n         printf(\"(average = %.2e sec)\\n\\n\",\n                wtavg/(double)((nc-first)/step+1));\n      }\n      check_endflag(&iend);\n\n      if (my_rank==0)\n      {\n         fflush(flog);\n         copy_file(log_file,log_save);\n         copy_file(dat_file,dat_save);\n         copy_file(rng_file,rng_save);\n      }\n   }\n\n\n   if (my_rank==0)\n   {\n      fclose(flog);\n   }\n\n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int nc,iend,status[4];\n   int nws,nwsd,nwv,nwvd;\n   double wt1,wt2,wtavg;\n   dfl_parms_t dfl;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   read_infile(argc,argv);\n   alloc_data();\n   check_files();\n   print_info();\n   dfl=dfl_parms();\n\n   geometry();\n   init_rng();\n\n   make_proplist();\n   wsize(&nws,&nwsd,&nwv,&nwvd);\n   alloc_ws(nws);\n   alloc_wsd(nwsd);\n   alloc_wv(nwv);\n   alloc_wvd(nwvd);\n\n   iend=0;\n   wtavg=0.0;\n\n   for (nc=first;(iend==0)&&(nc<=last);nc+=step)\n   {\n      MPI_Barrier(MPI_COMM_WORLD);\n      wt1=MPI_Wtime();\n\n      if (my_rank==0)\n         printf(\"Configuration no %d\\n\",nc);\n\n      if (noexp)\n      {\n         save_ranlux();\n         sprintf(cnfg_file,\"%s/%sn%d_%d\",loc_dir,nbase,nc,my_rank);\n         read_cnfg(cnfg_file);\n         restore_ranlux();\n      }\n      else\n      {\n         sprintf(cnfg_file,\"%s/%sn%d\",cnfg_dir,nbase,nc);\n         import_cnfg(cnfg_file);\n      }\n\n      if (dfl.Ns)\n      {\n         dfl_modes(status);\n         error_root(status[0]<0,1,\"main [mesons.c]\",\n                    \"Deflation subspace generation failed (status = %d)\",\n                    status[0]);\n\n         if (my_rank==0)\n            printf(\"Deflation subspace generation: status = %d\\n\",status[0]);\n      }\n\n      set_data(nc);\n      write_data();\n\n      export_ranlux(nc,rng_file);\n      error_chk();\n      \n      MPI_Barrier(MPI_COMM_WORLD);\n      wt2=MPI_Wtime();\n      wtavg+=(wt2-wt1);\n      \n\n      if (my_rank==0)\n      {\n         printf(\"Configuration no %d fully processed in %.2e sec \",\n                nc,wt2-wt1);\n         printf(\"(average = %.2e sec)\\n\\n\",\n                wtavg/(double)((nc-first)/step+1));\n      }\n      check_endflag(&iend);\n\n      if (my_rank==0)\n      {\n         fflush(flog);\n         copy_file(log_file,log_save);\n         copy_file(dat_file,dat_save);\n         copy_file(rng_file,rng_save);\n      }\n   }\n\n\n   if (my_rank==0)\n   {\n      fclose(flog);\n   }\n\n   MPI_Finalize();\n   exit(0);\n}"}
{"program": "nci-australia_506", "code": "int main(int argc, char **argv) {\n\n  int    \tsize=-1,rank=-1, left=-1, right=-1, you=-1;\n  int           ndata=127,ndata_max=127,seed;\n  int           rv, nsec=0, count, cmpl;\n  long long int i,j,k;\n  unsigned long long int  nflop=0,nmem=1,nsleep=0,nrep=1, myflops;\n  char \t\t*env_ptr, cbuf[4096];\n  double \t*sbuf, *rbuf,*x;\n  MPI_Status    *s;\n  MPI_Request   *r;\n  time_t\tts;\n\n\n   seed = time(&ts);\n\n\n \n x = (double *)malloc((size_t)(nmem*sizeof(double)));\n for(i=0;i<nmem;i++) {\n  x[i] = i;\n }\n s = (MPI_Status *)malloc((size_t)(sizeof(MPI_Status)*2*size));\n r = (MPI_Request *)malloc((size_t)(sizeof(MPI_Request)*2*size));\n sbuf = (double *)malloc((size_t)(ndata_max*sizeof(double)));\n rbuf = (double *)malloc((size_t)(ndata_max*sizeof(double)));\n\n  if(size>1) {\n  if(!rank) {left=size-1;} else { left = rank-1;}\n  if(rank == size-1) { right=0;} else {right=rank+1;}\n  you =  (rank < size/2)?(rank+size/2):(rank-size/2);\n  } else  {\n   you = left = right = rank;\n  }\n\n\n\n\n\n\n  free((char *)rbuf);\n  free((char *)sbuf);\n  free((char *)r);\n  free((char *)s);\n\n  return 0;   \n}", "label": "int main(int argc, char **argv) {\n\n  int    \tsize=-1,rank=-1, left=-1, right=-1, you=-1;\n  int           ndata=127,ndata_max=127,seed;\n  int           rv, nsec=0, count, cmpl;\n  long long int i,j,k;\n  unsigned long long int  nflop=0,nmem=1,nsleep=0,nrep=1, myflops;\n  char \t\t*env_ptr, cbuf[4096];\n  double \t*sbuf, *rbuf,*x;\n  MPI_Status    *s;\n  MPI_Request   *r;\n  time_t\tts;\n\n\n   seed = time(&ts);\n\n  MPI_Init(&argc,&argv);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n \n x = (double *)malloc((size_t)(nmem*sizeof(double)));\n for(i=0;i<nmem;i++) {\n  x[i] = i;\n }\n s = (MPI_Status *)malloc((size_t)(sizeof(MPI_Status)*2*size));\n r = (MPI_Request *)malloc((size_t)(sizeof(MPI_Request)*2*size));\n sbuf = (double *)malloc((size_t)(ndata_max*sizeof(double)));\n rbuf = (double *)malloc((size_t)(ndata_max*sizeof(double)));\n\n  if(size>1) {\n  if(!rank) {left=size-1;} else { left = rank-1;}\n  if(rank == size-1) { right=0;} else {right=rank+1;}\n  you =  (rank < size/2)?(rank+size/2):(rank-size/2);\n  } else  {\n   you = left = right = rank;\n  }\n\n  MPI_Irecv(rbuf,ndata,MPI_DOUBLE,MPI_ANY_SOURCE,0,MPI_COMM_WORLD,r);\n  MPI_Send(sbuf,ndata,MPI_DOUBLE,you,0,MPI_COMM_WORLD);\n  MPI_Wait(r,s);\n\n  MPI_Irecv(rbuf,ndata,MPI_DOUBLE,MPI_ANY_SOURCE,0,MPI_COMM_WORLD,r);\n  MPI_Send(sbuf,ndata,MPI_DOUBLE,you,0,MPI_COMM_WORLD);\n  MPI_Wait(r,NULL);\n\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  MPI_Finalize();\n\n  free((char *)rbuf);\n  free((char *)sbuf);\n  free((char *)r);\n  free((char *)s);\n\n  return 0;   \n}"}
{"program": "slitvinov_508", "code": "int main(int argc, char *argv[]) {\n  int rank;\n\n  if (rank == SEND) send();\n  else              recv();\n\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  int rank;\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(COMM, &rank);\n\n  if (rank == SEND) send();\n  else              recv();\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu3ra_511", "code": "int main( int argc, char *argv[] )\n{\n    int wrank, wsize, rank, size, color;\n    int tmp;\n    MPI_Comm newcomm;\n\n\n\n    \n\n    color = (wrank > 0) && (wrank <= wsize/2);\n\n    if (color) {\n\t\n\n\texit(1);\n    }\n    \n    \n\n\n    if (tmp != (size*(size+1)) / 2) {\n\tprintf( \"Allreduce gave %d but expected %d\\n\", tmp, (size*(size+1))/2);\n    }\n\n\n    printf( \" No Errors\\n\" );\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int wrank, wsize, rank, size, color;\n    int tmp;\n    MPI_Comm newcomm;\n\n    MPI_Init( &argc, &argv );\n\n    MPI_Comm_size( MPI_COMM_WORLD, &wsize );\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n\n    \n\n    color = (wrank > 0) && (wrank <= wsize/2);\n    MPI_Comm_split( MPI_COMM_WORLD, color, wrank, &newcomm );\n\n    MPI_Barrier( MPI_COMM_WORLD );\n    if (color) {\n\t\n\n\texit(1);\n    }\n    \n    \n\n    MPI_Comm_size( newcomm, &size );\n    MPI_Comm_rank( newcomm, &rank );\n\n    MPI_Allreduce( &rank, &tmp, 1, MPI_INT, MPI_SUM, newcomm );\n    if (tmp != (size*(size+1)) / 2) {\n\tprintf( \"Allreduce gave %d but expected %d\\n\", tmp, (size*(size+1))/2);\n    }\n\n    MPI_Comm_free( &newcomm );\n    MPI_Finalize();\n\n    printf( \" No Errors\\n\" );\n\n    return 0;\n}"}
{"program": "JasonRuonanWang_512", "code": "int main(int argc, char **argv)\n{\n    int err, i;\n\n\n    if (argc < 2)\n    {\n        Usage();\n        return 1;\n    }\n\n    errno = 0;\n    i = strtol(argv[1], NULL, 10);\n    if (errno || i < 1)\n    {\n        printf(\"Invalid 1st argument %s\\n\", argv[1]);\n        Usage();\n        return 1;\n    }\n    NVARS = i;\n\n    if (argc > 2)\n    {\n        i = strtol(argv[2], NULL, 10);\n        if (errno || i < 1)\n        {\n            printf(\"Invalid 2nd argument %s\\n\", argv[2]);\n            Usage();\n            return 1;\n        }\n        NBLOCKS = i;\n    }\n\n    if (argc > 3)\n    {\n        i = strtol(argv[3], NULL, 10);\n        if (errno || i < 1)\n        {\n            printf(\"Invalid 3rd argument %s\\n\", argv[3]);\n            Usage();\n            return 1;\n        }\n        NSTEPS = i;\n    }\n\n    if (argc > 4)\n    {\n        if (!strncasecmp(argv[4], \"redef\", 5))\n        {\n            printf(\"Delete and redefine variable definitions at each step.\\n\");\n            REDEFINE = 1;\n        }\n    }\n\n    alloc_vars();\n    adios2_adios *adiosH = adios2_init(MPI_COMM_WORLD, adios2_debug_mode_on);\n    ioW = adios2_declare_io(adiosH, \"multiblockwrite\"); \n\n    ioR = adios2_declare_io(adiosH, \"multiblockread\");  \n\n    set_gdim();\n\n    engineW = adios2_open(ioW, FILENAME, adios2_mode_write);\n\n    err = 0;\n    for (i = 0; i < NSTEPS; i++)\n    {\n        if (!err)\n        {\n            if (i == 0 || REDEFINE)\n            {\n                printf(\"-- Define variables.\\n\");\n                define_vars();\n            }\n\n            err = write_file(i);\n\n            if (REDEFINE)\n            {\n                printf(\"-- Delete variable definitions.\\n\");\n                adios2_remove_all_variables(ioW);\n                adios2_remove_all_attributes(ioW);\n            }\n        }\n    }\n    adios2_close(engineW);\n\n    if (!err)\n        err = read_file();\n\n    adios2_finalize(adiosH);\n    fini_vars();\n    return err;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, i;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    if (argc < 2)\n    {\n        Usage();\n        return 1;\n    }\n\n    errno = 0;\n    i = strtol(argv[1], NULL, 10);\n    if (errno || i < 1)\n    {\n        printf(\"Invalid 1st argument %s\\n\", argv[1]);\n        Usage();\n        return 1;\n    }\n    NVARS = i;\n\n    if (argc > 2)\n    {\n        i = strtol(argv[2], NULL, 10);\n        if (errno || i < 1)\n        {\n            printf(\"Invalid 2nd argument %s\\n\", argv[2]);\n            Usage();\n            return 1;\n        }\n        NBLOCKS = i;\n    }\n\n    if (argc > 3)\n    {\n        i = strtol(argv[3], NULL, 10);\n        if (errno || i < 1)\n        {\n            printf(\"Invalid 3rd argument %s\\n\", argv[3]);\n            Usage();\n            return 1;\n        }\n        NSTEPS = i;\n    }\n\n    if (argc > 4)\n    {\n        if (!strncasecmp(argv[4], \"redef\", 5))\n        {\n            printf(\"Delete and redefine variable definitions at each step.\\n\");\n            REDEFINE = 1;\n        }\n    }\n\n    alloc_vars();\n    adios2_adios *adiosH = adios2_init(MPI_COMM_WORLD, adios2_debug_mode_on);\n    ioW = adios2_declare_io(adiosH, \"multiblockwrite\"); \n\n    ioR = adios2_declare_io(adiosH, \"multiblockread\");  \n\n    set_gdim();\n\n    engineW = adios2_open(ioW, FILENAME, adios2_mode_write);\n\n    err = 0;\n    for (i = 0; i < NSTEPS; i++)\n    {\n        if (!err)\n        {\n            if (i == 0 || REDEFINE)\n            {\n                printf(\"-- Define variables.\\n\");\n                define_vars();\n            }\n\n            err = write_file(i);\n\n            if (REDEFINE)\n            {\n                printf(\"-- Delete variable definitions.\\n\");\n                adios2_remove_all_variables(ioW);\n                adios2_remove_all_attributes(ioW);\n            }\n        }\n    }\n    adios2_close(engineW);\n\n    if (!err)\n        err = read_file();\n\n    adios2_finalize(adiosH);\n    fini_vars();\n    MPI_Finalize();\n    return err;\n}"}
{"program": "fangohr_514", "code": "int main(int argc, char *argv[])\n{\n    int size, rank;\n\n\tchar *filename = malloc(strlen(argv[1]) + 4 + 1);\n\tchar *filenameWrite = malloc(strlen(argv[1]) + 8 + 1);\n\n    int x, y, z, xLocal, xGhostsAbove, xGhostsBelow;\n    long double *img, *imgBlurred, *imgLocal, *imgLocalBlurred;\n    int imgSize, imgLocalSize, ghostsAboveSize, ghostsBelowSize, blurFactorSize;\n\n    int *xLocals = (int*) malloc(sizeof(int) * size); \n\n                                                      \n\n                                                      \n\n    int *sendcounts = (int*) malloc(sizeof(int) * size);\n    int *displacements = (int*) malloc(sizeof(int) * size);\n\n    \n\n    \n\n    sprintf(filename, \"%s.txt\", argv[1]);\n    sprintf(filenameWrite, \"%s_out.txt\", argv[1]);\n    \n\n\tx = strtol(argv[2], NULL, 10);\n\ty = strtol(argv[3], NULL, 10);\n\tz = strtol(argv[4], NULL, 10);\n\n    imgSize = x * y * z; \n\n    blurFactorSize = blur_factor * y * z; \n\n                                          \n\n\n    \n\n    img = (long double*) malloc(sizeof(long double) * imgSize);\n    imgBlurred = (long double*) malloc(sizeof(long double) * imgSize);\n\n    \n\n    if (rank == 0){\n        read_file_to_array(x, y, z, filename, img);\n    }\n\n    \n\n    calculate_scatterv_variables(xLocals, x, y, z, sendcounts, displacements);\n    xLocal = xLocals[rank];\n    imgLocalSize = sendcounts[rank];\n\n    \n\n    define_ghost_variables(y, z, &ghostsAboveSize, &ghostsBelowSize, &xGhostsAbove, &xGhostsBelow);\n\n    \n\n    imgLocal = (long double*) malloc(sizeof(long double) * (imgLocalSize + ghostsBelowSize + ghostsAboveSize));\n    imgLocalBlurred = (long double*) malloc(sizeof(long double) * (imgLocalSize + ghostsBelowSize + ghostsAboveSize));\n\n    \n\n\n    \n\n    get_ghosts(imgLocal, imgLocalSize, ghostsAboveSize, blurFactorSize);\n\n    \n\n    blur_image(xLocal + xGhostsBelow + xGhostsAbove, y, z,\n               imgLocal,\n               imgLocalBlurred);\n\n    \n\n\n    \n\n    if (rank == 0){\n        write_array_to_file(x, y, z, filenameWrite, imgBlurred);\n        printf(\"saved\\n\");\n    }\n\n    free(filename);\n    free(filenameWrite);\n    free(img);\n    free(imgBlurred);\n    free(imgLocal);\n    free(imgLocalBlurred);\n    free(xLocals);\n    free(sendcounts);\n    free(displacements);\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int size, rank;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tchar *filename = malloc(strlen(argv[1]) + 4 + 1);\n\tchar *filenameWrite = malloc(strlen(argv[1]) + 8 + 1);\n\n    int x, y, z, xLocal, xGhostsAbove, xGhostsBelow;\n    long double *img, *imgBlurred, *imgLocal, *imgLocalBlurred;\n    int imgSize, imgLocalSize, ghostsAboveSize, ghostsBelowSize, blurFactorSize;\n\n    int *xLocals = (int*) malloc(sizeof(int) * size); \n\n                                                      \n\n                                                      \n\n    int *sendcounts = (int*) malloc(sizeof(int) * size);\n    int *displacements = (int*) malloc(sizeof(int) * size);\n\n    \n\n    \n\n    sprintf(filename, \"%s.txt\", argv[1]);\n    sprintf(filenameWrite, \"%s_out.txt\", argv[1]);\n    \n\n\tx = strtol(argv[2], NULL, 10);\n\ty = strtol(argv[3], NULL, 10);\n\tz = strtol(argv[4], NULL, 10);\n\n    imgSize = x * y * z; \n\n    blurFactorSize = blur_factor * y * z; \n\n                                          \n\n\n    \n\n    img = (long double*) malloc(sizeof(long double) * imgSize);\n    imgBlurred = (long double*) malloc(sizeof(long double) * imgSize);\n\n    \n\n    if (rank == 0){\n        read_file_to_array(x, y, z, filename, img);\n    }\n\n    \n\n    calculate_scatterv_variables(xLocals, x, y, z, sendcounts, displacements);\n    xLocal = xLocals[rank];\n    imgLocalSize = sendcounts[rank];\n\n    \n\n    define_ghost_variables(y, z, &ghostsAboveSize, &ghostsBelowSize, &xGhostsAbove, &xGhostsBelow);\n\n    \n\n    imgLocal = (long double*) malloc(sizeof(long double) * (imgLocalSize + ghostsBelowSize + ghostsAboveSize));\n    imgLocalBlurred = (long double*) malloc(sizeof(long double) * (imgLocalSize + ghostsBelowSize + ghostsAboveSize));\n\n    \n\n    MPI_Scatterv(img,\n                 sendcounts,\n                 displacements,\n                 MPI_LONG_DOUBLE,\n                 imgLocal + ghostsAboveSize,\n                 imgLocalSize,\n                 MPI_LONG_DOUBLE,\n                 0,\n                 MPI_COMM_WORLD);\n\n    \n\n    get_ghosts(imgLocal, imgLocalSize, ghostsAboveSize, blurFactorSize);\n\n    \n\n    blur_image(xLocal + xGhostsBelow + xGhostsAbove, y, z,\n               imgLocal,\n               imgLocalBlurred);\n\n    \n\n    MPI_Gatherv(imgLocalBlurred + ghostsAboveSize,\n               imgLocalSize,\n               MPI_LONG_DOUBLE,\n               imgBlurred,\n               sendcounts,\n               displacements,\n               MPI_LONG_DOUBLE,\n               0,\n               MPI_COMM_WORLD);\n\n    \n\n    if (rank == 0){\n        write_array_to_file(x, y, z, filenameWrite, imgBlurred);\n        printf(\"saved\\n\");\n    }\n\n    free(filename);\n    free(filenameWrite);\n    free(img);\n    free(imgBlurred);\n    free(imgLocal);\n    free(imgLocalBlurred);\n    free(xLocals);\n    free(sendcounts);\n    free(displacements);\n\n    MPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "scott-walker-llnl_518", "code": "int main(int argc, char** argv)\n{\n    struct rapl_data * rd = NULL;\n    uint64_t * rapl_flags = NULL;\n    uint64_t cores = 0, threads = 0, sockets = 0;\n    if (!sockets)\n    {\n        core_config(&cores, &threads, &sockets, NULL);\n    }\n\t#ifdef MPI\n    printf(\"mpi init done\\n\");\n\t#endif\n\n#ifdef SKIPMSR\ngoto csrpart;\n#endif\n\n\tif(init_msr())\n    {\n        fprintf(stderr, \"ERROR: Unable to initialize libmsr\\n\");\n        return -1;\n    }\n    printf(\"msr init done\\n\");\n    int ri_stat = 0;\n    ri_stat = rapl_init(&rd, &rapl_flags);\n    if (ri_stat < 0)\n    {\n        fprintf(stderr, \"ERROR: Unable to initialize rapl\\n\");\n        return -1;\n    }\n    printf(\"init done\\n\");\n    print_available_rapl();\n    print_available_counters();\n    enable_fixed_counters();\n\tget_limits();\n    unsigned i;\n    for(i = 0; i < sockets; i++)\n    {\n        fprintf(stdout, \"BEGINNING SOCKET %u TEST\\n\", i);\n\t    test_pkg_lower_limit(i);\n\t    test_pkg_upper_limit(i);\n\t    test_socket_0_limits(i);\n\t    test_socket_1_limits(i);\n        fprintf(stdout, \"FINISHED SOCKET %u TEST\\n\", i);\n    }\n    fprintf(stdout, \"TESTING ALL SETTINGS\\n\");\n    test_all_limits();\n    printf(\"set limits done\\n\");\n\trapl_r_test(&rd);\n    printf(\"rapl_r_test done\\n\");\n    printf(\"\\n\\nPOWER INFO\\n\");\n    dump_rapl_power_info(stdout);\n    printf(\"\\nEND POWER INFO\\n\\n\");\n    printf(\"thermal test\\n\");\n    thermal_test();\n\n    printf(\"clocks test\\n\");\n    clocks_test();\n    printf(\"counters test\\n\");\n    counters_test();\n    printf(\"turbo test\\n\");\n    turbo_test();\n    printf(\"misc test\\n\");\n    misc_test();\n\n    repeated_poll_test();\n    set_to_defaults();\n\n\tfinalize_msr();\n\t#ifdef MPI\n\ncsrpart:\n\t#endif\n    fprintf(stdout, \"Test Finished Successfully\\n\");\n    if (ri_stat)\n    {\n        fprintf(stdout, \"Found %d locked rapl register(s)\\n\", ri_stat);\n    }\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv)\n{\n    struct rapl_data * rd = NULL;\n    uint64_t * rapl_flags = NULL;\n    uint64_t cores = 0, threads = 0, sockets = 0;\n    if (!sockets)\n    {\n        core_config(&cores, &threads, &sockets, NULL);\n    }\n\t#ifdef MPI\n\tMPI_Init(&argc, &argv);\n    printf(\"mpi init done\\n\");\n\t#endif\n\n#ifdef SKIPMSR\ngoto csrpart;\n#endif\n\n\tif(init_msr())\n    {\n        fprintf(stderr, \"ERROR: Unable to initialize libmsr\\n\");\n        return -1;\n    }\n    printf(\"msr init done\\n\");\n    int ri_stat = 0;\n    ri_stat = rapl_init(&rd, &rapl_flags);\n    if (ri_stat < 0)\n    {\n        fprintf(stderr, \"ERROR: Unable to initialize rapl\\n\");\n        return -1;\n    }\n    printf(\"init done\\n\");\n    print_available_rapl();\n    print_available_counters();\n    enable_fixed_counters();\n\tget_limits();\n    unsigned i;\n    for(i = 0; i < sockets; i++)\n    {\n        fprintf(stdout, \"BEGINNING SOCKET %u TEST\\n\", i);\n\t    test_pkg_lower_limit(i);\n\t    test_pkg_upper_limit(i);\n\t    test_socket_0_limits(i);\n\t    test_socket_1_limits(i);\n        fprintf(stdout, \"FINISHED SOCKET %u TEST\\n\", i);\n    }\n    fprintf(stdout, \"TESTING ALL SETTINGS\\n\");\n    test_all_limits();\n    printf(\"set limits done\\n\");\n\trapl_r_test(&rd);\n    printf(\"rapl_r_test done\\n\");\n    printf(\"\\n\\nPOWER INFO\\n\");\n    dump_rapl_power_info(stdout);\n    printf(\"\\nEND POWER INFO\\n\\n\");\n    printf(\"thermal test\\n\");\n    thermal_test();\n\n    printf(\"clocks test\\n\");\n    clocks_test();\n    printf(\"counters test\\n\");\n    counters_test();\n    printf(\"turbo test\\n\");\n    turbo_test();\n    printf(\"misc test\\n\");\n    misc_test();\n\n    repeated_poll_test();\n    set_to_defaults();\n\n\tfinalize_msr();\n\t#ifdef MPI\n\ncsrpart:\n\tMPI_Finalize();\n\t#endif\n    fprintf(stdout, \"Test Finished Successfully\\n\");\n    if (ri_stat)\n    {\n        fprintf(stdout, \"Found %d locked rapl register(s)\\n\", ri_stat);\n    }\n\n\treturn 0;\n}"}
{"program": "alatar-_520", "code": "main(int argc, char **argv)\n{\n\n    if (argc < 2) {\n        if (!rank) {\n            printf(\"Usage: ./nazwa [liczba ukladow] [liczba planet]\\n\");\n        }\n        exit(-1);\n    }\n\n    systems = atoi(argv[1]);\n    planets = atoi(argv[1]);\n    \n\n    total_energy = RAND_ENERGY + (RAND_ENERGY / 2) * (planets * (planets - 1) / 2);\n    dockplace_spaces = 1;\n\n    if (systems * planets > rank) {\n        srand(time(NULL) + rank);\n        run();\n    }\n\n    return 0;\n}\n", "label": "main(int argc, char **argv)\n{\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Get_processor_name(processor, &i);\n\n    if (argc < 2) {\n        if (!rank) {\n            printf(\"Usage: ./nazwa [liczba ukladow] [liczba planet]\\n\");\n        }\n        MPI_Finalize();\n        exit(-1);\n    }\n\n    systems = atoi(argv[1]);\n    planets = atoi(argv[1]);\n    \n\n    total_energy = RAND_ENERGY + (RAND_ENERGY / 2) * (planets * (planets - 1) / 2);\n    dockplace_spaces = 1;\n\n    if (systems * planets > rank) {\n        srand(time(NULL) + rank);\n        run();\n    }\n\n    MPI_Finalize();\n    return 0;\n}\n"}
{"program": "RWTH-ELP_521", "code": "int main(int argc, char **argv)\n{\n  \n\n\n  \n\n  return EXIT_SUCCESS;\n}", "label": "int main(int argc, char **argv)\n{\n  \n\n  MPI_Init(&argc, &argv);\n  MPI_Finalize();\n\n  \n\n  return EXIT_SUCCESS;\n}"}
{"program": "gnu3ra_523", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = subarray_1d_c_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 1d subarray c test 1.\\n\", err);\n    errs += err;\n\n    err = subarray_1d_fortran_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 1d subarray fortran test 1.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = subarray_2d_c_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 2d subarray c test 1.\\n\", err);\n    errs += err;\n\n    err = subarray_2d_fortran_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 2d subarray fortran test 1.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = subarray_2d_c_test2();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 2d subarray c test 2.\\n\", err);\n    errs += err;\n\n    err = subarray_4d_c_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 4d subarray c test 1.\\n\", err);\n    errs += err;\n\n    err = subarray_4d_fortran_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 4d subarray fortran test 1.\\n\", err);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = subarray_1d_c_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 1d subarray c test 1.\\n\", err);\n    errs += err;\n\n    err = subarray_1d_fortran_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 1d subarray fortran test 1.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = subarray_2d_c_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 2d subarray c test 1.\\n\", err);\n    errs += err;\n\n    err = subarray_2d_fortran_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 2d subarray fortran test 1.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = subarray_2d_c_test2();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 2d subarray c test 2.\\n\", err);\n    errs += err;\n\n    err = subarray_4d_c_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 4d subarray c test 1.\\n\", err);\n    errs += err;\n\n    err = subarray_4d_fortran_test1();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in 4d subarray fortran test 1.\\n\", err);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "bjoern-leder_524", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,n;\n   double eps,dev;\n   su3_dble *udb,**usv;\n   FILE *flog=NULL,*fin=NULL;\n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n      fin=freopen(\"check2.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Gauge covariance of the Wilson flow\\n\");\n      printf(\"-----------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"n\",\"%d\\n\",&n);\n      read_line(\"eps\",\"%lf\",&eps);      \n      fclose(fin);\n\n      printf(\"n = %d\\n\",n);\n      printf(\"eps = %.3e\\n\\n\",eps);      \n   }\n\n   \n   start_ranlux(0,1234);\n   geometry();\n   alloc_g();\n   alloc_wud(2);\n   alloc_wfd(1);\n   usv=reserve_wud(2);\n   udb=udfld();\n\n   random_ud();\n   random_g();\n   cm3x3_assign(4*VOLUME,udb,usv[0]);\n   fwd_euler(n,eps);\n   transform_ud();\n   cm3x3_assign(4*VOLUME,udb,usv[1]);   \n   cm3x3_assign(4*VOLUME,usv[0],udb);\n   set_flags(UPDATED_UD);\n   transform_ud();   \n   fwd_euler(n,eps);\n\n   dev=max_dev_ud(usv[1]);\n   error_chk();\n\n   if (my_rank==0)\n   {\n      printf(\"Maximal absolute deviation of U(x,mu) = %.1e\\n\\n\",dev);\n      fclose(flog);\n   }\n   \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,n;\n   double eps,dev;\n   su3_dble *udb,**usv;\n   FILE *flog=NULL,*fin=NULL;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n      fin=freopen(\"check2.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Gauge covariance of the Wilson flow\\n\");\n      printf(\"-----------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"n\",\"%d\\n\",&n);\n      read_line(\"eps\",\"%lf\",&eps);      \n      fclose(fin);\n\n      printf(\"n = %d\\n\",n);\n      printf(\"eps = %.3e\\n\\n\",eps);      \n   }\n\n   MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);\n   MPI_Bcast(&eps,1,MPI_DOUBLE,0,MPI_COMM_WORLD);\n   \n   start_ranlux(0,1234);\n   geometry();\n   alloc_g();\n   alloc_wud(2);\n   alloc_wfd(1);\n   usv=reserve_wud(2);\n   udb=udfld();\n\n   random_ud();\n   random_g();\n   cm3x3_assign(4*VOLUME,udb,usv[0]);\n   fwd_euler(n,eps);\n   transform_ud();\n   cm3x3_assign(4*VOLUME,udb,usv[1]);   \n   cm3x3_assign(4*VOLUME,usv[0],udb);\n   set_flags(UPDATED_UD);\n   transform_ud();   \n   fwd_euler(n,eps);\n\n   dev=max_dev_ud(usv[1]);\n   error_chk();\n\n   if (my_rank==0)\n   {\n      printf(\"Maximal absolute deviation of U(x,mu) = %.1e\\n\\n\",dev);\n      fclose(flog);\n   }\n   \n   MPI_Finalize();    \n   exit(0);\n}"}
{"program": "dfroger_526", "code": "int\nmain(int argc, char **argv)\n{\n    \n\n    int rank, size;\n\n    \n\n    long problem_size;\n    bool debug_mode;\n    char* output_filepath;\n    if (!prb_parse_command_line(argc, argv, &problem_size, &debug_mode, \n                                &output_filepath)) {\n        exit(0);\n    }\n\n    long array_size = problem_size *  problem_size;\n\n    \n\n    size_t i0, i1;\n    prb_partition_index(size, rank, array_size, &i0, &i1);\n\n    \n\n    size_t partition_size = i1 - i0;\n    double* partition = (double*) malloc(partition_size*sizeof(double));\n\n    \n\n    size_t i;\n    for (i = 0 ; i < partition_size ; i++)\n        partition[i] = i0 + i;\n\n    int noperations = 1;\n    enum operations {WRITING};\n    char* operation_names[1] = {\"writting\"};\n\n    prb_stopwatch_t* sw = prb_stopwatch_new(noperations);\n    prb_stopwatch_start(sw, WRITING);\n\n    \n\n    MPI_File f;\n    int mode = MPI_MODE_CREATE | MPI_MODE_WRONLY;\n    char filename[] = \"data.bin\";\n\n    \n\n\n    \n\n    MPI_Status status;\n\n    \n\n\n    prb_stopwatch_stop(sw, WRITING);\n    prb_stopwatch_write_csv(sw, output_filepath,\n                            noperations, operation_names,\n                            MPI_COMM_WORLD, 0);\n    \n    prb_stopwatch_free(sw);\n\n    \n\n\n    return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n    \n\n    MPI_Init(&argc, &argv);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    long problem_size;\n    bool debug_mode;\n    char* output_filepath;\n    if (!prb_parse_command_line(argc, argv, &problem_size, &debug_mode, \n                                &output_filepath)) {\n        MPI_Finalize();\n        exit(0);\n    }\n\n    long array_size = problem_size *  problem_size;\n\n    \n\n    size_t i0, i1;\n    prb_partition_index(size, rank, array_size, &i0, &i1);\n\n    \n\n    size_t partition_size = i1 - i0;\n    double* partition = (double*) malloc(partition_size*sizeof(double));\n\n    \n\n    size_t i;\n    for (i = 0 ; i < partition_size ; i++)\n        partition[i] = i0 + i;\n\n    int noperations = 1;\n    enum operations {WRITING};\n    char* operation_names[1] = {\"writting\"};\n\n    prb_stopwatch_t* sw = prb_stopwatch_new(noperations);\n    prb_stopwatch_start(sw, WRITING);\n\n    \n\n    MPI_File f;\n    int mode = MPI_MODE_CREATE | MPI_MODE_WRONLY;\n    char filename[] = \"data.bin\";\n    MPI_File_open(MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &f);\n\n    \n\n    MPI_File_set_view(f, i0*sizeof(double), MPI_DOUBLE, MPI_DOUBLE,\n                      \"native\", MPI_INFO_NULL);\n\n    \n\n    MPI_Status status;\n    MPI_File_write(f, partition, partition_size, MPI_DOUBLE, &status);\n\n    \n\n    MPI_File_close(&f);\n\n    prb_stopwatch_stop(sw, WRITING);\n    prb_stopwatch_write_csv(sw, output_filepath,\n                            noperations, operation_names,\n                            MPI_COMM_WORLD, 0);\n    \n    prb_stopwatch_free(sw);\n\n    \n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "syftalent_527", "code": "int main(int argc, char *argv[])\n{\n    int i, size, num, name_len, desc_len, count, verb, thread_support;\n    int varclass, bind, readonly, continuous, atomic;\n    int fbox_idx = -1;\n    char name[STR_LEN], desc[STR_LEN];\n    MPI_Datatype dtype;\n    MPI_T_enum enumtype;\n\n\n    if (rank == 0) {\n        printf(\"MPIT pvar test: nem_fbox_fall_back_to_queue_count\\n\");\n        fflush(stdout);\n    }\n\n    \n\n    assert(size == 2);\n\n    \n\n    TRY(MPI_T_init_thread(MPI_THREAD_SINGLE, &thread_support));\n    TRY(MPI_T_pvar_get_num(&num));\n\n    \n\n    for (i = 0; i < num; i++) {\n        name_len = desc_len = STR_LEN;\n        TRY(MPI_T_pvar_get_info(i, name, &name_len, &verb, &varclass, &dtype,\n                                &enumtype, desc, &desc_len, &bind, &readonly,\n                                &continuous, &atomic));\n\n        if (strcmp(name, \"nem_fbox_fall_back_to_queue_count\") == 0)\n            fbox_idx = i;\n    }\n\n    \n\n    assert(fbox_idx != -1);\n\n    \n\n\n    \n\n    assert(count == 2);\n\n    \n\n    \n\n    send_first_test();\n    \n\n\n    \n\n\n    if (rank == 0) {\n        printf(\"finished\\n\");\n        fflush(stdout);\n    }\n\n    TRY(MPI_T_finalize());\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int i, size, num, name_len, desc_len, count, verb, thread_support;\n    int varclass, bind, readonly, continuous, atomic;\n    int fbox_idx = -1;\n    char name[STR_LEN], desc[STR_LEN];\n    MPI_Datatype dtype;\n    MPI_T_enum enumtype;\n\n    MPI_Init(NULL, NULL);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        printf(\"MPIT pvar test: nem_fbox_fall_back_to_queue_count\\n\");\n        fflush(stdout);\n    }\n\n    \n\n    assert(size == 2);\n\n    \n\n    TRY(MPI_T_init_thread(MPI_THREAD_SINGLE, &thread_support));\n    TRY(MPI_T_pvar_get_num(&num));\n\n    \n\n    for (i = 0; i < num; i++) {\n        name_len = desc_len = STR_LEN;\n        TRY(MPI_T_pvar_get_info(i, name, &name_len, &verb, &varclass, &dtype,\n                                &enumtype, desc, &desc_len, &bind, &readonly,\n                                &continuous, &atomic));\n\n        if (strcmp(name, \"nem_fbox_fall_back_to_queue_count\") == 0)\n            fbox_idx = i;\n    }\n\n    \n\n    assert(fbox_idx != -1);\n\n    \n\n    MPI_T_pvar_session_create(&session);\n    MPI_T_pvar_handle_alloc(session, fbox_idx, NULL, &fbox_handle, &count);\n\n    \n\n    assert(count == 2);\n\n    \n\n    \n\n    send_first_test();\n    \n\n\n    \n\n    MPI_T_pvar_handle_free(session, &fbox_handle);\n    MPI_T_pvar_session_free(&session);\n\n    if (rank == 0) {\n        printf(\"finished\\n\");\n        fflush(stdout);\n    }\n\n    TRY(MPI_T_finalize());\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_528", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tList*\t\t\tlist;\n\t\tIndex\t\t\tidx;\n\t\tStream*\t\t\tstream;\n\t\t\n\t\tstream = Journal_Register( Info_Type, \"myStream\" );\n\t\t\n\t\tlist = List_New( sizeof(unsigned) );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tList_Resize( list, List_Size( list ) + 1 );\n\t\t\tList_ElementAt( list, unsigned, idx ) = 100 - idx;\n\t\t}\n\t\t\n\t\tPrint( list, stream );\n\t\t\n\t\tprintf( \"List Data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ )\n\t\t\tprintf( \"\\telements[%d]: %d\\n\", idx, List_ElementAt( list, unsigned, idx ) );\n\t\t\n\t\tStg_Class_Delete( list );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tList*\t\t\tlist;\n\t\tIndex\t\t\tidx;\n\t\tStream*\t\t\tstream;\n\t\t\n\t\tstream = Journal_Register( Info_Type, \"myStream\" );\n\t\t\n\t\tlist = List_New( sizeof(unsigned) );\n\t\tfor( idx = 0; idx < 100; idx++ ) {\n\t\t\tList_Resize( list, List_Size( list ) + 1 );\n\t\t\tList_ElementAt( list, unsigned, idx ) = 100 - idx;\n\t\t}\n\t\t\n\t\tPrint( list, stream );\n\t\t\n\t\tprintf( \"List Data:\\n\" );\n\t\tfor( idx = 0; idx < 100; idx++ )\n\t\t\tprintf( \"\\telements[%d]: %d\\n\", idx, List_ElementAt( list, unsigned, idx ) );\n\t\t\n\t\tStg_Class_Delete( list );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "decarlof_529", "code": "int main(int argc, char **argv)\n{\n\tuint8_t restart;\n\tint32_t proj_rows, proj_cols, proj_num, recon_num, remove_rings, quad_convex, nodes_num, nodes_rank;\n\tfloat *object, *projections, *weights, *proj_angles, *proj_times, *recon_times, vox_wid, rot_center, sig_s, sig_t, c_s, c_t, convg_thresh, huber_delta, huber_T;\n\tFILE *debug_msg_ptr;\n\n\t\n\t\n\t\n\n\t\n\t\n\n\n\n\tdebug_msg_ptr = stdout;\n\t\n\t\n\n\tread_command_line_args (argc, argv, &proj_rows, &proj_cols, &proj_num, &recon_num, &vox_wid, &rot_center, &sig_s, &sig_t, &c_s, &c_t, &convg_thresh, &remove_rings, &quad_convex, &huber_delta, &huber_T, &restart, debug_msg_ptr);\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Number of nodes is %d and command line input argument values are proj_rows = %d, proj_cols = %d, proj_num = %d, recon_num = %d, vox_wid = %f, rot_center = %f, sig_s = %f, sig_t = %f, c_s = %f, c_t = %f, convg_thresh = %f, remove_rings = %d, quad_convex = %d, huber_delta = %f, huber_T = %f, restart = %d\\n\", nodes_num, proj_rows, proj_cols, proj_num, recon_num, vox_wid, rot_center, sig_s, sig_t, c_s, c_t, convg_thresh, remove_rings, quad_convex, huber_delta, huber_T, restart);\t\n\t\n\t\n\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Allocating memory for data ....\\n\");\n\tprojections = (float*)calloc ((proj_num*proj_rows*proj_cols)/nodes_num, sizeof(float));\n\tweights = (float*)calloc ((proj_num*proj_rows*proj_cols)/nodes_num, sizeof(float));\n\tproj_angles = (float*)calloc (proj_num, sizeof(float));\n\tproj_times = (float*)calloc (proj_num, sizeof(float));\n\trecon_times = (float*)calloc (recon_num + 1, sizeof(float));\n\n\t\n\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Reading data ....\\n\");\n\tread_data (projections, weights, proj_angles, proj_times, recon_times, proj_rows, proj_cols, proj_num, recon_num, debug_msg_ptr);\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Reconstructing the data ....\\n\");\n\t\n\n\treconstruct (&object, projections, weights, proj_angles, proj_times, recon_times, proj_rows, proj_cols, proj_num, recon_num, vox_wid, rot_center, sig_s, sig_t, c_s, c_t, convg_thresh, remove_rings, quad_convex, huber_delta, huber_T, restart, debug_msg_ptr);\n\t\n\n\n\tfclose (debug_msg_ptr); \n\treturn (0);\n}", "label": "int main(int argc, char **argv)\n{\n\tuint8_t restart;\n\tint32_t proj_rows, proj_cols, proj_num, recon_num, remove_rings, quad_convex, nodes_num, nodes_rank;\n\tfloat *object, *projections, *weights, *proj_angles, *proj_times, *recon_times, vox_wid, rot_center, sig_s, sig_t, c_s, c_t, convg_thresh, huber_delta, huber_T;\n\tFILE *debug_msg_ptr;\n\n\t\n\t\n\tMPI_Init(&argc, &argv);\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nodes_num);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &nodes_rank);\n\t\n\t\n\n\n\n\tdebug_msg_ptr = stdout;\n\t\n\t\n\n\tread_command_line_args (argc, argv, &proj_rows, &proj_cols, &proj_num, &recon_num, &vox_wid, &rot_center, &sig_s, &sig_t, &c_s, &c_t, &convg_thresh, &remove_rings, &quad_convex, &huber_delta, &huber_T, &restart, debug_msg_ptr);\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Number of nodes is %d and command line input argument values are proj_rows = %d, proj_cols = %d, proj_num = %d, recon_num = %d, vox_wid = %f, rot_center = %f, sig_s = %f, sig_t = %f, c_s = %f, c_t = %f, convg_thresh = %f, remove_rings = %d, quad_convex = %d, huber_delta = %f, huber_T = %f, restart = %d\\n\", nodes_num, proj_rows, proj_cols, proj_num, recon_num, vox_wid, rot_center, sig_s, sig_t, c_s, c_t, convg_thresh, remove_rings, quad_convex, huber_delta, huber_T, restart);\t\n\t\n\t\n\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Allocating memory for data ....\\n\");\n\tprojections = (float*)calloc ((proj_num*proj_rows*proj_cols)/nodes_num, sizeof(float));\n\tweights = (float*)calloc ((proj_num*proj_rows*proj_cols)/nodes_num, sizeof(float));\n\tproj_angles = (float*)calloc (proj_num, sizeof(float));\n\tproj_times = (float*)calloc (proj_num, sizeof(float));\n\trecon_times = (float*)calloc (recon_num + 1, sizeof(float));\n\n\t\n\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Reading data ....\\n\");\n\tread_data (projections, weights, proj_angles, proj_times, recon_times, proj_rows, proj_cols, proj_num, recon_num, debug_msg_ptr);\n\tif (nodes_rank == 0) fprintf(debug_msg_ptr, \"main: Reconstructing the data ....\\n\");\n\t\n\n\treconstruct (&object, projections, weights, proj_angles, proj_times, recon_times, proj_rows, proj_cols, proj_num, recon_num, vox_wid, rot_center, sig_s, sig_t, c_s, c_t, convg_thresh, remove_rings, quad_convex, huber_delta, huber_T, restart, debug_msg_ptr);\n\t\n\n\n\tfclose (debug_msg_ptr); \n\tMPI_Finalize();\n\treturn (0);\n}"}
{"program": "sigorilla_530", "code": "int main (int argc, char** argv) {\n\n    int world_size, world_rank;\n    int N, N_master, N_slave;\n    double startwtime = 0.0, endwtime;\n\n    N = (argc >= 2) ? atoi(argv[1]) : 1000;\n\n    double local_sum = 0.0, global_sum;\n\n\n\n    int count = 1, tag = 0;\n\n    if (world_rank == 0) {\n        startwtime =\n\n        N_slave = (int) (N / world_size);\n        N_master = N_slave + N % world_size;\n\n        int i = 1;\n        while (i < world_size ) {\n            int n_start = N_master + 1 + (i - 1) * N_slave;\n            i++;\n        }\n\n        int n = 1;\n        while (n <= N_master) {\n            local_sum += (1.0 / n);\n            n++;\n        }\n    } else {\n        int n = N_master + 1 + (world_rank - 1) * N_slave;\n        int n_start = n;\n        while (n < (n_start + N_slave)) {\n            local_sum += (1.0 / n);\n            n++;\n        }\n    }\n\n\n    if (world_rank == 0) {\n        endwtime =\n\n        printf(\"Sum of harmonical series 1/n = %.4lf for N = %d\\n\",\n                global_sum, N);\n        printf(\"Time: %lf s\\n\", (endwtime - startwtime) * 1000);\n    }\n\n    return 0;\n}", "label": "int main (int argc, char** argv) {\n\n    int world_size, world_rank;\n    int N, N_master, N_slave;\n    double startwtime = 0.0, endwtime;\n\n    N = (argc >= 2) ? atoi(argv[1]) : 1000;\n\n    double local_sum = 0.0, global_sum;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int count = 1, tag = 0;\n\n    if (world_rank == 0) {\n        startwtime = MPI_Wtime();\n\n        N_slave = (int) (N / world_size);\n        N_master = N_slave + N % world_size;\n\n        int i = 1;\n        while (i < world_size ) {\n            int n_start = N_master + 1 + (i - 1) * N_slave;\n            MPI_Send(&n_start, count, MPI_INT, i, tag, MPI_COMM_WORLD);\n            i++;\n        }\n\n        int n = 1;\n        while (n <= N_master) {\n            local_sum += (1.0 / n);\n            n++;\n        }\n    } else {\n        int n = N_master + 1 + (world_rank - 1) * N_slave;\n        int n_start = n;\n        while (n < (n_start + N_slave)) {\n            local_sum += (1.0 / n);\n            n++;\n        }\n    }\n\n    MPI_Reduce(&local_sum, &global_sum, count, MPI_DOUBLE,\n            MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        endwtime = MPI_Wtime();\n\n        printf(\"Sum of harmonical series 1/n = %.4lf for N = %d\\n\",\n                global_sum, N);\n        printf(\"Time: %lf s\\n\", (endwtime - startwtime) * 1000);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "gnu3ra_532", "code": "int main(int argc, char* argv[]) {\n\n  assert(COUNT <= MAXELEMS);\n\n  if (me == 0 && verbose) {\n    printf(\"Test starting on %d processes\\n\", nproc); \n    fflush(stdout);\n  }\n\n  test_put();\n\n\n\n  if (me == 0 && verbose) {\n    printf(\"Test completed.\\n\");\n    fflush(stdout);\n  }\n\n  if (me == 0)\n    printf(\" No Errors\\n\");\n\n  return 0;\n}", "label": "int main(int argc, char* argv[]) {\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n\n  assert(COUNT <= MAXELEMS);\n\n  if (me == 0 && verbose) {\n    printf(\"Test starting on %d processes\\n\", nproc); \n    fflush(stdout);\n  }\n\n  test_put();\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  MPI_Finalize();\n\n  if (me == 0 && verbose) {\n    printf(\"Test completed.\\n\");\n    fflush(stdout);\n  }\n\n  if (me == 0)\n    printf(\" No Errors\\n\");\n\n  return 0;\n}"}
{"program": "enesates_533", "code": "int main( int argc, char* argv[] )\n{\t\n    int\t\t\tmy_rank;\t\t\n\n    int\t\t\tp;\t\t\t\t\n\n    int\t\t\tsubSum = 0;\t\t\n\n    int \t\tsum = 0;\t\t\n\n    int \t\ti;\t\t\t\t\n\n    MPI_Status\t\tstatus;\t\t\t\n\n    int err;\n\n\t\n\n    err =\n\n\t\n\n\n\t\n\n\t\n\tint ARRAY_SIZE = p * k; \n\n\tint\tnumbers[ARRAY_SIZE]; \n\n\t\n\t\n\n\trandomNumbers(numbers, ARRAY_SIZE);\n\t\n\t\n\n    \n    \n\n    subSum = subTotal(numbers);   \n    printf(\"I am %d. process, partial sum = %d\\n\", my_rank, subSum);       \n\t\n\t\n\n\t\n\tif(my_rank == 0) {\n\t\t\n\n\t\t\n\t\t\n\n\t\tprintf(\"Total : %d\\n\",sum);\t\n\t\t\n\t\tprintf(\"Numbers: [ \");\n\t\tfor(i=0; i<ARRAY_SIZE; i++)\t\n\t\t\tprintf(\"%d, \",numbers[i]);\n\t\tprintf(\"]\\n\");\n\t\t\n\t} \n\n\t\n\n    err =\n    return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\t\n    int\t\t\tmy_rank;\t\t\n\n    int\t\t\tp;\t\t\t\t\n\n    int\t\t\tsubSum = 0;\t\t\n\n    int \t\tsum = 0;\t\t\n\n    int \t\ti;\t\t\t\t\n\n    MPI_Status\t\tstatus;\t\t\t\n\n    int err;\n\n\t\n\n    err = MPI_Init( &argc, &argv );\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\t\n\tint ARRAY_SIZE = p * k; \n\n\tint\tnumbers[ARRAY_SIZE]; \n\n\t\n\t\n\n\trandomNumbers(numbers, ARRAY_SIZE);\n\t\n\t\n\n\tMPI_Scatter (numbers, k, MPI_INT, numbers,\n              ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD); \n    \n    \n\n    subSum = subTotal(numbers);   \n    printf(\"I am %d. process, partial sum = %d\\n\", my_rank, subSum);       \n\t\n\t\n\n\tMPI_Reduce(&subSum,&sum,1,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n\t\n\tif(my_rank == 0) {\n\t\t\n\n\t\t\n\t\t\n\n\t\tprintf(\"Total : %d\\n\",sum);\t\n\t\t\n\t\tprintf(\"Numbers: [ \");\n\t\tfor(i=0; i<ARRAY_SIZE; i++)\t\n\t\t\tprintf(\"%d, \",numbers[i]);\n\t\tprintf(\"]\\n\");\n\t\t\n\t} \n\n\t\n\n    err = MPI_Finalize();\n    return 0;\n}"}
{"program": "d-meiser_534", "code": "int main(int argn, char **argv)\n{\n#ifdef BL_WITH_MPI\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  TestSuite *suite = create_test_suite();\n  add_test_with_context(suite, Diagnostics, canBeCreated);\n  add_test_with_context(suite, Diagnostics, worksForMultipleOfDumpPeriodicity);\n  add_test_with_context(suite, Diagnostics, doesntDumpWhenNotMultipleOfDumpPeriodicity);\n  add_test_with_context(suite, Diagnostics, canCreateMultipleDiagnostics);\n  add_test_with_context(suite, Diagnostics, canProcessSeveral);\n  int result = run_test_suite(suite, create_text_reporter());\n  destroy_test_suite(suite);\n#ifdef BL_WITH_MPI\n#endif\n  return result;\n}", "label": "int main(int argn, char **argv)\n{\n#ifdef BL_WITH_MPI\n  MPI_Init(&argn, &argv);\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  TestSuite *suite = create_test_suite();\n  add_test_with_context(suite, Diagnostics, canBeCreated);\n  add_test_with_context(suite, Diagnostics, worksForMultipleOfDumpPeriodicity);\n  add_test_with_context(suite, Diagnostics, doesntDumpWhenNotMultipleOfDumpPeriodicity);\n  add_test_with_context(suite, Diagnostics, canCreateMultipleDiagnostics);\n  add_test_with_context(suite, Diagnostics, canProcessSeveral);\n  int result = run_test_suite(suite, create_text_reporter());\n  destroy_test_suite(suite);\n#ifdef BL_WITH_MPI\n  MPI_Finalize();\n#endif\n  return result;\n}"}
{"program": "ghisvail_535", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 2; n[1] = 2; n[2] = 4;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with 4 processes.\\n\");\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT, g_hat\", MPI_COMM_WORLD);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_apr_complex_permuted_3d(\n      out, local_no, local_o_start, 1, 2, 0,\n      \"PFFT, g\", MPI_COMM_WORLD);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT^H, g_hat\", MPI_COMM_WORLD);\n\n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 2; n[1] = 2; n[2] = 4;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with 4 processes.\\n\");\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT, g_hat\", MPI_COMM_WORLD);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_apr_complex_permuted_3d(\n      out, local_no, local_o_start, 1, 2, 0,\n      \"PFFT, g\", MPI_COMM_WORLD);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT^H, g_hat\", MPI_COMM_WORLD);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "alucas_537", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tstarpu_mpi_req req;\n\n\t\tif ((loop % 2) == rank)\n\t\t{\n                        starpu_mpi_isend(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\t\t\tstarpu_mpi_irecv(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\n\t\tint finished = 0;\n\t\tdo {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_test(&req, &finished, &status);\n\t\t} while (!finished);\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tstarpu_mpi_req req;\n\n\t\tif ((loop % 2) == rank)\n\t\t{\n                        starpu_mpi_isend(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\t\t\tstarpu_mpi_irecv(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\n\t\tint finished = 0;\n\t\tdo {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_test(&req, &finished, &status);\n\t\t} while (!finished);\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "afarah1_538", "code": "int main(int argc, char **argv) {\n  int proc, me;\n\n\n  func(me, proc);\n    \n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  int proc, me;\n\n  MPI_Init (&argc, & argv);\n  MPI_Comm_size (MPI_COMM_WORLD, &proc);\n  MPI_Comm_rank (MPI_COMM_WORLD, &me);\n\n  func(me, proc);\n    \n  MPI_Finalize ();\n  return 0;\n}"}
{"program": "dnoliver_539", "code": "int main( int argc, char *argv[] )\n{\n\tchar idstr[32];\n\tchar buff[BUFSIZE];\n\tint numprocs;\n\tint myid;\n\tint i;\n\n\tMPI_Status stat;\n\t\n\n\t\n\n\t\n\n\t\n\n\tif(myid == 0)\n\t{\n\t\tprintf(\"%d: We have %d processors\\n\", myid, numprocs);\n\t\tfor(i=1;i<numprocs;i++)\n\t\t{\n\t\t\tsprintf(buff, \"Hello %d! \", i);\n\t\t}\n\t\tfor(i=1;i<numprocs;i++)\n\t\t{\n\t\t}\n\t}\n\telse\n\t{\n\t\t\n\n\t\tsprintf(idstr, \"Processor %d \", myid);\n\t\tstrncat(buff, idstr, BUFSIZE-1);\n\t\tstrncat(buff, \"reporting for duty\\n\", BUFSIZE-1);\n\t\t\n\n\t}\n\t\n\n\treturn 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n\tchar idstr[32];\n\tchar buff[BUFSIZE];\n\tint numprocs;\n\tint myid;\n\tint i;\n\n\tMPI_Status stat;\n\t\n\n\tMPI_Init(&argc,&argv);\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\t\n\n\tif(myid == 0)\n\t{\n\t\tprintf(\"%d: We have %d processors\\n\", myid, numprocs);\n\t\tfor(i=1;i<numprocs;i++)\n\t\t{\n\t\t\tsprintf(buff, \"Hello %d! \", i);\n\t\t\tMPI_Send(buff, BUFSIZE, MPI_CHAR, i, TAG, MPI_COMM_WORLD);\n\t\t}\n\t\tfor(i=1;i<numprocs;i++)\n\t\t{\n\t\t\tMPI_Recv(buff, BUFSIZE, MPI_CHAR, i, TAG, MPI_COMM_WORLD,&stat);\n\t\t}\n\t}\n\telse\n\t{\n\t\t\n\n\t\tMPI_Recv(buff, BUFSIZE, MPI_CHAR, 0, TAG, MPI_COMM_WORLD, &stat);\n\t\tsprintf(idstr, \"Processor %d \", myid);\n\t\tstrncat(buff, idstr, BUFSIZE-1);\n\t\tstrncat(buff, \"reporting for duty\\n\", BUFSIZE-1);\n\t\t\n\n\t\tMPI_Send(buff, BUFSIZE, MPI_CHAR, 0, TAG, MPI_COMM_WORLD);\n\t}\n\t\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "tcsiwula_541", "code": "int main(void)\n{\n   char       greeting[MAX_STRING];\n   int        my_rank, p, dest, source;\n\n   \n\n\n   \n\n\n   \n\n\n   if (p == 1)\n   {\n      printf(\"Greetings from process %d of %d\\n\", my_rank, p);\n   }\n   else if (p > 1)\n   {\n      if(my_rank == 0)\n      {\n        dest = 1;\n        source = my_rank+1;\n        printf(\"Greetings from process %d of %d\\n\", my_rank, p);\n\tprintf(\"%s\\n\", greeting); \n     }\n     \telse\n      {\n        dest = my_rank -1;\n        source = my_rank+1;\n        sprintf(greeting, \"Greetings from process %d of %d \\n\", my_rank, p);\n\t\n\tif(my_rank < p-1)\n\t{\n\t\tprintf(\"%s\\n\", greeting);\n\t}\n\t}\t\n   }\n\n   \n\n   return 0;\n}", "label": "int main(void)\n{\n   char       greeting[MAX_STRING];\n   int        my_rank, p, dest, source;\n\n   \n\n   MPI_Init(NULL, NULL);\n\n   \n\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   if (p == 1)\n   {\n      printf(\"Greetings from process %d of %d\\n\", my_rank, p);\n   }\n   else if (p > 1)\n   {\n      if(my_rank == 0)\n      {\n        dest = 1;\n        source = my_rank+1;\n        printf(\"Greetings from process %d of %d\\n\", my_rank, p);\n      \tMPI_Recv(greeting, MAX_STRING, MPI_CHAR, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\tprintf(\"%s\\n\", greeting); \n     }\n     \telse\n      {\n        dest = my_rank -1;\n        source = my_rank+1;\n        sprintf(greeting, \"Greetings from process %d of %d \\n\", my_rank, p);\n        MPI_Send(greeting, strlen(greeting)+1, MPI_CHAR, dest, 0, MPI_COMM_WORLD);\n\t\n\tif(my_rank < p-1)\n\t{\n\t\tMPI_Recv(greeting, MAX_STRING, MPI_CHAR, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tprintf(\"%s\\n\", greeting);\n\t}\n\t}\t\n   }\n\n   \n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "gnu3ra_542", "code": "int main( int argc, char **argv )\n{\n    int              rank, size, i,j;\n    int              table[MAX_PROCESSES][MAX_PROCESSES];\n    int              errors=0;\n    int              participants;\n\n\n    \n\n    if ( size > MAX_PROCESSES ) participants = MAX_PROCESSES;\n    else              participants = size;\n    if (MAX_PROCESSES % participants) {\n\tfprintf( stderr, \"Number of processors must divide %d\\n\",\n\t\tMAX_PROCESSES );\n\t}\n    \n\n    if ( (rank < participants) ) {\n\n      \n\n      int block_size = MAX_PROCESSES / participants;\n      int begin_row  = rank * block_size;\n      int end_row    = (rank+1) * block_size;\n      int send_count = block_size * MAX_PROCESSES;\n      int recv_count = send_count;\n\n      \n\n      for (i=begin_row; i<end_row ;i++)\n\tfor (j=0; j<MAX_PROCESSES; j++)\n\t  table[i][j] = rank + 10;\n\n      \n\n\n      \n\n      \n\n      \n\n      for (i=0; i<MAX_PROCESSES;i++) {\n\tif ( (table[i][0] - table[i][MAX_PROCESSES-1] !=0) ) \n\t  errors++;\n      }\n    } \n\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}", "label": "int main( int argc, char **argv )\n{\n    int              rank, size, i,j;\n    int              table[MAX_PROCESSES][MAX_PROCESSES];\n    int              errors=0;\n    int              participants;\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n\n    \n\n    if ( size > MAX_PROCESSES ) participants = MAX_PROCESSES;\n    else              participants = size;\n    if (MAX_PROCESSES % participants) {\n\tfprintf( stderr, \"Number of processors must divide %d\\n\",\n\t\tMAX_PROCESSES );\n\tMPI_Abort( MPI_COMM_WORLD, 1 );\n\t}\n    \n\n    if ( (rank < participants) ) {\n\n      \n\n      int block_size = MAX_PROCESSES / participants;\n      int begin_row  = rank * block_size;\n      int end_row    = (rank+1) * block_size;\n      int send_count = block_size * MAX_PROCESSES;\n      int recv_count = send_count;\n\n      \n\n      for (i=begin_row; i<end_row ;i++)\n\tfor (j=0; j<MAX_PROCESSES; j++)\n\t  table[i][j] = rank + 10;\n\n      \n\n      MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL,\n                    &table[0][0], recv_count, MPI_INT, MPI_COMM_WORLD);\n\n      \n\n      \n\n      \n\n      for (i=0; i<MAX_PROCESSES;i++) {\n\tif ( (table[i][0] - table[i][MAX_PROCESSES-1] !=0) ) \n\t  errors++;\n      }\n    } \n\n    MPI_Finalize();\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}"}
{"program": "hunsa_543", "code": "int main(int argc, char* argv[]) {\n    int my_rank, nprocs, p;\n    int master_rank;\n    double runtime_s;\n    reprompib_st_opts_t opts;\n    reprompib_st_error_t ret;\n    reprompib_sync_options_t sync_opts;\n    FILE* f;\n\n    double *all_runtimes = NULL;\n    int dummy_nrep = 1;\n\n    \n\n    reprompib_sync_functions_t sync_f;\n    initialize_sync_implementation(&sync_f);\n\n    init_timer();\n\n    \n\n    master_rank = 0;\n\n\n    if (my_rank == master_rank) {\n        all_runtimes = (double*) calloc(nprocs, sizeof(double));\n    }\n\n    ret = parse_test_options(&opts, argc, argv);\n    \n\n\n    sync_f.parse_sync_params(argc, argv, &sync_opts);\n    sync_f.init_sync_module(sync_opts, dummy_nrep);\n    print_initial_settings(argc, argv, sync_f.print_sync_info);\n\n\n    runtime_s = get_time();\n    init_timer();\n    sync_f.sync_clocks();\n    sync_f.init_sync();\n    runtime_s = get_time() - runtime_s;\n\n\n    f = stdout;\n    if (my_rank == master_rank) {\n        fprintf(f, \"p runtime\\n\");\n        for (p = 0; p < nprocs; p++) {\n            fprintf(f, \"%3d %14.9f\\n\", p, all_runtimes[p]);\n        }\n\n        free(all_runtimes);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char* argv[]) {\n    int my_rank, nprocs, p;\n    int master_rank;\n    double runtime_s;\n    reprompib_st_opts_t opts;\n    reprompib_st_error_t ret;\n    reprompib_sync_options_t sync_opts;\n    FILE* f;\n\n    double *all_runtimes = NULL;\n    int dummy_nrep = 1;\n\n    \n\n    reprompib_sync_functions_t sync_f;\n    initialize_sync_implementation(&sync_f);\n\n    init_timer();\n\n    \n\n    MPI_Init(&argc, &argv);\n    master_rank = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    if (my_rank == master_rank) {\n        all_runtimes = (double*) calloc(nprocs, sizeof(double));\n    }\n\n    ret = parse_test_options(&opts, argc, argv);\n    \n\n\n    sync_f.parse_sync_params(argc, argv, &sync_opts);\n    sync_f.init_sync_module(sync_opts, dummy_nrep);\n    print_initial_settings(argc, argv, sync_f.print_sync_info);\n\n\n    runtime_s = get_time();\n    init_timer();\n    sync_f.sync_clocks();\n    sync_f.init_sync();\n    runtime_s = get_time() - runtime_s;\n\n    MPI_Gather(&runtime_s, 1, MPI_DOUBLE, all_runtimes, 1, MPI_DOUBLE, 0,\n            MPI_COMM_WORLD);\n\n    f = stdout;\n    if (my_rank == master_rank) {\n        fprintf(f, \"p runtime\\n\");\n        for (p = 0; p < nprocs; p++) {\n            fprintf(f, \"%3d %14.9f\\n\", p, all_runtimes[p]);\n        }\n\n        free(all_runtimes);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ClaudioNahmad_545", "code": "int main( int argc, char* argv[] )\n{\n    int run_tests = 0xffffffff;  \n\n    int length, rank, size;\n    MPI_Datatype ddt;\n    \n\n\n\n\n    if( rank != 0 ) {\n        exit(0);\n    }\n\n    if( run_tests & DO_CONTIG ) {\n        printf( \"\\ncontiguous datatype\\n\\n\" );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( MPI_INT, MPI_INT, length );\n    }\n\n    if( run_tests & DO_INDEXED_GAP ) {\n        printf( \"\\nindexed gap\\n\\n\" );\n        ddt = create_indexed_gap_ddt();\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n    }\n\n    if( run_tests & DO_OPTIMIZED_INDEXED_GAP ) {\n        printf( \"\\noptimized indexed gap\\n\\n\" );\n        ddt = create_indexed_gap_optimized_ddt();\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n    }\n\n    if( run_tests & DO_CONSTANT_GAP ) {\n        printf( \"\\nconstant indexed gap\\n\\n\" );\n        ddt = create_indexed_constant_gap_ddt( 80, 100, 1 );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n    }\n\n    if( run_tests & DO_CONSTANT_GAP ) {\n        printf( \"\\noptimized constant indexed gap\\n\\n\" );\n        ddt = create_optimized_indexed_constant_gap_ddt( 80, 100, 1 );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n    }\n\n    if( run_tests & DO_STRUCT_CONSTANT_GAP_RESIZED ) {\n        printf( \"\\nstruct constant gap resized\\n\\n\" );\n        ddt = create_struct_constant_gap_resized_ddt( 0 \n, 0 \n, 0 \n );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n    }\n\n    exit(0);\n}", "label": "int main( int argc, char* argv[] )\n{\n    int run_tests = 0xffffffff;  \n\n    int length, rank, size;\n    MPI_Datatype ddt;\n    \n\n\n    MPI_Init (&argc, &argv);\n\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n    MPI_Comm_size (MPI_COMM_WORLD, &size);\n\n    if( rank != 0 ) {\n        MPI_Finalize();\n        exit(0);\n    }\n\n    if( run_tests & DO_CONTIG ) {\n        printf( \"\\ncontiguous datatype\\n\\n\" );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( MPI_INT, MPI_INT, length );\n    }\n\n    if( run_tests & DO_INDEXED_GAP ) {\n        printf( \"\\nindexed gap\\n\\n\" );\n        ddt = create_indexed_gap_ddt();\n        MPI_DDT_DUMP( ddt );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n        MPI_Type_free( &ddt );\n    }\n\n    if( run_tests & DO_OPTIMIZED_INDEXED_GAP ) {\n        printf( \"\\noptimized indexed gap\\n\\n\" );\n        ddt = create_indexed_gap_optimized_ddt();\n        MPI_DDT_DUMP( ddt );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n        MPI_Type_free( &ddt );\n    }\n\n    if( run_tests & DO_CONSTANT_GAP ) {\n        printf( \"\\nconstant indexed gap\\n\\n\" );\n        ddt = create_indexed_constant_gap_ddt( 80, 100, 1 );\n        MPI_DDT_DUMP( ddt );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n        MPI_Type_free( &ddt );\n    }\n\n    if( run_tests & DO_CONSTANT_GAP ) {\n        printf( \"\\noptimized constant indexed gap\\n\\n\" );\n        ddt = create_optimized_indexed_constant_gap_ddt( 80, 100, 1 );\n        MPI_DDT_DUMP( ddt );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n        MPI_Type_free( &ddt );\n    }\n\n    if( run_tests & DO_STRUCT_CONSTANT_GAP_RESIZED ) {\n        printf( \"\\nstruct constant gap resized\\n\\n\" );\n        ddt = create_struct_constant_gap_resized_ddt( 0 \n, 0 \n, 0 \n );\n        MPI_DDT_DUMP( ddt );\n        for( length = MIN_LENGTH; length < MAX_LENGTH; length <<=1 )\n            do_test_for_ddt( ddt, ddt, length );\n        MPI_Type_free( &ddt );\n    }\n\n    MPI_Finalize ();\n    exit(0);\n}"}
{"program": "syftalent_546", "code": "int main(int argc, char **argv) {\n    int size, rank, msg, cancelled;\n    MPI_Request request;\n    MPI_Status status;\n\n\n    if (size != 2) {\n        fprintf(stderr,\"ERROR: must be run with 2 processes\");\n    }\n\n    if (rank == 0) {\n        msg = -1;\n        \n\n        assert(cancelled);\n\n        assert(msg == 42);\n    } else {\n        msg = 42;\n    }\n\n    if (rank == 0)\n        printf(\" No Errors\\n\");\n\n}", "label": "int main(int argc, char **argv) {\n    int size, rank, msg, cancelled;\n    MPI_Request request;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (size != 2) {\n        fprintf(stderr,\"ERROR: must be run with 2 processes\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    if (rank == 0) {\n        msg = -1;\n        \n\n        MPI_Irecv(&msg, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &request);\n        MPI_Cancel(&request);\n        MPI_Wait(&request, &status);\n        MPI_Test_cancelled(&status, &cancelled);\n        assert(cancelled);\n\n        MPI_Barrier(MPI_COMM_WORLD);\n        MPI_Irecv(&msg, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &request);\n        MPI_Wait(&request, &status);\n        assert(msg == 42);\n    } else {\n        MPI_Barrier(MPI_COMM_WORLD);\n        msg = 42;\n        MPI_Send(&msg, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n}"}
{"program": "gentryx_548", "code": "int main(int argc, char * argv[])\n{\n\n    int rank, size;\n\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    if (size<2) {\n        printf(\"Use 2 or more processes. \\n\");\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf = NULL;\n\n\n    memset(buf, rank, (size_t)n);\n\n    size_t errors = 0;\n    for (int r = 1; r < size; r++) {\n\n        MPI_Request req;\n\n        \n\n        if (rank==r) {\n        }\n        else if (rank==0) {\n        }\n\n        if (rank == 0 || rank==r) {\n        }\n\n        if (rank==0) {\n            errors += verify_buffer(buf, n, r);\n            if (errors > 0) {\n                printf(\"There were %zu errors!\", errors);\n            }\n        }\n    }\n\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    if (size<2) {\n        printf(\"Use 2 or more processes. \\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf = NULL;\n\n    MPI_Alloc_mem((MPI_Aint)n, MPI_INFO_NULL, &buf);\n\n    memset(buf, rank, (size_t)n);\n\n    size_t errors = 0;\n    for (int r = 1; r < size; r++) {\n\n        MPI_Request req;\n\n        \n\n        if (rank==r) {\n            MPIX_Irsend_x(buf, n, MPI_CHAR, 0 \n, r \n, MPI_COMM_WORLD, &req);\n        }\n        else if (rank==0) {\n            MPIX_Irecv_x(buf, n, MPI_CHAR, r \n, r \n, MPI_COMM_WORLD, &req);\n        }\n\n        if (rank == 0 || rank==r) {\n            MPI_Wait(&req, MPI_STATUS_IGNORE);\n        }\n\n        if (rank==0) {\n            errors += verify_buffer(buf, n, r);\n            if (errors > 0) {\n                printf(\"There were %zu errors!\", errors);\n            }\n        }\n    }\n\n    MPI_Free_mem(buf);\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "fiber-miniapp_549", "code": "int main(int argc, char **argv)\n{\n  int n = 1000000;\n  double a, b;\n\n#ifdef USE_MPI\n#endif\n\n  maprof_time_start(ALL);\n\n  maprof_time_start(FUNC1);\n  a = func1(n);\n  maprof_time_stop(FUNC1);\n\n  maprof_time_start(FUNC2);\n  b = func2(n);\n  maprof_time_stop(FUNC2);\n\n  printf(\"a + b = %f\\n\", a + b);\n\n  maprof_time_stop(ALL);\n\n  maprof_print(ALL, \"all:\");\n#ifdef USE_MPI\n  maprof_print_time_mpi(FUNC1, \"func1: \");\n  maprof_print_time_mpi(FUNC2, \"func2: \");\n  maprof_print_time_mpi_full(ALL, \"all: \");\n#else\n  maprof_print_time(FUNC1, \"func1: \");\n  maprof_print_time(FUNC2, \"func2: \");\n  maprof_print_time(ALL, \"all:   \");\n#endif\n\n  maprof_setup(\"c_test\", \"1.0.0\");\n  maprof_add_section(\"fucn1\", FUNC1);\n  maprof_add_section(\"fucn2\", FUNC2);\n  maprof_profile_add_problem_size(\"n\", n);\n  maprof_output();\n\n#ifdef USE_MPI\n#endif\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int n = 1000000;\n  double a, b;\n\n#ifdef USE_MPI\n  MPI_Init(&argc, &argv);\n#endif\n\n  maprof_time_start(ALL);\n\n  maprof_time_start(FUNC1);\n  a = func1(n);\n  maprof_time_stop(FUNC1);\n\n  maprof_time_start(FUNC2);\n  b = func2(n);\n  maprof_time_stop(FUNC2);\n\n  printf(\"a + b = %f\\n\", a + b);\n\n  maprof_time_stop(ALL);\n\n  maprof_print(ALL, \"all:\");\n#ifdef USE_MPI\n  maprof_print_time_mpi(FUNC1, \"func1: \");\n  maprof_print_time_mpi(FUNC2, \"func2: \");\n  maprof_print_time_mpi_full(ALL, \"all: \");\n#else\n  maprof_print_time(FUNC1, \"func1: \");\n  maprof_print_time(FUNC2, \"func2: \");\n  maprof_print_time(ALL, \"all:   \");\n#endif\n\n  maprof_setup(\"c_test\", \"1.0.0\");\n  maprof_add_section(\"fucn1\", FUNC1);\n  maprof_add_section(\"fucn2\", FUNC2);\n  maprof_profile_add_problem_size(\"n\", n);\n  maprof_output();\n\n#ifdef USE_MPI\n  MPI_Finalize();\n#endif\n\n  return 0;\n}"}
{"program": "linhbngo_550", "code": "int main(int argc, char * argv[] ) {\n  int rank;     \n\n  int size;     \n\n  double a, b;  \n\n  int n;        \n\n  double h;        \n\n  double local_a, local_b; \n\n  int local_n;  \n\n  double result;       \n\n  double local_result; \n\n  int p;        \n\n  MPI_Status status;\n\n\n  a = atof(argv[1]);\n  b = atof(argv[2]);\n  n = atoi(argv[3]);\n\n  \n\n  h = (b - a) / n;\n  local_n = n / size;\n  local_a = a + rank * local_n * h;\n  local_b = local_a + local_n * h;\n  local_result = Trap(local_a,local_b,local_n);\n\n  \n\n  if (rank == 0){\n    result = local_result;\n    for (p = 1; p < size; p++){\n      result += local_result;\n    }\n  } \n  else{\n  }\n\n  \n\n  if (rank == 0){\n    printf(\"Calculating the integral of f(x) from %lf to %lf\\n\", a, b);\n    printf(\"The integral is %lf\\n\", result);  \n  }\n}", "label": "int main(int argc, char * argv[] ) {\n  int rank;     \n\n  int size;     \n\n  double a, b;  \n\n  int n;        \n\n  double h;        \n\n  double local_a, local_b; \n\n  int local_n;  \n\n  double result;       \n\n  double local_result; \n\n  int p;        \n\n  MPI_Status status;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n\n  a = atof(argv[1]);\n  b = atof(argv[2]);\n  n = atoi(argv[3]);\n\n  \n\n  h = (b - a) / n;\n  local_n = n / size;\n  local_a = a + rank * local_n * h;\n  local_b = local_a + local_n * h;\n  local_result = Trap(local_a,local_b,local_n);\n\n  \n\n  if (rank == 0){\n    result = local_result;\n    for (p = 1; p < size; p++){\n      MPI_Recv(&local_result,1,MPI_DOUBLE,p,MPI_ANY_TAG,MPI_COMM_WORLD,&status);\n      result += local_result;\n    }\n  } \n  else{\n    MPI_Send(&local_result,1,MPI_DOUBLE,0,0,MPI_COMM_WORLD);  \n  }\n\n  \n\n  if (rank == 0){\n    printf(\"Calculating the integral of f(x) from %lf to %lf\\n\", a, b);\n    printf(\"The integral is %lf\\n\", result);  \n  }\n  MPI_Finalize();\n}"}
{"program": "cmcantalupo_551", "code": "int main(int argc, char **argv)\n{\n    int size = 0;\n    int rank = 0;\n\n    int err =\n    if (!err) {\n        err =\n    }\n    if (!err) {\n        err =\n    }\n    if (!err && !rank) {\n        printf(\"MPI_COMM_WORLD size: %d\\n\", size);\n    }\n\n    uint64_t dgemm_rid;\n    if (!err) {\n        err = geopm_prof_region(\"tutorial_dgemm\",\n                                GEOPM_REGION_HINT_COMPUTE,\n                                &dgemm_rid);\n    }\n\n    int num_iter = 500;\n    double dgemm_big_o = 8.0;\n    if (rank < size / 2) {\n        dgemm_big_o *= 1.1;\n    }\n\n    if (!rank) {\n        printf(\"Beginning loop of %d iterations.\\n\", num_iter);\n        fflush(stdout);\n    }\n    for (int i = 0; !err && i < num_iter; ++i) {\n        err = geopm_prof_epoch();\n        if (!err) {\n            err = geopm_prof_enter(dgemm_rid);\n        }\n        if (!err) {\n            err = tutorial_dgemm_static(dgemm_big_o, 0);\n        }\n        if (!err) {\n            err = geopm_prof_exit(dgemm_rid);\n        }\n        if (!err) {\n            err =\n        }\n        if (!err && !rank) {\n            printf(\"Iteration=%.3d\\r\", i);\n            fflush(stdout);\n        }\n    }\n    if (!err && !rank) {\n        printf(\"Completed loop.                    \\n\");\n        fflush(stdout);\n    }\n    if (!err) {\n        err = tutorial_dgemm_static(0.0, 0);\n    }\n\n    int err_fin =\n    err = err ? err : err_fin;\n\n    return err;\n}", "label": "int main(int argc, char **argv)\n{\n    int size = 0;\n    int rank = 0;\n\n    int err = MPI_Init(&argc, &argv);\n    if (!err) {\n        err = MPI_Comm_size(MPI_COMM_WORLD, &size);\n    }\n    if (!err) {\n        err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    }\n    if (!err && !rank) {\n        printf(\"MPI_COMM_WORLD size: %d\\n\", size);\n    }\n\n    uint64_t dgemm_rid;\n    if (!err) {\n        err = geopm_prof_region(\"tutorial_dgemm\",\n                                GEOPM_REGION_HINT_COMPUTE,\n                                &dgemm_rid);\n    }\n\n    int num_iter = 500;\n    double dgemm_big_o = 8.0;\n    if (rank < size / 2) {\n        dgemm_big_o *= 1.1;\n    }\n\n    if (!rank) {\n        printf(\"Beginning loop of %d iterations.\\n\", num_iter);\n        fflush(stdout);\n    }\n    for (int i = 0; !err && i < num_iter; ++i) {\n        err = geopm_prof_epoch();\n        if (!err) {\n            err = geopm_prof_enter(dgemm_rid);\n        }\n        if (!err) {\n            err = tutorial_dgemm_static(dgemm_big_o, 0);\n        }\n        if (!err) {\n            err = geopm_prof_exit(dgemm_rid);\n        }\n        if (!err) {\n            err = MPI_Barrier(MPI_COMM_WORLD);\n        }\n        if (!err && !rank) {\n            printf(\"Iteration=%.3d\\r\", i);\n            fflush(stdout);\n        }\n    }\n    if (!err && !rank) {\n        printf(\"Completed loop.                    \\n\");\n        fflush(stdout);\n    }\n    if (!err) {\n        err = tutorial_dgemm_static(0.0, 0);\n    }\n\n    int err_fin = MPI_Finalize();\n    err = err ? err : err_fin;\n\n    return err;\n}"}
{"program": "praveendath92_554", "code": "int main(int argc, char* argv[])\n{\n\tchar **mat_1;\n\tchar **mat_2;\n\tunsigned long long **prod;\n\n\tmy_rank;\n\tcomm_size;\n\n\n\n\tunsigned long long start_time = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Starting app at time: %llu\\n\", start_time);\n\t}\n\t\n\trow_div = 1;\n\tcol_div = 1;\n\n\tif(my_rank != 0)\n\t{\n\t\tcol_div = (int)floor(sqrt((double)comm_size));\n\t\twhile(comm_size % col_div != 0)\n\t\t{\n\t\t\tcol_div--;\n\t\t}\n\t\tif(col_div == 0)\n\t\t{\n\t\t\tfprintf(stderr, \"Error: col_div is zero!!\\n\");\n\t\t}\n\t\trow_div = comm_size / col_div;\n\t}\n\n\tmat_1 = malloc(sizeof(char *) * DIM_LEN / row_div);\n\tmat_2 = malloc(sizeof(char *) * DIM_LEN / col_div);\n\tprod = malloc(sizeof(unsigned long long *) * DIM_LEN / row_div);\n\n\tint i;\n\tfor(i = 0; i < DIM_LEN / col_div || i < DIM_LEN / row_div; i++)\n\t{\n\t\tif(i < DIM_LEN / col_div)\n\t\t{\n\t\t\tmat_2[i] = malloc(sizeof(char) * DIM_LEN);\n\t\t\tif(mat_2[i] == NULL)\n\t\t\t{\n\t\t\t\tfprintf(stderr, \"Error: could not allocate memory to array at %d\\n\", i);\n\t\t\t}\n\t\t}\n\t\tif(i < DIM_LEN / row_div)\n\t\t{\n\t\t\tmat_1[i] = malloc(sizeof(char) * DIM_LEN);\n\t\t\tprod[i] = malloc(sizeof(unsigned long long) * DIM_LEN / col_div);\n\n\t\t\tif(mat_1[i] == NULL || prod[i] == NULL)\n\t\t\t{\n\t\t\t\tfprintf(stderr, \"Error: could not allocate memory to array at %d\\n\", i);\n\t\t\t}\n\t\t}\n\t}\n\n\tunsigned long long atime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in allocation: %llu\\n\", atime - start_time);\n\t}\n\n\tif(my_rank == 0)\n\t{\n\t\tinit_mat(mat_1);\n\t\tinit_mat(mat_2);\n\t}\n\n\tif(my_rank == 0)\n\t{\n\t\tcol_div = (int)floor(sqrt((double)comm_size));\n\t\twhile(comm_size % col_div != 0)\n\t\t{\n\t\t\tcol_div--;\n\t\t}\n\t\tif(col_div == 0)\n\t\t{\n\t\t\tfprintf(stderr, \"Error: col_div is zero!!\\n\");\n\t\t}\n\t\trow_div = comm_size / col_div;\n\t}\n\tunsigned long long itime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in initialization: %llu\\n\", itime - atime);\n\t}\n\n\tdistribute_matrix_rows(mat_1);\n\tdistribute_matrix_cols(mat_2);\n\n\tunsigned long long dtime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in distributing matrices: %llu\\n\", dtime - itime);\n\t}\n\tcal_prod(mat_1, mat_2, prod);\n\tunsigned long long ptime = get_time();\n\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in multiplication: %llu\\n\", ptime - dtime);\n\t}\n\n\tgather_product(prod);\n\tunsigned long long gtime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in gathering: %llu\\n\", gtime - ptime);\n\t\tprintf(\"Total time spent in application: %llu\\n\", gtime - start_time);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char* argv[])\n{\n\tchar **mat_1;\n\tchar **mat_2;\n\tunsigned long long **prod;\n\n\tmy_rank;\n\tcomm_size;\n\n\tMPI_Init(&argc, &argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n\tunsigned long long start_time = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Starting app at time: %llu\\n\", start_time);\n\t}\n\t\n\trow_div = 1;\n\tcol_div = 1;\n\n\tif(my_rank != 0)\n\t{\n\t\tcol_div = (int)floor(sqrt((double)comm_size));\n\t\twhile(comm_size % col_div != 0)\n\t\t{\n\t\t\tcol_div--;\n\t\t}\n\t\tif(col_div == 0)\n\t\t{\n\t\t\tfprintf(stderr, \"Error: col_div is zero!!\\n\");\n\t\t}\n\t\trow_div = comm_size / col_div;\n\t}\n\n\tmat_1 = malloc(sizeof(char *) * DIM_LEN / row_div);\n\tmat_2 = malloc(sizeof(char *) * DIM_LEN / col_div);\n\tprod = malloc(sizeof(unsigned long long *) * DIM_LEN / row_div);\n\n\tint i;\n\tfor(i = 0; i < DIM_LEN / col_div || i < DIM_LEN / row_div; i++)\n\t{\n\t\tif(i < DIM_LEN / col_div)\n\t\t{\n\t\t\tmat_2[i] = malloc(sizeof(char) * DIM_LEN);\n\t\t\tif(mat_2[i] == NULL)\n\t\t\t{\n\t\t\t\tfprintf(stderr, \"Error: could not allocate memory to array at %d\\n\", i);\n\t\t\t}\n\t\t}\n\t\tif(i < DIM_LEN / row_div)\n\t\t{\n\t\t\tmat_1[i] = malloc(sizeof(char) * DIM_LEN);\n\t\t\tprod[i] = malloc(sizeof(unsigned long long) * DIM_LEN / col_div);\n\n\t\t\tif(mat_1[i] == NULL || prod[i] == NULL)\n\t\t\t{\n\t\t\t\tfprintf(stderr, \"Error: could not allocate memory to array at %d\\n\", i);\n\t\t\t}\n\t\t}\n\t}\n\n\tunsigned long long atime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in allocation: %llu\\n\", atime - start_time);\n\t}\n\n\tif(my_rank == 0)\n\t{\n\t\tinit_mat(mat_1);\n\t\tinit_mat(mat_2);\n\t}\n\n\tif(my_rank == 0)\n\t{\n\t\tcol_div = (int)floor(sqrt((double)comm_size));\n\t\twhile(comm_size % col_div != 0)\n\t\t{\n\t\t\tcol_div--;\n\t\t}\n\t\tif(col_div == 0)\n\t\t{\n\t\t\tfprintf(stderr, \"Error: col_div is zero!!\\n\");\n\t\t}\n\t\trow_div = comm_size / col_div;\n\t}\n\tunsigned long long itime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in initialization: %llu\\n\", itime - atime);\n\t}\n\n\tdistribute_matrix_rows(mat_1);\n\tdistribute_matrix_cols(mat_2);\n\n\tunsigned long long dtime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in distributing matrices: %llu\\n\", dtime - itime);\n\t}\n\tcal_prod(mat_1, mat_2, prod);\n\tunsigned long long ptime = get_time();\n\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in multiplication: %llu\\n\", ptime - dtime);\n\t}\n\n\tgather_product(prod);\n\tunsigned long long gtime = get_time();\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"Time spent in gathering: %llu\\n\", gtime - ptime);\n\t\tprintf(\"Total time spent in application: %llu\\n\", gtime - start_time);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "gyaikhom_555", "code": "int main(int argc, char *argv[])\n{\n\tint *in, *out, i;\n\n\t\n\n\tbc_init(BC_ERR);\n\n\tif (bc_size != 9) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: 9 processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c 9 pipeline_farm\\n\\n\");\n\t\tbc_final();\n\t\treturn -1;\n\t}\n\n\t\n\n\tswitch(bc_rank) {\n\tcase 0:\n\t\tin = (int *) malloc(sizeof(int)*100);\n\t\tprintf(\"[%d] Input:\\n\", bc_rank);\n\t\tfor (i = 0; i < 100; i++) {\n\t\t\tin[i] = i;\n  \t\t\tprintf(\"%d \", in[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\t\tstage_0(in);\n\t\tfree(in);\n\t\tbreak;\n\tcase 1:\n\t\tstage_1();\n\t\tbreak;\n\tcase 2:\n\t\tstage_3();\n\t\tbreak;\n\tcase 3:\n\t\tout = (int *) malloc(sizeof(int)*2);\n\t\tstage_4(out);\n \t\tprintf(\"[%d] Sum: %d\\n\", bc_rank, out[0]);\n\t\tfree(out);\n\t\tbreak;\n\tcase 4:\n\tcase 5:\n\tcase 6:\n\tcase 7:\n\tcase 8:\n\t\tstage_2();\n\t\tbreak;\n\tdefault :\n\t\tbreak;\n\t}\n\tbc_final();\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint *in, *out, i;\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\tbc_init(BC_ERR);\n\n\tif (bc_size != 9) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: 9 processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c 9 pipeline_farm\\n\\n\");\n\t\tbc_final();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n\t\n\n\tswitch(bc_rank) {\n\tcase 0:\n\t\tin = (int *) malloc(sizeof(int)*100);\n\t\tprintf(\"[%d] Input:\\n\", bc_rank);\n\t\tfor (i = 0; i < 100; i++) {\n\t\t\tin[i] = i;\n  \t\t\tprintf(\"%d \", in[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\t\tstage_0(in);\n\t\tfree(in);\n\t\tbreak;\n\tcase 1:\n\t\tstage_1();\n\t\tbreak;\n\tcase 2:\n\t\tstage_3();\n\t\tbreak;\n\tcase 3:\n\t\tout = (int *) malloc(sizeof(int)*2);\n\t\tstage_4(out);\n \t\tprintf(\"[%d] Sum: %d\\n\", bc_rank, out[0]);\n\t\tfree(out);\n\t\tbreak;\n\tcase 4:\n\tcase 5:\n\tcase 6:\n\tcase 7:\n\tcase 8:\n\t\tstage_2();\n\t\tbreak;\n\tdefault :\n\t\tbreak;\n\t}\n\tbc_final();\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "yinyanlong_557", "code": "int\nmain(int     argc,\n     char ** argv)\n{\n    int           i;\n    IOR_queue_t * tests;\n\n    \n\n    for (i = 1; i < argc; i++) {\n        if (strcmp(argv[i], \"-h\") == 0) {\n            DisplayUsage(argv);\n            return(0);\n        }\n    }\n\n    \n\n    \n\n    \n\n    \n    \n\n    tests = SetupTests(argc, argv);\n    verbose = tests->testParameters.verbose;\n    tests->testParameters.testComm = MPI_COMM_WORLD;\n    \n    \n\n    if (rank == 0 && tests->testParameters.showHelp == TRUE) {\n      DisplayUsage(argv);\n    }\n    \n    \n\n    while (tests != NULL) {\n      verbose = tests->testParameters.verbose;\n      if (rank == 0 && verbose >= VERBOSE_0) {\n          ShowInfo(argc, argv, &tests->testParameters);\n      }\n      if (rank == 0 && verbose >= VERBOSE_3) {\n        ShowTest(&tests->testParameters);\n      }\n      TestIoSys(&tests->testParameters);\n      tests = tests->nextTest;\n    }\n\n    \n\n    if (rank == 0 && verbose >= VERBOSE_0) {\n      fprintf(stdout, \"Run finished: %s\", CurrentTimeString());\n    }\n\n\n    return(totalErrorCount);\n\n}", "label": "int\nmain(int     argc,\n     char ** argv)\n{\n    int           i;\n    IOR_queue_t * tests;\n\n    \n\n    for (i = 1; i < argc; i++) {\n        if (strcmp(argv[i], \"-h\") == 0) {\n            DisplayUsage(argv);\n            return(0);\n        }\n    }\n\n    \n\n    MPI_CHECK(MPI_Init(&argc, &argv), \"cannot initialize MPI\");\n    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &numTasksWorld),\n              \"cannot get number of tasks\");\n    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank), \"cannot get rank\");\n    \n\n    \n\n    \n    \n\n    tests = SetupTests(argc, argv);\n    verbose = tests->testParameters.verbose;\n    tests->testParameters.testComm = MPI_COMM_WORLD;\n    \n    \n\n    if (rank == 0 && tests->testParameters.showHelp == TRUE) {\n      DisplayUsage(argv);\n    }\n    \n    \n\n    while (tests != NULL) {\n      verbose = tests->testParameters.verbose;\n      if (rank == 0 && verbose >= VERBOSE_0) {\n          ShowInfo(argc, argv, &tests->testParameters);\n      }\n      if (rank == 0 && verbose >= VERBOSE_3) {\n        ShowTest(&tests->testParameters);\n      }\n      TestIoSys(&tests->testParameters);\n      tests = tests->nextTest;\n    }\n\n    \n\n    if (rank == 0 && verbose >= VERBOSE_0) {\n      fprintf(stdout, \"Run finished: %s\", CurrentTimeString());\n    }\n\n    MPI_CHECK(MPI_Finalize(), \"cannot finalize MPI\");\n\n    return(totalErrorCount);\n\n}"}
{"program": "POFK_558", "code": "int main(int argc, char **argv)\n{\n\tvoid run(char *fname, char *fout);\n\tchar Basedir[100] = \"/data/dell5/userdir/maotx/ICR/fiducial_ICR/output\";\n\tchar fname[200], buf[200];\n\tint i;\n\n\tfor(i = 0; i < 1; i++)\n\t{\n\t\tsprintf(fname, \"%s/snapdir_%03d/snapshot_%03d\", Basedir, i, i);\n\t\tsprintf(buf, \"data/snap%03d.bin\", i);\n\t\trun(fname, buf);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &ThisTask);\n\tMPI_Comm_size(MPI_COMM_WORLD, &NTask);\n\tvoid run(char *fname, char *fout);\n\tchar Basedir[100] = \"/data/dell5/userdir/maotx/ICR/fiducial_ICR/output\";\n\tchar fname[200], buf[200];\n\tint i;\n\n\tfor(i = 0; i < 1; i++)\n\t{\n\t\tsprintf(fname, \"%s/snapdir_%03d/snapshot_%03d\", Basedir, i, i);\n\t\tsprintf(buf, \"data/snap%03d.bin\", i);\n\t\trun(fname, buf);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "eddelbuettel_559", "code": "void main( int argc, char **argv ) {\n    PGAContext *ctx;    \n\n    int testnum;        \n\n    int maxiter;        \n\n\n\n    testnum = GetIntegerParameter(\"Which test? (1 - 5)\\n\") - 1;\n    gray_on = GetIntegerParameter(\"Gray-coded? (0 = no)\\n\");\n    maxiter = GetIntegerParameter(\"How many iterations?\\n\");\n\n    ctx = PGACreate(&argc, argv, PGA_DATATYPE_BINARY, \n\t\t    BinLen[testnum]*NumCoords[testnum], PGA_MINIMIZE);\n    \n    PGASetMaxGAIterValue(ctx, maxiter);\n    PGASetRandomSeed(ctx, 1);\n    \n    PGASetUp(ctx);\n\n    if (testnum == 0)    PGARun(ctx, dejong1);\n    if (testnum == 1)    PGARun(ctx, dejong2);\n    if (testnum == 2)    PGARun(ctx, dejong3);\n    if (testnum == 3)    PGARun(ctx, dejong4);\n    if (testnum == 4)    PGARun(ctx, dejong5);\n\n    printResultInterpretation(ctx, testnum);\n\n    PGADestroy(ctx);\n    \n}", "label": "void main( int argc, char **argv ) {\n    PGAContext *ctx;    \n\n    int testnum;        \n\n    int maxiter;        \n\n\n    MPI_Init(&argc, &argv); \n\n    testnum = GetIntegerParameter(\"Which test? (1 - 5)\\n\") - 1;\n    gray_on = GetIntegerParameter(\"Gray-coded? (0 = no)\\n\");\n    maxiter = GetIntegerParameter(\"How many iterations?\\n\");\n\n    ctx = PGACreate(&argc, argv, PGA_DATATYPE_BINARY, \n\t\t    BinLen[testnum]*NumCoords[testnum], PGA_MINIMIZE);\n    \n    PGASetMaxGAIterValue(ctx, maxiter);\n    PGASetRandomSeed(ctx, 1);\n    \n    PGASetUp(ctx);\n\n    if (testnum == 0)    PGARun(ctx, dejong1);\n    if (testnum == 1)    PGARun(ctx, dejong2);\n    if (testnum == 2)    PGARun(ctx, dejong3);\n    if (testnum == 3)    PGARun(ctx, dejong4);\n    if (testnum == 4)    PGARun(ctx, dejong5);\n\n    printResultInterpretation(ctx, testnum);\n\n    PGADestroy(ctx);\n    \n    MPI_Finalize();\n}"}
{"program": "jeremiedecock_560", "code": "int main(int argc, char ** argv)\n{\n\n    int rank;\n\n\n    int buffer[10];\n    int root = 0;\n\n    if(rank == root) {\n        int i;\n        for(i=0 ; i<10 ; i++)\n            buffer[i] = i*2;\n    }\n\n\n    if(rank!=0){\n        int i;\n        for(i=0 ; i<10 ; i++) {\n            printf(\"Process %d: buffer[%d]=%d\\n\", rank, i, buffer[i]);\n        }\n    }\n\n    \n    return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n    MPI_Init(&argc, &argv);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); \n\n\n    int buffer[10];\n    int root = 0;\n\n    if(rank == root) {\n        int i;\n        for(i=0 ; i<10 ; i++)\n            buffer[i] = i*2;\n    }\n\n    MPI_Bcast(buffer, 10, MPI_INT, root, MPI_COMM_WORLD);\n\n    if(rank!=0){\n        int i;\n        for(i=0 ; i<10 ; i++) {\n            printf(\"Process %d: buffer[%d]=%d\\n\", rank, i, buffer[i]);\n        }\n    }\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "hariseldon99_561", "code": "int\nmain (int argc, char **argv)\n{\n  double local_time,global_time;\n  double runtime_avg;\n  drive_and_tower param;\n  int i, j, n, m, argv_count = 1;\n  int pid, nprocs;\n  double cosmatrix;\n  double y[2*DIMS], y_real[DIMS], y_imag[DIMS];\n  double period;\n  double kappa, lambda, omega;\n  int parameter_begin, parameter_end, probsize,dim;\n  FILE *benchmark;\n  benchmark = fopen (argv[argv_count++], \"w\");\n\n  \n  local_time = -gtod_timer();\n\n parameter_begin = atof (argv[argv_count++]);\n parameter_end = atof (argv[argv_count++]);\n kappa=atof (argv[argv_count++]);\n lambda = atof (argv[argv_count++]);\n omega = atof (argv[argv_count++]);\n\n  period = 2.0 * M_PI / omega;\n  param.kappa = kappa; \n  param.lambda = lambda;\n  param.omega = omega;\n\n  \n\n  for (n = 0; n < DIMS; n++)\n    {\n      param.psq[n] = psq_mat (n);\n      for (m = 0; m <=n; m++)\n\t{\n\t  cosmatrix = c_mat (n, m);\n\t  param.cosmatrix[n + DIMS * m] = cosmatrix;\n\t  param.cosmatrix[m + DIMS * n] = cosmatrix;\n\t}\n    }\n    local_time+=gtod_timer();\n\n  \n\n  for (probsize = parameter_begin; probsize < parameter_end;probsize=probsize+5)\n    {\n\t    local_time = -gtod_timer();\n\t    dim=2*probsize+1;\n\t    param.dim=dim;\n\t    if(pid==0) printf (\"\\nintegrating with problem size =  %d, dimensionality= %d\\n\", probsize,dim);\n    \n\t    j = pid;\n\t\n\n\t\n\n\tfor (i = 0; i < 2 * dim; i++)\n\t  {\n\t    if (i < dim)\n\t      {\n\t\ty[i] = kdel (i, j);\n\t      }\n\t    else\n\t      {\n\t\ty[i] = 0.0;\n\t      }\n\t  }\n\n\t\n\n\tintegrate (y, 0.0, period, &param);\n\t \n\n\n\t  \n\n\t  for (i = 0; i < dim; i++)\n\t  {\n\t    y_real[i] = y[i];\n\t    y_imag[i] = y[i + dim];\n\t  }\n\t  local_time+=gtod_timer();\n\t  if(pid==0){\n\t\t  runtime_avg=global_time/nprocs;\n\t \t  fprintf(benchmark,\"\\n %d %lf\",probsize,runtime_avg);\n\t  \t}\n\tif(pid==0) printf (\"\\nELAPSED TIME = %lf\\n\", local_time);\n    }\n  \n\n  fclose (benchmark);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  double local_time,global_time;\n  double runtime_avg;\n  drive_and_tower param;\n  int i, j, n, m, argv_count = 1;\n  int pid, nprocs;\n  double cosmatrix;\n  double y[2*DIMS], y_real[DIMS], y_imag[DIMS];\n  double period;\n  double kappa, lambda, omega;\n  int parameter_begin, parameter_end, probsize,dim;\n  FILE *benchmark;\n  benchmark = fopen (argv[argv_count++], \"w\");\n\n  MPI_Init (&argc, &argv);\n  MPI_Barrier (MPI_COMM_WORLD);\n  MPI_Comm_rank (MPI_COMM_WORLD, &pid);\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  \n  local_time = -gtod_timer();\n\n parameter_begin = atof (argv[argv_count++]);\n parameter_end = atof (argv[argv_count++]);\n kappa=atof (argv[argv_count++]);\n lambda = atof (argv[argv_count++]);\n omega = atof (argv[argv_count++]);\n\n  period = 2.0 * M_PI / omega;\n  param.kappa = kappa; \n  param.lambda = lambda;\n  param.omega = omega;\n\n  \n\n  for (n = 0; n < DIMS; n++)\n    {\n      param.psq[n] = psq_mat (n);\n      for (m = 0; m <=n; m++)\n\t{\n\t  cosmatrix = c_mat (n, m);\n\t  param.cosmatrix[n + DIMS * m] = cosmatrix;\n\t  param.cosmatrix[m + DIMS * n] = cosmatrix;\n\t}\n    }\n    local_time+=gtod_timer();\n\n  \n\n  for (probsize = parameter_begin; probsize < parameter_end;probsize=probsize+5)\n    {\n\t    local_time = -gtod_timer();\n\t    dim=2*probsize+1;\n\t    param.dim=dim;\n\t    if(pid==0) printf (\"\\nintegrating with problem size =  %d, dimensionality= %d\\n\", probsize,dim);\n    \n\t    j = pid;\n\t\n\n\t\n\n\tfor (i = 0; i < 2 * dim; i++)\n\t  {\n\t    if (i < dim)\n\t      {\n\t\ty[i] = kdel (i, j);\n\t      }\n\t    else\n\t      {\n\t\ty[i] = 0.0;\n\t      }\n\t  }\n\n\t\n\n\tintegrate (y, 0.0, period, &param);\n\t \n\n\n\t  \n\n\t  for (i = 0; i < dim; i++)\n\t  {\n\t    y_real[i] = y[i];\n\t    y_imag[i] = y[i + dim];\n\t  }\n\t  local_time+=gtod_timer();\n\t  MPI_Reduce (&local_time,&global_time,1, MPI_DOUBLE, MPI_SUM,0,MPI_COMM_WORLD);\n\t  if(pid==0){\n\t\t  runtime_avg=global_time/nprocs;\n\t \t  fprintf(benchmark,\"\\n %d %lf\",probsize,runtime_avg);\n\t  \t}\n\tif(pid==0) printf (\"\\nELAPSED TIME = %lf\\n\", local_time);\n    }\n  \n\n  MPI_Finalize(); \n  fclose (benchmark);\n\n  return 0;\n}"}
{"program": "gnu3ra_562", "code": "int main(int argc, char ** argv) {\n  int rank, nproc, i, j;\n  armcix_mutex_hdl_t mhdl;\n  ARMCI_Group world_group;\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCIX mutex test with %d processes\\n\", nproc);\n\n  ARMCI_Group_get_world(&world_group);\n  mhdl = ARMCIX_Create_mutexes_hdl(NUM_MUTEXES, &world_group);\n\n  for (i = 0; i < nproc; i++)\n    for (j = 0; j < NUM_MUTEXES; j++) {\n      while (ARMCIX_Trylock_hdl(mhdl, j, (rank+i)%nproc))\n        ;\n      ARMCIX_Unlock_hdl(mhdl, j, (rank+i)%nproc);\n    }\n\n  printf(\" + %3d done\\n\", rank);\n  fflush(NULL);\n\n  ARMCIX_Destroy_mutexes_hdl(mhdl);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int rank, nproc, i, j;\n  armcix_mutex_hdl_t mhdl;\n  ARMCI_Group world_group;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCIX mutex test with %d processes\\n\", nproc);\n\n  ARMCI_Group_get_world(&world_group);\n  mhdl = ARMCIX_Create_mutexes_hdl(NUM_MUTEXES, &world_group);\n\n  for (i = 0; i < nproc; i++)\n    for (j = 0; j < NUM_MUTEXES; j++) {\n      while (ARMCIX_Trylock_hdl(mhdl, j, (rank+i)%nproc))\n        ;\n      ARMCIX_Unlock_hdl(mhdl, j, (rank+i)%nproc);\n    }\n\n  printf(\" + %3d done\\n\", rank);\n  fflush(NULL);\n\n  ARMCIX_Destroy_mutexes_hdl(mhdl);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "TUM-I5_563", "code": "int main (int argc, char** argv)\n{\n\t\n\tasagi_grid* grid = asagi_grid_create(ASAGI_FLOAT);\n\t\n\tif (asagi_grid_open(grid, \"tests/2dgrid.nc\", 0) != ASAGI_SUCCESS) {\n\t\tprintf(\"Could not load file\\n\");\n\t\treturn 1;\n\t}\n\t\n\tprintf(\"Range X: %f-%f\\n\", asagi_grid_min(grid, 0), asagi_grid_max(grid, 0));\n\tprintf(\"Range Y: %f-%f\\n\", asagi_grid_min(grid, 1), asagi_grid_max(grid, 1));\n\t\n\tdouble pos[] = {5, 10};\n\tprintf(\"Value at 5x10: %f\\n\", asagi_grid_get_float(grid, pos, 0));\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\tasagi_grid_close(grid);\n\t\n\t\n\treturn 0;\n}", "label": "int main (int argc, char** argv)\n{\n\tMPI_Init(&argc, &argv);\n\t\n\tasagi_grid* grid = asagi_grid_create(ASAGI_FLOAT);\n\t\n\tif (asagi_grid_open(grid, \"tests/2dgrid.nc\", 0) != ASAGI_SUCCESS) {\n\t\tprintf(\"Could not load file\\n\");\n\t\treturn 1;\n\t}\n\t\n\tprintf(\"Range X: %f-%f\\n\", asagi_grid_min(grid, 0), asagi_grid_max(grid, 0));\n\tprintf(\"Range Y: %f-%f\\n\", asagi_grid_min(grid, 1), asagi_grid_max(grid, 1));\n\t\n\tdouble pos[] = {5, 10};\n\tprintf(\"Value at 5x10: %f\\n\", asagi_grid_get_float(grid, pos, 0));\n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\tasagi_grid_close(grid);\n\t\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "gnu3ra_565", "code": "int main( int argc, char *argv[] )\n{\n    int wrank, wsize, rank, size, color;\n    int j, tmp;\n    MPI_Comm newcomm;\n\n\n\n    \n\n    color = (wrank > 0) && (wrank <= wsize/2);\n\n    if (color) {\n\t\n\n\texit(1);\n    }\n    \n    \n\n\n    for (j=0; j<rank; j++) {\n    }\n    for (j=rank+1; j<size; j++) {\n    }\n\n\n    printf( \" No Errors\\n\" );\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int wrank, wsize, rank, size, color;\n    int j, tmp;\n    MPI_Comm newcomm;\n\n    MPI_Init( &argc, &argv );\n\n    MPI_Comm_size( MPI_COMM_WORLD, &wsize );\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n\n    \n\n    color = (wrank > 0) && (wrank <= wsize/2);\n    MPI_Comm_split( MPI_COMM_WORLD, color, wrank, &newcomm );\n\n    MPI_Barrier( MPI_COMM_WORLD );\n    if (color) {\n\t\n\n\texit(1);\n    }\n    \n    \n\n    MPI_Comm_size( newcomm, &size );\n    MPI_Comm_rank( newcomm, &rank );\n\n    for (j=0; j<rank; j++) {\n\tMPI_Recv( &tmp, 1, MPI_INT, j, 0, newcomm, MPI_STATUS_IGNORE );\n    }\n    for (j=rank+1; j<size; j++) {\n\tMPI_Send( &rank, 1, MPI_INT, j, 0, newcomm );\n    }\n\n    MPI_Comm_free( &newcomm );\n    MPI_Finalize();\n\n    printf( \" No Errors\\n\" );\n\n    return 0;\n}"}
{"program": "bjoern-leder_566", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,irw,nm,ie,iea,p,nrw,nmx;\n   double kappa0,kappa,kappa2,mu,mu0,gamma;\n   double d,dmx;\n   FILE *flog=NULL,*fin=NULL;\n   mrw_parms_t rw;\n   \n   \n   if (my_rank==0)\n   {\n      flog=freopen(\"check11.log\",\"w\",stdout);\n      fin=freopen(\"check11.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Check interpolations\\n\");\n      printf(\"----------------------\\n\\n\");\n   }\n   \n   if (my_rank==0)\n   {\n      find_section(\"Configurations\");\n      read_line(\"nrw\",\"%d\",&nrw);\n   }\n\n\n   for (irw=0;irw<nrw;irw++)\n      read_mrw_parms(irw);\n      \n   if (my_rank==0)\n      fclose(fin);\n\n   print_mrw_parms();\n\n   dmx=0.0;\n   nmx=0;\n   iea=0;\n\n   for (irw=0;irw<nrw;irw++)\n   {\n      rw=mrw_parms(irw);\n      \n      if (my_rank==0)\n      {            \n         printf(\"Reweighting factor %d, %s\",irw,mrwfacts[rw.mrwfact]);\n      }   \n\n      d=check_mrw_inter(irw,&ie);\n      \n      if (d>dmx)\n         dmx=d;\n      \n      if (rw.nm>nmx)\n         nmx=rw.nm;\n      \n      iea+=ie;\n\n      if (my_rank==0)\n         printf(\"\\n\");\n   }\n\n   if (my_rank==0)\n   {\n      printf(\"\\n\");\n      if (iea)\n         printf(\"%d errors found\\n\",iea);\n      else\n         printf(\"all ok\\n\");\n      printf(\"max diff = %.1e\\n\",dmx);\n      printf(\"(should be smaller than %.1e)\\n\\n\",sqrt((double)(4*(4*5+2)*nmx))*DBL_EPSILON);\n   }\n   \n   if (my_rank==0)\n      printf(\"Generic test of all types and powers:\\n\");\n\n   kappa0=0.1315;\n   kappa=0.131;\n   mu0=0.05;\n   mu=0.0;\n   gamma=0.8;\n   kappa2=0.1316;\n   nm=24;\n   \n   dmx=0.0;\n   iea=0;\n  \n   for (irw=0;irw<MRWFACTS;irw++)\n   {\n      if (my_rank==0)\n      {            \n         printf(\"%s, p: \",mrwfacts[irw]);\n      }   \n      \n      for (p=0;p<=4;p++)\n      {\n         if (my_rank==0)\n         {            \n            printf(\"%d \",p);\n         }   \n\n         init_mrw();\n         set_mrw_parms(irw,mrwfact[irw],kappa0,kappa,mu0,mu,gamma,kappa2,0,0,nm,p,1,0);\n         \n         d=check_mrw_inter(irw,&ie);\n         \n         if (d>dmx)\n            dmx=d;\n         \n         iea+=ie;\n      }\n\n      if (my_rank==0)\n         printf(\"\\n\");\n   }\n   \n   if (my_rank==0)\n   {\n      printf(\"\\n\");\n      if (iea)\n         printf(\"%d errors found\\n\",iea);\n      else\n         printf(\"all ok\\n\");\n      printf(\"max diff = %.1e\\n\",dmx);\n      printf(\"(should be smaller than %.1e)\\n\\n\",sqrt((double)(4*(4*5+2)*nm))*DBL_EPSILON);\n      fclose(flog);\n   }\n   \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,irw,nm,ie,iea,p,nrw,nmx;\n   double kappa0,kappa,kappa2,mu,mu0,gamma;\n   double d,dmx;\n   FILE *flog=NULL,*fin=NULL;\n   mrw_parms_t rw;\n   \n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n   \n   if (my_rank==0)\n   {\n      flog=freopen(\"check11.log\",\"w\",stdout);\n      fin=freopen(\"check11.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Check interpolations\\n\");\n      printf(\"----------------------\\n\\n\");\n   }\n   \n   if (my_rank==0)\n   {\n      find_section(\"Configurations\");\n      read_line(\"nrw\",\"%d\",&nrw);\n   }\n\n   MPI_Bcast(&nrw,1,MPI_INT,0,MPI_COMM_WORLD);   \n\n   for (irw=0;irw<nrw;irw++)\n      read_mrw_parms(irw);\n      \n   if (my_rank==0)\n      fclose(fin);\n\n   print_mrw_parms();\n\n   dmx=0.0;\n   nmx=0;\n   iea=0;\n\n   for (irw=0;irw<nrw;irw++)\n   {\n      rw=mrw_parms(irw);\n      \n      if (my_rank==0)\n      {            \n         printf(\"Reweighting factor %d, %s\",irw,mrwfacts[rw.mrwfact]);\n      }   \n\n      d=check_mrw_inter(irw,&ie);\n      \n      if (d>dmx)\n         dmx=d;\n      \n      if (rw.nm>nmx)\n         nmx=rw.nm;\n      \n      iea+=ie;\n\n      if (my_rank==0)\n         printf(\"\\n\");\n   }\n\n   if (my_rank==0)\n   {\n      printf(\"\\n\");\n      if (iea)\n         printf(\"%d errors found\\n\",iea);\n      else\n         printf(\"all ok\\n\");\n      printf(\"max diff = %.1e\\n\",dmx);\n      printf(\"(should be smaller than %.1e)\\n\\n\",sqrt((double)(4*(4*5+2)*nmx))*DBL_EPSILON);\n   }\n   \n   if (my_rank==0)\n      printf(\"Generic test of all types and powers:\\n\");\n\n   kappa0=0.1315;\n   kappa=0.131;\n   mu0=0.05;\n   mu=0.0;\n   gamma=0.8;\n   kappa2=0.1316;\n   nm=24;\n   \n   dmx=0.0;\n   iea=0;\n  \n   for (irw=0;irw<MRWFACTS;irw++)\n   {\n      if (my_rank==0)\n      {            \n         printf(\"%s, p: \",mrwfacts[irw]);\n      }   \n      \n      for (p=0;p<=4;p++)\n      {\n         if (my_rank==0)\n         {            \n            printf(\"%d \",p);\n         }   \n\n         init_mrw();\n         set_mrw_parms(irw,mrwfact[irw],kappa0,kappa,mu0,mu,gamma,kappa2,0,0,nm,p,1,0);\n         \n         d=check_mrw_inter(irw,&ie);\n         \n         if (d>dmx)\n            dmx=d;\n         \n         iea+=ie;\n      }\n\n      if (my_rank==0)\n         printf(\"\\n\");\n   }\n   \n   if (my_rank==0)\n   {\n      printf(\"\\n\");\n      if (iea)\n         printf(\"%d errors found\\n\",iea);\n      else\n         printf(\"all ok\\n\");\n      printf(\"max diff = %.1e\\n\",dmx);\n      printf(\"(should be smaller than %.1e)\\n\\n\",sqrt((double)(4*(4*5+2)*nm))*DBL_EPSILON);\n      fclose(flog);\n   }\n   \n   MPI_Finalize();    \n   exit(0);\n}"}
{"program": "seckin206_567", "code": "int main(int argc, char *argv[])\n{\n\n\tdouble time;\n\n\tif (me == 0)\n\t{\n\t\tprintf(\"MPI version\\n\");\n\t\ttime =\n\t}\n\n\n\tdefault_wolf_breeding_period = atoi(argv[2]);\n\tdefault_squirrel_breeding_period = atoi(argv[3]);\n\tdefault_wolf_starvation_period = atoi(argv[4]);\n\n\tint gen_total;\n\tgen_total = atoi(argv[5]);\n\n\tread_and_distribute_world(argv[1]);\n\n\tsync_world();\n\t\n\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\tint curr_gen, i, j;\n\tfor (curr_gen = 1; curr_gen <= gen_total; curr_gen++)\n\t\tsimulate_world();\n\t\n\n\n\t\n\n\tassemble_and_output_world(argv[6]);\n\t\n\n\n\tif (me == 0)\n\t{\n\t\ttime +=\n\t\tFILE *fp = fopen(argv[6], \"a\");\n\t\tfprintf(fp, \"%f\\n\", time);\n\t\tprintf(\"%f\\n\", time);\n\t\tfclose(fp);\n\t}\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tMPI_Init(&argc, &argv);\n\n\tdouble time;\n\n\tif (me == 0)\n\t{\n\t\tprintf(\"MPI version\\n\");\n\t\ttime = -MPI_Wtime();\n\t}\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &me);\n\n\tdefault_wolf_breeding_period = atoi(argv[2]);\n\tdefault_squirrel_breeding_period = atoi(argv[3]);\n\tdefault_wolf_starvation_period = atoi(argv[4]);\n\n\tint gen_total;\n\tgen_total = atoi(argv[5]);\n\n\tread_and_distribute_world(argv[1]);\n\n\tsync_world();\n\t\n\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\tint curr_gen, i, j;\n\tfor (curr_gen = 1; curr_gen <= gen_total; curr_gen++)\n\t\tsimulate_world();\n\t\n\n\n\t\n\n\tassemble_and_output_world(argv[6]);\n\t\n\n\n\tif (me == 0)\n\t{\n\t\ttime += MPI_Wtime();\n\t\tFILE *fp = fopen(argv[6], \"a\");\n\t\tfprintf(fp, \"%f\\n\", time);\n\t\tprintf(\"%f\\n\", time);\n\t\tfclose(fp);\n\t}\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "aksiazek_568", "code": "int main(int argc, char** argv) {\n    char buffer[BUFFSIZE];\n    int world_rank;\n    int world_size;\n\n    int numOfCom=200;\n    int maxMsgSize=262144;\n    int mode=SYNCHRONISED_MODE;\n\n    double start=0;\n    double end=0;\n\n    int i=0,j=0;\n\n    if ( argc != 4 )\n    {\n        printf( \"usage: %s times_to_send max_message_size communication_mode\\nUsing defaults: %d %d %d\\n\", argv[0],numOfCom, maxMsgSize, mode );\n    } else {\n        numOfCom = atoi(argv[1]);\n        maxMsgSize = atoi(argv[2]);\n        mode = atoi(argv[3]);\n    }\n\n    char msgBuf[maxMsgSize];\n\n    int buffsize;\n    if(mode==BUFFERED_MODE) {\n        buffsize = BUFFSIZE;\n    }\n\n    if (world_size < 2) {\n        fprintf(stderr, \"World size must be greater than 1 for %s\\n\", argv[0]);\n    }\n    for(i=0; i<maxMsgSize; i++) {\n        msgBuf[i]=i;\n    }\n    if (world_rank == 0) {\n        for(j=1; j<maxMsgSize; j*=2) {\n            for(i=0; i<numOfCom; i++) {\n                if(mode==BUFFERED_MODE) {\n                } else {\n                }\n            }\n        }\n    } else if (world_rank == 1) {\n        for(j=1; j<maxMsgSize; j*=2) {\n            start =\n            for(i=0; i<numOfCom; i++) {\n            }\n            end =\n            double time = end-start;\n            printf(\"%f\\n\", (j*8*numOfCom)/ time / BITS_IN_MBIT);\n        }\n    }\n\n    if(mode==BUFFERED_MODE) {\n    }\n\n}", "label": "int main(int argc, char** argv) {\n    MPI_Init(&argc, &argv);\n    char buffer[BUFFSIZE];\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int numOfCom=200;\n    int maxMsgSize=262144;\n    int mode=SYNCHRONISED_MODE;\n\n    double start=0;\n    double end=0;\n\n    int i=0,j=0;\n\n    if ( argc != 4 )\n    {\n        printf( \"usage: %s times_to_send max_message_size communication_mode\\nUsing defaults: %d %d %d\\n\", argv[0],numOfCom, maxMsgSize, mode );\n    } else {\n        numOfCom = atoi(argv[1]);\n        maxMsgSize = atoi(argv[2]);\n        mode = atoi(argv[3]);\n    }\n\n    char msgBuf[maxMsgSize];\n\n    int buffsize;\n    if(mode==BUFFERED_MODE) {\n        buffsize = BUFFSIZE;\n        MPI_Buffer_attach(buffer,BUFFSIZE);\n    }\n\n    if (world_size < 2) {\n        fprintf(stderr, \"World size must be greater than 1 for %s\\n\", argv[0]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    for(i=0; i<maxMsgSize; i++) {\n        msgBuf[i]=i;\n    }\n    if (world_rank == 0) {\n        for(j=1; j<maxMsgSize; j*=2) {\n            MPI_Barrier(MPI_COMM_WORLD);\n            for(i=0; i<numOfCom; i++) {\n                if(mode==BUFFERED_MODE) {\n                    MPI_Bsend(msgBuf, j, MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n                } else {\n                    MPI_Ssend(msgBuf, j, MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n                }\n            }\n        }\n    } else if (world_rank == 1) {\n        for(j=1; j<maxMsgSize; j*=2) {\n            MPI_Barrier(MPI_COMM_WORLD);\n            start = MPI_Wtime();\n            for(i=0; i<numOfCom; i++) {\n                MPI_Recv(msgBuf, j, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            end = MPI_Wtime();\n            double time = end-start;\n            printf(\"%f\\n\", (j*8*numOfCom)/ time / BITS_IN_MBIT);\n        }\n    }\n\n    if(mode==BUFFERED_MODE) {\n        MPI_Buffer_detach(buffer, &buffsize);\n    }\n\n    MPI_Finalize();\n}"}
{"program": "ghisvail_570", "code": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  pfft_init();\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 8;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_2d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= 3;\n\n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, data, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one gcell exchange and reduce of size n=(%td, %td, %td),\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"gc_below = (%td, %td, %td), gc_above = (%td, %td, %td):\\n\", gc_below[0], gc_below[1], gc_below[2], gc_above[0], gc_above[1], gc_above[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  pfft_free(data);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  MPI_Init(&argc, &argv);\n  pfft_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &rnk_self);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 8;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_2d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= 3;\n\n  \n\n  MPI_Barrier(comm_cart_2d);\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, data, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one gcell exchange and reduce of size n=(%td, %td, %td),\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"gc_below = (%td, %td, %td), gc_above = (%td, %td, %td):\\n\", gc_below[0], gc_below[1], gc_below[2], gc_above[0], gc_above[1], gc_above[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(data);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "cotdp_572", "code": "int main(int argc, char *argv[]) {\n   char idstr[32], buff[128];\n   int numprocs, rank, namelen, i;\n   char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n   MPI_Status stat;\n\n   \n\n   if (rank == 0) {\n      \n\n      printf(\"We have %d processors\\n\", numprocs);\n      \n\n      for(i = 1; i < numprocs; i++) {\n         sprintf(buff, \"Hello %d... \", i);\n      }\n      \n\n      for(i = 1; i < numprocs; i++) {\n         printf(\"%s\\n\", buff);\n      }\n   } else {\n      \n\n      \n\n      sprintf(idstr, \"Processor %d \", rank);\n      strcat(buff, idstr);\n      strcat(buff, \"reporting!\");\n      \n\n   }\n\n\n   return 0;\n}", "label": "int main(int argc, char *argv[]) {\n   char idstr[32], buff[128];\n   int numprocs, rank, namelen, i;\n   char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n   MPI_Status stat;\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Get_processor_name(processor_name, &namelen);\n\n   \n\n   if (rank == 0) {\n      \n\n      printf(\"We have %d processors\\n\", numprocs);\n      \n\n      for(i = 1; i < numprocs; i++) {\n         sprintf(buff, \"Hello %d... \", i);\n         MPI_Send(buff, 128, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n      }\n      \n\n      for(i = 1; i < numprocs; i++) {\n         MPI_Recv(buff, 128, MPI_CHAR, i, 0, MPI_COMM_WORLD, &stat);\n         printf(\"%s\\n\", buff);\n      }\n   } else {\n      \n\n      MPI_Recv(buff, 128, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &stat);\n      \n\n      sprintf(idstr, \"Processor %d \", rank);\n      strcat(buff, idstr);\n      strcat(buff, \"reporting!\");\n      \n\n      MPI_Send(buff, 128, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n   }\n\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "markfasheh_573", "code": "int main(int argc, char *argv[])\n{\n\tint ret;\n\n\tret =\n\tif (ret != MPI_SUCCESS) {\n\t\tfprintf(stderr, \"MPI_Init failed: %d\\n\", ret);\n\t\texit(1);\n\t}\n\n\tprog = strrchr(argv[0], '/');\n\tif (prog == NULL)\n\t\tprog = argv[0];\n\telse\n\t\tprog++;\n\n\tif (parse_opts(argc, argv))\n\t\tusage();\n\n\tif (gethostname(hostname, HOSTNAME_SIZE) < 0) {\n\t\tfprintf(stderr, \"gethostname failed: %s\\n\",\n\t\t\tstrerror(errno));\n\t\texit(1);\n\t}\n\n\tret =\n        if (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_rank failed: %d\\n\", ret);\n\n\tret =\n\tif (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_size failed: %d\\n\", ret);\n\n        printf(\"%s: rank: %d, procs: %d, path \\\"%s\\\"\\n\",\n\t       hostname, rank, num_procs, path);\n\n\trun_test();\n\n        return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint ret;\n\n\tret = MPI_Init(&argc, &argv);\n\tif (ret != MPI_SUCCESS) {\n\t\tfprintf(stderr, \"MPI_Init failed: %d\\n\", ret);\n\t\texit(1);\n\t}\n\n\tprog = strrchr(argv[0], '/');\n\tif (prog == NULL)\n\t\tprog = argv[0];\n\telse\n\t\tprog++;\n\n\tif (parse_opts(argc, argv))\n\t\tusage();\n\n\tif (gethostname(hostname, HOSTNAME_SIZE) < 0) {\n\t\tfprintf(stderr, \"gethostname failed: %s\\n\",\n\t\t\tstrerror(errno));\n\t\texit(1);\n\t}\n\n\tret = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        if (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_rank failed: %d\\n\", ret);\n\n\tret = MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\tif (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_size failed: %d\\n\", ret);\n\n        printf(\"%s: rank: %d, procs: %d, path \\\"%s\\\"\\n\",\n\t       hostname, rank, num_procs, path);\n\n\trun_test();\n\n        MPI_Finalize();\n        return 0;\n}"}
{"program": "tcsiwula_574", "code": "int main(int argc, char* argv[])\n{\n   int p, my_rank;\n   MPI_Comm comm;\n   int result, in_val;\n\n   comm = MPI_COMM_WORLD;\n\n   if (my_rank == 0)\n   {\n      printf(\"Enter an int\\n\");\n      scanf(\"%d\", &in_val);\n   }\n   result = Bcast(in_val, my_rank, p, comm);\n\n   printf(\"Proc %d > result = %d\\n\", my_rank, result); \n\n   return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n   int p, my_rank;\n   MPI_Comm comm;\n   int result, in_val;\n\n   MPI_Init(&argc, &argv);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &p);\n   MPI_Comm_rank(comm, &my_rank);\n\n   if (my_rank == 0)\n   {\n      printf(\"Enter an int\\n\");\n      scanf(\"%d\", &in_val);\n   }\n   MPI_Barrier(comm);\n   result = Bcast(in_val, my_rank, p, comm);\n\n   printf(\"Proc %d > result = %d\\n\", my_rank, result); \n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "thiagowfx_575", "code": "int main(int argc, char *argv[]) {\n\n\tdouble starttime, endtime;\n\tstarttime =\n\n\t\n\n\tint rank;\n\n\t\n\n\tint p;\n\n\n\t\n\n\tint *v1 = NULL, *v2 = NULL;\n\n\t\n\n\tint *w1 = NULL, *w2 = NULL;\n\n\t\n\n\tint *sendcounts = NULL, *displs = NULL;\n\n\t\n\n\tint s = 0;\n\n\t\n\n\tconst int n = N / p;\n\n\t\n\n\tconst int n0 = n + N % p;\n\n\t\n\n\tif (rank == 0) {\n\t\tv1 = (int*) malloc(N * sizeof(int));\n\t\tv2 = (int*) malloc(N * sizeof(int));\n\n\t\tsrand(time(NULL));\n\n\t\t\n\n\t\tfor(int i = 0; i < N; ++i) {\n\t\t\tv1[i] = get_random_int(0, MAX_RANDOM_NUMBER);\n\t\t\tv2[i] = get_random_int(0, MAX_RANDOM_NUMBER);\n\t\t}\n\n\t\tDEBUGPP(0, N, v1, rank)\n\t\tDEBUGPP(0, N, v2, rank)\n\n\t\tw1 = (int*) malloc(n0 * sizeof(int));\n\t\tw2 = (int*) malloc(n0 * sizeof(int));\n\t}\n\telse {\n\t\tw1 = (int*) malloc(n * sizeof(int));\n\t\tw2 = (int*) malloc(n * sizeof(int));\n\t}\n\n\t\n\n\tsendcounts = (int*) malloc(p * sizeof(int));\n\tdispls = (int*) malloc(p * sizeof(int));\n\n\t\n\n\tdispls[0] = 0;\n\tsendcounts[0] = n0;\n\tfor(int i = 1; i < p; ++i) {\n\t\tsendcounts[i] = n;\n\n\t\t\n\n\t\tdispls[i] = displs[i - 1] + sendcounts[i - 1];\n\t}\n\n\n\tDEBUGPP(0, sendcounts[rank], w1, rank)\n\tDEBUGPP(0, sendcounts[rank], w2, rank)\n\n\t\n\n\tfor(int i = 0; i < (!rank ? n0 : n); ++i) {\n\t\ts += w1[i] * w2[i];\n\t}\n\n\tDEBUGS(s, rank)\n\n\t\n\n\tendtime =\n\n\t\n\n#ifdef DEBUG\n\tprintf(\"Process #%d: the scalar product is %d\\n\", rank, s);\n#else\n\tif (rank == 0) {\n\t\tprintf(\"Process #%d: the scalar product is %d\\n\", rank, s);\n\t}\n#endif\n\n\t\n\n\t\tprintf(\"Process #%d: total time: %lf seconds\\n\", rank, endtime - starttime);\n\t\n\n\n\t\n\n\tif(v1)\n\t\tfree(v1);\n\n\tif(v2)\n\t\tfree(v2);\n\n\tif(w1)\n\t\tfree(w1);\n\n\tif(w2)\n\t\tfree(w2);\n\n\tif(sendcounts)\n\t\tfree(sendcounts);\n\n\tif(displs)\n\t\tfree(displs);\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tMPI_Init(&argc, &argv);\n\n\tdouble starttime, endtime;\n\tstarttime = MPI_Wtime();\n\n\t\n\n\tint rank;\n\n\t\n\n\tint p;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\n\t\n\n\tint *v1 = NULL, *v2 = NULL;\n\n\t\n\n\tint *w1 = NULL, *w2 = NULL;\n\n\t\n\n\tint *sendcounts = NULL, *displs = NULL;\n\n\t\n\n\tint s = 0;\n\n\t\n\n\tconst int n = N / p;\n\n\t\n\n\tconst int n0 = n + N % p;\n\n\t\n\n\tif (rank == 0) {\n\t\tv1 = (int*) malloc(N * sizeof(int));\n\t\tv2 = (int*) malloc(N * sizeof(int));\n\n\t\tsrand(time(NULL));\n\n\t\t\n\n\t\tfor(int i = 0; i < N; ++i) {\n\t\t\tv1[i] = get_random_int(0, MAX_RANDOM_NUMBER);\n\t\t\tv2[i] = get_random_int(0, MAX_RANDOM_NUMBER);\n\t\t}\n\n\t\tDEBUGPP(0, N, v1, rank)\n\t\tDEBUGPP(0, N, v2, rank)\n\n\t\tw1 = (int*) malloc(n0 * sizeof(int));\n\t\tw2 = (int*) malloc(n0 * sizeof(int));\n\t}\n\telse {\n\t\tw1 = (int*) malloc(n * sizeof(int));\n\t\tw2 = (int*) malloc(n * sizeof(int));\n\t}\n\n\t\n\n\tsendcounts = (int*) malloc(p * sizeof(int));\n\tdispls = (int*) malloc(p * sizeof(int));\n\n\t\n\n\tdispls[0] = 0;\n\tsendcounts[0] = n0;\n\tfor(int i = 1; i < p; ++i) {\n\t\tsendcounts[i] = n;\n\n\t\t\n\n\t\tdispls[i] = displs[i - 1] + sendcounts[i - 1];\n\t}\n\n\tMPI_Scatterv(v1, sendcounts, displs, MPI_INT, w1, sendcounts[rank], MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Scatterv(v2, sendcounts, displs, MPI_INT, w2, sendcounts[rank], MPI_INT, 0, MPI_COMM_WORLD);\n\n\tDEBUGPP(0, sendcounts[rank], w1, rank)\n\tDEBUGPP(0, sendcounts[rank], w2, rank)\n\n\t\n\n\tfor(int i = 0; i < (!rank ? n0 : n); ++i) {\n\t\ts += w1[i] * w2[i];\n\t}\n\n\tDEBUGS(s, rank)\n\n\t\n\n\tMPI_Allreduce(&s, &s, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tendtime = MPI_Wtime();\n\n\t\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n#ifdef DEBUG\n\tprintf(\"Process #%d: the scalar product is %d\\n\", rank, s);\n#else\n\tif (rank == 0) {\n\t\tprintf(\"Process #%d: the scalar product is %d\\n\", rank, s);\n\t}\n#endif\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\t\n\n\t\tprintf(\"Process #%d: total time: %lf seconds\\n\", rank, endtime - starttime);\n\t\n\n\n\t\n\n\tif(v1)\n\t\tfree(v1);\n\n\tif(v2)\n\t\tfree(v2);\n\n\tif(w1)\n\t\tfree(w1);\n\n\tif(w2)\n\t\tfree(w2);\n\n\tif(sendcounts)\n\t\tfree(sendcounts);\n\n\tif(displs)\n\t\tfree(displs);\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "centaure_576", "code": "int main(int argc, char *argv[]){\r\n\tchar buff[256];\r\n\tint c;\r\n\tint index, error;\r\n\tunsigned int max_threads = get_max_threads();\r\n\tchar *arg = NULL;\r\n\tint mpi_size = 1, mpi_rank = 0;\r\n\tlua_State *L;\r\n\t\r\n#ifdef HAVE_MPI\r\n#endif\r\n\t\r\n\topterr = 0;\r\n\twhile((c = getopt(argc, argv, \"a:ht:v\")) != -1){\r\n\t\tswitch(c){\r\n\t\tcase 'a':\r\n\t\t\targ = strdup(optarg);\r\n\t\t\tbreak;\r\n\t\tcase 'h':\r\n\t\t\tusage();\r\n\t\t\treturn EXIT_SUCCESS;\r\n\t\tcase 't':\r\n\t\t\tmax_threads = atoi(optarg);\r\n\t\t\tbreak;\r\n\t\tcase 'v':\r\n\t\t\tversion();\r\n\t\t\treturn EXIT_SUCCESS;\r\n\t\tcase '?':\r\n\t\t\tif('t' == optopt){\r\n\t\t\t\tfprintf(stderr, \"Option -%c requires an argument.\\n\", optopt);\r\n\t\t\t}else if(isprint(optopt)){\r\n\t\t\t\tfprintf(stderr, \"Unknown option -%c.\\n\", optopt);\r\n\t\t\t}else{\r\n\t\t\t\tfprintf(stderr, \"Unknown option character '\\\\x%x'.\\n\", optopt);\r\n\t\t\t}\r\n\t\t\tusage();\r\n\t\t\treturn EXIT_FAILURE;\r\n\t\tdefault:\r\n\t\t\tabort();\r\n\t\t}\r\n\t}\r\n\t\r\n\tL = new_S4_lua_state(); \n\r\n\t\r\n\tthreadsafe_init();\r\n\tS4_threads_init(L, max_threads);\r\n\t\r\n\t\n\r\n\tif(NULL != arg){\r\n\t\tlua_getglobal(L, \"S4\");\r\n\t\tlua_pushstring(L, \"arg\");\r\n\t\tlua_pushstring(L, arg);\r\n\t\tlua_settable(L, -3);\r\n\t\tlua_pop(L, 1);\r\n\t}\r\n\t\r\n\tlua_getglobal(L, \"S4\");\r\n\tlua_pushstring(L, \"MPIRank\");\r\n\tlua_pushinteger(L, mpi_rank);\r\n\tlua_settable(L, -3);\r\n\tlua_pushstring(L, \"MPISize\");\r\n\tlua_pushinteger(L, mpi_size);\r\n\tlua_settable(L, -3);\r\n\tlua_pop(L, 1);\r\n\t\r\n\tif(optind < argc){ \n\r\n\t\tS4L_set_interactive(L, 0);\r\n\t\t\r\n\t\tfor(index = optind; index < argc; ++index){\r\n\t\t\terror = luaL_loadfile(L, argv[index]);\r\n\t\t\tif(error){\r\n\t\t\t\tfprintf(stderr, \"%s\\n\", lua_tostring(L, -1));\r\n\t\t\t\tlua_pop(L, 1); \n\r\n\t\t\t}else{\r\n\t\t\t\terror = lua_pcall(L, 0, 0, 0);\r\n\t\t\t\tif(error){\r\n\t\t\t\t\tfprintf(stderr, \"%s\\n\", lua_tostring(L, -1));\r\n\t\t\t\t\tlua_pop(L, 1); \n\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}else{ \n\r\n\t\tfprintf(stdout, \"No input file given, running in interactive mode\\n\");\r\n\t\tS4L_set_interactive(L, 1);\r\n\t\twhile(fgets(buff, sizeof(buff), stdin) != NULL){\r\n\t\t\terror = luaL_loadbuffer(L, buff, strlen(buff), \"line\") || lua_pcall(L, 0, 0, 0);\r\n\t\t\tif(error){\r\n\t\t\t\tfprintf(stderr, \"%s\", lua_tostring(L, -1));\r\n\t\t\t\tlua_pop(L, 1); \n\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t\r\n\tS4_threads_destroy(L);\r\n\t\r\n\tlua_close(L);\r\n\t\r\n\tthreadsafe_destroy();\r\n\t\r\n\tif(NULL != arg){ free(arg); }\r\n#ifdef HAVE_MPI\r\n#endif\r\n\treturn EXIT_SUCCESS;\r\n}", "label": "int main(int argc, char *argv[]){\r\n\tchar buff[256];\r\n\tint c;\r\n\tint index, error;\r\n\tunsigned int max_threads = get_max_threads();\r\n\tchar *arg = NULL;\r\n\tint mpi_size = 1, mpi_rank = 0;\r\n\tlua_State *L;\r\n\t\r\n#ifdef HAVE_MPI\r\n\tMPI_Init(&argc, &argv);\r\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\r\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\r\n#endif\r\n\t\r\n\topterr = 0;\r\n\twhile((c = getopt(argc, argv, \"a:ht:v\")) != -1){\r\n\t\tswitch(c){\r\n\t\tcase 'a':\r\n\t\t\targ = strdup(optarg);\r\n\t\t\tbreak;\r\n\t\tcase 'h':\r\n\t\t\tusage();\r\n\t\t\treturn EXIT_SUCCESS;\r\n\t\tcase 't':\r\n\t\t\tmax_threads = atoi(optarg);\r\n\t\t\tbreak;\r\n\t\tcase 'v':\r\n\t\t\tversion();\r\n\t\t\treturn EXIT_SUCCESS;\r\n\t\tcase '?':\r\n\t\t\tif('t' == optopt){\r\n\t\t\t\tfprintf(stderr, \"Option -%c requires an argument.\\n\", optopt);\r\n\t\t\t}else if(isprint(optopt)){\r\n\t\t\t\tfprintf(stderr, \"Unknown option -%c.\\n\", optopt);\r\n\t\t\t}else{\r\n\t\t\t\tfprintf(stderr, \"Unknown option character '\\\\x%x'.\\n\", optopt);\r\n\t\t\t}\r\n\t\t\tusage();\r\n\t\t\treturn EXIT_FAILURE;\r\n\t\tdefault:\r\n\t\t\tabort();\r\n\t\t}\r\n\t}\r\n\t\r\n\tL = new_S4_lua_state(); \n\r\n\t\r\n\tthreadsafe_init();\r\n\tS4_threads_init(L, max_threads);\r\n\t\r\n\t\n\r\n\tif(NULL != arg){\r\n\t\tlua_getglobal(L, \"S4\");\r\n\t\tlua_pushstring(L, \"arg\");\r\n\t\tlua_pushstring(L, arg);\r\n\t\tlua_settable(L, -3);\r\n\t\tlua_pop(L, 1);\r\n\t}\r\n\t\r\n\tlua_getglobal(L, \"S4\");\r\n\tlua_pushstring(L, \"MPIRank\");\r\n\tlua_pushinteger(L, mpi_rank);\r\n\tlua_settable(L, -3);\r\n\tlua_pushstring(L, \"MPISize\");\r\n\tlua_pushinteger(L, mpi_size);\r\n\tlua_settable(L, -3);\r\n\tlua_pop(L, 1);\r\n\t\r\n\tif(optind < argc){ \n\r\n\t\tS4L_set_interactive(L, 0);\r\n\t\t\r\n\t\tfor(index = optind; index < argc; ++index){\r\n\t\t\terror = luaL_loadfile(L, argv[index]);\r\n\t\t\tif(error){\r\n\t\t\t\tfprintf(stderr, \"%s\\n\", lua_tostring(L, -1));\r\n\t\t\t\tlua_pop(L, 1); \n\r\n\t\t\t}else{\r\n\t\t\t\terror = lua_pcall(L, 0, 0, 0);\r\n\t\t\t\tif(error){\r\n\t\t\t\t\tfprintf(stderr, \"%s\\n\", lua_tostring(L, -1));\r\n\t\t\t\t\tlua_pop(L, 1); \n\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}else{ \n\r\n\t\tfprintf(stdout, \"No input file given, running in interactive mode\\n\");\r\n\t\tS4L_set_interactive(L, 1);\r\n\t\twhile(fgets(buff, sizeof(buff), stdin) != NULL){\r\n\t\t\terror = luaL_loadbuffer(L, buff, strlen(buff), \"line\") || lua_pcall(L, 0, 0, 0);\r\n\t\t\tif(error){\r\n\t\t\t\tfprintf(stderr, \"%s\", lua_tostring(L, -1));\r\n\t\t\t\tlua_pop(L, 1); \n\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\t\r\n\tS4_threads_destroy(L);\r\n\t\r\n\tlua_close(L);\r\n\t\r\n\tthreadsafe_destroy();\r\n\t\r\n\tif(NULL != arg){ free(arg); }\r\n#ifdef HAVE_MPI\r\n\tMPI_Finalize();\r\n#endif\r\n\treturn EXIT_SUCCESS;\r\n}"}
{"program": "syftalent_578", "code": "int main( int argc, char ** argv ) {\n    MPI_Comm tmp, comm, startComm;\n    char * fname;\n    char * actualFname = NULL;\n    char * globalFname = NULL;\n    int totalSize, expectedRank, size, cachedRank;\n    char portName[MPI_MAX_PORT_NAME];\n    int rankToAccept = 1;\n\n    \n\n#ifdef MPICHLIBSTR\n    msg( \"MPICH library taken from: %s\\n\", MPICHLIBSTR );\n#endif\n\n    if( argc != 4 ) {\n        printf( \"Usage: %s <fname> <totalSize> <idx-1-based>\\n\", argv[0] );\n        exit( 1 );\n    }\n\n    \n\n    fname = argv[1];\n    \n\n    totalSize = atoi( argv[2] );\n    \n\n    expectedRank = atoi( argv[3] )-1;\n\n    \n\n    startWatchdog( 120 );\n\n    \n\n    msg( \"Waiting for: %d - my rank is %d\\n\", totalSize, expectedRank );\n\n    \n\n\n    \n\n\n    \n    if( expectedRank == 0 ) {\n        \n\n        \n        \n\n        actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n\n        \n\n        globalFname = writePortToFile( portName, fname );\n        installExitHandler( globalFname );\n\n        comm = startComm;\n    } else {\n        char * readPort;\n        readPort = getPortFromFile( \"%s.%d\", fname, expectedRank );\n        strncpy( portName, readPort, MPI_MAX_PORT_NAME );\n        free( readPort );\n        msg( \"Read port <%s>\\n\", portName );\n        \n        comm = tmp;\n        msg( \"After my first merge, size is now: %d\\n\", size );\n    }\n    while( size < totalSize ) {\n        \n\n        strokeWatchdog();\n\n        \n\n\n        \n\n\n        \n\n\n        \n\n\n        if( expectedRank == 0 ) {\n            msg( \"Up to size: %d\\n\", size );\n\n            \n\n            unlink( actualFname );\n            free( actualFname );\n\n            \n\n            actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n        }\n    }\n\n    msg( \"All done - I got rank: %d.\\n\", cachedRank );\n\n\n    if( expectedRank == 0 ) {\n\n        \n\n        MTestSleep( 4 );\n        unlink( actualFname );\n        free( actualFname );\n        unlink( globalFname );\n        free( globalFname );\n\n        \n\n        indicateConnectSucceeded();\n    }\n\n    return 0;\n}", "label": "int main( int argc, char ** argv ) {\n    MPI_Comm tmp, comm, startComm;\n    char * fname;\n    char * actualFname = NULL;\n    char * globalFname = NULL;\n    int totalSize, expectedRank, size, cachedRank;\n    char portName[MPI_MAX_PORT_NAME];\n    int rankToAccept = 1;\n\n    \n\n#ifdef MPICHLIBSTR\n    msg( \"MPICH library taken from: %s\\n\", MPICHLIBSTR );\n#endif\n\n    if( argc != 4 ) {\n        printf( \"Usage: %s <fname> <totalSize> <idx-1-based>\\n\", argv[0] );\n        exit( 1 );\n    }\n\n    \n\n    fname = argv[1];\n    \n\n    totalSize = atoi( argv[2] );\n    \n\n    expectedRank = atoi( argv[3] )-1;\n\n    \n\n    startWatchdog( 120 );\n\n    \n\n    msg( \"Waiting for: %d - my rank is %d\\n\", totalSize, expectedRank );\n\n    \n\n    MPI_Init( 0, 0 );\n\n    \n\n    MPI_Comm_dup( MPI_COMM_SELF, &startComm );\n\n    \n    if( expectedRank == 0 ) {\n        \n\n        MPI_Open_port( MPI_INFO_NULL, portName );\n        \n        \n\n        actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n\n        \n\n        globalFname = writePortToFile( portName, fname );\n        installExitHandler( globalFname );\n\n        comm = startComm;\n    } else {\n        char * readPort;\n        readPort = getPortFromFile( \"%s.%d\", fname, expectedRank );\n        strncpy( portName, readPort, MPI_MAX_PORT_NAME );\n        free( readPort );\n        msg( \"Read port <%s>\\n\", portName );\n        \n        MPI_Comm_connect( portName, MPI_INFO_NULL, 0, startComm, &comm );\n        MPI_Intercomm_merge( comm, 1, &tmp );\n        comm = tmp;\n        MPI_Comm_size( comm, &size );\n        msg( \"After my first merge, size is now: %d\\n\", size );\n    }\n    while( size < totalSize ) {\n        \n\n        strokeWatchdog();\n\n        \n\n        MPI_Comm_accept( portName, MPI_INFO_NULL, 0, comm, &tmp );\n\n        \n\n        MPI_Intercomm_merge( tmp, 0, &comm );\n\n        \n\n        MPI_Comm_free( &tmp );\n\n        \n\n        MPI_Comm_rank( comm, &cachedRank );\n        MPI_Comm_size( comm, &size );\n\n        if( expectedRank == 0 ) {\n            msg( \"Up to size: %d\\n\", size );\n\n            \n\n            unlink( actualFname );\n            free( actualFname );\n\n            \n\n            actualFname = writePortToFile( portName, \"%s.%d\", fname, rankToAccept++ );\n        }\n    }\n    MPI_Comm_rank( comm, &cachedRank );\n\n    msg( \"All done - I got rank: %d.\\n\", cachedRank );\n\n    MPI_Barrier( comm );\n\n    if( expectedRank == 0 ) {\n\n        \n\n        MTestSleep( 4 );\n        unlink( actualFname );\n        free( actualFname );\n        unlink( globalFname );\n        free( globalFname );\n\n        \n\n        indicateConnectSucceeded();\n    }\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_579", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tDictionary*\t\tdictionary;\n\tGeometry*\t\tgeometry;\n\tElementLayout*\t\teLayout;\n\tTopology*\t\tnTopology;\n\tNodeLayout*\t\tnLayout;\n\tMeshDecomp*\t\tmeshDecomp;\t\n\tIndex\t\t\ti;\n\t\n\t\n\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 13 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"maxX\", Dictionary_Entry_Value_FromUnsignedInt( 6 ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"isPeriodicI\", Dictionary_Entry_Value_FromBool( True ) );\n\t\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPQuadEL\", 2, dictionary );\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tmeshDecomp = (MeshDecomp*)HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\n\tElementLayout_Build( eLayout, meshDecomp );\n\n\tif (rank == procToWatch) {\n\t\tprintf( \"Element with point:\\n\" );\n\t}\n\tgeometry = eLayout->geometry;\n\tfor( i = 0; i < geometry->pointCount; i++ ) {\n\t\tCoord point;\n\t\tIndex excEl, incEl;\n\t\t\n\t\tgeometry->pointAt( geometry, i, point );\n\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tNode %u (%0.2f,%0.2f):\\n\", i, point[0], point[1] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\n\t\t}\n\n\t\tpoint[0] += 0.1;\n\t\tpoint[1] += 0.1;\n\t\tpoint[2] += 0.1;\n\t\t\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tTest Point %u (%0.2f,%0.2f):\\n\", i, point[0], point[1] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\n\t\t}\n\t}\n\tif (rank == procToWatch) {\n\t\tprintf( \"\\n\" );\n\t}\t\n\t\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( meshDecomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( eLayout );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tDictionary*\t\tdictionary;\n\tGeometry*\t\tgeometry;\n\tElementLayout*\t\teLayout;\n\tTopology*\t\tnTopology;\n\tNodeLayout*\t\tnLayout;\n\tMeshDecomp*\t\tmeshDecomp;\t\n\tIndex\t\t\ti;\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size(CommWorld, &procCount);\n\tMPI_Comm_rank(CommWorld, &rank);\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 13 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"maxX\", Dictionary_Entry_Value_FromUnsignedInt( 6 ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"isPeriodicI\", Dictionary_Entry_Value_FromBool( True ) );\n\t\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPQuadEL\", 2, dictionary );\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tmeshDecomp = (MeshDecomp*)HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\n\tElementLayout_Build( eLayout, meshDecomp );\n\n\tif (rank == procToWatch) {\n\t\tprintf( \"Element with point:\\n\" );\n\t}\n\tgeometry = eLayout->geometry;\n\tfor( i = 0; i < geometry->pointCount; i++ ) {\n\t\tCoord point;\n\t\tIndex excEl, incEl;\n\t\t\n\t\tgeometry->pointAt( geometry, i, point );\n\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tNode %u (%0.2f,%0.2f):\\n\", i, point[0], point[1] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\n\t\t}\n\n\t\tpoint[0] += 0.1;\n\t\tpoint[1] += 0.1;\n\t\tpoint[2] += 0.1;\n\t\t\n\t\tif (rank == procToWatch) {\n\t\t\tprintf( \"\\tTest Point %u (%0.2f,%0.2f):\\n\", i, point[0], point[1] );\n\t\t\texcEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   EXCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tincEl = eLayout->elementWithPoint( eLayout, meshDecomp, point, NULL, \n\t\t\t\t\t\t\t   INCLUSIVE_UPPER_BOUNDARY, 0, NULL );\n\t\t\tprintf( \"\\t\\tIncl %4u, Excl %4u\\n\", incEl, excEl );\n\t\t}\n\t}\n\tif (rank == procToWatch) {\n\t\tprintf( \"\\n\" );\n\t}\t\n\t\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( meshDecomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( eLayout );\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "mpip_580", "code": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  pfft_init();\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  \n\n  n[0] = n[1] = n[2] = 2; \n\n  np[0]=5; np[1]=5; np[2] = 1;\n  verbose = 1;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 8;\n\n  \n\n  gc_below[0] = 2;\n  gc_above[0] = 0;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_2d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= gc_below[0]+gc_above[0]+(np[0]-1);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, data, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  pfft_free(data);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  MPI_Init(&argc, &argv);\n  pfft_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &rnk_self);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  \n\n  n[0] = n[1] = n[2] = 2; \n\n  np[0]=5; np[1]=5; np[2] = 1;\n  verbose = 1;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 8;\n\n  \n\n  gc_below[0] = 2;\n  gc_above[0] = 0;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_2d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= gc_below[0]+gc_above[0]+(np[0]-1);\n\n  \n\n  MPI_Barrier(comm_cart_2d);\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, data, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(data);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "pf-aics-riken_581", "code": "int\nmain(int argc, char *argv[])\n{\n    int nprocs, rank;\n\n    printf(\"Running PI on rank=%d/%d\\n\", rank, nprocs);\n    fflush(0);\n\n    int n;\n    if (rank == 0) {\n\tn = 1000;\n    }\n\n    int c;\n    c = 0;\n    for (int i = 0; i < n; i++) {\n\tdouble x = drand48();\n\tdouble y = drand48();\n\tif (x*x + y*y < 1e0) {\n\t    c++;\n\t}\n    }\n    double pi0 = (4e0 * ((double)c / (double)n));\n    double tot;\n    if (rank == 0) {\n\tdouble pi = ((double)tot / (double)nprocs);\n\tprintf(\"PI is pi=%e (error=%e)\\n\", pi, fabs(pi - PI));\n\tfflush(0);\n\tsleep(1);\n    } else {\n\tsleep(1);\n    }\n\n\n    exit(0);\n    return 0;\n}", "label": "int\nmain(int argc, char *argv[])\n{\n    int nprocs, rank;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    printf(\"Running PI on rank=%d/%d\\n\", rank, nprocs);\n    fflush(0);\n\n    int n;\n    if (rank == 0) {\n\tn = 1000;\n    }\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int c;\n    c = 0;\n    for (int i = 0; i < n; i++) {\n\tdouble x = drand48();\n\tdouble y = drand48();\n\tif (x*x + y*y < 1e0) {\n\t    c++;\n\t}\n    }\n    double pi0 = (4e0 * ((double)c / (double)n));\n    double tot;\n    MPI_Reduce(&pi0, &tot, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n\tdouble pi = ((double)tot / (double)nprocs);\n\tprintf(\"PI is pi=%e (error=%e)\\n\", pi, fabs(pi - PI));\n\tfflush(0);\n\tsleep(1);\n    } else {\n\tsleep(1);\n    }\n\n    MPI_Finalize();\n\n    exit(0);\n    return 0;\n}"}
{"program": "byu-vv-lab_583", "code": "int main( int argc, char *argv[] )\n{\n    int rank, size;\n    int chunk = 128;\n    int i;\n    int *sb;\n    int *rb;\n    int status, gstatus;\n\n    for ( i=1 ; i < argc ; ++i ) {\n        if ( argv[i][0] != '-' )\n            continue;\n        switch(argv[i][1]) {\n            case 'm':\n                chunk = atoi(argv[++i]);\n                break;\n            default:\n                fprintf(stderr, \"Unrecognized argument %s\\n\", argv[i]);fflush(stderr);\n        }\n    }\n    sb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !sb ) {\n        perror( \"can't allocate send buffer\" );fflush(stderr);\n    }\n    rb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !rb ) {\n        perror( \"can't allocate recv buffer\");fflush(stderr);\n        free(sb);\n    }\n    for ( i=0 ; i < size*chunk ; ++i ) {\n        sb[i] = rank + 1;\n        rb[i] = 0;\n    }\n    status =\n    if (rank == 0) {\n        if (gstatus != 0) {\n            printf(\"all_to_all returned %d\\n\",gstatus);fflush(stdout);\n        }\n    }\n    free(sb);\n    free(rb);\n    return(EXIT_SUCCESS);\n}", "label": "int main( int argc, char *argv[] )\n{\n    int rank, size;\n    int chunk = 128;\n    int i;\n    int *sb;\n    int *rb;\n    int status, gstatus;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    MPI_Comm_size(MPI_COMM_WORLD,&size);\n    for ( i=1 ; i < argc ; ++i ) {\n        if ( argv[i][0] != '-' )\n            continue;\n        switch(argv[i][1]) {\n            case 'm':\n                chunk = atoi(argv[++i]);\n                break;\n            default:\n                fprintf(stderr, \"Unrecognized argument %s\\n\", argv[i]);fflush(stderr);\n                MPI_Abort(MPI_COMM_WORLD,EXIT_FAILURE);\n        }\n    }\n    sb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !sb ) {\n        perror( \"can't allocate send buffer\" );fflush(stderr);\n        MPI_Abort(MPI_COMM_WORLD,EXIT_FAILURE);\n    }\n    rb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !rb ) {\n        perror( \"can't allocate recv buffer\");fflush(stderr);\n        free(sb);\n        MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n    }\n    for ( i=0 ; i < size*chunk ; ++i ) {\n        sb[i] = rank + 1;\n        rb[i] = 0;\n    }\n    status = MPI_Alltoall(sb, chunk, MPI_INT, rb, chunk, MPI_INT, MPI_COMM_WORLD);\n    MPI_Allreduce( &status, &gstatus, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n        if (gstatus != 0) {\n            printf(\"all_to_all returned %d\\n\",gstatus);fflush(stdout);\n        }\n    }\n    free(sb);\n    free(rb);\n    MPI_Finalize();\n    return(EXIT_SUCCESS);\n}"}
{"program": "neurodebian_584", "code": "void work_around() {\n\tMPI_Comm c = MPI_COMM_WORLD;\n\n}", "label": "void work_around() {\n\tMPI_Comm c = MPI_COMM_WORLD;\n\n\tMPI_Abort(c, 0);\n\tMPI_Address(0,0);\n\tMPI_Allgather(0,0,0,0,0,0,c);\n\tMPI_Allgatherv(0,0,0,0,0,0,0,c);\n\tMPI_Allreduce(0,0,0,0,0,c);\n\tMPI_Barrier(c);\n\tMPI_Bcast(0,0,0,0,c);\n\tMPI_Comm_dup(c, 0);\n\tMPI_Comm_rank(c, 0);\n\tMPI_Comm_size(c, 0);\n\tMPI_Finalize();\n\tMPI_Gather(0,0,0,0,0,0,0,c);\n\tMPI_Gatherv(0,0,0,0,0,0,0,0,c);\n\tMPI_Get_count(0, 0, 0);\n\tMPI_Init(0, 0);\n\tMPI_Initialized(0);\n\tMPI_Iprobe(0,0,c,0,0);\n\tMPI_Irecv(0,0,0,0,0,c,0);\n\tMPI_Isend(0,0,0,0,0,c,0);\n\tMPI_Op_create(0, 0, 0);\n\tMPI_Pack(0, 0, 0, 0, 0, 0, c);\n\tMPI_Pack_size(0, 0, c, 0);\n\tMPI_Probe(0, 0, c, 0);\n\tMPI_Recv(0,0,0,0,0,c,0);\n\tMPI_Request_free(0);\n\tMPI_Send(0,0,0,0,0,c);\n\tMPI_Sendrecv(0,0,0,0,0,0,0,0,0,0,c,0);\n\tMPI_Type_commit(0);\n\tMPI_Type_struct(0,0,0,0,0);\n\tMPI_Unpack(0, 0, 0, 0, 0, 0, c);\n\tMPI_Wait(0, 0);\n\tMPI_Wtime();\n}"}
{"program": "bsc-performance-tools_587", "code": "int main(int argc, char *argv[])\n{\n\tint v, rank, size;\n\tif (size != 2)\n\t\treturn 1;\n\tif (rank == 0)\n\t{\n\t\tsleep(5);\n\t}\n\telse\n\t{\n\t\tMPI_Request r;\n\t\tMPI_Status s;\n\t\tint flag = 0;\n\t\twhile (!flag)\n\t}\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint v, rank, size;\n\tMPI_Init (&argc, &argv);\n\tMPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size (MPI_COMM_WORLD, &size);\n\tif (size != 2)\n\t\treturn 1;\n\tif (rank == 0)\n\t{\n\t\tsleep(5);\n\t\tMPI_Send (&v, 1, MPI_INT, 1, 1234, MPI_COMM_WORLD);\n\t}\n\telse\n\t{\n\t\tMPI_Request r;\n\t\tMPI_Status s;\n\t\tint flag = 0;\n\t\tMPI_Iprobe (0, 1234, MPI_COMM_WORLD, &flag, &s);\n\t\twhile (!flag)\n\t\t\tMPI_Iprobe (0, 1234, MPI_COMM_WORLD, &flag, &s);\n\t\tMPI_Irecv (&v, 1, MPI_INT, 0, 1234, MPI_COMM_WORLD, &r);\n\t\tMPI_Wait (&r, &s);\n\t}\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "luizirber_589", "code": "int main(int argc, char **argv) {\n  int rank, procs;\n  long long int my_sum = 0;\n  long long int other_sum = 0;\n  int vector[VECTOR_SIZE];\n  int chunk_size;\n\n  MPI_Status status;\n\n  chunk_size = calc_chunk_for_proc(rank, VECTOR_SIZE, procs);\n  if (rank == 0) {\n    init_vector(vector);\n    distribute_vector(vector, procs);\n  } else {\n    receive_vector(vector, chunk_size);\n  }\n\n  my_sum = sum_vector(vector, chunk_size);\n\n  unsigned int m = (int)log2(procs);\n  unsigned int sender = 0, receiver = 0;\n  unsigned int divider, partner, offset;\n\n  for (int i = 1; i <= m; ++i ) {\n    sender = 0;\n    receiver = 0;\n    divider = 2 << (i - 1); \n\n    offset = divider >> 1; \n\n\n    if (rank % divider == 0) {\n      receiver = 1;\n      partner = rank + offset;\n    } else if (rank % divider == offset) {\n      sender = 1;\n      partner = rank - offset;\n    }\n\n    if (receiver) {\n      my_sum += other_sum;\n    } else if (sender) {\n    }\n  }\n\n  \n\n\n\n  return(0);\n}", "label": "int main(int argc, char **argv) {\n  int rank, procs;\n  long long int my_sum = 0;\n  long long int other_sum = 0;\n  int vector[VECTOR_SIZE];\n  int chunk_size;\n\n  MPI_Status status;\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  chunk_size = calc_chunk_for_proc(rank, VECTOR_SIZE, procs);\n  if (rank == 0) {\n    init_vector(vector);\n    distribute_vector(vector, procs);\n  } else {\n    receive_vector(vector, chunk_size);\n  }\n\n  my_sum = sum_vector(vector, chunk_size);\n\n  unsigned int m = (int)log2(procs);\n  unsigned int sender = 0, receiver = 0;\n  unsigned int divider, partner, offset;\n\n  for (int i = 1; i <= m; ++i ) {\n    sender = 0;\n    receiver = 0;\n    divider = 2 << (i - 1); \n\n    offset = divider >> 1; \n\n\n    if (rank % divider == 0) {\n      receiver = 1;\n      partner = rank + offset;\n    } else if (rank % divider == offset) {\n      sender = 1;\n      partner = rank - offset;\n    }\n\n    if (receiver) {\n      MPI_Recv(&other_sum, 1, MPI_LONG_LONG_INT, partner,\n               SUM_BASE_STEP_TAG + i, MPI_COMM_WORLD, &status);\n      my_sum += other_sum;\n    } else if (sender) {\n      MPI_Send(&my_sum, 1, MPI_LONG_LONG_INT, partner,\n               SUM_BASE_STEP_TAG + i, MPI_COMM_WORLD);\n    }\n  }\n\n  \n\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "itsjareds_590", "code": "int main(int argc, char *argv[]){\n\n  srand(time(NULL));\n\n  int rank, size;\n  int tmp;\n  int tag;\n  int i;\n\n  int arrayZero[16];\n  int arrayOne[16];\n  int fromArray[2];\n  int fromOne;\n  int sum;\n\n  MPI_Status status;\n\n\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.1\\n\\n\");\n\n  if (rank == 0) {\n    \n\n    int i;\n    for (i = 0; i < 16; i++) {\n      arrayZero[i] = rand() % RMAX + 1;\n    }\n    printf(\"[%d/%d] arrayZero[1] = %d\\n\", rank, size, arrayZero[1]);\n  } else if (rank == 1) {\n    printf(\"[%d/%d] fromOne = %d\\n\", rank, size, fromOne);\n  }\n\n  \n\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.2\\n\\n\");\n\n  printf(\"[%d/%d] fromOne = %d\\n\", rank, size, fromOne);\n\n  \n\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.3\\n\\n\");\n\n  if (rank == 2)\n    printf(\"[%d/%d] sum = %d\\n\", rank, size, sum);\n\n  \n\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.4\\n\\n\");\n\n  printf(\"[%d/%d] fromArray = \", rank, size);\n  printArr(fromArray, 2);\n  fflush(stdout);\n  printf(\"\\n\");\n\n  \n\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.5\\n\\n\");\n\n  if (rank == 0) {\n    printf(\"[%d/%d] \", rank, size);\n    printArr(arrayZero, 16);\n    fflush(stdout);\n    printf(\"\\n[%d/%d] \", rank, size);\n    printArr(arrayOne, 16);\n    fflush(stdout);\n    printf(\"\\n\");\n  }\n\n  return 0;\n}", "label": "int main(int argc, char *argv[]){\n\n  srand(time(NULL));\n\n  int rank, size;\n  int tmp;\n  int tag;\n  int i;\n\n  int arrayZero[16];\n  int arrayOne[16];\n  int fromArray[2];\n  int fromOne;\n  int sum;\n\n  MPI_Status status;\n\n  MPI_Init(&argc,&argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.1\\n\\n\");\n\n  if (rank == 0) {\n    \n\n    int i;\n    for (i = 0; i < 16; i++) {\n      arrayZero[i] = rand() % RMAX + 1;\n    }\n    MPI_Send(&arrayZero[1], 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n    printf(\"[%d/%d] arrayZero[1] = %d\\n\", rank, size, arrayZero[1]);\n  } else if (rank == 1) {\n    MPI_Recv(&fromOne, 1, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n    printf(\"[%d/%d] fromOne = %d\\n\", rank, size, fromOne);\n  }\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.2\\n\\n\");\n\n  MPI_Bcast(&fromOne, 1, MPI_INT, 1, MPI_COMM_WORLD);\n  printf(\"[%d/%d] fromOne = %d\\n\", rank, size, fromOne);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.3\\n\\n\");\n\n  MPI_Reduce(&fromOne, &sum, 1, MPI_INT, MPI_SUM, 2, MPI_COMM_WORLD);\n  if (rank == 2)\n    printf(\"[%d/%d] sum = %d\\n\", rank, size, sum);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.4\\n\\n\");\n\n  MPI_Scatter(&arrayZero, 2, MPI_INT, &fromArray, 2, MPI_INT, 0, MPI_COMM_WORLD);\n  printf(\"[%d/%d] fromArray = \", rank, size);\n  printArr(fromArray, 2);\n  fflush(stdout);\n  printf(\"\\n\");\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  if (rank == 0)\n    printf(\"\\nPart 1.5\\n\\n\");\n\n  MPI_Gather(&fromArray, 2, MPI_INT, &arrayOne, 2, MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    printf(\"[%d/%d] \", rank, size);\n    printArr(arrayZero, 16);\n    fflush(stdout);\n    printf(\"\\n[%d/%d] \", rank, size);\n    printArr(arrayOne, 16);\n    fflush(stdout);\n    printf(\"\\n\");\n  }\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "samjcus_591", "code": "int main (int argc, char *argv[])\n{\n  int N,i,ThisN,ThisStart;\n  int ThisTask,NTask;\n  double *list,Thisres,res,kgo;\n  double t_start,t_end;\n\n\n  N=100000000;\n\n  ThisN=N/NTask;\n  ThisStart=ThisTask*ThisN;\n  if(ThisTask==NTask-1)\n    {\n      ThisN+=N%NTask;\n    }\n  \n  list=malloc(ThisN*sizeof(double));\n  \n  for(i=0;i<ThisN;i++)\n    {\n      list[i] = (double) (i+ThisStart);\n    }\n\n  Thisres=sum(list, ThisN);\n  kgo=0.5* N*(N-1);\n  if(ThisTask==0)\n    {\n      printf(\"error=%g\\n\",res-kgo);\n      printf(\"Took %g s\\n\",t_end - t_start);\n    }\n  free(list);\n\n  \n  return EXIT_SUCCESS;\n}", "label": "int main (int argc, char *argv[])\n{\n  int N,i,ThisN,ThisStart;\n  int ThisTask,NTask;\n  double *list,Thisres,res,kgo;\n  double t_start,t_end;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&ThisTask);\n  MPI_Comm_size(MPI_COMM_WORLD,&NTask);\n\n  t_start=MPI_Wtime();\n  N=100000000;\n\n  ThisN=N/NTask;\n  ThisStart=ThisTask*ThisN;\n  if(ThisTask==NTask-1)\n    {\n      ThisN+=N%NTask;\n    }\n  \n  list=malloc(ThisN*sizeof(double));\n  \n  for(i=0;i<ThisN;i++)\n    {\n      list[i] = (double) (i+ThisStart);\n    }\n\n  Thisres=sum(list, ThisN);\n  MPI_Reduce(&Thisres,&res,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n  t_end=MPI_Wtime();\n  kgo=0.5* N*(N-1);\n  if(ThisTask==0)\n    {\n      printf(\"error=%g\\n\",res-kgo);\n      printf(\"Took %g s\\n\",t_end - t_start);\n    }\n  free(list);\n\n  MPI_Finalize();\n  \n  return EXIT_SUCCESS;\n}"}
{"program": "ghisvail_592", "code": "int main(int argc, char **argv){\n  int np[3];\n  ptrdiff_t N[3], local_M;\n  ptrdiff_t local_N[3], local_N_start[3];\n  double lower_border[3], upper_border[3];\n  MPI_Comm comm_cart_3d;\n  pnfft_complex *f_hat, *f;\n  double *x;\n  pnfft_plan pnfft;\n  \n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  np[0]=2; np[1]=2; np[2]=2;\n  local_M = N[0]*N[1]*N[2]/(np[0]*np[1]*np[2]);\n  \n  \n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] x np[2] = %td x %td x %td processes\\n\", np[0], np[1], np[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n  \n\n  if( pnfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: Procmesh of size %d x %d x %d does not fit to number of allocated processes.\\n\", np[0], np[1], np[2]);\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"       Please allocate %d processes (mpiexec -np %d ...) or change the procmesh (with -pnfft_np * * *).\\n\", np[0]*np[1]*np[2], np[0]*np[1]*np[2]);\n    return 1;\n  }\n\n  \n\n  pnfft_local_size_3d(N, comm_cart_3d, PNFFT_TRANSPOSED_NONE,\n      local_N, local_N_start, lower_border, upper_border);\n\n  \n\n  pnfft = pnfft_init_3d(N, local_M, comm_cart_3d);\n\n  \n\n  f_hat = pnfft_get_f_hat(pnfft);\n  f     = pnfft_get_f(pnfft);\n  x     = pnfft_get_x(pnfft);\n\n  \n\n  pnfft_init_f_hat_3d(N, local_N, local_N_start, PNFFT_TRANSPOSED_NONE,\n      f_hat);\n\n  \n\n  init_random_x(lower_border, upper_border, local_M,\n      x);\n\n  \n\n  vpr_complex(comm_cart_3d, 8, f_hat,\n      \"Input Fourier coefficients on process 1:\");\n\n  \n\n  pnfft_trafo(pnfft);\n\n  \n\n  vpr_complex(comm_cart_3d, 8, f,\n      \"PNFFT Results on process 1:\");\n\n  \n\n  pnfft_adj(pnfft);\n\n  \n\n  for(ptrdiff_t l=0; l < local_N[0] * local_N[1] * local_N[2]; l++)\n    f_hat[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  vpr_complex(comm_cart_3d, 8, f_hat,\n      \"Fourier coefficients after one forward and backward PNFFT on process 1:\");\n\n  \n\n  pnfft_finalize(pnfft, PNFFT_FREE_X | PNFFT_FREE_F_HAT| PNFFT_FREE_F);\n  pnfft_cleanup();\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[3];\n  ptrdiff_t N[3], local_M;\n  ptrdiff_t local_N[3], local_N_start[3];\n  double lower_border[3], upper_border[3];\n  MPI_Comm comm_cart_3d;\n  pnfft_complex *f_hat, *f;\n  double *x;\n  pnfft_plan pnfft;\n  \n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  np[0]=2; np[1]=2; np[2]=2;\n  local_M = N[0]*N[1]*N[2]/(np[0]*np[1]*np[2]);\n  \n  \n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] x np[2] = %td x %td x %td processes\\n\", np[0], np[1], np[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n  \n\n  if( pnfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: Procmesh of size %d x %d x %d does not fit to number of allocated processes.\\n\", np[0], np[1], np[2]);\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"       Please allocate %d processes (mpiexec -np %d ...) or change the procmesh (with -pnfft_np * * *).\\n\", np[0]*np[1]*np[2], np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  pnfft_local_size_3d(N, comm_cart_3d, PNFFT_TRANSPOSED_NONE,\n      local_N, local_N_start, lower_border, upper_border);\n\n  \n\n  pnfft = pnfft_init_3d(N, local_M, comm_cart_3d);\n\n  \n\n  f_hat = pnfft_get_f_hat(pnfft);\n  f     = pnfft_get_f(pnfft);\n  x     = pnfft_get_x(pnfft);\n\n  \n\n  pnfft_init_f_hat_3d(N, local_N, local_N_start, PNFFT_TRANSPOSED_NONE,\n      f_hat);\n\n  \n\n  init_random_x(lower_border, upper_border, local_M,\n      x);\n\n  \n\n  vpr_complex(comm_cart_3d, 8, f_hat,\n      \"Input Fourier coefficients on process 1:\");\n\n  \n\n  pnfft_trafo(pnfft);\n\n  \n\n  vpr_complex(comm_cart_3d, 8, f,\n      \"PNFFT Results on process 1:\");\n\n  \n\n  pnfft_adj(pnfft);\n\n  \n\n  for(ptrdiff_t l=0; l < local_N[0] * local_N[1] * local_N[2]; l++)\n    f_hat[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  vpr_complex(comm_cart_3d, 8, f_hat,\n      \"Fourier coefficients after one forward and backward PNFFT on process 1:\");\n\n  \n\n  pnfft_finalize(pnfft, PNFFT_FREE_X | PNFFT_FREE_F_HAT| PNFFT_FREE_F);\n  MPI_Comm_free(&comm_cart_3d);\n  pnfft_cleanup();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bmi-forum_593", "code": "int main(int argc, char *argv[])\n{\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tDictionary*\t\tdictionary;\n\tExtensionManager_Register*\textensionMgr_Register;\n\tTopology*\t\tnTopology;\n\tElementLayout*\t\teLayout;\n\tNodeLayout*\t\tnLayout;\n\tMeshDecomp*\t\tdecomp;\n\tMeshLayout*\t\tml;\n\tMesh*\t\t\tmesh;\n\tMeshCoarsener_Hexa*\tcoarsener;\n\tStream*\t\t\tstream;\n\t\n\t\n\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"rank\", Dictionary_Entry_Value_FromUnsignedInt( rank ) );\n\tDictionary_Add( dictionary, \"numProcessors\", Dictionary_Entry_Value_FromUnsignedInt( procCount ) );\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 21 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 21 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 21 ) );\n\tDictionary_Add( dictionary, \"allowUnusedCPUs\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnElement\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( False ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 0 ) );\n\t\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tdecomp = (MeshDecomp*)HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\tml = MeshLayout_New( \"MeshLayout\", eLayout, nLayout, decomp );\n\t\n\textensionMgr_Register = ExtensionManager_Register_New();\n\tmesh = Mesh_New( \"Mesh\", ml, sizeof(Node), sizeof(Element), extensionMgr_Register, dictionary );\n\t\n\tmesh->buildNodeLocalToGlobalMap = True;\n\tmesh->buildNodeDomainToGlobalMap = True;\n\tmesh->buildNodeGlobalToLocalMap = True;\n\tmesh->buildNodeGlobalToDomainMap = True;\n\tmesh->buildNodeNeighbourTbl = True;\n\tmesh->buildNodeElementTbl = True;\n\tmesh->buildElementLocalToGlobalMap = True;\n\tmesh->buildElementDomainToGlobalMap = True;\n\tmesh->buildElementGlobalToDomainMap = True;\n\tmesh->buildElementGlobalToLocalMap = True;\n\tmesh->buildElementNeighbourTbl = True;\n\tmesh->buildElementNodeTbl = True;\n\t\n\t\n\n\tcoarsener = MeshCoarsener_Hexa_New(\"meshCoarsener\");\n\tMeshCoarsener_Hexa_Coarsen( coarsener, mesh, 1, NULL, NULL );\n\tStg_Class_Delete( coarsener );\n\t\n\tBuild( mesh, 0, False );\n\tInitialise(mesh, 0, False );\n\t\n\tif (rank == procToWatch)\n\t{\n\t\tPrint(mesh, stream);\n\t}\n\t\n\tStg_Class_Delete(mesh);\n\tStg_Class_Delete(ml);\n\tStg_Class_Delete(decomp);\n\tStg_Class_Delete(nLayout);\n\tStg_Class_Delete(eLayout);\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete(dictionary);\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main(int argc, char *argv[])\n{\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tDictionary*\t\tdictionary;\n\tExtensionManager_Register*\textensionMgr_Register;\n\tTopology*\t\tnTopology;\n\tElementLayout*\t\teLayout;\n\tNodeLayout*\t\tnLayout;\n\tMeshDecomp*\t\tdecomp;\n\tMeshLayout*\t\tml;\n\tMesh*\t\t\tmesh;\n\tMeshCoarsener_Hexa*\tcoarsener;\n\tStream*\t\t\tstream;\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size(CommWorld, &procCount);\n\tMPI_Comm_rank(CommWorld, &rank);\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\tMPI_Barrier( CommWorld );\n\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"rank\", Dictionary_Entry_Value_FromUnsignedInt( rank ) );\n\tDictionary_Add( dictionary, \"numProcessors\", Dictionary_Entry_Value_FromUnsignedInt( procCount ) );\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 21 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 21 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 21 ) );\n\tDictionary_Add( dictionary, \"allowUnusedCPUs\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnElement\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( False ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 0 ) );\n\t\n\tnTopology = (Topology*)IJK6Topology_New( \"IJK6Topology\", dictionary );\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tdecomp = (MeshDecomp*)HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\tml = MeshLayout_New( \"MeshLayout\", eLayout, nLayout, decomp );\n\t\n\textensionMgr_Register = ExtensionManager_Register_New();\n\tmesh = Mesh_New( \"Mesh\", ml, sizeof(Node), sizeof(Element), extensionMgr_Register, dictionary );\n\t\n\tmesh->buildNodeLocalToGlobalMap = True;\n\tmesh->buildNodeDomainToGlobalMap = True;\n\tmesh->buildNodeGlobalToLocalMap = True;\n\tmesh->buildNodeGlobalToDomainMap = True;\n\tmesh->buildNodeNeighbourTbl = True;\n\tmesh->buildNodeElementTbl = True;\n\tmesh->buildElementLocalToGlobalMap = True;\n\tmesh->buildElementDomainToGlobalMap = True;\n\tmesh->buildElementGlobalToDomainMap = True;\n\tmesh->buildElementGlobalToLocalMap = True;\n\tmesh->buildElementNeighbourTbl = True;\n\tmesh->buildElementNodeTbl = True;\n\t\n\t\n\n\tcoarsener = MeshCoarsener_Hexa_New(\"meshCoarsener\");\n\tMeshCoarsener_Hexa_Coarsen( coarsener, mesh, 1, NULL, NULL );\n\tStg_Class_Delete( coarsener );\n\t\n\tBuild( mesh, 0, False );\n\tInitialise(mesh, 0, False );\n\t\n\tif (rank == procToWatch)\n\t{\n\t\tPrint(mesh, stream);\n\t}\n\t\n\tStg_Class_Delete(mesh);\n\tStg_Class_Delete(ml);\n\tStg_Class_Delete(decomp);\n\tStg_Class_Delete(nLayout);\n\tStg_Class_Delete(eLayout);\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete(dictionary);\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "scafacos_599", "code": "int main(int argc, char** argv)\n{\n#ifdef DCMF\n      int desired = MPI_THREAD_MULTIPLE;\n      int provided;\n      if ( provided != MPI_THREAD_MULTIPLE ) printf(\"provided != MPI_THREAD_MULTIPLE\\n\");\n#else\n\n#endif\n      if(me==0)printf(\"Testing IPCs (%d MPI processes)\\n\\n\",nproc);\n      ARMCI_Init();\n      test();\n      ARMCI_Finalize();\n      return 0;\n\n}", "label": "int main(int argc, char** argv)\n{\n#ifdef DCMF\n      int desired = MPI_THREAD_MULTIPLE;\n      int provided;\n      printf(\"using MPI_Init_thread\\n\");       MPI_Init_thread(&argc, &argv, desired, &provided);\n      if ( provided != MPI_THREAD_MULTIPLE ) printf(\"provided != MPI_THREAD_MULTIPLE\\n\");\n#else\n      MPI_Init (&argc, &argv);  \n\n#endif\n      MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n      MPI_Comm_rank(MPI_COMM_WORLD, &me);\n      if(me==0)printf(\"Testing IPCs (%d MPI processes)\\n\\n\",nproc);\n      ARMCI_Init();\n      test();\n      ARMCI_Finalize();\n      MPI_Finalize();\n      return 0;\n\n}"}
{"program": "ahmadia_600", "code": "int main(int argc, char *argv[])\n{\n  int err,verbosity,rank,size;\n  double tstart,tend,elapsed;\n  char path[MAXPATHLEN];\n\n  if (argc == 2) {\n    verbosity = atoi(argv[1]);\n  }\n  collfs_initialize(verbosity, NULL);\n\n  if (!getcwd(path,sizeof path)) ERR(\"getcwd failed\");\n  strcat(path,\"/libminimal_thefunc.so\");\n\n  tstart =\n  collfs_comm_push(MPI_COMM_WORLD);\n  err = run_tests(verbosity,path);CHK(err);\n  collfs_comm_pop();\n  tend =\n  elapsed = tend - tstart;\n  if (verbosity > 1) printf(\"[%d] elapsed = %g\\n\",rank,elapsed);\n  {\n    struct {double time; int rank; } loc,gmax,gmin;\n    double gsum;\n    loc.time = elapsed;\n    loc.rank = rank;\n    if (!rank) printf(\"NumProcs %d  Min %g@%d  Max %g@%d  Ratio %g  Ave %g\\n\",size,gmin.time,gmin.rank,gmax.time,gmax.rank,gmax.time/gmin.time,gsum/size);\n  }\n\n  collfs_finalize();\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n  int err,verbosity,rank,size;\n  double tstart,tend,elapsed;\n  char path[MAXPATHLEN];\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n  if (argc == 2) {\n    verbosity = atoi(argv[1]);\n  }\n  collfs_initialize(verbosity, NULL);\n\n  if (!getcwd(path,sizeof path)) ERR(\"getcwd failed\");\n  strcat(path,\"/libminimal_thefunc.so\");\n\n  tstart = MPI_Wtime();\n  collfs_comm_push(MPI_COMM_WORLD);\n  err = run_tests(verbosity,path);CHK(err);\n  collfs_comm_pop();\n  tend = MPI_Wtime();\n  elapsed = tend - tstart;\n  if (verbosity > 1) printf(\"[%d] elapsed = %g\\n\",rank,elapsed);\n  {\n    struct {double time; int rank; } loc,gmax,gmin;\n    double gsum;\n    loc.time = elapsed;\n    loc.rank = rank;\n    MPI_Reduce(&loc,&gmax,1,MPI_DOUBLE_INT,MPI_MAXLOC,0,MPI_COMM_WORLD);\n    MPI_Reduce(&loc,&gmin,1,MPI_DOUBLE_INT,MPI_MINLOC,0,MPI_COMM_WORLD);\n    MPI_Reduce(&elapsed,&gsum,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n    if (!rank) printf(\"NumProcs %d  Min %g@%d  Max %g@%d  Ratio %g  Ave %g\\n\",size,gmin.time,gmin.rank,gmax.time,gmax.rank,gmax.time/gmin.time,gsum/size);\n  }\n\n  collfs_finalize();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bmi-forum_601", "code": "int main(int argc, char *argv[])\n{\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tDictionary*\tdictionary;\n\tMeshTopology*\tmt;\n\tMeshGeometry*\tmg;\n\tMeshDecomp*\tmd;\n\tMeshLayout*\tml;\n\tCoord\t\tpoint[8] = {{0.25, 0.25, 0.25}, {0.75, 0.25, 0.25}, {0.25, 0.75, 0.25}, {0.75, 0.75, 0.25}, \n\t\t\t\t{0.25, 0.25, 0.75}, {0.75, 0.25, 0.75}, {0.25, 0.75, 0.75}, {0.75, 0.75, 0.75}};\n\t\n\t\n\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\t\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"rank\", Dictionary_Entry_Value_FromUnsignedInt( rank ) );\n\tDictionary_Add( dictionary, \"numProcessors\", Dictionary_Entry_Value_FromUnsignedInt( procCount ) );\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tDictionary_Add( dictionary, \"allowUnusedCPUs\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnElement\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( False ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 0 ) );\n\t\n\tmt = (MeshTopology*)HexaMeshTopology_New(dictionary);\n\tmg = (MeshGeometry*)HexaMeshGeometry_New(dictionary);\n\tmd = (MeshDecomp*)RegularMeshDecomp_New(dictionary, MPI_COMM_WORLD, mt);\n\tml = MeshLayout_New(mt, mg, md);\n\t\n\tif (rank == procToWatch)\n\t{\n\t\tElement_GlobalIndex\telt;\n\t\tIndex\t\t\ti;\n\t\t\n\t\tPrint(ml);\n\t\t\n\t\tfor (i = 0; i < 8; i++)\n\t\t{\n\t\t\telt = MeshLayout_ElementWithPoint(ml, point[i]);\n\t\t\tprintf(\"Point: {%g, %g, %g} - element: \", point[i][0], point[i][1], point[i][2]);\n\t\t\tif (elt < ml->decomp->nodeGlobalCount)\n\t\t\t\tprintf(\"%u\\n\", elt);\n\t\t\telse\n\t\t\t\tprintf(\"X\\n\");\n\t\t}\n\t}\n\t\n\tStg_Class_Delete(ml);\n\tStg_Class_Delete(md);\n\tStg_Class_Delete(mg);\n\tStg_Class_Delete(mt);\n\tStg_Class_Delete(dictionary);\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main(int argc, char *argv[])\n{\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tDictionary*\tdictionary;\n\tMeshTopology*\tmt;\n\tMeshGeometry*\tmg;\n\tMeshDecomp*\tmd;\n\tMeshLayout*\tml;\n\tCoord\t\tpoint[8] = {{0.25, 0.25, 0.25}, {0.75, 0.25, 0.25}, {0.25, 0.75, 0.25}, {0.75, 0.75, 0.25}, \n\t\t\t\t{0.25, 0.25, 0.75}, {0.75, 0.25, 0.75}, {0.25, 0.75, 0.75}, {0.75, 0.75, 0.75}};\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\t\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"rank\", Dictionary_Entry_Value_FromUnsignedInt( rank ) );\n\tDictionary_Add( dictionary, \"numProcessors\", Dictionary_Entry_Value_FromUnsignedInt( procCount ) );\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tDictionary_Add( dictionary, \"allowUnusedCPUs\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnElement\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowPartitionOnNode\", Dictionary_Entry_Value_FromBool( True ) );\n\tDictionary_Add( dictionary, \"allowUnbalancing\", Dictionary_Entry_Value_FromBool( False ) );\n\tDictionary_Add( dictionary, \"shadowDepth\", Dictionary_Entry_Value_FromUnsignedInt( 0 ) );\n\t\n\tmt = (MeshTopology*)HexaMeshTopology_New(dictionary);\n\tmg = (MeshGeometry*)HexaMeshGeometry_New(dictionary);\n\tmd = (MeshDecomp*)RegularMeshDecomp_New(dictionary, MPI_COMM_WORLD, mt);\n\tml = MeshLayout_New(mt, mg, md);\n\t\n\tif (rank == procToWatch)\n\t{\n\t\tElement_GlobalIndex\telt;\n\t\tIndex\t\t\ti;\n\t\t\n\t\tPrint(ml);\n\t\t\n\t\tfor (i = 0; i < 8; i++)\n\t\t{\n\t\t\telt = MeshLayout_ElementWithPoint(ml, point[i]);\n\t\t\tprintf(\"Point: {%g, %g, %g} - element: \", point[i][0], point[i][1], point[i][2]);\n\t\t\tif (elt < ml->decomp->nodeGlobalCount)\n\t\t\t\tprintf(\"%u\\n\", elt);\n\t\t\telse\n\t\t\t\tprintf(\"X\\n\");\n\t\t}\n\t}\n\t\n\tStg_Class_Delete(ml);\n\tStg_Class_Delete(md);\n\tStg_Class_Delete(mg);\n\tStg_Class_Delete(mt);\n\tStg_Class_Delete(dictionary);\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "bmi-forum_602", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tStream* myDebug;\t\t\n\n\t\tJournal_Enable_TypedStream( Debug_Type, True );\n\t\t\n\t\tmyDebug = Journal_Register( Debug_Type, \"MyDebug\" );\n\n\t\tJournal_Printf( myDebug, \"Hello world\\n\" );\n\t\tJournal_Printf( myDebug, \"\\n\\n\\n\" );\n\t\tJournal_Printf( myDebug, \"abc\\ndef\\nghi\" );\n\t\tJournal_Printf( myDebug, \"jkl\\n\\n\\n\" );\n\t}\n\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tStream* myDebug;\t\t\n\n\t\tJournal_Enable_TypedStream( Debug_Type, True );\n\t\t\n\t\tmyDebug = Journal_Register( Debug_Type, \"MyDebug\" );\n\n\t\tJournal_Printf( myDebug, \"Hello world\\n\" );\n\t\tJournal_Printf( myDebug, \"\\n\\n\\n\" );\n\t\tJournal_Printf( myDebug, \"abc\\ndef\\nghi\" );\n\t\tJournal_Printf( myDebug, \"jkl\\n\\n\\n\" );\n\t}\n\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "germasch_603", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"simple\");\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  switch (testcase) {\n  case 1:\n    mrc_crds_set_type(crds, \"uniform\");\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    test_read_write(domain);\n    break;\n  case 2: ;\n    mrc_crds_set_type(crds, \"rectilinear\");\n    mrc_crds_set_param_int(crds, \"sw\", 2);\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    mrctest_set_crds_rectilinear_1(domain);\n    test_read_write(domain);\n    break;\n  }\n  mrc_domain_destroy(domain);\n\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"simple\");\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  switch (testcase) {\n  case 1:\n    mrc_crds_set_type(crds, \"uniform\");\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    test_read_write(domain);\n    break;\n  case 2: ;\n    mrc_crds_set_type(crds, \"rectilinear\");\n    mrc_crds_set_param_int(crds, \"sw\", 2);\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    mrctest_set_crds_rectilinear_1(domain);\n    test_read_write(domain);\n    break;\n  }\n  mrc_domain_destroy(domain);\n\n  MPI_Finalize();\n}"}
{"program": "gnu3ra_604", "code": "int main( int argc, char *argv[] )\n{\n    char str[10];\n    int err=0, errcodes[256], rank, rank1, nprocs;\n    MPI_Comm intercomm1, intercomm2, intracomm;\n\n\n\n    if (nprocs != 2) {\n        printf(\"Run this program with 2 processes\\n\");\n    }\n\n    if (rank1 == 0) {\n        printf(\"Parents spawning 2 children...\\n\");\n        fflush(stdout);\n    }\n\n    err =  \n    if (err) printf(\"Error in MPI_Comm_spawn\\n\");\n\n    if (rank1 == 0) {\n        printf(\"Parents and children merging to form new intracommunicator...\\n\");\n        fflush(stdout);\n    }\n\n\n    if (rank1 == 0) {\n        printf(\"Merged parents spawning 2 more children and communicating with them...\\n\");\n        fflush(stdout);\n    }\n\n    err =  \n    if (err) printf(\"Error in MPI_Comm_spawn\\n\");\n\n\n    if (rank == 2) {\n        err =\n        printf(\"Parent received from child: %s\\n\", str);\n        fflush(stdout);\n        \n        err = \n    }\n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    char str[10];\n    int err=0, errcodes[256], rank, rank1, nprocs;\n    MPI_Comm intercomm1, intercomm2, intracomm;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD,&nprocs); \n    MPI_Comm_rank(MPI_COMM_WORLD,&rank1); \n\n    if (nprocs != 2) {\n        printf(\"Run this program with 2 processes\\n\");\n        MPI_Abort(MPI_COMM_WORLD,1);\n    }\n\n    if (rank1 == 0) {\n        printf(\"Parents spawning 2 children...\\n\");\n        fflush(stdout);\n    }\n\n    err = MPI_Comm_spawn(\"spawn_merge_child1\", MPI_ARGV_NULL, 2,\n                         MPI_INFO_NULL, 1, MPI_COMM_WORLD,\n                         &intercomm1, errcodes);  \n    if (err) printf(\"Error in MPI_Comm_spawn\\n\");\n\n    if (rank1 == 0) {\n        printf(\"Parents and children merging to form new intracommunicator...\\n\");\n        fflush(stdout);\n    }\n\n    MPI_Intercomm_merge(intercomm1, 0, &intracomm);\n\n    if (rank1 == 0) {\n        printf(\"Merged parents spawning 2 more children and communicating with them...\\n\");\n        fflush(stdout);\n    }\n\n    err = MPI_Comm_spawn(\"spawn_merge_child2\", MPI_ARGV_NULL, 2,\n                         MPI_INFO_NULL, 2, intracomm,\n                         &intercomm2, errcodes);  \n    if (err) printf(\"Error in MPI_Comm_spawn\\n\");\n\n    MPI_Comm_rank(intercomm2, &rank);\n\n    if (rank == 2) {\n        err = MPI_Recv(str, 3, MPI_CHAR, 1, 0, intercomm2, MPI_STATUS_IGNORE);\n        printf(\"Parent received from child: %s\\n\", str);\n        fflush(stdout);\n        \n        err = MPI_Send(\"bye\", 4, MPI_CHAR, 1, 0, intercomm2); \n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "lapesd_605", "code": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing Time Recorder test...\\n\", stdout);\n\n  ctx_init ();\n  trec_init ();\n\n  int num_exp_repl = 1;\n  int num_run = 1;\n  int num_repts = 1;\n  fputs (\"Recording performance metrics...\\n\", stdout);\n  record (num_exp_repl, num_run, num_repts);\n  fputs (\"Performance metrics recorded.\\n\", stdout);\n\n  fputs (\"Display recorded metrics...\\n\", stdout);\n  show ();\n  fputs (\"Metrics displayed.\\n\", stdout);\n\n  fputs (\"Displaying to string...\\n\", stdout);\n  tostr ();\n  fputs (\"String displayed.\\n\", stdout);\n\n  trec_destroy ();\n\n  fputs (\"Finalizing Time Recorder test.\\n\", stdout);\n}", "label": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing Time Recorder test...\\n\", stdout);\n\n  MPI_Init (&argc, &argv);\n  ctx_init ();\n  trec_init ();\n\n  int num_exp_repl = 1;\n  int num_run = 1;\n  int num_repts = 1;\n  fputs (\"Recording performance metrics...\\n\", stdout);\n  record (num_exp_repl, num_run, num_repts);\n  fputs (\"Performance metrics recorded.\\n\", stdout);\n\n  fputs (\"Display recorded metrics...\\n\", stdout);\n  show ();\n  fputs (\"Metrics displayed.\\n\", stdout);\n\n  fputs (\"Displaying to string...\\n\", stdout);\n  tostr ();\n  fputs (\"String displayed.\\n\", stdout);\n\n  trec_destroy ();\n  MPI_Finalize ();\n\n  fputs (\"Finalizing Time Recorder test.\\n\", stdout);\n}"}
{"program": "CFDEMproject_607", "code": "int main(int argc, char *argv[])\n\n{\n  int globalstrategy;\n  long int local_N;\n  realtype fnormtol, scsteptol;\n  N_Vector cc, sc, constraints;\n  UserData data;\n  int flag, maxl, maxlrst;\n  int my_pe, npes, npelast = NPEX*NPEY-1;\n  void *kmem;\n  MPI_Comm comm;\n\n  cc = sc = constraints = NULL;\n  data = NULL;\n  kmem = NULL;\n\n  \n\n  comm = MPI_COMM_WORLD;\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      fprintf(stderr, \"\\nMPI_ERROR(0); npes = %d is not equal to NPEX*NPEY = %d\\n\",\n\t      npes,NPEX*NPEY);\n    return(1);\n  }\n\n  \n \n\n  \n\n  local_N = NUM_SPECIES*MXSUB*MYSUB;\n\n  \n\n  data = AllocUserData();\n  InitUserData(my_pe, comm, data);\n\n  \n\n  globalstrategy = KIN_NONE;\n  \n  \n\n  cc = N_VNew_Parallel(comm, local_N, NEQ);\n  sc = N_VNew_Parallel(comm, local_N, NEQ);\n  data->rates = N_VNew_Parallel(comm, local_N, NEQ);\n  constraints = N_VNew_Parallel(comm, local_N, NEQ);\n  N_VConst(ZERO, constraints);\n  \n  SetInitialProfiles(cc, sc);\n\n  fnormtol=FTOL; scsteptol=STOL;\n\n \n\n  kmem = KINCreate();\n  \n\n  flag = KINInit(kmem, funcprpr, cc);\n\n  flag = KINSetNumMaxIters(kmem, 250);\n  flag = KINSetUserData(kmem, data);\n  flag = KINSetConstraints(kmem, constraints);\n  flag = KINSetFuncNormTol(kmem, fnormtol);\n  flag = KINSetScaledStepTol(kmem, scsteptol);\n\n  \n\n  N_VDestroy_Parallel(constraints);\n\n  \n\n  maxl = 20; maxlrst = 2;\n  flag = KINSpgmr(kmem, maxl);\n\n  flag = KINSpilsSetMaxRestarts(kmem, maxlrst);\n  flag = KINSpilsSetPreconditioner(kmem,\n\t\t\t\t   Precondbd,\n\t\t\t\t   PSolvebd);\n\n  \n\n  if (my_pe == 0) \n    PrintHeader(globalstrategy, maxl, maxlrst, fnormtol, scsteptol);\n\n  \n\n  flag = KINSol(kmem,           \n\n                cc,             \n\n                globalstrategy, \n\n                sc,             \n\n                sc);            \n\n\n  if (my_pe == 0) \n     printf(\"\\n\\nComputed equilibrium species concentrations:\\n\");\n  if (my_pe == 0 || my_pe == npelast) \n     PrintOutput(my_pe, comm, cc);\n\n  \n  \n  if (my_pe == 0) \n     PrintFinalStats(kmem);\n\n  N_VDestroy_Parallel(cc);\n  N_VDestroy_Parallel(sc);\n  KINFree(&kmem);\n  FreeUserData(data);\n\n\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n\n{\n  int globalstrategy;\n  long int local_N;\n  realtype fnormtol, scsteptol;\n  N_Vector cc, sc, constraints;\n  UserData data;\n  int flag, maxl, maxlrst;\n  int my_pe, npes, npelast = NPEX*NPEY-1;\n  void *kmem;\n  MPI_Comm comm;\n\n  cc = sc = constraints = NULL;\n  data = NULL;\n  kmem = NULL;\n\n  \n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &my_pe);\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      fprintf(stderr, \"\\nMPI_ERROR(0); npes = %d is not equal to NPEX*NPEY = %d\\n\",\n\t      npes,NPEX*NPEY);\n    MPI_Finalize();\n    return(1);\n  }\n\n  \n \n\n  \n\n  local_N = NUM_SPECIES*MXSUB*MYSUB;\n\n  \n\n  data = AllocUserData();\n  if (check_flag((void *)data, \"AllocUserData\", 0, my_pe)) MPI_Abort(comm, 1);\n  InitUserData(my_pe, comm, data);\n\n  \n\n  globalstrategy = KIN_NONE;\n  \n  \n\n  cc = N_VNew_Parallel(comm, local_N, NEQ);\n  if (check_flag((void *)cc, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  sc = N_VNew_Parallel(comm, local_N, NEQ);\n  if (check_flag((void *)sc, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  data->rates = N_VNew_Parallel(comm, local_N, NEQ);\n  if (check_flag((void *)data->rates, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  constraints = N_VNew_Parallel(comm, local_N, NEQ);\n  if (check_flag((void *)constraints, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  N_VConst(ZERO, constraints);\n  \n  SetInitialProfiles(cc, sc);\n\n  fnormtol=FTOL; scsteptol=STOL;\n\n \n\n  kmem = KINCreate();\n  if (check_flag((void *)kmem, \"KINCreate\", 0, my_pe)) MPI_Abort(comm, 1);\n  \n\n  flag = KINInit(kmem, funcprpr, cc);\n  if (check_flag(&flag, \"KINInit\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  flag = KINSetNumMaxIters(kmem, 250);\n  if (check_flag(&flag, \"KINSetNumMaxIters\", 1, my_pe)) MPI_Abort(comm, 1);\n  flag = KINSetUserData(kmem, data);\n  if (check_flag(&flag, \"KINSetUserData\", 1, my_pe)) MPI_Abort(comm, 1);\n  flag = KINSetConstraints(kmem, constraints);\n  if (check_flag(&flag, \"KINSetConstraints\", 1, my_pe)) MPI_Abort(comm, 1);\n  flag = KINSetFuncNormTol(kmem, fnormtol);\n  if (check_flag(&flag, \"KINSetFuncNormTol\", 1, my_pe)) MPI_Abort(comm, 1);\n  flag = KINSetScaledStepTol(kmem, scsteptol);\n  if (check_flag(&flag, \"KINSetScaledStepTop\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  N_VDestroy_Parallel(constraints);\n\n  \n\n  maxl = 20; maxlrst = 2;\n  flag = KINSpgmr(kmem, maxl);\n  if (check_flag(&flag, \"KINSpgmr\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  flag = KINSpilsSetMaxRestarts(kmem, maxlrst);\n  if (check_flag(&flag, \"KINSpilsSetMaxRestarts\", 1, my_pe)) MPI_Abort(comm, 1);\n  flag = KINSpilsSetPreconditioner(kmem,\n\t\t\t\t   Precondbd,\n\t\t\t\t   PSolvebd);\n  if (check_flag(&flag, \"KINSpilsSetPreconditioner\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  if (my_pe == 0) \n    PrintHeader(globalstrategy, maxl, maxlrst, fnormtol, scsteptol);\n\n  \n\n  flag = KINSol(kmem,           \n\n                cc,             \n\n                globalstrategy, \n\n                sc,             \n\n                sc);            \n\n  if (check_flag(&flag, \"KINSol\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  if (my_pe == 0) \n     printf(\"\\n\\nComputed equilibrium species concentrations:\\n\");\n  if (my_pe == 0 || my_pe == npelast) \n     PrintOutput(my_pe, comm, cc);\n\n  \n  \n  if (my_pe == 0) \n     PrintFinalStats(kmem);\n\n  N_VDestroy_Parallel(cc);\n  N_VDestroy_Parallel(sc);\n  KINFree(&kmem);\n  FreeUserData(data);\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "bmi-forum_608", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tStream* stream;\n\n\tMockContext* context;\n\n\tDictionary* dictionary;\n\tIO_Handler* ioHandler;\n\n\tStg_ComponentFactory* cf;\n\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n        RegisterParent( MockContext_Type, Stg_Component_Type );\n\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, __FILE__ );\n\n\tdictionary = Dictionary_New();\n\tioHandler = (IO_Handler*)XML_IO_Handler_New();\n\n\tIO_Handler_ReadAllFromCommandLine( ioHandler, argc, argv, dictionary );\n\n\tcontext = MockContext_New( dictionary );\n\n\t\n\n\n\tcf = Stg_ComponentFactory_New( dictionary, Dictionary_GetDictionary( dictionary, \"components\" ), Stg_ObjectList_New() );\n\tLiveComponentRegister_Add( cf->LCRegister, (Stg_Component*)context );\n\tPluginsManager_Load( context->plugins, context, dictionary );\n\n\tStg_ComponentFactory_CreateComponents( cf );\n\tStg_ComponentFactory_ConstructComponents( cf, 0 \n );\n\tPluginsManager_ConstructPlugins( context->plugins, cf, 0 \n );\n\n\t((EntryPoint_VoidPtr_CallCast*) context->ep->run)( context->ep, context );\n\n\tPrint( context->plugins, stream );\n\n\tStg_Class_Delete( ioHandler );\n\tStg_Class_Delete( context );\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n                                                                                                                                    \n\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tStream* stream;\n\n\tMockContext* context;\n\n\tDictionary* dictionary;\n\tIO_Handler* ioHandler;\n\n\tStg_ComponentFactory* cf;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n        RegisterParent( MockContext_Type, Stg_Component_Type );\n\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, __FILE__ );\n\n\tdictionary = Dictionary_New();\n\tioHandler = (IO_Handler*)XML_IO_Handler_New();\n\n\tIO_Handler_ReadAllFromCommandLine( ioHandler, argc, argv, dictionary );\n\n\tcontext = MockContext_New( dictionary );\n\n\t\n\n\n\tcf = Stg_ComponentFactory_New( dictionary, Dictionary_GetDictionary( dictionary, \"components\" ), Stg_ObjectList_New() );\n\tLiveComponentRegister_Add( cf->LCRegister, (Stg_Component*)context );\n\tPluginsManager_Load( context->plugins, context, dictionary );\n\n\tStg_ComponentFactory_CreateComponents( cf );\n\tStg_ComponentFactory_ConstructComponents( cf, 0 \n );\n\tPluginsManager_ConstructPlugins( context->plugins, cf, 0 \n );\n\n\t((EntryPoint_VoidPtr_CallCast*) context->ep->run)( context->ep, context );\n\n\tPrint( context->plugins, stream );\n\n\tStg_Class_Delete( ioHandler );\n\tStg_Class_Delete( context );\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n                                                                                                                                    \n\n\treturn 0;\n}"}
{"program": "gnu3ra_609", "code": "int main(int argc, char **argv )\n{\n    char *p;\n    int errs = 0, toterrs;\n    int size, rank;\n\n    \n    if (size != 2) {\n\terrs++;\n\tprintf( \"Communicator size is %d, should be 2\\n\", size );\n    }\n\n    p = getenv(\"TMP_ENV_VAR\");\n    if (!p) {\n\terrs++;\n\tprintf( \"Did not find TMP_ENV_VAR\\n\" );\n    }\n    else if (strcmp(p,\"1\") != 0) {\n\terrs++;\n\tprintf( \"Value of TMP_ENV_VAR was %s, expected 1\\n\", p );\n    }\n\n    if (rank == 0) {\n\tif (toterrs == 0) {\n\t    printf( \" No Errors\\n\" );\n\t}\n\telse {\n\t    printf( \" Found %d errors\\n\", toterrs );\n\t}\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv )\n{\n    char *p;\n    int errs = 0, toterrs;\n    int size, rank;\n\n    MPI_Init( &argc, &argv );\n    \n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n    if (size != 2) {\n\terrs++;\n\tprintf( \"Communicator size is %d, should be 2\\n\", size );\n    }\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\n    p = getenv(\"TMP_ENV_VAR\");\n    if (!p) {\n\terrs++;\n\tprintf( \"Did not find TMP_ENV_VAR\\n\" );\n    }\n    else if (strcmp(p,\"1\") != 0) {\n\terrs++;\n\tprintf( \"Value of TMP_ENV_VAR was %s, expected 1\\n\", p );\n    }\n\n    MPI_Reduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif (toterrs == 0) {\n\t    printf( \" No Errors\\n\" );\n\t}\n\telse {\n\t    printf( \" Found %d errors\\n\", toterrs );\n\t}\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "predicador37_610", "code": "int main(int argc, char **argv) {\n    int rank, size, version, subversion, namelen, universe_size;\n    char processor_name[MPI_MAX_PROCESSOR_NAME], worker_program[100];\n    MPI_Comm esclavos_comm;\n\n\n\n    printf(\"[maestro] Iniciado proceso maestro %d de %d en %s ejecutando MPI %d.%d\\n\", rank, size, processor_name,\n           version,\n           subversion);\n    strcpy(worker_program, \"./Debug/esclavo\");\n\n    \n\n    int n_esclavo = (N +1)  / ESCLAVOS;\n    printf(\"PUNTOS POR ESCLAVO: %d\\n\", n_esclavo);\n\n    \n\n    double dx = (double)(B - A) / (double)N ;\n    double h = ((double)B - (double)A) / (2*(double)N);\n\n    printf(\"DIFERENCIAL DE X: %f\\n\", dx);\n    int i = 0;\n    double y[N+1], y_esclavo[n_esclavo]; \n\n    double x = (double) A;\n\n    for(i=0;i<N+1;i++){\n        y[i]= x * x;\n        x+=dx;\n        printf(\"VALOR DE F(X) EN PUNTO i %d: %f\\n\", i, y[i]);\n    }\n\n\n\n\n\n    double suma;\n    printf(\"SUMA REDUCIDA ES: %f\\n\", suma);\n\n    double integral = (double) dx/3 * ((double)A*(double)A + suma + (double) B * (double) B);\n    printf(\"RESULTADO DE LA INTEGRAL: %f\\n\", integral);\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int rank, size, version, subversion, namelen, universe_size;\n    char processor_name[MPI_MAX_PROCESSOR_NAME], worker_program[100];\n    MPI_Comm esclavos_comm;\n    MPI_Init(&argc, &argv);    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);    \n\n    MPI_Get_processor_name(processor_name, &namelen);\n    MPI_Get_version(&version, &subversion);\n    printf(\"[maestro] Iniciado proceso maestro %d de %d en %s ejecutando MPI %d.%d\\n\", rank, size, processor_name,\n           version,\n           subversion);\n    strcpy(worker_program, \"./Debug/esclavo\");\n    MPI_Comm_spawn(worker_program, MPI_ARGV_NULL,ESCLAVOS, MPI_INFO_NULL, 0, MPI_COMM_SELF, &esclavos_comm,\n                   MPI_ERRCODES_IGNORE);\n\n    \n\n    int n_esclavo = (N +1)  / ESCLAVOS;\n    printf(\"PUNTOS POR ESCLAVO: %d\\n\", n_esclavo);\n\n    \n\n    double dx = (double)(B - A) / (double)N ;\n    double h = ((double)B - (double)A) / (2*(double)N);\n\n    printf(\"DIFERENCIAL DE X: %f\\n\", dx);\n    int i = 0;\n    double y[N+1], y_esclavo[n_esclavo]; \n\n    double x = (double) A;\n\n    for(i=0;i<N+1;i++){\n        y[i]= x * x;\n        x+=dx;\n        printf(\"VALOR DE F(X) EN PUNTO i %d: %f\\n\", i, y[i]);\n    }\n\n\n\n    MPI_Bcast(&n_esclavo, 1, MPI_INT, MPI_ROOT, esclavos_comm);\n\n    MPI_Scatter(y, n_esclavo, MPI_DOUBLE, y_esclavo, n_esclavo, MPI_DOUBLE, MPI_ROOT, esclavos_comm);\n\n    double suma;\n    MPI_Reduce(NULL, &suma, 1, MPI_DOUBLE, MPI_SUM, MPI_ROOT, esclavos_comm);\n    printf(\"SUMA REDUCIDA ES: %f\\n\", suma);\n\n    double integral = (double) dx/3 * ((double)A*(double)A + suma + (double) B * (double) B);\n    printf(\"RESULTADO DE LA INTEGRAL: %f\\n\", integral);\n    MPI_Comm_disconnect(&esclavos_comm);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "Wrent_611", "code": "int main(int argc, char **argv) {\n\n\tinit();\n\tif(rank==0) {\n\t\tset_f();\n\t}\n\tcalc_g();\n\tif(rank==0) {\n\t\tshow_g();\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv) {\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &np);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tinit();\n\tif(rank==0) {\n\t\tset_f();\n\t}\n\tMPI_Scatter(&f[begin], length, MPI_FLOAT, &f[begin], length, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\tcalc_g();\n\tMPI_Gather(&f[begin], length, MPI_FLOAT, &f[begin], length, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\tif(rank==0) {\n\t\tshow_g();\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "mpip_612", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu3ra_613", "code": "int main( int argc, char *argv[] )\n{\n    MPI_Info i1, i2;\n    int errs = 0;\n    char value[64];\n    int flag;\n\n    \n\n\n    if (flag) {\n\tprintf( \"Found key2 in info1\\n\" );\n\terrs ++;\n    }\n    if (!flag) {\n\terrs++;\n\tprintf( \"Did not find key1 in info1\\n\" );\n    }\n    else if (strcmp( value, \"value1\" )) {\n\terrs++;\n\tprintf( \"Found wrong value (%s), expected value1\\n\", value );\n    }\n\n    if (errs) {\n\tprintf( \" Found %d errors\\n\", errs );\n    }\n    else {\n\tprintf( \" No Errors\\n\" );\n    }\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    MPI_Info i1, i2;\n    int errs = 0;\n    char value[64];\n    int flag;\n\n    MPI_Init( 0, 0 );\n    \n    MPI_Info_create( &i1 );\n    MPI_Info_create( &i2 );\n\n    MPI_Info_set( i1, (char*)\"key1\", (char*)\"value1\" );\n    MPI_Info_set( i2, (char*)\"key2\", (char*)\"value2\" );\n\n    MPI_Info_get( i1, (char*)\"key2\", 64, value, &flag );\n    if (flag) {\n\tprintf( \"Found key2 in info1\\n\" );\n\terrs ++;\n    }\n    MPI_Info_get( i1, (char*)\"key1\", 64, value, &flag );\n    if (!flag) {\n\terrs++;\n\tprintf( \"Did not find key1 in info1\\n\" );\n    }\n    else if (strcmp( value, \"value1\" )) {\n\terrs++;\n\tprintf( \"Found wrong value (%s), expected value1\\n\", value );\n    }\n\n    MPI_Info_free( &i1 );\n    MPI_Info_free( &i2 );\n    if (errs) {\n\tprintf( \" Found %d errors\\n\", errs );\n    }\n    else {\n\tprintf( \" No Errors\\n\" );\n    }\n    MPI_Finalize( );\n    return 0;\n}"}
{"program": "gentryx_616", "code": "int main(int argc, char * argv[])\n{\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n\n    int rank, size;\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    double * baseptr = NULL;\n    MPI_Win win;\n    MPI_Aint winsize = (rank==0 ? n : 0);\n#if MPI_VERSION >= 3\n    \n\n#else\n#endif\n\n    if (rank==0) {\n        for (size_t i=0; i<(n/sizeof(double)); i++) {\n            baseptr[i] = 0.0;\n        }\n#if MPI_VERSION >= 3\n#endif\n    }\n\n    double * buf = NULL;\n    for (size_t i=0; i<(n/sizeof(double)); i++) {\n        buf[i] = 1.0;\n    }\n\n#if MPI_VERSION < 3\n#endif\n#if MPI_VERSION >= 3\n#else\n#endif\n\n\n    if (rank==0) {\n#if MPI_VERSION >= 3\n#endif\n        double expected = size;\n        size_t errors = verify_doubles(baseptr, n/sizeof(double), expected);\n        if (errors > 0) {\n            printf(\"There were %zu errors!\", errors);\n            for (size_t i=0; i<(n/(sizeof(double))); i++) {\n                printf(\"baseptr[%zu] = %lf (expected %lf)\\n\", i, baseptr[i], expected);\n            }\n        }\n        if (errors==0) {\n            printf(\"SUCCESS\\n\");\n        }\n    }\n\n#if MPI_VERSION >= 3\n#endif\n#if MPI_VERSION < 3\n#endif\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    double * baseptr = NULL;\n    MPI_Win win;\n    MPI_Aint winsize = (rank==0 ? n : 0);\n#if MPI_VERSION >= 3\n    \n\n    MPI_Win_allocate(winsize, sizeof(double), MPI_INFO_NULL, MPI_COMM_WORLD, &baseptr, &win);\n    MPI_Win_lock_all(0, win);\n#else\n    MPI_Alloc_mem(winsize*sizeof(double), MPI_INFO_NULL, &baseptr);\n    MPI_Win_create(baseptr, size, sizeof(double), MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n#endif\n\n    if (rank==0) {\n        for (size_t i=0; i<(n/sizeof(double)); i++) {\n            baseptr[i] = 0.0;\n        }\n#if MPI_VERSION >= 3\n        MPI_Win_sync(win);\n#endif\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    double * buf = NULL;\n    MPI_Alloc_mem((MPI_Aint)n, MPI_INFO_NULL, &buf);\n    for (size_t i=0; i<(n/sizeof(double)); i++) {\n        buf[i] = 1.0;\n    }\n\n#if MPI_VERSION < 3\n    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0 \n, 0 \n, win);\n#endif\n    MPIX_Accumulate_x(buf, n/sizeof(double), MPI_DOUBLE,\n                      0 \n, 0 \n, n/sizeof(double), MPI_DOUBLE, MPI_SUM, win);\n#if MPI_VERSION >= 3\n    MPI_Win_flush(0,win);\n#else\n    MPI_Win_unlock(0,win);\n#endif\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank==0) {\n#if MPI_VERSION >= 3\n        MPI_Win_sync(win);\n#endif\n        double expected = size;\n        size_t errors = verify_doubles(baseptr, n/sizeof(double), expected);\n        if (errors > 0) {\n            printf(\"There were %zu errors!\", errors);\n            for (size_t i=0; i<(n/(sizeof(double))); i++) {\n                printf(\"baseptr[%zu] = %lf (expected %lf)\\n\", i, baseptr[i], expected);\n            }\n        }\n        if (errors==0) {\n            printf(\"SUCCESS\\n\");\n        }\n    }\n\n#if MPI_VERSION >= 3\n    MPI_Win_unlock_all(win);\n#endif\n    MPI_Win_free(&win);\n#if MPI_VERSION < 3\n    MPI_Free_mem(baseptr);\n#endif\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "OnRampOrg_618", "code": "int main(int argc, char* argv[])\n{\n    int rank, size, len;\n    char processor[MPI_MAX_PROCESSOR_NAME];\n    char *name = NULL;\n\n    \n\n\n    \n\n    if( argc > 1 ) {\n        name = strdup(argv[1]);\n    }\n    else {\n        name = strdup(\"World\");\n    }\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n    printf(\"Hello, %s! I am %2d of %d on %s!\\n\", name, rank, size, processor);\n\n    \n\n\n    \n\n    if( NULL != name ) {\n        free(name);\n        name = NULL;\n    }\n\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    int rank, size, len;\n    char processor[MPI_MAX_PROCESSOR_NAME];\n    char *name = NULL;\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    if( argc > 1 ) {\n        name = strdup(argv[1]);\n    }\n    else {\n        name = strdup(\"World\");\n    }\n\n    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    MPI_Get_processor_name(processor, &len);\n\n    \n\n    printf(\"Hello, %s! I am %2d of %d on %s!\\n\", name, rank, size, processor);\n\n    \n\n    MPI_Finalize();\n\n    \n\n    if( NULL != name ) {\n        free(name);\n        name = NULL;\n    }\n\n    return 0;\n}"}
{"program": "EvaBr_619", "code": "int main(int argc, char* argv[]) {\n   \n   int i, myrank, nproc;\n   int NeighborID_li, NeighborID_re;\n   int recv_NeighborID_li, recv_NeighborID_re;\n   double pi; \n   MPI_Status status;\n\n\n   \n\n\n\n\n\n\n\n\n\n\n   \n\n\n   if ( 0 < myrank )\n      NeighborID_li = myrank - 1;\n   else \n      NeighborID_li = MPI_PROC_NULL;             \n\n\n   if ( (nproc - 1) > myrank )\n      NeighborID_re = myrank + 1; \n   else\n      NeighborID_re = MPI_PROC_NULL;             \n\n\n   \n   \n\n\n   if ( 0 == myrank )\n      pi = PI;\n   else\n      pi = 0.0;\n\n   printf(\"Proc %2d : Before the broadcast pi = %e\\n\", myrank, pi);\n\n\n   printf(\"Proc %2d : After the broadcast pi = %e\\n\", myrank, pi);   \n\n\n   \n\n\n   \n\n\n\n   if ( MPI_PROC_NULL != NeighborID_re )\n      printf(\"Proc %2d : ID right neighbor = %2d\\n\", myrank, recv_NeighborID_re);\n   else\n      printf(\"Proc %2d : ID right neighbor = MPI_PROC_NULL\\n\", myrank);\n\n\n   if ( MPI_PROC_NULL != NeighborID_li )\n      printf(\"Proc %2d : ID left neighbor = %2d\\n\", myrank, recv_NeighborID_li);\n   else\n      printf(\"Proc %2d : ID left neighbor = MPI_PROC_NULL\\n\", myrank);\n  \n \n   \n\n   fflush(stdout);                       \n\n   fflush(stderr);\n\n\n \n   return 0;\n}", "label": "int main(int argc, char* argv[]) {\n   \n   int i, myrank, nproc;\n   int NeighborID_li, NeighborID_re;\n   int recv_NeighborID_li, recv_NeighborID_re;\n   double pi; \n   MPI_Status status;\n\n\n   \n\n\n\n   MPI_Init( &argc, &argv );                    \n\n   MPI_Comm_size( MPI_COMM_WORLD, &nproc );     \n\n   MPI_Comm_rank( MPI_COMM_WORLD, &myrank );    \n\n\n\n\n\n   \n\n\n   if ( 0 < myrank )\n      NeighborID_li = myrank - 1;\n   else \n      NeighborID_li = MPI_PROC_NULL;             \n\n\n   if ( (nproc - 1) > myrank )\n      NeighborID_re = myrank + 1; \n   else\n      NeighborID_re = MPI_PROC_NULL;             \n\n\n   \n   \n\n\n   if ( 0 == myrank )\n      pi = PI;\n   else\n      pi = 0.0;\n\n   printf(\"Proc %2d : Before the broadcast pi = %e\\n\", myrank, pi);\n\n   MPI_Bcast( &pi, 1,MPI_DOUBLE, 0, MPI_COMM_WORLD );\n\n   printf(\"Proc %2d : After the broadcast pi = %e\\n\", myrank, pi);   \n\n\n   \n\n\n   \n\n\n   MPI_Send( &myrank, 1, MPI_INT, NeighborID_li, 1, MPI_COMM_WORLD );\n   MPI_Recv( &recv_NeighborID_re, 1, MPI_INT, NeighborID_re, 1, \n             MPI_COMM_WORLD, &status );\n\n   if ( MPI_PROC_NULL != NeighborID_re )\n      printf(\"Proc %2d : ID right neighbor = %2d\\n\", myrank, recv_NeighborID_re);\n   else\n      printf(\"Proc %2d : ID right neighbor = MPI_PROC_NULL\\n\", myrank);\n\n   MPI_Send( &myrank, 1, MPI_INT, NeighborID_re, 2, MPI_COMM_WORLD );\n   MPI_Recv( &recv_NeighborID_li, 1, MPI_INT, NeighborID_li, 2,\n             MPI_COMM_WORLD, &status );\n\n   if ( MPI_PROC_NULL != NeighborID_li )\n      printf(\"Proc %2d : ID left neighbor = %2d\\n\", myrank, recv_NeighborID_li);\n   else\n      printf(\"Proc %2d : ID left neighbor = MPI_PROC_NULL\\n\", myrank);\n  \n \n   \n\n   fflush(stdout);                       \n\n   fflush(stderr);\n   MPI_Barrier( MPI_COMM_WORLD );        \n\n   MPI_Finalize();                       \n\n \n   return 0;\n}"}
{"program": "YuKill_620", "code": "int main(int argc, char **argv) {\n  int rank, nof_slaves;\n  double out_start, out_end;\n\n\n  nof_slaves = nof_processes -1;\n  if (rank == MASTER) {\n    out_start = get_time();\n\n    int i, j, k;\n    double start, end, stime, rtime; \n\n    get_params(argc, argv);\n    double *a = (double*) malloc(sizeof(double)*(n*n)), \n           *l = (double*) malloc(sizeof(double)*(n*n)),\n           *u = (double*) malloc(sizeof(double)*(n*n)),\n           r[n], var[n], *pkg = NULL;\n    read_input(a, l, u, r);\n\n    start = get_time();\n\n\n    stime = get_time();    \n    for (j = 0; j < n; j++) \n    stime = get_time()-stime;\n\n    for (i = 0; i < n-1; i++) \n\n    double *current = u;\n    rtime = get_time();\n    for (i = 0; i < n; i++) {\n      current += n;\n    }\n    rtime = get_time()-rtime;\n\n    end = get_time();\n\n    \n\n\n    \n\n\n    solve(n, l, u, var, r);\n    if (!verify_ls(n, a, var, r)) {\n      printf(\"Master: error in solving system.\\n\");\n    } \n\n\n    fprint_vector(output, n, var);\n\n    free(a);\n    free(l);\n    free(u);\n\n    out_end = get_time();\n\n    printf(\"%s %d %.4lf %.4lf %.4lf %.4lf\\n\", input_name, nof_slaves, \n        end-start, out_end-out_start, stime, rtime);\n  } else {\n    lu_parallel(n, rank, nof_slaves);\n  }\n\n  \n\n\n}", "label": "int main(int argc, char **argv) {\n  int rank, nof_slaves;\n  double out_start, out_end;\n\n  MPI_Init(NULL, NULL);\n  MPI_Comm_size(MPI_COMM_WORLD, &nof_processes);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  nof_slaves = nof_processes -1;\n  if (rank == MASTER) {\n    out_start = get_time();\n\n    int i, j, k;\n    double start, end, stime, rtime; \n\n    get_params(argc, argv);\n    double *a = (double*) malloc(sizeof(double)*(n*n)), \n           *l = (double*) malloc(sizeof(double)*(n*n)),\n           *u = (double*) malloc(sizeof(double)*(n*n)),\n           r[n], var[n], *pkg = NULL;\n    read_input(a, l, u, r);\n\n    start = get_time();\n\n    MPI_Bcast(&n, 1, MPI_INT, MASTER, MPI_COMM_WORLD);\n\n    stime = get_time();    \n    for (j = 0; j < n; j++) \n      MPI_Send(&u[j*n], n, MPI_DOUBLE, (j % nof_slaves) + 1, DATA, MPI_COMM_WORLD);\n    stime = get_time()-stime;\n\n    for (i = 0; i < n-1; i++) \n      MPI_Bcast(&l[i*n + i+1], n-(i+1), MPI_DOUBLE, (i % nof_slaves) + 1, MPI_COMM_WORLD); \n\n    double *current = u;\n    rtime = get_time();\n    for (i = 0; i < n; i++) {\n      MPI_Recv(current, n, MPI_DOUBLE, (i % nof_slaves) + 1, DATA, \n          MPI_COMM_WORLD, MPI_STATUS_IGNORE); \n      current += n;\n    }\n    rtime = get_time()-rtime;\n\n    end = get_time();\n\n    \n\n\n    \n\n\n    solve(n, l, u, var, r);\n    if (!verify_ls(n, a, var, r)) {\n      printf(\"Master: error in solving system.\\n\");\n    } \n\n\n    fprint_vector(output, n, var);\n\n    free(a);\n    free(l);\n    free(u);\n\n    out_end = get_time();\n\n    printf(\"%s %d %.4lf %.4lf %.4lf %.4lf\\n\", input_name, nof_slaves, \n        end-start, out_end-out_start, stime, rtime);\n  } else {\n    MPI_Bcast(&n, 1, MPI_INT, MASTER, MPI_COMM_WORLD);\n    lu_parallel(n, rank, nof_slaves);\n  }\n\n  \n\n\n  MPI_Finalize();\n}"}
{"program": "SwipeX_622", "code": "int main(int argc, char* argv[]){\n\t\n\n\tint n = 100;\n\tdouble d = 0.85; \n\n\tdouble eps = 0.000000001;\n\tif (argc <= 1 || argc > 4) {\n\t\tprintf(\"usage: %s num-pages damping-factor(0.85) epsilon(0.0001)\\n\", argv[0]);\n\t\treturn 0;\n\t}\n\tif (argc > 1) n = atoi(argv[1]);\n\tif (argc > 2) d = atof(argv[2]);\n\tif (argc > 3) eps = atof(argv[3]);\n\n\t\n\n\tdouble *y0  = (double *) malloc(n*sizeof(double)); \n\n\tdouble *y  = (double *) malloc(n*sizeof(double)); \n\n\tuniform(y0, n);\n\tprintf(\"initial probabilities\\n\");\n\tprintvec(y0, n);\n\n\t\n\n\tstruct SparseMatrixHandle SH;\n\tSparseMatrix S = &SH;\n\trandomLM(S, n); \n\n\tif (n <= 100) writeSM(S, 50);\n\tprintf(\"dimension is %d, nnz is %d, damper is %f, epsilon is %g\\n\\n\", S->coldim, S->nnz, d, eps);\n\n\tdouble *C = (double *) malloc(S->coldim*sizeof(double));\n\tscale(C, S); \n\n\n\tdouble start, elapsed, elapsed2;\n\tint iters; \n\t\n\n\tint size;\n\tint rank;\n\n\t\n\tstart =\n\titers = solve(S, C, d, y0, y, eps, strawman_mvpSM, size, rank);\n\telapsed = MPI_Wtime() - start;\n\tprintf(\"final (page rank) probabilities, %d iterations in time %f\\n\", iters, elapsed);\n\tprintvec(y, n); printf(\"\\n\");\n\n\t\n\n\tfree(C); free(y); free(y0); free(SH.row); free(SH.col); free(SH.val);\n\treturn 0;\n}", "label": "int main(int argc, char* argv[]){\n\t\n\n\tint n = 100;\n\tdouble d = 0.85; \n\n\tdouble eps = 0.000000001;\n\tif (argc <= 1 || argc > 4) {\n\t\tprintf(\"usage: %s num-pages damping-factor(0.85) epsilon(0.0001)\\n\", argv[0]);\n\t\treturn 0;\n\t}\n\tif (argc > 1) n = atoi(argv[1]);\n\tif (argc > 2) d = atof(argv[2]);\n\tif (argc > 3) eps = atof(argv[3]);\n\n\t\n\n\tdouble *y0  = (double *) malloc(n*sizeof(double)); \n\n\tdouble *y  = (double *) malloc(n*sizeof(double)); \n\n\tuniform(y0, n);\n\tprintf(\"initial probabilities\\n\");\n\tprintvec(y0, n);\n\n\t\n\n\tstruct SparseMatrixHandle SH;\n\tSparseMatrix S = &SH;\n\trandomLM(S, n); \n\n\tif (n <= 100) writeSM(S, 50);\n\tprintf(\"dimension is %d, nnz is %d, damper is %f, epsilon is %g\\n\\n\", S->coldim, S->nnz, d, eps);\n\n\tdouble *C = (double *) malloc(S->coldim*sizeof(double));\n\tscale(C, S); \n\n\n\tMPI_Init(0,0);\n\tdouble start, elapsed, elapsed2;\n\tint iters; \n\t\n\n\tint size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t\n\tstart = MPI_Wtime();\n\titers = solve(S, C, d, y0, y, eps, strawman_mvpSM, size, rank);\n\telapsed = MPI_Wtime() - start;\n\tprintf(\"final (page rank) probabilities, %d iterations in time %f\\n\", iters, elapsed);\n\tprintvec(y, n); printf(\"\\n\");\n\n\t\n\n\tfree(C); free(y); free(y0); free(SH.row); free(SH.col); free(SH.val);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "blue42u_627", "code": "int\nmain(int argc, char *argv[])\n{\n  int rc;\n\n  int am_server;\n\n  int num_types = 1;\n  int type_vect[2] = { CMDLINE };\n\n\n  printf(\"batcher...\\n\");\n\n  rc =\n  assert(rc == MPI_SUCCESS);\n\n  int my_world_rank, worker_rank;\n\n  int num_servers = 1;\n\n  MPI_Comm worker_comm;\n  rc = ADLB_Init(num_servers, num_types, type_vect, &am_server,\n                  MPI_COMM_WORLD, &worker_comm);\n  if (! am_server)\n    \n\n\n  double start_time =\n\n  if (am_server )\n  {\n    \n\n    ADLB_Server(3000000);\n  }\n  else\n  {\n    \n\n    if (worker_rank == 0)\n      \n\n      put_commands();\n\n    \n\n    \n\n    worker_loop();\n\n    if (worker_rank == 0)\n    {\n      double end_time =\n      printf(\"TOOK: %.3f\\n\", end_time-start_time);\n    }\n  }\n\n  ADLB_Finalize();\n  return 0;\n}", "label": "int\nmain(int argc, char *argv[])\n{\n  int rc;\n\n  int am_server;\n\n  int num_types = 1;\n  int type_vect[2] = { CMDLINE };\n\n\n  printf(\"batcher...\\n\");\n\n  rc = MPI_Init( &argc, &argv );\n  assert(rc == MPI_SUCCESS);\n\n  int my_world_rank, worker_rank;\n  MPI_Comm_rank( MPI_COMM_WORLD, &my_world_rank );\n\n  int num_servers = 1;\n\n  MPI_Comm worker_comm;\n  rc = ADLB_Init(num_servers, num_types, type_vect, &am_server,\n                  MPI_COMM_WORLD, &worker_comm);\n  if (! am_server)\n    \n\n    MPI_Comm_rank(worker_comm, &worker_rank);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  double start_time = MPI_Wtime();\n\n  if (am_server )\n  {\n    \n\n    ADLB_Server(3000000);\n  }\n  else\n  {\n    \n\n    if (worker_rank == 0)\n      \n\n      put_commands();\n\n    \n\n    \n\n    worker_loop();\n\n    if (worker_rank == 0)\n    {\n      double end_time = MPI_Wtime();\n      printf(\"TOOK: %.3f\\n\", end_time-start_time);\n    }\n  }\n\n  ADLB_Finalize();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "konstantin-psu_630", "code": "int main(int argc, char *argv[])\n{\n  int cnt=4, buf[4], i, rank;\n  char fname[10];\n  MPI_File fh;\n  MPI_Status st;\n\n\n  \n\n  for (i=0; i<cnt; i++) \n    buf[i] = rank*100 + i;\n  \n\n  sprintf(fname, \"%s.%d\", \"out\", \"gen\");\n  \n\n  \n\n  \n\n\n  return 0; \n}", "label": "int main(int argc, char *argv[])\n{\n  int cnt=4, buf[4], i, rank;\n  char fname[10];\n  MPI_File fh;\n  MPI_Status st;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  for (i=0; i<cnt; i++) \n    buf[i] = rank*100 + i;\n  \n\n  sprintf(fname, \"%s.%d\", \"out\", \"gen\");\n  \n\n  MPI_File_open(MPI_COMM_SELF, fname, MPI_MODE_CREATE|MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n  \n\n  MPI_File_set_view(fh, rank*16, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);\n  \n\n  MPI_File_write(fh, buf, cnt, MPI_INT, &st);\n  MPI_File_close(&fh);\n\n  MPI_Finalize();\n  return 0; \n}"}
{"program": "nschloe_631", "code": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag; \n  MPI_Status status;\n\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  \n  printf(\"%s\\n\", message2);\n\n  return 0;\n}", "label": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag; \n  MPI_Status status;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  MPI_Sendrecv(NULL, strlen(message1)+1, MPI_CHAR, dest, tag, message2, 50, MPI_CHAR, source, tag, MPI_COMM_WORLD, &status);\n  \n  printf(\"%s\\n\", message2);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "hunsa_632", "code": "int main(int argc, char *argv[])\n{\n    int i, j, count, k;\n    int n_procs = 0;\n    void *send_buffer, *recv_buffer;\n    int rank;\n    char* meas_functions[] = {\"MPI_Alltoall\", \"MPI_Bcast\"};\n    int n_calls = 2;\n    int max_count = 50;\n    int inc_count=10;\n    int call_rep = 4;\n    \n\n\n\n\n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    for (i=0; i<n_calls; i++) {\n        for (j=1; j< call_rep; j++ ) {\n            for (count=0; count<max_count; count+=inc_count) {\n\n                \n\n                \n\n\n                send_buffer = malloc( n_procs * count);\n                recv_buffer = malloc( n_procs * count);\n\n                \n\n                \n\n\n                \n\n                for (k=0; k< j; k++) {\n                    if (k == 1) {\n                        \n\n                    }\n                    if (strcmp(meas_functions[i], \"MPI_Bcast\") == 0) {\n                    }\n                    else {\n                    }\n                }\n                if (k == 1) {\n                    \n\n                }\n                \n\n\n                \n\n                \n\n\n                \n\n\n                \n\n                \n\n\n                free( send_buffer);\n                send_buffer = NULL;\n                free( recv_buffer);\n                recv_buffer = NULL;\n            }\n        }\n    }\n\n    \n\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int i, j, count, k;\n    int n_procs = 0;\n    void *send_buffer, *recv_buffer;\n    int rank;\n    char* meas_functions[] = {\"MPI_Alltoall\", \"MPI_Bcast\"};\n    int n_calls = 2;\n    int max_count = 50;\n    int inc_count=10;\n    int call_rep = 4;\n    \n\n\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size( MPI_COMM_WORLD, &n_procs);\n\n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    for (i=0; i<n_calls; i++) {\n        for (j=1; j< call_rep; j++ ) {\n            for (count=0; count<max_count; count+=inc_count) {\n\n                \n\n                \n\n\n                send_buffer = malloc( n_procs * count);\n                recv_buffer = malloc( n_procs * count);\n\n                \n\n                \n\n\n                \n\n                for (k=0; k< j; k++) {\n                    if (k == 1) {\n                        \n\n                    }\n                    if (strcmp(meas_functions[i], \"MPI_Bcast\") == 0) {\n                        MPI_Bcast(send_buffer, count, MPI_BYTE, 0, MPI_COMM_WORLD);\n                    }\n                    else {\n                        MPI_Alltoall( send_buffer, count, MPI_BYTE, recv_buffer,\n                                count, MPI_BYTE, MPI_COMM_WORLD);\n                    }\n                }\n                if (k == 1) {\n                    \n\n                }\n                \n\n\n                \n\n                \n\n\n                \n\n\n                \n\n                \n\n\n                free( send_buffer);\n                send_buffer = NULL;\n                free( recv_buffer);\n                recv_buffer = NULL;\n            }\n        }\n    }\n\n    \n\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mpip_633", "code": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3], N[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  fftw_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_r2r(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_r2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = fftw_alloc_real(alloc_local);\n  out = fftw_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_r2r(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_r2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real_3d(ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real_3d(ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_real_3d(ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  fftw_free(in); fftw_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3], N[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  fftw_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_r2r(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_r2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = fftw_alloc_real(alloc_local);\n  out = fftw_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_r2r(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_r2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real_3d(ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real_3d(ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real_3d(ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  fftw_free(in); fftw_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "mpip_634", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = in;\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = in;\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_635", "code": "nt main(int argc, char **argv)\n{\n    int i, num_posted, num_completed;\n    int wrank, wsize;\n    unsigned int seed = 0x10bc;\n    unsigned int post_seq, complete_seq;\n#if defined(TEST_NBC_ROUTINES)\n    struct laundry larr[WINDOW];\n#endif\n    MPI_Request reqs[WINDOW];\n    int outcount;\n    int indices[WINDOW];\n    MPI_Comm comms[NUM_COMMS];\n    MPI_Comm comm;\n\n\n#if defined(TEST_NBC_ROUTINES)\n\n    \n\n    post_seq = complete_seq = gen_prn(seed);\n\n    num_completed = 0;\n    num_posted = 0;\n\n    \n\n    for (i = 0; i < NUM_COMMS; ++i) {\n    }\n\n    \n\n    for (i = 0; i < WINDOW; ++i) {\n        reqs[i] = MPI_REQUEST_NULL;\n        memset(&larr[i], 0, sizeof(struct laundry));\n        larr[i].case_num = -1;\n\n        \n\n        comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n\n        start_random_nonblocking(comm, post_seq, &reqs[i], &larr[i]);\n        ++num_posted;\n        post_seq = gen_prn(post_seq);\n    }\n\n    \n\n    while (num_completed < MAIN_ITERATIONS) {\n        complete_something_somehow(complete_seq, WINDOW, reqs, &outcount, indices);\n        complete_seq = gen_prn(complete_seq);\n        for (i = 0; i < outcount; ++i) {\n            int idx = indices[i];\n            assert(reqs[idx] == MPI_REQUEST_NULL);\n            if (larr[idx].case_num != -1) {\n                check_after_completion(&larr[idx]);\n                cleanup_laundry(&larr[idx]);\n                ++num_completed;\n                if (num_posted < MAIN_ITERATIONS) {\n                    comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n                    start_random_nonblocking(comm, post_seq, &reqs[idx], &larr[idx]);\n                    ++num_posted;\n                    post_seq = gen_prn(post_seq);\n                }\n            }\n        }\n\n        \n\n        if (0 == rand_range(gen_prn(complete_seq + wrank), 0, CHANCE_OF_SLEEP)) {\n            usleep(JITTER_DELAY); \n\n        }\n    }\n\n    for (i = 0; i < NUM_COMMS; ++i) {\n    }\n\n#endif \n\n\n    if (wrank == 0) {\n        if (errs)\n            printf(\"found %d errors\\n\", errs);\n        else\n            printf(\" No errors\\n\");\n    }\n\n\n    return 0;\n}\n\n", "label": "nt main(int argc, char **argv)\n{\n    int i, num_posted, num_completed;\n    int wrank, wsize;\n    unsigned int seed = 0x10bc;\n    unsigned int post_seq, complete_seq;\n#if defined(TEST_NBC_ROUTINES)\n    struct laundry larr[WINDOW];\n#endif\n    MPI_Request reqs[WINDOW];\n    int outcount;\n    int indices[WINDOW];\n    MPI_Comm comms[NUM_COMMS];\n    MPI_Comm comm;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &wrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &wsize);\n\n#if defined(TEST_NBC_ROUTINES)\n\n    \n\n    post_seq = complete_seq = gen_prn(seed);\n\n    num_completed = 0;\n    num_posted = 0;\n\n    \n\n    for (i = 0; i < NUM_COMMS; ++i) {\n        MPI_Comm_dup(MPI_COMM_WORLD, &comms[i]);\n    }\n\n    \n\n    for (i = 0; i < WINDOW; ++i) {\n        reqs[i] = MPI_REQUEST_NULL;\n        memset(&larr[i], 0, sizeof(struct laundry));\n        larr[i].case_num = -1;\n\n        \n\n        comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n\n        start_random_nonblocking(comm, post_seq, &reqs[i], &larr[i]);\n        ++num_posted;\n        post_seq = gen_prn(post_seq);\n    }\n\n    \n\n    while (num_completed < MAIN_ITERATIONS) {\n        complete_something_somehow(complete_seq, WINDOW, reqs, &outcount, indices);\n        complete_seq = gen_prn(complete_seq);\n        for (i = 0; i < outcount; ++i) {\n            int idx = indices[i];\n            assert(reqs[idx] == MPI_REQUEST_NULL);\n            if (larr[idx].case_num != -1) {\n                check_after_completion(&larr[idx]);\n                cleanup_laundry(&larr[idx]);\n                ++num_completed;\n                if (num_posted < MAIN_ITERATIONS) {\n                    comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n                    start_random_nonblocking(comm, post_seq, &reqs[idx], &larr[idx]);\n                    ++num_posted;\n                    post_seq = gen_prn(post_seq);\n                }\n            }\n        }\n\n        \n\n        if (0 == rand_range(gen_prn(complete_seq + wrank), 0, CHANCE_OF_SLEEP)) {\n            usleep(JITTER_DELAY); \n\n        }\n    }\n\n    for (i = 0; i < NUM_COMMS; ++i) {\n        MPI_Comm_free(&comms[i]);\n    }\n\n#endif \n\n\n    if (wrank == 0) {\n        if (errs)\n            printf(\"found %d errors\\n\", errs);\n        else\n            printf(\" No errors\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}\n\n"}
{"program": "mellanox-hpc_638", "code": "int run_mpi(int argc, char **argv)\n{\n#if HAVE_MPICC\n    int num_ranks;\n    int ret;\n\n    \n\n    if (isatty(0)) {\n        return -ENOTSUP;\n    }\n\n    ret =\n    if (ret != 0) {\n        return -ENOTSUP;\n    }\n\n    \n\n    if (num_ranks == 1) {\n        ret = -ENOTSUP;\n        goto out;\n    }\n\n\n    if (my_rank < 2) {\n    \t\n\n    \t\n\n    \tret = run_task_pingpong_app(my_rank, QP_EXCHANGE_OVER_IB);\n    } else {\n    \tfprintf(stderr, \"%s Rank %d waiting in barrier.  only 2 ranks supported\\n\", hostname, my_rank);\n    }\n\n\nout:\n    return ret;\n#else\n    return -ENOTSUP;\n#endif\n}", "label": "int run_mpi(int argc, char **argv)\n{\n#if HAVE_MPICC\n    int num_ranks;\n    int ret;\n\n    \n\n    if (isatty(0)) {\n        return -ENOTSUP;\n    }\n\n    ret = MPI_Init(&argc, &argv);\n    if (ret != 0) {\n        return -ENOTSUP;\n    }\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    if (num_ranks == 1) {\n        ret = -ENOTSUP;\n        goto out;\n    }\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (my_rank < 2) {\n    \t\n\n    \t\n\n    \tret = run_task_pingpong_app(my_rank, QP_EXCHANGE_OVER_IB);\n    } else {\n    \tfprintf(stderr, \"%s Rank %d waiting in barrier.  only 2 ranks supported\\n\", hostname, my_rank);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\nout:\n    MPI_Finalize();\n    return ret;\n#else\n    return -ENOTSUP;\n#endif\n}"}
{"program": "leiverandres_639", "code": "int main() {\n  int world_rank = -1, world_size = -1;\n  int chunk_cols = COLB / (world_size - 1);\n  int offset = COLB % (world_size - 1);\n\n  if (world_rank == MASTER) {\n    int *mat_a = (int *) malloc(ROWA * COLA * sizeof(int));\n    int *mat_b = (int *) malloc(ROWB * COLB * sizeof(int));\n    int *mat_c = (int *) malloc(ROWA * COLB * sizeof(int));\n    int *trans_b = (int *) malloc(COLB * ROWB * sizeof(int));\n    int *trans_c = (int *) malloc(COLB * ROWA * sizeof(int));\n    fillRandMat(mat_a, ROWA, COLA);\n    fillRandMat(mat_b, ROWB, COLB);\n    trans(mat_b, trans_b, ROWB, COLB);\n    showMat(mat_a, ROWA, COLA, \"A\");\n    showMat(mat_b, ROWB, COLB, \"B\");\n    for (int i = 1; i < world_size; i++) {\n    }\n\n    for (int i = 1; i < world_size; i++) {\n    }\n\n    if (offset) {\n      for (int i = 0; i < offset; i++) {\n        matrixVecMult(mat_a, trans_b + (COLB - offset + i) * COLA, trans_c + (COLB - offset + i) * ROWA);\n      }\n    }\n\n    trans(trans_c, mat_c, COLB, ROWA);\n    showMat(mat_c, ROWA, COLB, \"C\");\n    free(mat_a); free(mat_b); free(mat_c);\n    free(trans_b); free(trans_c);\n  } else {\n    int *mat_a_proc = (int *) malloc(ROWA * COLA * sizeof(int));\n    int *cols = (int *) malloc(chunk_cols * ROWB * sizeof(int));\n    int *result = (int *) malloc(ROWA * chunk_cols * sizeof(int));\n    for (int i = 0; i < chunk_cols; i++) {\n      matrixVecMult(mat_a_proc, cols + i * COLA, result + i * ROWA);\n    }\n    free(mat_a_proc);\n    free(cols);\n    free(result);\n  }\n  return 0;\n}", "label": "int main() {\n  int world_rank = -1, world_size = -1;\n  MPI_Init(NULL, NULL);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int chunk_cols = COLB / (world_size - 1);\n  int offset = COLB % (world_size - 1);\n\n  if (world_rank == MASTER) {\n    int *mat_a = (int *) malloc(ROWA * COLA * sizeof(int));\n    int *mat_b = (int *) malloc(ROWB * COLB * sizeof(int));\n    int *mat_c = (int *) malloc(ROWA * COLB * sizeof(int));\n    int *trans_b = (int *) malloc(COLB * ROWB * sizeof(int));\n    int *trans_c = (int *) malloc(COLB * ROWA * sizeof(int));\n    fillRandMat(mat_a, ROWA, COLA);\n    fillRandMat(mat_b, ROWB, COLB);\n    trans(mat_b, trans_b, ROWB, COLB);\n    showMat(mat_a, ROWA, COLA, \"A\");\n    showMat(mat_b, ROWB, COLB, \"B\");\n    for (int i = 1; i < world_size; i++) {\n      MPI_Send(mat_a, ROWA * COLA, MPI_INT, i, MULT_TAG, MPI_COMM_WORLD);\n      MPI_Send(trans_b + (i - 1) * (chunk_cols * ROWB), chunk_cols * ROWB, MPI_INT, i, MULT_TAG, MPI_COMM_WORLD);\n    }\n\n    for (int i = 1; i < world_size; i++) {\n      MPI_Recv(trans_c + (i - 1) * (chunk_cols * ROWA), chunk_cols * ROWA, MPI_INT, i, RESULT_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (offset) {\n      for (int i = 0; i < offset; i++) {\n        matrixVecMult(mat_a, trans_b + (COLB - offset + i) * COLA, trans_c + (COLB - offset + i) * ROWA);\n      }\n    }\n\n    trans(trans_c, mat_c, COLB, ROWA);\n    showMat(mat_c, ROWA, COLB, \"C\");\n    free(mat_a); free(mat_b); free(mat_c);\n    free(trans_b); free(trans_c);\n  } else {\n    int *mat_a_proc = (int *) malloc(ROWA * COLA * sizeof(int));\n    int *cols = (int *) malloc(chunk_cols * ROWB * sizeof(int));\n    int *result = (int *) malloc(ROWA * chunk_cols * sizeof(int));\n    MPI_Recv(mat_a_proc, ROWA * COLA, MPI_INT, MASTER, MULT_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(cols, chunk_cols * ROWB, MPI_INT, MASTER, MULT_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for (int i = 0; i < chunk_cols; i++) {\n      matrixVecMult(mat_a_proc, cols + i * COLA, result + i * ROWA);\n    }\n    MPI_Send(result, chunk_cols * ROWA, MPI_INT, MASTER, RESULT_TAG, MPI_COMM_WORLD); \n    free(mat_a_proc);\n    free(cols);\n    free(result);\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "indiependente_640", "code": "int main(int argc, char *argv[])\n{\n    int rank, size;\n    int i;\n    char c[100];\n    char buffer[110];\n    int position = 0;\n    MPI_Status status;\n\n    if (size < 2)\n    {\n        printf(\"Please run with 2 processes.\\n\");fflush(stdout);\n        return 1;\n    }\n\n    if (rank == 0)\n    {\n        for (i=0; i<100; i++)\n            c[i] = i;\n        i = 123;\n    }\n\n    if (rank == 1)\n    {\n        printf(\"i=%d\\nc[0] = %d\\n...\\nc[99] = %d\\n\", i, (int)c[0], (int)c[99]);fflush(stdout);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, size;\n    int i;\n    char c[100];\n    char buffer[110];\n    int position = 0;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size < 2)\n    {\n        printf(\"Please run with 2 processes.\\n\");fflush(stdout);\n        MPI_Finalize();\n        return 1;\n    }\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0)\n    {\n        for (i=0; i<100; i++)\n            c[i] = i;\n        i = 123;\n        MPI_Pack(&i, 1, MPI_INT, buffer, 110, &position, MPI_COMM_WORLD);\n        MPI_Pack(c, 100, MPI_CHAR, buffer, 110, &position, MPI_COMM_WORLD);\n        MPI_Send(buffer, position, MPI_PACKED, 1, 100, MPI_COMM_WORLD);\n    }\n\n    if (rank == 1)\n    {\n        MPI_Recv(buffer, 110, MPI_PACKED, 0, 100, MPI_COMM_WORLD, &status);\n        MPI_Unpack(buffer, 110, &position, &i, 1, MPI_INT, MPI_COMM_WORLD);\n        MPI_Unpack(buffer, 110, &position, c, 100, MPI_CHAR, MPI_COMM_WORLD);\n        printf(\"i=%d\\nc[0] = %d\\n...\\nc[99] = %d\\n\", i, (int)c[0], (int)c[99]);fflush(stdout);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "lstorchi_642", "code": "int main (int argc, char ** argv)\n{\n  int i, j, n, rank, size, rlen; \n  char myname[MPI_MAX_PROCESSOR_NAME];\n\n  double * d, stime, etime, dim;\n\n  MPI_Status status;\n\n\n\n  if (argc == 2)\n    n = atoi (argv[1]);\n  else\n    n = 10;\n\n  fprintf(stdout, \"Hello world from process %d of %d [%s]\\n\", rank, size, \n          myname);\n\n  srand (10);\n\n  dim = (n * n * sizeof(double)) / (1024.0*1024.0*1024.0);\n  if ( (d = (double *) malloc (sizeof (double) * n * n)) == NULL) \n  {\n    fprintf (stderr, \"Memory allocation error\\n\");\n    return EXIT_FAILURE;\n  }\n\n  if (rank == 0) \n  {\n    for (i=0; i<n; i++)\n      for (j=0; j<n; j++)\n        d[(i*n) + j] = 1.0e0 + (double) (1.0e0 * n * (rand() / (RAND_MAX + 1.0e0)));   \n    fprintf (stdout, \"Sending: %f GB\\n\", dim);\n  }\n\n\n  if (rank == 0) \n  {\n    stime =\n    etime =\n    fprintf (stdout, \"Send time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n\n    stime =\n    etime =\n    fprintf (stdout, \"Send time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime)); \n\n    stime =\n    etime =\n    fprintf (stdout, \"Recv time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n  }\n  else \n  {\n    stime =\n    etime =\n    fprintf (stdout, \"Recv time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n\n    stime =\n    etime =\n    fprintf (stdout, \"Recv time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n\n    stime =\n    etime =\n    fprintf (stdout, \"Send time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime)); \n  }\n\n\n\n  free (d);\n\n  return 0;\n}", "label": "int main (int argc, char ** argv)\n{\n  int i, j, n, rank, size, rlen; \n  char myname[MPI_MAX_PROCESSOR_NAME];\n\n  double * d, stime, etime, dim;\n\n  MPI_Status status;\n\n  MPI_Init (&argc, &argv);   \n  MPI_Comm_rank (MPI_COMM_WORLD, &rank); \n  MPI_Comm_size (MPI_COMM_WORLD, &size); \n\n  MPI_Get_processor_name (myname, &rlen);\n\n  if (argc == 2)\n    n = atoi (argv[1]);\n  else\n    n = 10;\n\n  fprintf(stdout, \"Hello world from process %d of %d [%s]\\n\", rank, size, \n          myname);\n\n  srand (10);\n\n  dim = (n * n * sizeof(double)) / (1024.0*1024.0*1024.0);\n  if ( (d = (double *) malloc (sizeof (double) * n * n)) == NULL) \n  {\n    fprintf (stderr, \"Memory allocation error\\n\");\n    return EXIT_FAILURE;\n  }\n\n  if (rank == 0) \n  {\n    for (i=0; i<n; i++)\n      for (j=0; j<n; j++)\n        d[(i*n) + j] = 1.0e0 + (double) (1.0e0 * n * (rand() / (RAND_MAX + 1.0e0)));   \n    fprintf (stdout, \"Sending: %f GB\\n\", dim);\n  }\n\n  MPI_Barrier (MPI_COMM_WORLD);\n\n  if (rank == 0) \n  {\n    stime = MPI_Wtime ();\n    MPI_Ssend (d, n*n, MPI_DOUBLE, 1, 1, MPI_COMM_WORLD);\n    etime = MPI_Wtime ();\n    fprintf (stdout, \"Send time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n\n    stime = MPI_Wtime ();\n    MPI_Ssend (d, n*n, MPI_DOUBLE, 1, 2, MPI_COMM_WORLD);\n    etime = MPI_Wtime ();\n    fprintf (stdout, \"Send time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime)); \n\n    stime = MPI_Wtime ();\n    MPI_Recv (d, n*n, MPI_DOUBLE, 1, 3, MPI_COMM_WORLD, \n        &status);\n    etime = MPI_Wtime ();\n    fprintf (stdout, \"Recv time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n  }\n  else \n  {\n    stime = MPI_Wtime ();\n    MPI_Recv (d, n*n, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, \n        &status);\n    etime = MPI_Wtime ();\n    fprintf (stdout, \"Recv time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n\n    stime = MPI_Wtime ();\n    MPI_Recv (d, n*n, MPI_DOUBLE, 0, 2, MPI_COMM_WORLD, \n        &status);\n    etime = MPI_Wtime ();\n    fprintf (stdout, \"Recv time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime));\n\n    stime = MPI_Wtime ();\n    MPI_Ssend (d, n*n, MPI_DOUBLE, 0, 3, MPI_COMM_WORLD);\n    etime = MPI_Wtime ();\n    fprintf (stdout, \"Send time 3: %f s [%f GB/s] \\n\", etime - stime, \n            dim/(etime - stime)); \n  }\n\n  MPI_Barrier (MPI_COMM_WORLD);\n\n  MPI_Finalize ();\n\n  free (d);\n\n  return 0;\n}"}
{"program": "byu-vv-lab_643", "code": "int main(int argc, char *argv[]) {\n  long double *a;\n\n#ifndef _CIVL\n  if(argc < 3) \n    printf(\"Expecting the arguments: numberOfRows  numberOfColumns\\n\");\n  numRow = atoi(argv[1]);\n  numCol = atoi(argv[2]);\n  \n\n  \n\n  \n\n#endif\n  first = firstForProc(rank);\n  localRow = countForProc(rank);\n  a = (long double*)malloc(numCol*localRow*sizeof(long double));\n  loc = (int*)malloc(numRow*sizeof(int));\n  idx = (int*)malloc(numRow*sizeof(int));\n  initialization(argc, argv, a, loc, idx);\n  gaussianElimination(a);\n  backwardReduce(a);\n  if(!rank)printf(\"After backward reduction, the matrix in reduced row echelon form is:\\n\");\n  printSystem(a);\n  free(loc);\n  free(idx);\n  free(a);\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  long double *a;\n\n#ifndef _CIVL\n  if(argc < 3) \n    printf(\"Expecting the arguments: numberOfRows  numberOfColumns\\n\");\n  numRow = atoi(argv[1]);\n  numCol = atoi(argv[2]);\n  \n\n  \n\n  \n\n#endif\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  first = firstForProc(rank);\n  localRow = countForProc(rank);\n  a = (long double*)malloc(numCol*localRow*sizeof(long double));\n  loc = (int*)malloc(numRow*sizeof(int));\n  idx = (int*)malloc(numRow*sizeof(int));\n  initialization(argc, argv, a, loc, idx);\n  gaussianElimination(a);\n  backwardReduce(a);\n  if(!rank)printf(\"After backward reduction, the matrix in reduced row echelon form is:\\n\");\n  printSystem(a);\n  MPI_Finalize();\n  free(loc);\n  free(idx);\n  free(a);\n  return 0;\n}"}
{"program": "JohnPJenkins_644", "code": "int\nmain(int argc, char** argv)\n{\n  int rc;\n\n  char* filename = \"tests/mpi-io.data\";\n  MPI_File file;\n\n\n  int mpi_size;\n  int mpi_rank;\n\n  printf(\"mpi: %i/%i\\n\", mpi_rank, mpi_size);\n\n  MPI_Offset size = 100;\n\n  if (mpi_rank == 0)\n  {\n    struct stat s;\n    rc = stat(filename, &s);\n    size = s.st_size;\n  }\n  int size_int = (int) size;\n  printf(\"file size: %i\\n\", size_int);\n\n  int chunk_size = 4;\n  MPI_Datatype chunk;\n  rc =\n\n  int chunks = (int)size/chunk_size + 1;\n  MPI_Datatype strides;\n  \n\n  \n\n  rc =\n\n  rc =\n\n  int disp = mpi_rank*chunk_size;\n  rc =\n\n  char buffer[10000]; \n\n  memset(buffer, '\\0', 10000);\n\n  printf(\"read\\n\");\n\n  MPI_Status status;\n\n  rc =\n  int r;\n  printf(\"r: %i\\n\", r);\n\n  \n\n  printf(\"c: '%s'\\n\", buffer);\n  \n\n\n  \n\n\n\n\n  return 0;\n}", "label": "int\nmain(int argc, char** argv)\n{\n  int rc;\n\n  char* filename = \"tests/mpi-io.data\";\n  MPI_File file;\n\n  MPI_Init(&argc, &argv);\n\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  int mpi_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  printf(\"mpi: %i/%i\\n\", mpi_rank, mpi_size);\n\n  MPI_Offset size = 100;\n\n  if (mpi_rank == 0)\n  {\n    struct stat s;\n    rc = stat(filename, &s);\n    size = s.st_size;\n  }\n  MPI_Bcast(&size, sizeof(MPI_Offset), MPI_BYTE, 0, MPI_COMM_WORLD);\n  int size_int = (int) size;\n  printf(\"file size: %i\\n\", size_int);\n\n  int chunk_size = 4;\n  MPI_Datatype chunk;\n  rc = MPI_Type_contiguous(chunk_size, MPI_BYTE, &chunk);\n  MPI_ASSERT(rc);\n  MPI_Type_commit(&chunk);\n\n  int chunks = (int)size/chunk_size + 1;\n  MPI_Datatype strides;\n  \n\n  \n\n  rc = MPI_Type_vector(chunks, 1, 2, chunk, &strides);\n  MPI_ASSERT(rc);\n  MPI_Type_commit(&strides);\n\n  rc = MPI_File_open(MPI_COMM_WORLD, filename,\n\t\t     MPI_MODE_RDONLY,\n\t\t     MPI_INFO_NULL, &file);\n  MPI_ASSERT_MSG(rc, \"could not open file\");\n\n  int disp = mpi_rank*chunk_size;\n  rc = MPI_File_set_view(file, disp, MPI_BYTE, strides,\n\t\t\t \"native\", MPI_INFO_NULL);\n  MPI_ASSERT(rc);\n\n  char buffer[10000]; \n\n  memset(buffer, '\\0', 10000);\n\n  printf(\"read\\n\");\n\n  MPI_Status status;\n\n  rc = MPI_File_read_all(file, buffer, chunk_size*2, MPI_BYTE, &status);\n  MPI_ASSERT(rc);\n  int r;\n  MPI_Get_count(&status, MPI_BYTE, &r);\n  printf(\"r: %i\\n\", r);\n\n  \n\n  printf(\"c: '%s'\\n\", buffer);\n  \n\n\n  \n\n\n  MPI_Type_free(&strides);\n  MPI_Type_free(&chunk);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "aarestad_645", "code": "int main(int argc, char* argv[]) {\n    int rank;\n    int size;\n    int i, j;\n    int numrows;\n    MPI_Datatype matrixcol;\n    MPI_Status status;\n\n\n    if (rank == 0) {\n        printf(\"Number of rows: \");\n        scanf(\"%d\", &numrows);\n    }\n\n\n    if (rank == 0) {\n        int matrix[numrows][size];\n\n        for (i = 0; i < size; ++i) {\n            printf(\"process 0: row %d: \", i);\n            for (j = 0; j < size; ++j) {\n                matrix[i][j] = j + i*size;\n                printf(\"[%d]\", matrix[i][j]);\n            }\n            printf(\"\\n\");\n        }\n        for (i = 1; i < size; ++i) {\n            printf(\"sent column %d to proc %d\\n\", i, i);\n        }\n    } else {\n        int col[numrows];\n        printf(\"process %d: received column %d: \", rank, rank);\n        for (i = 0; i < size; ++i) {\n            printf(\"[%d]\", col[i]);\n        }\n        printf(\"\\n\");\n    }\n\n    return 0;\n}", "label": "int main(int argc, char* argv[]) {\n    int rank;\n    int size;\n    int i, j;\n    int numrows;\n    MPI_Datatype matrixcol;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        printf(\"Number of rows: \");\n        scanf(\"%d\", &numrows);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        int matrix[numrows][size];\n        MPI_Type_vector(numrows, 1, size, MPI_INT, &matrixcol);\n        MPI_Type_commit(&matrixcol);\n\n        for (i = 0; i < size; ++i) {\n            printf(\"process 0: row %d: \", i);\n            for (j = 0; j < size; ++j) {\n                matrix[i][j] = j + i*size;\n                printf(\"[%d]\", matrix[i][j]);\n            }\n            printf(\"\\n\");\n        }\n        for (i = 1; i < size; ++i) {\n            MPI_Send(matrix[0] + i, 1, matrixcol, i, 0, MPI_COMM_WORLD);\n            printf(\"sent column %d to proc %d\\n\", i, i);\n        }\n    } else {\n        int col[numrows];\n        MPI_Recv(col, numrows, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        printf(\"process %d: received column %d: \", rank, rank);\n        for (i = 0; i < size; ++i) {\n            printf(\"[%d]\", col[i]);\n        }\n        printf(\"\\n\");\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ClaudioNahmad_646", "code": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    MPI_Status status;\n    MPI_Offset size;\n    long long *buf, i;\n    char *filename;\n    int j, mynod, nprocs, len, flag, err;\n\n\n\n    if (nprocs != 1) {\n\tfprintf(stderr, \"Run this program on one process only\\n\");\n    }\n\n    i = 1;\n    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n\ti++;\n\targv++;\n    }\n    if (i >= argc) {\n\tfprintf(stderr, \"\\n*#  Usage: large -fname filename\\n\\n\");\n    }\n    argv++;\n    len = strlen(*argv);\n    filename = (char *) malloc(len+1);\n    strcpy(filename, *argv);\n    fprintf(stderr, \"This program creates an 4 Gbyte file. Don't run it if you don't have that much disk space!\\n\");\n\n    buf = (long long *) malloc(SIZE * sizeof(long long));\n    if (!buf) {\n\tfprintf(stderr, \"not enough memory to allocate buffer\\n\");\n    }\n\n\n    for (i=0; i<NTIMES; i++) {\n\tfor (j=0; j<SIZE; j++)\n\t    buf[j] = i*SIZE + j;\n\n\terr =\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t}\n    }\n\n    fprintf(stderr, \"file size = %lld bytes\\n\", size);\n\n\n    for (j=0; j<SIZE; j++) buf[j] = -1;\n\n    flag = 0;\n    for (i=0; i<NTIMES; i++) {\n\terr =\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t}\n\tfor (j=0; j<SIZE; j++)\n\t    if (buf[j] != i*SIZE + j) {\n\t\tfprintf(stderr, \"error: buf %d is %lld, should be %lld \\n\", j, buf[j],\n                                 i*SIZE + j);\n\t\tflag = 1;\n\t    }\n    }\n\n    if (!flag) fprintf(stderr, \"Data read back is correct\\n\");\n\n    free(buf);\n    free(filename);\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    MPI_Status status;\n    MPI_Offset size;\n    long long *buf, i;\n    char *filename;\n    int j, mynod, nprocs, len, flag, err;\n\n    MPI_Init(&argc,&argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    if (nprocs != 1) {\n\tfprintf(stderr, \"Run this program on one process only\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    i = 1;\n    while ((i < argc) && strcmp(\"-fname\", *argv)) {\n\ti++;\n\targv++;\n    }\n    if (i >= argc) {\n\tfprintf(stderr, \"\\n*#  Usage: large -fname filename\\n\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    argv++;\n    len = strlen(*argv);\n    filename = (char *) malloc(len+1);\n    strcpy(filename, *argv);\n    fprintf(stderr, \"This program creates an 4 Gbyte file. Don't run it if you don't have that much disk space!\\n\");\n\n    buf = (long long *) malloc(SIZE * sizeof(long long));\n    if (!buf) {\n\tfprintf(stderr, \"not enough memory to allocate buffer\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR,\n                  MPI_INFO_NULL, &fh);\n\n    for (i=0; i<NTIMES; i++) {\n\tfor (j=0; j<SIZE; j++)\n\t    buf[j] = i*SIZE + j;\n\n\terr = MPI_File_write(fh, buf, SIZE, MPI_DOUBLE, &status);\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n    }\n\n    MPI_File_get_size(fh, &size);\n    fprintf(stderr, \"file size = %lld bytes\\n\", size);\n\n    MPI_File_seek(fh, 0, MPI_SEEK_SET);\n\n    for (j=0; j<SIZE; j++) buf[j] = -1;\n\n    flag = 0;\n    for (i=0; i<NTIMES; i++) {\n\terr = MPI_File_read(fh, buf, SIZE, MPI_DOUBLE, &status);\n        \n\n        if (err != MPI_SUCCESS) {\n\t    fprintf(stderr, \"MPI_File_write returned error\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\tfor (j=0; j<SIZE; j++)\n\t    if (buf[j] != i*SIZE + j) {\n\t\tfprintf(stderr, \"error: buf %d is %lld, should be %lld \\n\", j, buf[j],\n                                 i*SIZE + j);\n\t\tflag = 1;\n\t    }\n    }\n\n    if (!flag) fprintf(stderr, \"Data read back is correct\\n\");\n    MPI_File_close(&fh);\n\n    free(buf);\n    free(filename);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "wesbland_647", "code": "int main(int argc, char** argv) {\n\n  int world_rank;\n  int world_size;\n\n  \n\n  srand(time(NULL) * world_rank);\n\n  float rand_num = rand() / (float)RAND_MAX;\n  int rank;\n  printf(\"Rank for %f on process %d - %d\\n\", rand_num, world_rank, rank);\n\n}", "label": "int main(int argc, char** argv) {\n  MPI_Init(NULL, NULL);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  srand(time(NULL) * world_rank);\n\n  float rand_num = rand() / (float)RAND_MAX;\n  int rank;\n  MPI_Rank(&rand_num, &rank, MPI_FLOAT, MPI_COMM_WORLD);\n  printf(\"Rank for %f on process %d - %d\\n\", rand_num, world_rank, rank);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}"}
{"program": "syftalent_649", "code": "int main(int argc, char **argv) {\n    int       i, j, rank, nproc;\n    int       errors = 0, all_errors = 0;\n    int       val = 0, one = 1;\n    int       iter;\n    MPI_Aint *val_ptrs;\n    MPI_Win   dyn_win;\n\n\n\n    iter = ITER_PER_RANK * nproc;\n\n    val_ptrs = malloc(nproc * sizeof(MPI_Aint));\n\n\n\n    for (i = 0; i < iter; i++) {\n    }\n\n\n    \n\n    if ( val != iter ) {\n        errors++;\n        printf(\"%d -- Got %d, expected %d\\n\", rank, val, iter);\n    }\n\n\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    free(val_ptrs);\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int       i, j, rank, nproc;\n    int       errors = 0, all_errors = 0;\n    int       val = 0, one = 1;\n    int       iter;\n    MPI_Aint *val_ptrs;\n    MPI_Win   dyn_win;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    iter = ITER_PER_RANK * nproc;\n\n    val_ptrs = malloc(nproc * sizeof(MPI_Aint));\n    MPI_Get_address(&val, &val_ptrs[rank]);\n\n    MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, val_ptrs, 1, MPI_AINT,\n                  MPI_COMM_WORLD);\n\n    MPI_Win_create_dynamic(MPI_INFO_NULL, MPI_COMM_WORLD, &dyn_win);\n    MPI_Win_attach(dyn_win, &val, sizeof(int));\n\n    for (i = 0; i < iter; i++) {\n            MPI_Win_fence(MPI_MODE_NOPRECEDE, dyn_win);\n            MPI_Accumulate(&one, 1, MPI_INT, i%nproc, val_ptrs[i%nproc], 1, MPI_INT, MPI_SUM, dyn_win);\n            MPI_Win_fence(MPI_MODE_NOSUCCEED, dyn_win);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    if ( val != iter ) {\n        errors++;\n        printf(\"%d -- Got %d, expected %d\\n\", rank, val, iter);\n    }\n\n    MPI_Win_detach(dyn_win, &val);\n    MPI_Win_free(&dyn_win);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    free(val_ptrs);\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "miguelzf_650", "code": "int main (int argc, char* argv[] )\n{\n\tscore_t** documents, **cabinets, **cabinetsums;\n\tdouble t1, t2, t3, t4;\n\tchar procname[64], *infname, *outfname;\n\tint nround, nchanged, i, namelen;\n\tint *assignments;\n\n\t\n\n\tdocuments = cabinets = cabinetsums = NULL;\n\tinfname = outfname = NULL;\n\tt1 = t2 = t3 = t4 = 0.0;\n\t\n\n\n\tsprintf(procname+namelen, \"-P%d\", rank);\n\tif (argc < 2)\n\t\treturn fprintf(stderr, \"Too few arguments!\\nUsage: %s\\n\", usage);\n\t}\n\n\tif (ROOT)\n\t\tprintf(\"Startup MPI process %d of %d on %s Running with 1 thread\\n\",\n\t\t\trank, nprocs, procname);\n\n\tt1 =\n\tif (ROOT)\t\n\n\t{\n\t\tint len = strlen(argv[1]);\n\t\tinfname = argv[1];\n\t\tif (strcmp(infname+len-3, \".in\"))\n\t\t\treturn fprintf(stderr, \"Invalid input file name! Must use extension '.in'\\n\");\n\t\n\t\toutfname = malloc(len+3);\n\t\tstrcpy(outfname, infname);\n\t\tstrcpy(outfname+len-2, \"out\");\n\n\t\tnum_cabinets = 0;\n\t\tif (argc > 2)\n\t\t\tif ((num_cabinets = atoi(argv[2])) <= 0)\n\t\t\t\treturn fprintf(stderr, \"Not a valid No. of Cabinets!\\nUsage: %s\\n\", usage);\n\t\n\t\tdocuments = read_and_send_input(infname);\n\n\t\tdprintf(\"[Process %d] In %s: %d documents, %d cabinets, %d subjects\\n\", \n\t\t\t\trank, procname, num_documents, num_cabinets, num_subjects);\n\t}\n\telse\n\t\tdocuments = receive_input();\n\n\n\tt2 =\n\n\tcabinets = create_score_matrix(num_cabinets, nsubjects1);\t\n\n\tcabinetsums = create_score_matrix(num_cabinets, nsubjects1);\n\tcompute_centroids(documents, cabinets, cabinetsums);\n\t\t\n#if 1\t\n\tfor (nround = 0; 1; nround++)\n\t{\n\t\tdprintf(\"\\n[Process %d] ROUND %d\\n\", rank, nround);\n\t\tnchanged = reassign_documents(documents, cabinets, cabinetsums);\n\n\t\tif (!DISTR && nchanged == 0)\t\n\n\t\t\tbreak;\n\n\t\tif (update_centroids(cabinets, cabinetsums, nchanged) == 0)\n\t\t\tbreak;\n\t}\n\n\tdprintf(\"[P%d] Finished\\n\", rank);\n\t\n\t\n\n\tassignments = malloc(nprocs*(blocksize+1)*sizeof(int));\n\tfor (i = 0; i < blocksize; i++)\n\t\tassignments[i] = (int) DOC_CABINET(documents[i]);\n\n\tif (DISTR)\n\t{\n\n\t\tif (!ROOT)\n\t\t\treturn 0;\n\t\t}\n\t}\n\t\n\tt3 =\n\tprint_result(assignments, outfname);\n\tt4 =\n#endif\n\n\tprintf(\"=== Finished %s ===\\n\", infname);\n\tprintf(\"Distribut: %.3lf secs\\n\", t2-t1);\n\n\n\tprintf(\"Computing: %.3lf secs\\n\", t3-t2);\n\tprintf(\"Writing  : %.3lf secs\\n\", t4-t3);\n\tprintf(\"Total    : %.3lf secs\\n\", t4-t1);\t\n\treturn 0;\n}", "label": "int main (int argc, char* argv[] )\n{\n\tscore_t** documents, **cabinets, **cabinetsums;\n\tdouble t1, t2, t3, t4;\n\tchar procname[64], *infname, *outfname;\n\tint nround, nchanged, i, namelen;\n\tint *assignments;\n\n\t\n\n\tdocuments = cabinets = cabinetsums = NULL;\n\tinfname = outfname = NULL;\n\tt1 = t2 = t3 = t4 = 0.0;\n\t\n\tMPI_Init(&argc, &argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tMPI_Get_processor_name(procname, &namelen);\n\n\tsprintf(procname+namelen, \"-P%d\", rank);\n\tif (argc < 2)\n\t{\tMPI_Finalize();\n\t\treturn fprintf(stderr, \"Too few arguments!\\nUsage: %s\\n\", usage);\n\t}\n\n\tif (ROOT)\n\t\tprintf(\"Startup MPI process %d of %d on %s Running with 1 thread\\n\",\n\t\t\trank, nprocs, procname);\n\n\tt1 = MPI_Wtime();\n\tif (ROOT)\t\n\n\t{\n\t\tint len = strlen(argv[1]);\n\t\tinfname = argv[1];\n\t\tif (strcmp(infname+len-3, \".in\"))\n\t\t\treturn fprintf(stderr, \"Invalid input file name! Must use extension '.in'\\n\");\n\t\n\t\toutfname = malloc(len+3);\n\t\tstrcpy(outfname, infname);\n\t\tstrcpy(outfname+len-2, \"out\");\n\n\t\tnum_cabinets = 0;\n\t\tif (argc > 2)\n\t\t\tif ((num_cabinets = atoi(argv[2])) <= 0)\n\t\t\t\treturn fprintf(stderr, \"Not a valid No. of Cabinets!\\nUsage: %s\\n\", usage);\n\t\n\t\tdocuments = read_and_send_input(infname);\n\n\t\tdprintf(\"[Process %d] In %s: %d documents, %d cabinets, %d subjects\\n\", \n\t\t\t\trank, procname, num_documents, num_cabinets, num_subjects);\n\t}\n\telse\n\t\tdocuments = receive_input();\n\n\tMPI_Barrier(MPI_COMM_WORLD);\t\n\n\tt2 = MPI_Wtime();\n\n\tcabinets = create_score_matrix(num_cabinets, nsubjects1);\t\n\n\tcabinetsums = create_score_matrix(num_cabinets, nsubjects1);\n\tcompute_centroids(documents, cabinets, cabinetsums);\n\t\t\n#if 1\t\n\tfor (nround = 0; 1; nround++)\n\t{\n\t\tdprintf(\"\\n[Process %d] ROUND %d\\n\", rank, nround);\n\t\tnchanged = reassign_documents(documents, cabinets, cabinetsums);\n\n\t\tif (!DISTR && nchanged == 0)\t\n\n\t\t\tbreak;\n\n\t\tif (update_centroids(cabinets, cabinetsums, nchanged) == 0)\n\t\t\tbreak;\n\t}\n\n\tdprintf(\"[P%d] Finished\\n\", rank);\n\t\n\t\n\n\tassignments = malloc(nprocs*(blocksize+1)*sizeof(int));\n\tfor (i = 0; i < blocksize; i++)\n\t\tassignments[i] = (int) DOC_CABINET(documents[i]);\n\n\tif (DISTR)\n\t{\n\t\tMPI_Gather(ROOT? MPI_IN_PLACE : assignments, blocksize, MPI_INT, assignments, blocksize, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t\tif (!ROOT)\n\t\t{\tMPI_Finalize();\n\t\t\treturn 0;\n\t\t}\n\t}\n\t\n\tt3 = MPI_Wtime();\n\tprint_result(assignments, outfname);\n\tt4 = MPI_Wtime();\n#endif\n\n\tprintf(\"=== Finished %s ===\\n\", infname);\n\tprintf(\"Distribut: %.3lf secs\\n\", t2-t1);\n\n\n\tprintf(\"Computing: %.3lf secs\\n\", t3-t2);\n\tprintf(\"Writing  : %.3lf secs\\n\", t4-t3);\n\tprintf(\"Total    : %.3lf secs\\n\", t4-t1);\t\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "luk51000_651", "code": "main(int argc, char* argv[]) {\n    float vector[100];\n    MPI_Status status;\n    int p;\n    int my_rank;\n    int i;\n\n\n    \n\n    if (my_rank == 0) {\n        for (i = 0; i < 50; i++)\n            vector[i] = 0.0;\n        for (i = 50; i < 100; i++)\n            vector[i] = 1.0;\n    } else { \n\n        for (i = 50; i < 100; i++)\n            printf(\"%3.1f \",vector[i]);\n        printf(\"\\n\");\n    }\n\n}", "label": "main(int argc, char* argv[]) {\n    float vector[100];\n    MPI_Status status;\n    int p;\n    int my_rank;\n    int i;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    \n\n    if (my_rank == 0) {\n        for (i = 0; i < 50; i++)\n            vector[i] = 0.0;\n        for (i = 50; i < 100; i++)\n            vector[i] = 1.0;\n        MPI_Send(vector+50, 50, MPI_FLOAT, 1, 0,\n            MPI_COMM_WORLD); \n    } else { \n\n        MPI_Recv(vector+50, 50, MPI_FLOAT, 0, 0,\n            MPI_COMM_WORLD, &status);\n        for (i = 50; i < 100; i++)\n            printf(\"%3.1f \",vector[i]);\n        printf(\"\\n\");\n    }\n\n    MPI_Finalize();\n}"}
{"program": "ansade_652", "code": "n (int argc, char *argv[]) {\n\n\n\n\n\n\n\n\n\n    long cuenta_temporal=0;\n    long cuenta_total=0;     \n\n    long inicio;\n    long final;      \n\n    long i;\n\n    int pid;          \n\n    int nump;          \n\n    int tam, tam_proc0;   \n\n\tdouble tiempo;         \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n\n\n        \n\n    \n\n    \n\n\n\n    tiempo =\n\n    \n    inicio= 2 + pid*(n-1)/nump;  \n\n    final = 1 + (pid+1)*(n-1)/nump;\n    tam = final - inicio+ 1;\n\n    \n\n    tam_proc0 = (n-1)/nump;\n\n    int j;\n    \n\n    for (j=inicio; j<=final; j++){\n     \n\t\tif (primo(j)==0){ \n\n\t\t\tcuenta_temporal++;\n\t\t}\n     }\n\n     \n\n\n     \n\n     tiempo +=\n\n\n\n\t\n\n    \n\n\n\n\n     \n\n     if (pid == 0) {\n            printf (\"Existen %ld numeros primos menores que %d\\n\", cuenta_total, n);\n        printf (\"El tiempo total de ejecucion es :  %f milisegundos\\n\", tiempo*1000);\n     }\n\n\n\n     return 0;\n   }\n\n\n \n", "label": "n (int argc, char *argv[]) {\n\n\n\n\n\n\n\n\n\n    long cuenta_temporal=0;\n    long cuenta_total=0;     \n\n    long inicio;\n    long final;      \n\n    long i;\n\n    int pid;          \n\n    int nump;          \n\n    int tam, tam_proc0;   \n\n\tdouble tiempo;         \n\n\n\n\n\n\n\n\n\n\n\n\n    \n    MPI_Init (NULL, NULL); \n\n\n    MPI_Comm_rank (MPI_COMM_WORLD, &pid); \n\n        \n    MPI_Comm_size (MPI_COMM_WORLD, &nump); \n\n    \n    MPI_Barrier(MPI_COMM_WORLD); \n\n    \n\n\n\n    tiempo = -MPI_Wtime();\n\n    \n    inicio= 2 + pid*(n-1)/nump;  \n\n    final = 1 + (pid+1)*(n-1)/nump;\n    tam = final - inicio+ 1;\n\n    \n\n    tam_proc0 = (n-1)/nump;\n\n    int j;\n    \n\n    for (j=inicio; j<=final; j++){\n     \n\t\tif (primo(j)==0){ \n\n\t\t\tcuenta_temporal++;\n\t\t}\n     }\n\n     \n\n         MPI_Reduce (&cuenta_temporal, &cuenta_total, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n     \n\n     tiempo += MPI_Wtime();\n\n\n\n\t\n\n    \n\n\n\n\n     \n\n     if (pid == 0) {\n            printf (\"Existen %ld numeros primos menores que %d\\n\", cuenta_total, n);\n        printf (\"El tiempo total de ejecucion es :  %f milisegundos\\n\", tiempo*1000);\n     }\n\n     MPI_Finalize ();\n\n\n     return 0;\n   }\n\n\n \n"}
{"program": "bmi-forum_655", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tStream* stream;\n\n\tStg_ObjectList* directories;\n\tPluginLoader* plugin;\n\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n\t\n\n\tstream =  Journal_Register( Info_Type, __FILE__ );\n\n\tdirectories = Stg_ObjectList_New();\n\tStg_ObjectList_PointerAppend( directories, StG_Strdup(LIB_DIR), \"default dir\", 0, 0, 0 );\n\t\n\tplugin = PluginLoader_NewLocal( \"LocalPlugin\", directories );\n\n\tJournal_Firewall( plugin != NULL, stream, \"Failed!\\n\" );\n\n\tJournal_Printf( stream, \"PluginLoader_GetName(): %s\\n\", PluginLoader_GetName( plugin ) );\n\tPrint( plugin, stream );\n\n\tStg_Class_Delete( plugin );\n\tStg_Class_Delete( directories );\n\t\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n                                                                                                                                    \n\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tStream* stream;\n\n\tStg_ObjectList* directories;\n\tPluginLoader* plugin;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n\t\n\n\tstream =  Journal_Register( Info_Type, __FILE__ );\n\n\tdirectories = Stg_ObjectList_New();\n\tStg_ObjectList_PointerAppend( directories, StG_Strdup(LIB_DIR), \"default dir\", 0, 0, 0 );\n\t\n\tplugin = PluginLoader_NewLocal( \"LocalPlugin\", directories );\n\n\tJournal_Firewall( plugin != NULL, stream, \"Failed!\\n\" );\n\n\tJournal_Printf( stream, \"PluginLoader_GetName(): %s\\n\", PluginLoader_GetName( plugin ) );\n\tPrint( plugin, stream );\n\n\tStg_Class_Delete( plugin );\n\tStg_Class_Delete( directories );\n\t\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n                                                                                                                                    \n\n\treturn 0;\n}"}
{"program": "ResearchComputing_656", "code": "int\r\nmain (int argc, char **argv)\r\n{\r\n    \n \t\r\n    hid_t       file_id, dset_id;         \n\r\n    hid_t       filespace, memspace;      \n\r\n    hsize_t     dimsf[2];                 \n\r\n    int         *data;                    \n\r\n    hsize_t\tcount[2];\t          \n\r\n    hsize_t\toffset[2];\r\n    hid_t\tplist_id;                 \n\r\n    int         i;\r\n    herr_t\tstatus;\r\n\r\n    int nx, ny;\r\n\r\n    \n\r\n    int mpi_size, mpi_rank;\r\n    MPI_Comm comm  = MPI_COMM_WORLD;\r\n    MPI_Info info  = MPI_INFO_NULL;\r\n    \n\r\n\r\n    if (mpi_rank == 0) {\r\n      printf(\"Running in parallel on %d processes\\n\", mpi_size);\r\n    }\r\n \r\n    \n\r\n     plist_id = H5Pcreate(H5P_FILE_ACCESS);\r\n     H5Pset_fapl_mpio(plist_id, comm, info);\r\n\r\n    \n\r\n    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);\r\n    H5Pclose(plist_id);\r\n   \r\n\r\n    \n\r\n    dimsf[0] = mpi_size*2;\r\n    dimsf[1] = NY;\r\n    filespace = H5Screate_simple(RANK, dimsf, NULL); \r\n\r\n    \n\r\n    dset_id = H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace,\r\n\t\t\tH5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\r\n    H5Sclose(filespace);\r\n\r\n    \n\r\n    count[0] = dimsf[0]/mpi_size;\r\n    count[1] = dimsf[1];\r\n    offset[0] = mpi_rank * count[0];\r\n    offset[1] = 0;\r\n    memspace = H5Screate_simple(RANK, count, NULL);\r\n\r\n    \n\r\n    filespace = H5Dget_space(dset_id);\r\n    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);\r\n\r\n    \n\r\n    data = (int *) malloc(sizeof(int)*count[0]*count[1]);\r\n    for (i=0; i < count[0]*count[1]; i++) {\r\n        data[i] = mpi_rank + 10;\r\n    }\r\n\r\n    \n\r\n    plist_id = H5Pcreate(H5P_DATASET_XFER);\r\n    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);\r\n    \r\n    status = H5Dwrite(dset_id, H5T_NATIVE_INT, memspace, filespace,\r\n\t\t      plist_id, data);\r\n    free(data);\r\n\r\n    \n\r\n    H5Dclose(dset_id);\r\n    H5Sclose(filespace);\r\n    H5Sclose(memspace);\r\n    H5Pclose(plist_id);\r\n    H5Fclose(file_id);\r\n \r\n\r\n    return 0;\r\n}", "label": "int\r\nmain (int argc, char **argv)\r\n{\r\n    \n \t\r\n    hid_t       file_id, dset_id;         \n\r\n    hid_t       filespace, memspace;      \n\r\n    hsize_t     dimsf[2];                 \n\r\n    int         *data;                    \n\r\n    hsize_t\tcount[2];\t          \n\r\n    hsize_t\toffset[2];\r\n    hid_t\tplist_id;                 \n\r\n    int         i;\r\n    herr_t\tstatus;\r\n\r\n    int nx, ny;\r\n\r\n    \n\r\n    int mpi_size, mpi_rank;\r\n    MPI_Comm comm  = MPI_COMM_WORLD;\r\n    MPI_Info info  = MPI_INFO_NULL;\r\n    \n\r\n    MPI_Init(&argc, &argv);\r\n    MPI_Comm_size(comm, &mpi_size);\r\n    MPI_Comm_rank(comm, &mpi_rank);\r\n\r\n    if (mpi_rank == 0) {\r\n      printf(\"Running in parallel on %d processes\\n\", mpi_size);\r\n    }\r\n \r\n    \n\r\n     plist_id = H5Pcreate(H5P_FILE_ACCESS);\r\n     H5Pset_fapl_mpio(plist_id, comm, info);\r\n\r\n    \n\r\n    file_id = H5Fcreate(H5FILE_NAME, H5F_ACC_TRUNC, H5P_DEFAULT, plist_id);\r\n    H5Pclose(plist_id);\r\n   \r\n\r\n    \n\r\n    dimsf[0] = mpi_size*2;\r\n    dimsf[1] = NY;\r\n    filespace = H5Screate_simple(RANK, dimsf, NULL); \r\n\r\n    \n\r\n    dset_id = H5Dcreate(file_id, DATASETNAME, H5T_NATIVE_INT, filespace,\r\n\t\t\tH5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);\r\n    H5Sclose(filespace);\r\n\r\n    \n\r\n    count[0] = dimsf[0]/mpi_size;\r\n    count[1] = dimsf[1];\r\n    offset[0] = mpi_rank * count[0];\r\n    offset[1] = 0;\r\n    memspace = H5Screate_simple(RANK, count, NULL);\r\n\r\n    \n\r\n    filespace = H5Dget_space(dset_id);\r\n    H5Sselect_hyperslab(filespace, H5S_SELECT_SET, offset, NULL, count, NULL);\r\n\r\n    \n\r\n    data = (int *) malloc(sizeof(int)*count[0]*count[1]);\r\n    for (i=0; i < count[0]*count[1]; i++) {\r\n        data[i] = mpi_rank + 10;\r\n    }\r\n\r\n    \n\r\n    plist_id = H5Pcreate(H5P_DATASET_XFER);\r\n    H5Pset_dxpl_mpio(plist_id, H5FD_MPIO_COLLECTIVE);\r\n    \r\n    status = H5Dwrite(dset_id, H5T_NATIVE_INT, memspace, filespace,\r\n\t\t      plist_id, data);\r\n    free(data);\r\n\r\n    \n\r\n    H5Dclose(dset_id);\r\n    H5Sclose(filespace);\r\n    H5Sclose(memspace);\r\n    H5Pclose(plist_id);\r\n    H5Fclose(file_id);\r\n \r\n    MPI_Finalize();\r\n\r\n    return 0;\r\n}"}
{"program": "mpip_657", "code": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  double *rdata;\n  pfft_gcplan ths;\n  \n  pfft_init();\n  \n  \n\n  n[0] = n[1] = n[2] = 8;\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 4;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  \n\n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  rdata = pfft_alloc_real(alloc_local_gc > 2*alloc_local ? alloc_local_gc : 2*alloc_local);\n\n  \n\n  \n\n  ths = pfft_plan_rgc_3d(n, gc_below, gc_above,\n      rdata, comm_cart_2d, PFFT_GC_TRANSPOSED_NONE);\n\n  \n\n  pfft_init_input_real_3d(n, local_ni, local_i_start,\n      rdata);\n\n  \n\n  if(verbose)\n    pfft_apr_real_3d(rdata, local_ni, local_i_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_real_3d(rdata, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_real_3d(rdata, local_ni, local_i_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    rdata[l] /= 2;\n\n  \n\n  err = pfft_check_output_real_3d(n, local_ni, local_i_start, rdata, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one gcell exchange and reduce of size n=(%td, %td, %td),\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"gc_below = (%td, %td, %td), gc_above = (%td, %td, %td):\\n\", gc_below[0], gc_below[1], gc_below[2], gc_above[0], gc_above[1], gc_above[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  pfft_free(rdata);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  double *rdata;\n  pfft_gcplan ths;\n  \n  MPI_Init(&argc, &argv);\n  pfft_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &rnk_self);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  \n\n  n[0] = n[1] = n[2] = 8;\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 4;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  \n\n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  rdata = pfft_alloc_real(alloc_local_gc > 2*alloc_local ? alloc_local_gc : 2*alloc_local);\n\n  \n\n  \n\n  ths = pfft_plan_rgc_3d(n, gc_below, gc_above,\n      rdata, comm_cart_2d, PFFT_GC_TRANSPOSED_NONE);\n\n  \n\n  pfft_init_input_real_3d(n, local_ni, local_i_start,\n      rdata);\n\n  \n\n  if(verbose)\n    pfft_apr_real_3d(rdata, local_ni, local_i_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_real_3d(rdata, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_real_3d(rdata, local_ni, local_i_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    rdata[l] /= 2;\n\n  \n\n  MPI_Barrier(comm_cart_2d);\n  err = pfft_check_output_real_3d(n, local_ni, local_i_start, rdata, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one gcell exchange and reduce of size n=(%td, %td, %td),\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"gc_below = (%td, %td, %td), gc_above = (%td, %td, %td):\\n\", gc_below[0], gc_below[1], gc_below[2], gc_above[0], gc_above[1], gc_above[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(rdata);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "dansarie_659", "code": "int main(int argc, char **argv) {\n  int rank, size;\n\n  create_g_mpi_work_type();\n\n  \n\n  if (rank != 0) {\n    mpi_worker();\n    return 0;\n  }\n\n  \n\n  options opt = {\n    .fname = {0},\n    .gfname = {0},\n    .iterations = 1,\n    .oneoutput = -1,\n    .permute = 0,\n    .metric = GATES,\n    .output_c = false,\n    .output_dot = false,\n    .lut_graph = false,\n    .randomize = true,\n    .try_nots = false,\n    .avail_gates = {{0}},\n    .avail_not = {{0}},\n    .avail_3 = {{0}},\n    .num_avail_3 = 0,\n    .verbosity = 0\n  };\n  create_avail_gates(2 + 64 + 128, &opt); \n\n  argp_parse(&argp, argc, argv, 0, 0, &opt);\n  if (opt.verbosity >= 1) {\n    printf(\"Available gates: NOT \");\n    for (int i = 0; opt.avail_gates[i].num_inputs != 0; i++) {\n      printf(\"%s \", gate_name[opt.avail_gates[i].fun]);\n    }\n    printf(\"\\nGenerated gates: \");\n    for (int i = 0; opt.avail_not[i].num_inputs != 0; i++) {\n      printf(\"%s \", gate_name[opt.avail_not[i].fun]);\n    }\n    printf(\"\\nGenerated 3-input gates: \");\n    for (int i = 0; opt.avail_3[i].num_inputs != 0; i++) {\n      printf(\"%02x \", opt.avail_3[i].fun);\n    }\n    printf(\"\\n\");\n  }\n\n  \n\n  if (opt.output_c || opt.output_dot) {\n    stop_workers();\n    state st;\n    if (!load_state(opt.fname, &st)) {\n      fprintf(stderr, \"Error when reading state file. (sboxgates.c:%d)\\n\", __LINE__);\n      return 1;\n    }\n    int retval = 0;\n    if (opt.output_c) {\n      retval = print_c_function(&st) ? 0 : 1;\n    } else {\n      print_digraph(&st);\n    }\n    return retval;\n  }\n\n  \n\n  uint32_t num_inputs; \n\n  if (!load_sbox(g_sbox_enc, &num_inputs, &opt)) {\n    stop_workers();\n    return 1;\n  }\n\n  \n\n  for (uint8_t i = 0; i < 8; i++) {\n    g_target[i] = generate_target(i, true);\n  }\n\n  if (opt.oneoutput >= get_num_outputs()) {\n    fprintf(stderr, \"Error: Can't generate output bit %d. Target S-box only has %d outputs. \"\n        \"(sboxgates.c:%d)\\n\", opt.oneoutput, get_num_outputs(), __LINE__);\n    stop_workers();\n    return 1;\n  }\n\n  \n\n  state st;\n  memset(&st, 0, sizeof(state));\n  if (strlen(opt.gfname) == 0) {\n    st.max_sat_metric = INT_MAX;\n    st.sat_metric = 0;\n    st.max_gates = MAX_GATES;\n    st.num_gates = num_inputs;\n    for (int i = 0; i < num_inputs; i++) {\n      st.gates[i].type = IN;\n      st.gates[i].table = generate_target(i, false);\n      st.gates[i].in1 = NO_GATE;\n      st.gates[i].in2 = NO_GATE;\n      st.gates[i].in3 = NO_GATE;\n      st.gates[i].function = 0;\n    }\n    for (int i = 0; i < 8; i++) {\n      st.outputs[i] = NO_GATE;\n    }\n  } else if (!load_state(opt.gfname, &st)) {\n    stop_workers();\n    return 1;\n  } else {\n    printf(\"Loaded %s.\\n\", opt.gfname);\n  }\n\n  \n\n  if (opt.oneoutput != -1) {\n    generate_graph_one_output(st, &opt);\n  } else {\n    generate_graph(st, &opt);\n  }\n\n  stop_workers();\n\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  MPI_Init(&argc, &argv);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  create_g_mpi_work_type();\n\n  \n\n  if (rank != 0) {\n    mpi_worker();\n    MPI_Finalize();\n    return 0;\n  }\n\n  \n\n  options opt = {\n    .fname = {0},\n    .gfname = {0},\n    .iterations = 1,\n    .oneoutput = -1,\n    .permute = 0,\n    .metric = GATES,\n    .output_c = false,\n    .output_dot = false,\n    .lut_graph = false,\n    .randomize = true,\n    .try_nots = false,\n    .avail_gates = {{0}},\n    .avail_not = {{0}},\n    .avail_3 = {{0}},\n    .num_avail_3 = 0,\n    .verbosity = 0\n  };\n  create_avail_gates(2 + 64 + 128, &opt); \n\n  argp_parse(&argp, argc, argv, 0, 0, &opt);\n  if (opt.verbosity >= 1) {\n    printf(\"Available gates: NOT \");\n    for (int i = 0; opt.avail_gates[i].num_inputs != 0; i++) {\n      printf(\"%s \", gate_name[opt.avail_gates[i].fun]);\n    }\n    printf(\"\\nGenerated gates: \");\n    for (int i = 0; opt.avail_not[i].num_inputs != 0; i++) {\n      printf(\"%s \", gate_name[opt.avail_not[i].fun]);\n    }\n    printf(\"\\nGenerated 3-input gates: \");\n    for (int i = 0; opt.avail_3[i].num_inputs != 0; i++) {\n      printf(\"%02x \", opt.avail_3[i].fun);\n    }\n    printf(\"\\n\");\n  }\n\n  \n\n  if (opt.output_c || opt.output_dot) {\n    stop_workers();\n    state st;\n    if (!load_state(opt.fname, &st)) {\n      fprintf(stderr, \"Error when reading state file. (sboxgates.c:%d)\\n\", __LINE__);\n      MPI_Finalize();\n      return 1;\n    }\n    int retval = 0;\n    if (opt.output_c) {\n      retval = print_c_function(&st) ? 0 : 1;\n    } else {\n      print_digraph(&st);\n    }\n    MPI_Finalize();\n    return retval;\n  }\n\n  \n\n  uint32_t num_inputs; \n\n  if (!load_sbox(g_sbox_enc, &num_inputs, &opt)) {\n    stop_workers();\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  for (uint8_t i = 0; i < 8; i++) {\n    g_target[i] = generate_target(i, true);\n  }\n\n  if (opt.oneoutput >= get_num_outputs()) {\n    fprintf(stderr, \"Error: Can't generate output bit %d. Target S-box only has %d outputs. \"\n        \"(sboxgates.c:%d)\\n\", opt.oneoutput, get_num_outputs(), __LINE__);\n    stop_workers();\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  state st;\n  memset(&st, 0, sizeof(state));\n  if (strlen(opt.gfname) == 0) {\n    st.max_sat_metric = INT_MAX;\n    st.sat_metric = 0;\n    st.max_gates = MAX_GATES;\n    st.num_gates = num_inputs;\n    for (int i = 0; i < num_inputs; i++) {\n      st.gates[i].type = IN;\n      st.gates[i].table = generate_target(i, false);\n      st.gates[i].in1 = NO_GATE;\n      st.gates[i].in2 = NO_GATE;\n      st.gates[i].in3 = NO_GATE;\n      st.gates[i].function = 0;\n    }\n    for (int i = 0; i < 8; i++) {\n      st.outputs[i] = NO_GATE;\n    }\n  } else if (!load_state(opt.gfname, &st)) {\n    stop_workers();\n    MPI_Finalize();\n    return 1;\n  } else {\n    printf(\"Loaded %s.\\n\", opt.gfname);\n  }\n\n  \n\n  if (opt.oneoutput != -1) {\n    generate_graph_one_output(st, &opt);\n  } else {\n    generate_graph(st, &opt);\n  }\n\n  stop_workers();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "qingu_660", "code": "int main( int argc, char **argv )\n{\n  double wscale = 10.0, scale;\n  int numprocs, myid,i;\n\n\n  for ( i=0; i<10000; i++) {\n  }\n\n  if (myid == 0) {\n      \n\n      printf( \" No Errors\\n\" );\n  }\n  \n  return 0;\n}", "label": "int main( int argc, char **argv )\n{\n  double wscale = 10.0, scale;\n  int numprocs, myid,i;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n  for ( i=0; i<10000; i++) {\n    MPI_Allreduce(&wscale,&scale,1,MPI_DOUBLE,MPI_SUM,MPI_COMM_WORLD);\n  }\n\n  if (myid == 0) {\n      \n\n      printf( \" No Errors\\n\" );\n  }\n  MPI_Finalize();\n  \n  return 0;\n}"}
{"program": "keremsahin1_661", "code": "int main(int iArgCnt, char* sArrArgs[])\n{\n\tint iIterationNo = 0, iCommNo = 0;\n\tdouble dNormOfResult = 0;\n\tdouble dTime0 = 0, dTime1 = 0, dTimeDiff = 0, dMinTimeDiff = DBL_MAX, dMaxTimeDiff = 0;\n\n\tparseInputs(iArgCnt, sArrArgs);\n\n\tGiSqrtProcCnt = (int)sqrt((double)GiProcessCnt);\n\n\tinitData();\n\tinitCommunicators();\n\n\tfor(iIterationNo = 0; iIterationNo < GiIterationCnt; iIterationNo++)\n\t{\n\t\tdTime0 =\n\n\t\tfor(iCommNo = 0; iCommNo < GiSqrtProcCnt; iCommNo++)\n\t\t{\n\t\t\tif((GiProcessRank % GiSqrtProcCnt) == iCommNo)\n\t\t\t{\n\t\t\t\t\n\n\t\t\t}\n\t\t}\n\n\t\tcblas_dgemv(CblasRowMajor, CblasNoTrans, GiRowColCntForOneProc, GiRowColCntForOneProc, 1.0, GdArrSubMatrix, GiRowColCntForOneProc, GdArrSubVector, 1, 0.0, GdArrSubResult, 1);\n\t\t\n\n\n\t\tfor(iCommNo = 0; iCommNo < GiSqrtProcCnt; iCommNo++)\n\t\t{\n\t\t\tif((GiProcessRank / GiSqrtProcCnt) == iCommNo)\n\t\t\t{\n\t\t\t\t\n\n\t\t\t}\n\t\t}\n\n\t\tif((GiProcessRank % GiSqrtProcCnt) == 0)\n\t\t{\n\t\t\t\n\n\t\t}\n\n\t\tdTime1 =\n\t\tdTimeDiff = (dTime1 - dTime0);\n\n\t\tif(dTimeDiff > dMaxTimeDiff)\n\t\t\tdMaxTimeDiff = dTimeDiff;\n\t\tif(dTimeDiff < dMinTimeDiff)\n\t\t\tdMinTimeDiff = dTimeDiff;\n\t}\n\n\tif(GiProcessRank == 0)\n\t{\n\t\t\n\n\t\tdNormOfResult = cblas_dnrm2(GiVectorLength, GdArrTotalResult, 1);\n\t\tprintf(\"Result=%f\\nMin Time=%f uSec\\nMax Time=%f uSec\\n\", dNormOfResult, (1.e6 * dMinTimeDiff), (1.e6 * dMaxTimeDiff));\n\t}\n\n\n\treturn 0;\n}", "label": "int main(int iArgCnt, char* sArrArgs[])\n{\n\tint iIterationNo = 0, iCommNo = 0;\n\tdouble dNormOfResult = 0;\n\tdouble dTime0 = 0, dTime1 = 0, dTimeDiff = 0, dMinTimeDiff = DBL_MAX, dMaxTimeDiff = 0;\n\n\tparseInputs(iArgCnt, sArrArgs);\n\n\tMPI_Init(&iArgCnt, &sArrArgs);\n\tMPI_Comm_size(MPI_COMM_WORLD, &GiProcessCnt);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &GiProcessRank);\n\tGiSqrtProcCnt = (int)sqrt((double)GiProcessCnt);\n\n\tinitData();\n\tinitCommunicators();\n\n\tfor(iIterationNo = 0; iIterationNo < GiIterationCnt; iIterationNo++)\n\t{\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t\tdTime0 = MPI_Wtime();\n\n\t\tfor(iCommNo = 0; iCommNo < GiSqrtProcCnt; iCommNo++)\n\t\t{\n\t\t\tif((GiProcessRank % GiSqrtProcCnt) == iCommNo)\n\t\t\t{\n\t\t\t\tMPI_Barrier(GarrColumnComms[iCommNo]);\n\t\t\t\tMPI_Bcast(GdArrSubVector, GiRowColCntForOneProc, MPI_DOUBLE, 0, GarrColumnComms[iCommNo]);\n\t\t\t\t\n\n\t\t\t}\n\t\t}\n\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t\tcblas_dgemv(CblasRowMajor, CblasNoTrans, GiRowColCntForOneProc, GiRowColCntForOneProc, 1.0, GdArrSubMatrix, GiRowColCntForOneProc, GdArrSubVector, 1, 0.0, GdArrSubResult, 1);\n\t\t\n\n\n\t\tfor(iCommNo = 0; iCommNo < GiSqrtProcCnt; iCommNo++)\n\t\t{\n\t\t\tif((GiProcessRank / GiSqrtProcCnt) == iCommNo)\n\t\t\t{\n\t\t\t\tMPI_Barrier(GarrRowComms[iCommNo]);\n\t\t\t\tMPI_Reduce(GdArrSubResult, GdArrSubTotalResult, GiRowColCntForOneProc, MPI_DOUBLE, MPI_SUM, 0, GarrRowComms[iCommNo]);\n\t\t\t\t\n\n\t\t\t}\n\t\t}\n\n\t\tif((GiProcessRank % GiSqrtProcCnt) == 0)\n\t\t{\n\t\t\tMPI_Barrier(GarrColumnComms[0]);\n\t\t\tMPI_Gather(GdArrSubTotalResult, GiRowColCntForOneProc, MPI_DOUBLE, GdArrTotalResult, GiRowColCntForOneProc, MPI_DOUBLE, 0, GarrColumnComms[0]);\n\t\t\t\n\n\t\t}\n\n\t\tdTime1 = MPI_Wtime();\n\t\tdTimeDiff = (dTime1 - dTime0);\n\n\t\tif(dTimeDiff > dMaxTimeDiff)\n\t\t\tdMaxTimeDiff = dTimeDiff;\n\t\tif(dTimeDiff < dMinTimeDiff)\n\t\t\tdMinTimeDiff = dTimeDiff;\n\t}\n\n\tif(GiProcessRank == 0)\n\t{\n\t\t\n\n\t\tdNormOfResult = cblas_dnrm2(GiVectorLength, GdArrTotalResult, 1);\n\t\tprintf(\"Result=%f\\nMin Time=%f uSec\\nMax Time=%f uSec\\n\", dNormOfResult, (1.e6 * dMinTimeDiff), (1.e6 * dMaxTimeDiff));\n\t}\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "nerscadmin_662", "code": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int *rcnt, *rdpl;\n  int dsize;\n\n  \n  PMPI_Type_size(DATATYPE, &dsize);\n\n  rbuf=0; \n  rcnt=0; rdpl=0;\n  if( myrank==ROOT )\n    {\n      rbuf=(char*)malloc(SIZE*dsize * ((nprocs*(nprocs+1))/2+nprocs) );\n      rcnt=(int*) malloc(sizeof(int)*nprocs);\n      rdpl=(int*) malloc(sizeof(int)*nprocs);\n\n      for( i=0; i<nprocs; i++ )\n\t{\n\t  rcnt[i] = SIZE*(i+1);\n\t  rdpl[i] = SIZE*(i*((i+1)/2)+i+1);\n\t}\n    }\n  sbuf=(char*)malloc(SIZE*dsize * (myrank+1) );\n\n\n  for( i=0; i<REPEAT; i++ )\n    {\n    }\n\n  return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int *rcnt, *rdpl;\n  int dsize;\n\n  MPI_Init( &argc, &argv );\n  \n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  PMPI_Type_size(DATATYPE, &dsize);\n\n  rbuf=0; \n  rcnt=0; rdpl=0;\n  if( myrank==ROOT )\n    {\n      rbuf=(char*)malloc(SIZE*dsize * ((nprocs*(nprocs+1))/2+nprocs) );\n      rcnt=(int*) malloc(sizeof(int)*nprocs);\n      rdpl=(int*) malloc(sizeof(int)*nprocs);\n\n      for( i=0; i<nprocs; i++ )\n\t{\n\t  rcnt[i] = SIZE*(i+1);\n\t  rdpl[i] = SIZE*(i*((i+1)/2)+i+1);\n\t}\n    }\n  sbuf=(char*)malloc(SIZE*dsize * (myrank+1) );\n\n\n  for( i=0; i<REPEAT; i++ )\n    {\n      MPI_Gatherv( sbuf, SIZE*(myrank+1), DATATYPE, \n\t\t   rbuf, rcnt, rdpl, DATATYPE, ROOT, MPI_COMM_WORLD );\n    }\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "ClaudioNahmad_663", "code": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nints, len;\n    char *filename, *tmp;\n    int  errs = 0, toterrs, errcode;\n    MPI_File fh;\n    MPI_Status status;\n\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+10);\n    }\n\n\n    buf = (int *) malloc(SIZE);\n    nints = SIZE/sizeof(int);\n    for (i=0; i<nints; i++) buf[i] = rank*100000 + i;\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode,\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_write\");\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode,\n\n    \n\n\n    for (i=0; i<nints; i++) buf[i] = 0;\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode,\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_read\");\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode,\n\n    \n\n    for (i=0; i<nints; i++) {\n\tif (buf[i] != (rank*100000 + i)) {\n\t    errs++;\n\t    fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\",\n\t\t    rank, buf[i], rank*100000+i);\n\t}\n    }\n\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(buf);\n    free(filename);\n    free(tmp);\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nints, len;\n    char *filename, *tmp;\n    int  errs = 0, toterrs, errcode;\n    MPI_File fh;\n    MPI_Status status;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+10);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n\n    buf = (int *) malloc(SIZE);\n    nints = SIZE/sizeof(int);\n    for (i=0; i<nints; i++) buf[i] = rank*100000 + i;\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    errcode = MPI_File_open(MPI_COMM_SELF, filename,\n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_open(1)\");\n\n    errcode = MPI_File_write(fh, buf, nints, MPI_INT, &status);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_write\");\n\n    errcode = MPI_File_close(&fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_clode (1)\");\n\n    \n\n\n    for (i=0; i<nints; i++) buf[i] = 0;\n    errcode = MPI_File_open(MPI_COMM_SELF, filename,\n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_open(2)\");\n\n    errcode = MPI_File_read(fh, buf, nints, MPI_INT, &status);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_read\");\n\n    errcode = MPI_File_close(&fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_close(2)\");\n\n    \n\n    for (i=0; i<nints; i++) {\n\tif (buf[i] != (rank*100000 + i)) {\n\t    errs++;\n\t    fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\",\n\t\t    rank, buf[i], rank*100000+i);\n\t}\n    }\n\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(buf);\n    free(filename);\n    free(tmp);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "gentryx_664", "code": "int main(int argc, char * argv[])\n{\n\n    int rank, size;\n\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    if (size<2) {\n        printf(\"Use 2 or more processes. \\n\");\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf = NULL;\n\n\n    memset(buf, rank, (size_t)n);\n\n    size_t errors = 0;\n    for (int r = 1; r < size; r++) {\n\n        MPI_Request req;\n\n        \n\n        if (rank==r) {\n        }\n        else if (rank==0) {\n        }\n\n        if (rank == 0 || rank==r) {\n        }\n\n        if (rank==0) {\n            errors += verify_buffer(buf, n, r);\n            if (errors > 0) {\n                printf(\"There were %zu errors!\", errors);\n            }\n        }\n    }\n\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    if (size<2) {\n        printf(\"Use 2 or more processes. \\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf = NULL;\n\n    MPI_Alloc_mem((MPI_Aint)n, MPI_INFO_NULL, &buf);\n\n    memset(buf, rank, (size_t)n);\n\n    size_t errors = 0;\n    for (int r = 1; r < size; r++) {\n\n        MPI_Request req;\n\n        \n\n        if (rank==r) {\n            MPIX_Isend_x(buf, n, MPI_CHAR, 0 \n, r \n, MPI_COMM_WORLD, &req);\n        }\n        else if (rank==0) {\n            MPIX_Irecv_x(buf, n, MPI_CHAR, r \n, r \n, MPI_COMM_WORLD, &req);\n        }\n\n        if (rank == 0 || rank==r) {\n            MPI_Wait(&req, MPI_STATUS_IGNORE);\n        }\n\n        if (rank==0) {\n            errors += verify_buffer(buf, n, r);\n            if (errors > 0) {\n                printf(\"There were %zu errors!\", errors);\n            }\n        }\n    }\n\n    MPI_Free_mem(buf);\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "csampez_665", "code": "int main(int argc, char* argv[]) {\n   int my_rank, p;\n   char g_i;\n   int *local_A;\n   int global_n;\n   int local_n;\n   MPI_Comm comm;\n   double start, finish;\n   \n\n   \n\n\n   comm = MPI_COMM_WORLD;\n\n   Get_args(argc, argv, &global_n, &local_n, &g_i, my_rank, p, comm);\n   local_A = (int*) malloc(local_n*sizeof(int));\n   if (g_i == 'g') {\n      Generate_list(local_A, local_n, my_rank);\n  } else if (g_i == 't'){\n      Read_file(local_A, local_n, my_rank, p, comm);\n  } else\n  {\n      Read_list(local_A, local_n, my_rank, p, comm);\n  }\n\n\n   \n\n\n\n\n    start =\n    Sort(local_A, local_n, my_rank, p, comm);\n    finish =\n    if (my_rank == 0)\n       printf(\"Elapsed time = %e seconds\\n\", finish-start);\n\n\n\n    \n\n\n\n\n\n\n   \n\n\n   free(local_A);\n\n\n   return 0;\n}", "label": "int main(int argc, char* argv[]) {\n   int my_rank, p;\n   char g_i;\n   int *local_A;\n   int global_n;\n   int local_n;\n   MPI_Comm comm;\n   double start, finish;\n   \n\n   \n\n\n   MPI_Init(&argc, &argv);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &p);\n   MPI_Comm_rank(comm, &my_rank);\n\n   Get_args(argc, argv, &global_n, &local_n, &g_i, my_rank, p, comm);\n   local_A = (int*) malloc(local_n*sizeof(int));\n   if (g_i == 'g') {\n      Generate_list(local_A, local_n, my_rank);\n  } else if (g_i == 't'){\n      Read_file(local_A, local_n, my_rank, p, comm);\n  } else\n  {\n      Read_list(local_A, local_n, my_rank, p, comm);\n  }\n\n\n   \n\n\n\n\n    start = MPI_Wtime();\n    Sort(local_A, local_n, my_rank, p, comm);\n    finish = MPI_Wtime();\n    if (my_rank == 0)\n       printf(\"Elapsed time = %e seconds\\n\", finish-start);\n\n\n\n    \n\n\n\n\n\n\n   \n\n\n   free(local_A);\n\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "syftalent_666", "code": "int main(int argc, char *argv[])\n{\n    int i, size, num, name_len, desc_len, verb, thread_support;\n    int varclass, bind, readonly, continuous, atomic, uqsize_idx, count;\n    char name[STR_LEN], desc[STR_LEN];\n    MPI_Datatype dtype;\n    MPI_T_enum enumtype;\n\n\n    if (rank == 0) {\n        printf(\"MPIT pvar test: unexpected_recvq_buffer_size\\n\");\n        fflush(stdout);\n    }\n\n    \n\n    \n\n    assert(size == 2);\n\n    \n\n    TRY(MPI_T_init_thread(MPI_THREAD_SINGLE, &thread_support));\n    TRY(MPI_T_pvar_get_num(&num));\n\n    int found = 0;\n\n    \n\n    for (i = 0; i < num; i++) {\n        name_len = desc_len = STR_LEN;\n        TRY(MPI_T_pvar_get_info(i, name, &name_len, &verb, &varclass, &dtype,\n                                &enumtype, desc, &desc_len, &bind, &readonly,\n                                &continuous, &atomic));\n\n        if (strcmp(name, \"unexpected_recvq_buffer_size\") == 0) {\n            uqsize_idx = i;\n            found = 1;\n        }\n    }\n\n    if (found) {\n        \n\n\n        \n\n        assert(count == 1);\n\n        \n\n        reversed_tags_test();\n        rndv_test();\n\n        \n\n    }\n\n    if (rank == 0) {\n        printf(\"finished\\n\");\n        fflush(stdout);\n    }\n\n    TRY(MPI_T_finalize());\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int i, size, num, name_len, desc_len, verb, thread_support;\n    int varclass, bind, readonly, continuous, atomic, uqsize_idx, count;\n    char name[STR_LEN], desc[STR_LEN];\n    MPI_Datatype dtype;\n    MPI_T_enum enumtype;\n\n    MPI_Init(NULL, NULL);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        printf(\"MPIT pvar test: unexpected_recvq_buffer_size\\n\");\n        fflush(stdout);\n    }\n\n    \n\n    \n\n    assert(size == 2);\n\n    \n\n    TRY(MPI_T_init_thread(MPI_THREAD_SINGLE, &thread_support));\n    TRY(MPI_T_pvar_get_num(&num));\n\n    int found = 0;\n\n    \n\n    for (i = 0; i < num; i++) {\n        name_len = desc_len = STR_LEN;\n        TRY(MPI_T_pvar_get_info(i, name, &name_len, &verb, &varclass, &dtype,\n                                &enumtype, desc, &desc_len, &bind, &readonly,\n                                &continuous, &atomic));\n\n        if (strcmp(name, \"unexpected_recvq_buffer_size\") == 0) {\n            uqsize_idx = i;\n            found = 1;\n        }\n    }\n\n    if (found) {\n        \n\n        MPI_T_pvar_session_create(&session);\n        MPI_T_pvar_handle_alloc(session, uqsize_idx, NULL, &uqsize_handle, &count);\n\n        \n\n        assert(count == 1);\n\n        \n\n        reversed_tags_test();\n        rndv_test();\n\n        \n\n        MPI_T_pvar_handle_free(session, &uqsize_handle);\n        MPI_T_pvar_session_free(&session);\n    }\n\n    if (rank == 0) {\n        printf(\"finished\\n\");\n        fflush(stdout);\n    }\n\n    TRY(MPI_T_finalize());\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ssrb_667", "code": "int main(int argc, char*argv[]){\n  int i;\t\t\t\t\n\n  char*buffer;\t\t\t\t\n\n  double T,Tot;\t\t\t\t\n\n  int rank,size;\t\t\t\n\n\n  Tot =\t\t\t\n\n\n  if (size<2){\n    printf(\"2 computers required\\n\");\n    exit(1);\n  }\n\n  if (!rank){\n    printf(\"_____________________________________\\n\");\n    system(\"env | egrep \\\"LAM|MPI\\\"\");\n    printf(\"_____________________________________\\n\");\n    printf(\"Sending 1MB...\"); fflush(stdout);\n  }\n\n  buffer=calloc(SIZE,sizeof(char));\n\n\n\n\n  if (!rank)\tfor (i=0; i<NTIMES; i++){\n\n  if (rank==1)\tfor (i=0; i<NTIMES; i++){\n  }\n\n  T=MPI_Wtime()-T;\t\t\t\n\n\n  free(buffer);\n  if (!rank){\n    printf(\"\\t...done\\n\");\n  }\n  Tot=MPI_Wtime()-Tot;\t\t\t\n\n\n  if (!rank){\n    printf(\"Message time: %8.3f s\\n\",T);\n    printf(\"BandWidth   : %8.3f MB/s\\n\",NTIMES*2*(SIZE)/(1<<20)/T);\n    printf(\"Elapsed time: %8.3f s\\n\",Tot);\n  }\n  exit(0);\n\n}", "label": "int main(int argc, char*argv[]){\n  int i;\t\t\t\t\n\n  char*buffer;\t\t\t\t\n\n  double T,Tot;\t\t\t\t\n\n  int rank,size;\t\t\t\n\n\n  Tot = MPI_Wtime();\t\t\t\n\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n  if (size<2){\n    printf(\"2 computers required\\n\");\n    exit(1);\n  }\n\n  if (!rank){\n    printf(\"_____________________________________\\n\");\n    system(\"env | egrep \\\"LAM|MPI\\\"\");\n    printf(\"_____________________________________\\n\");\n    printf(\"Sending 1MB...\"); fflush(stdout);\n  }\n\n  buffer=calloc(SIZE,sizeof(char));\n\n\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  T=MPI_Wtime();\t\t\t\n\n  if (!rank)\tfor (i=0; i<NTIMES; i++){\n    MPI_Send(buffer,SIZE,MPI_BYTE, 1,TAG,MPI_COMM_WORLD);\n    MPI_Recv(buffer,SIZE,MPI_BYTE, 1,TAG,MPI_COMM_WORLD, MPI_STATUS_IGNORE);}\n\n  if (rank==1)\tfor (i=0; i<NTIMES; i++){\n    MPI_Recv(buffer,SIZE,MPI_BYTE, 0,TAG,MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Send(buffer,SIZE,MPI_BYTE, 0,TAG,MPI_COMM_WORLD);\n  }\n\n  T=MPI_Wtime()-T;\t\t\t\n\n\n  free(buffer);\n  if (!rank){\n    printf(\"\\t...done\\n\");\n  }\n  MPI_Finalize();\n  Tot=MPI_Wtime()-Tot;\t\t\t\n\n\n  if (!rank){\n    printf(\"Message time: %8.3f s\\n\",T);\n    printf(\"BandWidth   : %8.3f MB/s\\n\",NTIMES*2*(SIZE)/(1<<20)/T);\n    printf(\"Elapsed time: %8.3f s\\n\",Tot);\n  }\n  exit(0);\n\n}"}
{"program": "joeladams_668", "code": "int main(int argc, char** argv) {\n    const int REPS = 8;\n    int id = -1, numProcesses = -1,\n        start = -1, stop = -1;\n\n\n    if (numProcesses > REPS) {\n      if (id == 0) {\n          printf(\"Please run with -np less than or equal to %d\\n.\", REPS);\n      }\n    } else {\n\n      \n\n      int chunkSize1 = (int)ceil(((double)REPS) / numProcesses);\n      \n\n      int chunkSize2 = chunkSize1 - 1;\n      int remainder = REPS % numProcesses;\n\n      \n\n      \n\n      \n\n      if (remainder == 0 || (remainder !=0 && id < remainder)) {\n        start = id * chunkSize1;\n        stop = start + chunkSize1;\n      } else {\n        \n\n        \n\n        start = (remainder * chunkSize1) + (chunkSize2 * (id - remainder));\n        stop = start + chunkSize2;\n      }\n\n      for (int i = start; i < stop; i++) {     \n\n          printf(\"Process %d is performing iteration %d\\n\", id, i);\n      }\n      \n    }\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    const int REPS = 8;\n    int id = -1, numProcesses = -1,\n        start = -1, stop = -1;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    if (numProcesses > REPS) {\n      if (id == 0) {\n          printf(\"Please run with -np less than or equal to %d\\n.\", REPS);\n      }\n    } else {\n\n      \n\n      int chunkSize1 = (int)ceil(((double)REPS) / numProcesses);\n      \n\n      int chunkSize2 = chunkSize1 - 1;\n      int remainder = REPS % numProcesses;\n\n      \n\n      \n\n      \n\n      if (remainder == 0 || (remainder !=0 && id < remainder)) {\n        start = id * chunkSize1;\n        stop = start + chunkSize1;\n      } else {\n        \n\n        \n\n        start = (remainder * chunkSize1) + (chunkSize2 * (id - remainder));\n        stop = start + chunkSize2;\n      }\n\n      for (int i = start; i < stop; i++) {     \n\n          printf(\"Process %d is performing iteration %d\\n\", id, i);\n      }\n      \n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "joeladams_669", "code": "int main(int argc, char** argv) {\n    int id = -1, numProcesses = -1;\n    double startTime = 0.0, localTime = 0.0, totalTime = 0.0;\n    int answer = 0.0;\n\n\n    startTime =\n\n    answer = solveProblem(id, numProcesses);\n\n    localTime = MPI_Wtime() - startTime;\n\n    if ( id == MASTER ) {\n        printf(\"\\nThe answer is %d; computing it took %f secs.\\n\\n\",\n                   answer, totalTime);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int id = -1, numProcesses = -1;\n    double startTime = 0.0, localTime = 0.0, totalTime = 0.0;\n    int answer = 0.0;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    startTime = MPI_Wtime();\n\n    answer = solveProblem(id, numProcesses);\n\n    localTime = MPI_Wtime() - startTime;\n    MPI_Reduce(&localTime, &totalTime, 1, MPI_DOUBLE,\n        MPI_MAX, 0, MPI_COMM_WORLD);\n\n    if ( id == MASTER ) {\n        printf(\"\\nThe answer is %d; computing it took %f secs.\\n\\n\",\n                   answer, totalTime);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "deborasetton_670", "code": "int main(int argc, char** argv) {\n\n  \n\n  examples[0] = ex_send;\n  examples[1] = ex_bsend;\n  examples[2] = ex_ssend;\n  examples[3] = ex_rsend;\n  examples[4] = ex_isend;\n  examples[5] = ex_ibsend;\n  examples[6] = ex_issend;\n  examples[7] = ex_irsend;\n\n\n  int my_rank;\n  int world_size;\n\n  if (world_size != 2) {\n    fprintf(stderr, \"World size must be two for %s\\n\", argv[0]);\n  }\n\n  int example_num;\n\n  if (my_rank == 0) {\n    example_num = choose_from_menu();\n  }\n\n  \n\n\n  printf(\"[%d] Running example %d \\n\", my_rank, example_num);\n  (*examples[example_num - 1])(my_rank);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n\n  \n\n  examples[0] = ex_send;\n  examples[1] = ex_bsend;\n  examples[2] = ex_ssend;\n  examples[3] = ex_rsend;\n  examples[4] = ex_isend;\n  examples[5] = ex_ibsend;\n  examples[6] = ex_issend;\n  examples[7] = ex_irsend;\n\n  MPI_Init(NULL, NULL);\n\n  int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  if (world_size != 2) {\n    fprintf(stderr, \"World size must be two for %s\\n\", argv[0]);\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n\n  int example_num;\n\n  if (my_rank == 0) {\n    example_num = choose_from_menu();\n  }\n\n  \n\n  MPI_Bcast(&example_num, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  printf(\"[%d] Running example %d \\n\", my_rank, example_num);\n  (*examples[example_num - 1])(my_rank);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "dfroger_671", "code": "int\nmain( int argc, char **argv )\n{\n    \n\n    int rank, size;\n\n    \n\n    long problem_size;\n    bool debug_mode;\n    char* output_filepath;\n    if (!prb_parse_command_line(argc, argv, &problem_size, &debug_mode, \n                                &output_filepath)) {\n        exit(0);\n    }\n\n    long array_size = problem_size *  problem_size;\n\n    \n\n    size_t i0, i1;\n    prb_partition_index(size, rank, array_size, &i0, &i1);\n\n    \n\n    size_t partition_size = i1 - i0;\n    double* partition = malloc((partition_size+1)*sizeof(double));\n\n    int noperations = 1;\n    enum operations {READING};\n    char* operation_names[1] = {\"reading\"};\n\n    prb_stopwatch_t* sw = prb_stopwatch_new(noperations);\n    prb_stopwatch_start(sw, READING);\n\n    \n\n    MPI_File f;\n\n    \n\n\n    \n\n\n    \n\n    MPI_Status status;\n\n    if (debug_mode) check_data_mean(partition, partition_size, array_size);\n\n    \n\n\n    prb_stopwatch_stop(sw, READING);\n    prb_stopwatch_write_csv(sw, output_filepath,\n                            noperations, operation_names,\n                            MPI_COMM_WORLD, 0);\n    \n    prb_stopwatch_free(sw);\n\n    \n\n\n    free(partition);\n\n    return 0;\n}", "label": "int\nmain( int argc, char **argv )\n{\n    \n\n    MPI_Init(&argc, &argv);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    long problem_size;\n    bool debug_mode;\n    char* output_filepath;\n    if (!prb_parse_command_line(argc, argv, &problem_size, &debug_mode, \n                                &output_filepath)) {\n        MPI_Finalize();\n        exit(0);\n    }\n\n    long array_size = problem_size *  problem_size;\n\n    \n\n    size_t i0, i1;\n    prb_partition_index(size, rank, array_size, &i0, &i1);\n\n    \n\n    size_t partition_size = i1 - i0;\n    double* partition = malloc((partition_size+1)*sizeof(double));\n\n    int noperations = 1;\n    enum operations {READING};\n    char* operation_names[1] = {\"reading\"};\n\n    prb_stopwatch_t* sw = prb_stopwatch_new(noperations);\n    prb_stopwatch_start(sw, READING);\n\n    \n\n    MPI_File f;\n    MPI_File_open(MPI_COMM_WORLD, \"data.bin\", MPI_MODE_RDONLY, MPI_INFO_NULL, &f);\n\n    \n\n    if (debug_mode) MPI_File_set_atomicity(f, 1);\n\n    \n\n    MPI_File_set_view(f, i0*sizeof(double), MPI_DOUBLE, MPI_DOUBLE,\n                      \"native\", MPI_INFO_NULL);\n\n    \n\n    MPI_Status status;\n    MPI_File_read(f, partition, partition_size, MPI_DOUBLE, &status);\n\n    if (debug_mode) check_data_mean(partition, partition_size, array_size);\n\n    \n\n    MPI_File_close(&f);\n\n    prb_stopwatch_stop(sw, READING);\n    prb_stopwatch_write_csv(sw, output_filepath,\n                            noperations, operation_names,\n                            MPI_COMM_WORLD, 0);\n    \n    prb_stopwatch_free(sw);\n\n    \n\n    MPI_Finalize();\n\n    free(partition);\n\n    return 0;\n}"}
{"program": "bmi-forum_672", "code": "int main( int argc, char* argv[] )\n{\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\n\tdouble*\t\tone2d;\n\tIndex i, j;\n\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\t}\n\n\tJournal_Printf( stream, \"2D as 1D\\n\" );\n\tone2d = Memory_Alloc_2DArrayAs1D_Unnamed( double, 3, 2 );\n\n\tMemory_Print();\n\n\t\n\t\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tfor ( j = 0; j < 2; ++j )\n\t\t{\n\t\t\tMemory_Access2D( one2d, i, j, 2 ) = i + (j / 10.0);\n\t\t}\n\t}\n\n\t\n\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tfor ( j = 0; j < 2; ++j )\n\t\t{\n\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access2D( one2d, i, j, 2 ) );\n\t\t}\n\t\tJournal_Printf( stream, \"\\n\" );\n\t}\n\n\tone2d = Memory_Realloc_2DArrayAs1D( one2d, double, 3, 2, 4, 4 );\n\n\tMemory_Print();\n\n\t\n\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tfor ( j = 0; j < 2; ++j )\n\t\t{\n\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access2D( one2d, i, j, 4 ) );\n\t\t}\n\t\tJournal_Printf( stream, \"\\n\" );\n\t}\n\t\n\t\n\t\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 4; ++j )\n\t\t{\n\t\t\tMemory_Access2D( one2d, i, j, 4 ) = i + (j / 10.0);\n\t\t}\n\t}\n\n\t\n\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 4; ++j )\n\t\t{\n\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access2D( one2d, i, j, 4 ) );\n\t\t}\n\t\tJournal_Printf( stream, \"\\n\" );\n\t}\n\n\n\tMemory_Free( one2d );\n\n\tMemory_Print();\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] )\n{\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\n\tdouble*\t\tone2d;\n\tIndex i, j;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\t}\n\n\tJournal_Printf( stream, \"2D as 1D\\n\" );\n\tone2d = Memory_Alloc_2DArrayAs1D_Unnamed( double, 3, 2 );\n\n\tMemory_Print();\n\n\t\n\t\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tfor ( j = 0; j < 2; ++j )\n\t\t{\n\t\t\tMemory_Access2D( one2d, i, j, 2 ) = i + (j / 10.0);\n\t\t}\n\t}\n\n\t\n\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tfor ( j = 0; j < 2; ++j )\n\t\t{\n\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access2D( one2d, i, j, 2 ) );\n\t\t}\n\t\tJournal_Printf( stream, \"\\n\" );\n\t}\n\n\tone2d = Memory_Realloc_2DArrayAs1D( one2d, double, 3, 2, 4, 4 );\n\n\tMemory_Print();\n\n\t\n\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tfor ( j = 0; j < 2; ++j )\n\t\t{\n\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access2D( one2d, i, j, 4 ) );\n\t\t}\n\t\tJournal_Printf( stream, \"\\n\" );\n\t}\n\t\n\t\n\t\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 4; ++j )\n\t\t{\n\t\t\tMemory_Access2D( one2d, i, j, 4 ) = i + (j / 10.0);\n\t\t}\n\t}\n\n\t\n\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 4; ++j )\n\t\t{\n\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access2D( one2d, i, j, 4 ) );\n\t\t}\n\t\tJournal_Printf( stream, \"\\n\" );\n\t}\n\n\n\tMemory_Free( one2d );\n\n\tMemory_Print();\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "mpip_673", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n \n\n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n \n\n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_674", "code": "int main(int argc, char *argv[]) \n{ \n    int n, myid, numprocs, i, ierr; \n    double PI25DT = 3.141592653589793238462643; \n    double mypi, pi, h, sum, x; \n    MPI_Win nwin, piwin; \n \n \n    if (myid == 0) { \n    } \n    else { \n    } \n    while (1) { \n        if (myid == 0) { \n            fprintf(stdout, \"Enter the number of intervals: (0 quits) \");\n\t    fflush(stdout); \n            ierr=scanf(\"%d\",&n); \n\t    pi = 0.0;\t\t\t \n        } \n\tif (myid != 0)  \n        if (n == 0) \n            break; \n        else { \n            h   = 1.0 / (double) n; \n            sum = 0.0; \n            for (i = myid + 1; i <= n; i += numprocs) { \n                x = h * ((double)i - 0.5); \n                sum += (4.0 / (1.0 + x*x)); \n            } \n            mypi = h * sum; \n            if (myid == 0) { \n                fprintf(stdout, \"pi is approximately %.16f, Error is %.16f\\n\", \n                       pi, fabs(pi - PI25DT)); \n\t\tfflush(stdout);\n\t    }\n        } \n    } \n    return 0; \n}", "label": "int main(int argc, char *argv[]) \n{ \n    int n, myid, numprocs, i, ierr; \n    double PI25DT = 3.141592653589793238462643; \n    double mypi, pi, h, sum, x; \n    MPI_Win nwin, piwin; \n \n    MPI_Init(&argc,&argv); \n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs); \n    MPI_Comm_rank(MPI_COMM_WORLD,&myid); \n \n    if (myid == 0) { \n\tMPI_Win_create(&n, sizeof(int), 1, MPI_INFO_NULL, \n\t\t       MPI_COMM_WORLD, &nwin); \n\tMPI_Win_create(&pi, sizeof(double), 1, MPI_INFO_NULL, \n\t\t       MPI_COMM_WORLD, &piwin);  \n    } \n    else { \n\tMPI_Win_create(MPI_BOTTOM, 0, 1, MPI_INFO_NULL, \n\t\t       MPI_COMM_WORLD, &nwin); \n\tMPI_Win_create(MPI_BOTTOM, 0, 1, MPI_INFO_NULL, \n\t\t       MPI_COMM_WORLD, &piwin); \n    } \n    while (1) { \n        if (myid == 0) { \n            fprintf(stdout, \"Enter the number of intervals: (0 quits) \");\n\t    fflush(stdout); \n            ierr=scanf(\"%d\",&n); \n\t    pi = 0.0;\t\t\t \n        } \n\tMPI_Win_fence(0, nwin); \n\tif (myid != 0)  \n\t    MPI_Get(&n, 1, MPI_INT, 0, 0, 1, MPI_INT, nwin); \n\tMPI_Win_fence(0, nwin); \n        if (n == 0) \n            break; \n        else { \n            h   = 1.0 / (double) n; \n            sum = 0.0; \n            for (i = myid + 1; i <= n; i += numprocs) { \n                x = h * ((double)i - 0.5); \n                sum += (4.0 / (1.0 + x*x)); \n            } \n            mypi = h * sum; \n\t    MPI_Win_fence( 0, piwin); \n\t    MPI_Accumulate(&mypi, 1, MPI_DOUBLE, 0, 0, 1, MPI_DOUBLE, \n\t\t\t   MPI_SUM, piwin); \n\t    MPI_Win_fence(0, piwin); \n            if (myid == 0) { \n                fprintf(stdout, \"pi is approximately %.16f, Error is %.16f\\n\", \n                       pi, fabs(pi - PI25DT)); \n\t\tfflush(stdout);\n\t    }\n        } \n    } \n    MPI_Win_free(&nwin); \n    MPI_Win_free(&piwin); \n    MPI_Finalize(); \n    return 0; \n}"}
{"program": "NLeSC_675", "code": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n\n\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n       for (i=0;i<10;i++) {\n          buffer[i] = 42+i;\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"Sending to %d\\n\", i);\n\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Send failed! %d\\n\", error);\n          }\n       }\n    } else {\n       fprintf(stderr, \"Receiving from to %d\\n\", 0);\n\n       error =\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Receive failed! %d\\n\", error);\n       } else {\n          fprintf(stderr, \"Received from 0: {\");\n\n          for (i=0;i<10;i++) {\n             fprintf(stderr, \"%d \", buffer[i]);\n          }\n\n          fprintf(stderr, \"}\\n\");\n       }\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n       for (i=0;i<10;i++) {\n          buffer[i] = 42+i;\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"Sending to %d\\n\", i);\n\n          error = MPI_Send(buffer, 10, MPI_INTEGER, i, 0, MPI_COMM_WORLD);\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Send failed! %d\\n\", error);\n          }\n       }\n    } else {\n       fprintf(stderr, \"Receiving from to %d\\n\", 0);\n\n       error = MPI_Recv(buffer, 10, MPI_INTEGER, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Receive failed! %d\\n\", error);\n       } else {\n          fprintf(stderr, \"Received from 0: {\");\n\n          for (i=0;i<10;i++) {\n             fprintf(stderr, \"%d \", buffer[i]);\n          }\n\n          fprintf(stderr, \"}\\n\");\n       }\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "VictorRodriguez_678", "code": "int main(int argc, char ** argv)\n{\n  int n;\n  int * data = NULL;\n  int c, s;\n  int * chunk;\n  int o;\n  int * other;\n  int step;\n  int p, id;\n  MPI_Status status;\n  double elapsed_time;\n  FILE * file = NULL;\n  int i;\n\n  if (argc!=3) {\n    fprintf(stderr, \"Usage: mpirun -np <num_procs> %s <in_file> <out_file>\\n\", argv[0]);\n    exit(1);\n  }\n\n\n  if (id == 0) {\n    \n\n    file = fopen(argv[1], \"r\");\n    fscanf(file, \"%d\", &n);\n    \n\n    c = n/p; if (n%p) c++;\n    \n\n    data = (int *)malloc(p*c * sizeof(int));\n    for (i = 0; i < n; i++)\n      fscanf(file, \"%d\", &(data[i]));\n    fclose(file);\n    \n\n    for (i = n; i < p*c; i++)\n      data[i] = 0;\n  }\n\n  \n\n  elapsed_time = -\n\n  \n\n\n  \n\n  c = n/p; if (n%p) c++;\n\n  \n\n  chunk = (int *)malloc(c * sizeof(int));\n  free(data);\n  data = NULL;\n\n  \n\n  s = (n >= c * (id+1)) ? c : n - c * id;\n  bubblesort(chunk, s);\n\n  \n\n  for (step = 1; step < p; step = 2*step) {\n    if (id % (2*step)!=0) {\n      \n\n      break;\n    }\n    \n\n    if (id+step < p) {\n      \n\n      o = (n >= c * (id+2*step)) ? c * step : n - c * (id+step);\n      \n\n      other = (int *)malloc(o * sizeof(int));\n      \n\n      data = merge(chunk, s, other, o);\n      free(chunk);\n      free(other);\n      chunk = data;\n      s = s + o;\n    }\n  }\n\n  \n\n  elapsed_time +=\n\n  \n\n  if (id == 0) {\n    file = fopen(argv[2], \"w\");\n    fprintf(file, \"%d\\n\", s);   \n\n    for (i = 0; i < s; i++)\n      fprintf(file, \"%d\\n\", chunk[i]);\n    fclose(file);\n    printf(\"Bubblesort %d ints on %d procs: %f secs\\n\", n, p, elapsed_time);\n    \n\n  }\n\n  return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n  int n;\n  int * data = NULL;\n  int c, s;\n  int * chunk;\n  int o;\n  int * other;\n  int step;\n  int p, id;\n  MPI_Status status;\n  double elapsed_time;\n  FILE * file = NULL;\n  int i;\n\n  if (argc!=3) {\n    fprintf(stderr, \"Usage: mpirun -np <num_procs> %s <in_file> <out_file>\\n\", argv[0]);\n    exit(1);\n  }\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n\n  if (id == 0) {\n    \n\n    file = fopen(argv[1], \"r\");\n    fscanf(file, \"%d\", &n);\n    \n\n    c = n/p; if (n%p) c++;\n    \n\n    data = (int *)malloc(p*c * sizeof(int));\n    for (i = 0; i < n; i++)\n      fscanf(file, \"%d\", &(data[i]));\n    fclose(file);\n    \n\n    for (i = n; i < p*c; i++)\n      data[i] = 0;\n  }\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  elapsed_time = - MPI_Wtime();\n\n  \n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  \n\n  c = n/p; if (n%p) c++;\n\n  \n\n  chunk = (int *)malloc(c * sizeof(int));\n  MPI_Scatter(data, c, MPI_INT, chunk, c, MPI_INT, 0, MPI_COMM_WORLD);\n  free(data);\n  data = NULL;\n\n  \n\n  s = (n >= c * (id+1)) ? c : n - c * id;\n  bubblesort(chunk, s);\n\n  \n\n  for (step = 1; step < p; step = 2*step) {\n    if (id % (2*step)!=0) {\n      \n\n      MPI_Send(chunk, s, MPI_INT, id-step, 0, MPI_COMM_WORLD);\n      break;\n    }\n    \n\n    if (id+step < p) {\n      \n\n      o = (n >= c * (id+2*step)) ? c * step : n - c * (id+step);\n      \n\n      other = (int *)malloc(o * sizeof(int));\n      MPI_Recv(other, o, MPI_INT, id+step, 0, MPI_COMM_WORLD, &status);\n      \n\n      data = merge(chunk, s, other, o);\n      free(chunk);\n      free(other);\n      chunk = data;\n      s = s + o;\n    }\n  }\n\n  \n\n  elapsed_time += MPI_Wtime();\n\n  \n\n  if (id == 0) {\n    file = fopen(argv[2], \"w\");\n    fprintf(file, \"%d\\n\", s);   \n\n    for (i = 0; i < s; i++)\n      fprintf(file, \"%d\\n\", chunk[i]);\n    fclose(file);\n    printf(\"Bubblesort %d ints on %d procs: %f secs\\n\", n, p, elapsed_time);\n    \n\n  }\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "syftalent_679", "code": "int main(int argc, char *argv[])\n{\n    MPI_Request request;\n    int size, rank;\n    int one = 1, two = 2, isum, sum;\n    int errs = 0;\n\n    assert(size == 2);\n\n    assert(isum == 2);\n    assert(sum == 4);\n\n    if (MPI_SUCCESS == MPI_Iallreduce(&one, &one, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &request))         errs++;\n\n    if (rank == 0 && errs == 0)\n        printf(\" No errors\\n\");\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Request request;\n    int size, rank;\n    int one = 1, two = 2, isum, sum;\n    int errs = 0;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    assert(size == 2);\n    MPI_Iallreduce(&one,&isum,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD,&request);\n    MPI_Allreduce(&two,&sum,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);\n    MPI_Wait(&request,MPI_STATUS_IGNORE);\n\n    assert(isum == 2);\n    assert(sum == 4);\n\n    MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n    if (MPI_SUCCESS == MPI_Iallreduce(&one, &one, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &request))\n        errs++;\n\n    if (rank == 0 && errs == 0)\n        printf(\" No errors\\n\");\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "savila_680", "code": "int main(int argc, char **argv)\n{\n\n  if(argc < 2)\n    {\n      if(ThisTask == 0)\n\t{\n\t  fprintf(stdout, \"\\nParameters are missing.\\n\");\n\t  fprintf(stdout, \"Call with <ParameterFile>\\n\\n\");\n\t}\n      exit(0);\n    }\n\n  read_parameterfile(argv[1]);\n\n  set_units();\n\n  initialize_powerspectrum();\n\n  initialize_ffts();\n\n  read_glass(GlassFile);\n\n  displacement_fields();\n\n  write_particle_data();\n\n  if(NumPart)\n    free(P);\n\n  free_ffts();\n\n\n  if(ThisTask == 0)\n    {\n      printf(\"\\nIC's generated.\\n\\n\");\n      printf(\"Initial scale factor = %g\\n\", InitTime);\n      printf(\"\\n\");\n    }\n\n  print_spec();\n\n\n  exit(0);\n}", "label": "int main(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &ThisTask);\n  MPI_Comm_size(MPI_COMM_WORLD, &NTask);\n\n  if(argc < 2)\n    {\n      if(ThisTask == 0)\n\t{\n\t  fprintf(stdout, \"\\nParameters are missing.\\n\");\n\t  fprintf(stdout, \"Call with <ParameterFile>\\n\\n\");\n\t}\n      MPI_Finalize();\n      exit(0);\n    }\n\n  read_parameterfile(argv[1]);\n\n  set_units();\n\n  initialize_powerspectrum();\n\n  initialize_ffts();\n\n  read_glass(GlassFile);\n\n  displacement_fields();\n\n  write_particle_data();\n\n  if(NumPart)\n    free(P);\n\n  free_ffts();\n\n\n  if(ThisTask == 0)\n    {\n      printf(\"\\nIC's generated.\\n\\n\");\n      printf(\"Initial scale factor = %g\\n\", InitTime);\n      printf(\"\\n\");\n    }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  print_spec();\n\n  MPI_Finalize();\t\t\n\n  exit(0);\n}"}
{"program": "qingu_681", "code": "int main(int argc, char **argv) {\n  int me, nproc, i;\n  int gsize, *glist;\n  MPI_Comm group;\n\n\n\n  gsize = nproc/2 + (nproc % 2);\n  glist = malloc(gsize*sizeof(int));\n\n  for (i = 0; i < nproc; i += 2)\n    glist[i/2] = i;\n\n  if (me % 2 == 0) {\n    int gme, gnproc;\n\n    PGroup_create(gsize, glist, &group);\n\n    if (verbose) \n      printf(\"[%d] Group rank = %d, size = %d\\n\", me, gme, gnproc);\n\n    if (group != MPI_COMM_SELF)\n  }\n\n  free(glist);\n\n\n  if (me == 0)\n    printf(\" No Errors\\n\");\n\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  int me, nproc, i;\n  int gsize, *glist;\n  MPI_Comm group;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  gsize = nproc/2 + (nproc % 2);\n  glist = malloc(gsize*sizeof(int));\n\n  for (i = 0; i < nproc; i += 2)\n    glist[i/2] = i;\n\n  if (me % 2 == 0) {\n    int gme, gnproc;\n\n    PGroup_create(gsize, glist, &group);\n    MPI_Barrier(group);\n\n    MPI_Comm_rank(group, &gme);\n    MPI_Comm_size(group, &gnproc);\n    if (verbose) \n      printf(\"[%d] Group rank = %d, size = %d\\n\", me, gme, gnproc);\n\n    if (group != MPI_COMM_SELF)\n      MPI_Comm_free(&group);\n  }\n\n  free(glist);\n\n  MPI_Finalize();\n\n  if (me == 0)\n    printf(\" No Errors\\n\");\n\n  return 0;\n}"}
{"program": "germasch_682", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct sph_harm_params par;\n  mrc_params_parse(&par, sph_harm_params_descr, \"sph_harm\", MPI_COMM_WORLD);\n  mrc_params_print(&par, sph_harm_params_descr, \"sph_harm\", MPI_COMM_WORLD);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"simple\");\n  mrc_domain_set_from_options(domain);\n  mrc_domain_view(domain);\n  mrc_domain_setup(domain);\n\n  struct mrc_fld *fld = mrc_domain_fld_create(domain, SW_0, \"phi:ex:ey:ez\");\n  mrc_fld_setup(fld);\n  if (strcmp(par.ic, \"one\") == 0) {\n    ini_one(fld, &par);\n  } else if (strcmp(par.ic, \"semi\") == 0) {\n    ini_semi(fld, &par);\n  } else if (strcmp(par.ic, \"hydrogen\") == 0) {\n    ini_hydrogen(fld, &par);\n  } else {\n    assert(0);\n  }\n  calc_grad(fld);\n\n  struct mrc_io *io = mrc_io_create(MPI_COMM_WORLD);\n  mrc_io_setup(io);\n  mrc_io_open(io, \"w\", 0, 0.);\n  mrc_fld_write(fld, io);\n  mrc_io_close(io);\n  mrc_io_destroy(io);\n\n  mrc_fld_destroy(fld);\n  mrc_domain_destroy(domain);\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct sph_harm_params par;\n  mrc_params_parse(&par, sph_harm_params_descr, \"sph_harm\", MPI_COMM_WORLD);\n  mrc_params_print(&par, sph_harm_params_descr, \"sph_harm\", MPI_COMM_WORLD);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"simple\");\n  mrc_domain_set_from_options(domain);\n  mrc_domain_view(domain);\n  mrc_domain_setup(domain);\n\n  struct mrc_fld *fld = mrc_domain_fld_create(domain, SW_0, \"phi:ex:ey:ez\");\n  mrc_fld_setup(fld);\n  if (strcmp(par.ic, \"one\") == 0) {\n    ini_one(fld, &par);\n  } else if (strcmp(par.ic, \"semi\") == 0) {\n    ini_semi(fld, &par);\n  } else if (strcmp(par.ic, \"hydrogen\") == 0) {\n    ini_hydrogen(fld, &par);\n  } else {\n    assert(0);\n  }\n  calc_grad(fld);\n\n  struct mrc_io *io = mrc_io_create(MPI_COMM_WORLD);\n  mrc_io_setup(io);\n  mrc_io_open(io, \"w\", 0, 0.);\n  mrc_fld_write(fld, io);\n  mrc_io_close(io);\n  mrc_io_destroy(io);\n\n  mrc_fld_destroy(fld);\n  mrc_domain_destroy(domain);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "tancheng_684", "code": "int main(int argc, char *argv[])\n{   \n\tint rank;\n    int size;\n\tint n_ranks = 6;\n\tint n = 10;\n\tint i;\n\n\n#ifndef DUMP\n#endif\n\n\tu8 rk[RK_LEN] = {\n\t\t0x33, 0xc6, 0x6d, 0x09, \n\t\t0x3e, 0x68, 0x5c, 0x0f, \n\t\t0x63, 0x03, 0x13, 0x79, \n\t\t0x92, 0x7c, 0x2a, 0x09, \n\t};\n\n\t\n\n\n\n\tif(rank == 0)\n\t{\n\t\trijndaelKeySetupEnc_Master(encpt_rk,rk, rank);\n\t}\n\t\n\n\telse\n\t{\n\t\trijndaelKeySetupEnc_Middle(encpt_rk, rank);\n\t}\n\n\n\n\n\n#ifdef DUMP\n        if(rank == 0)\n        {       m5_dump_stats(0, 0);\n                m5_reset_stats(0, 0);\n        }\n#endif\n\n\tfor(n = 0; n < 1000; n++)\n\t{\n\t\tif(rank == 0)\n\t\t{\n\t\t\tu8 plain_text[16] = {\n\t\t\t\t0x25, 0x5c, 0x31, 0x1d,\n\t\t\t\t0x71, 0xd8, 0x04, 0xb7,\n\t\t\t\t0x43, 0x11, 0x40, 0x86,\n\t\t\t\t0x12, 0xa2, 0xc8, 0x3d\n\t\t\t};\n\t\t\trijndaelEncrypt_Master(encpt_rk, plain_text, rank);\n\n\n\t\t}\n\t\telse if(rank == n_ranks - 1)\n\t\t{\n\t\t\tu8 cipher_text[16] = {0};\n\t\t\trijndaelEncrypt_Final(encpt_rk, cipher_text, rank);\n\n\t\t\t\n\n\t\t\t\n\n\n\t\t}\n\t\telse\n        {\n\t\t\trijndaelEncrypt_Middle(encpt_rk, rank);\n        }\n        if(n%7 == 0)\n\n\t}\n\n#ifdef DUMP\n\tif(rank == n_ranks -1)\n\t{       m5_dump_stats(0, 0);\n\t        m5_reset_stats(0, 0);\n\t}\n#endif\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{   \n\tint rank;\n    int size;\n\tint n_ranks = 6;\n\tint n = 10;\n\tint i;\n\n\tMPI_Init(&argc, &argv);\n\n#ifndef DUMP\n\tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n#endif\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tu8 rk[RK_LEN] = {\n\t\t0x33, 0xc6, 0x6d, 0x09, \n\t\t0x3e, 0x68, 0x5c, 0x0f, \n\t\t0x63, 0x03, 0x13, 0x79, \n\t\t0x92, 0x7c, 0x2a, 0x09, \n\t};\n\n\t\n\n\n\n\tif(rank == 0)\n\t{\n\t\trijndaelKeySetupEnc_Master(encpt_rk,rk, rank);\n\t}\n\t\n\n\telse\n\t{\n\t\trijndaelKeySetupEnc_Middle(encpt_rk, rank);\n\t}\n\n\n\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n#ifdef DUMP\n        if(rank == 0)\n        {       m5_dump_stats(0, 0);\n                m5_reset_stats(0, 0);\n        }\n#endif\n\n\tfor(n = 0; n < 1000; n++)\n\t{\n\t\tif(rank == 0)\n\t\t{\n\t\t\tu8 plain_text[16] = {\n\t\t\t\t0x25, 0x5c, 0x31, 0x1d,\n\t\t\t\t0x71, 0xd8, 0x04, 0xb7,\n\t\t\t\t0x43, 0x11, 0x40, 0x86,\n\t\t\t\t0x12, 0xa2, 0xc8, 0x3d\n\t\t\t};\n\t\t\trijndaelEncrypt_Master(encpt_rk, plain_text, rank);\n\n\n\t\t}\n\t\telse if(rank == n_ranks - 1)\n\t\t{\n\t\t\tu8 cipher_text[16] = {0};\n\t\t\trijndaelEncrypt_Final(encpt_rk, cipher_text, rank);\n\n\t\t\t\n\n\t\t\t\n\n\n\t\t}\n\t\telse\n        {\n\t\t\trijndaelEncrypt_Middle(encpt_rk, rank);\n        }\n        if(n%7 == 0)\n            MPI_Barrier(MPI_COMM_WORLD);\n\n\t}\n\n#ifdef DUMP\n\tif(rank == n_ranks -1)\n\t{       m5_dump_stats(0, 0);\n\t        m5_reset_stats(0, 0);\n\t}\n#endif\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\tMPI_Finalize();   \n\treturn 0;\n}"}
{"program": "ClaudioNahmad_686", "code": "int main(int argc, char **argv)\n{\n    int i, rank, len, err;\n    int errs = 0;\n    char *filename, *tmp;\n    MPI_File fh;\n    char string[MPI_MAX_ERROR_STRING];\n\n\n#if VERBOSE\n    if (!rank) {\n\tfprintf(stderr, \"Tests if errors are reported correctly...\\n\");\n\tfprintf(stderr, \"Should say \\\"Invalid displacement argument\\\"\\n\\n\");\n    }\n#endif\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+10);\n    }\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    err =\n    err =\n    \n\n\n    \n\n    if (err != MPI_SUCCESS) {\n\tif (!rank) {\n#if VERBOSE\n\t    fprintf(stderr, \"%s\\n\", string);\n#else\n\t    \n\n\t    if (strstr( string, \"displacement\" ) == 0) {\n\t\tfprintf( stderr, \"Unexpected error message %s\\n\", string );\n\t\terrs++;\n\t    }\n#endif\n\t}\n    }\n    else {\n\terrs++;\n\tfprintf( stderr, \"File set view did not return an error\\n\" );\n    }\n\n\n    free(filename);\n    free(tmp);\n\n    if (!rank) {\n\tif (errs == 0) {\n\t    printf( \" No Errors\\n\" );\n\t}\n\telse {\n\t    printf( \" Found %d errors\\n\", errs );\n\t}\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int i, rank, len, err;\n    int errs = 0;\n    char *filename, *tmp;\n    MPI_File fh;\n    char string[MPI_MAX_ERROR_STRING];\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n#if VERBOSE\n    if (!rank) {\n\tfprintf(stderr, \"Tests if errors are reported correctly...\\n\");\n\tfprintf(stderr, \"Should say \\\"Invalid displacement argument\\\"\\n\\n\");\n    }\n#endif\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+10);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE+MPI_MODE_RDWR,\n\t\t        MPI_INFO_NULL, &fh);\n    err = MPI_File_set_view(fh, -1, MPI_BYTE, MPI_BYTE, \"native\",\n                            MPI_INFO_NULL);\n    \n\n\n    \n\n    if (err != MPI_SUCCESS) {\n\tMPI_Error_string(err, string, &len);\n\tif (!rank) {\n#if VERBOSE\n\t    fprintf(stderr, \"%s\\n\", string);\n#else\n\t    \n\n\t    if (strstr( string, \"displacement\" ) == 0) {\n\t\tfprintf( stderr, \"Unexpected error message %s\\n\", string );\n\t\terrs++;\n\t    }\n#endif\n\t}\n    }\n    else {\n\terrs++;\n\tfprintf( stderr, \"File set view did not return an error\\n\" );\n    }\n\n    MPI_File_close(&fh);\n\n    free(filename);\n    free(tmp);\n\n    if (!rank) {\n\tif (errs == 0) {\n\t    printf( \" No Errors\\n\" );\n\t}\n\telse {\n\t    printf( \" Found %d errors\\n\", errs );\n\t}\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "jiajuncao_687", "code": "int\nmain(int argc, char *argv[])\n{\n  int rank;\n  int size;\n  int i = 1;\n\n\n  if (rank == 0) {\n    printf(\"*** Will print ten rows of dots.\\n\");  \n\n  }\n  printf(\"Hello, world, I am %d of %d\\n\", rank, size);\n\n  for (i = 1; i < (int)1e7; i++) {\n    int buf;\n    MPI_Status status;\n\n    buf = i;\n    if (rank == 0) {\n      \n\n    }\n\n    \n\n    if (i != buf) {\n      fprintf(stderr, \"****** INCORRECT RESULT:  %d\\n\", i);\n      exit(1);\n    }\n    if (rank != 0) {\n      \n\n    }\n\n    if (rank == 0) {\n      if (i % (int)1e5 == 0) {\n        printf(\".\"); fflush(stdout);\n      }\n      if (i % (int)5e6 == 0) {\n        printf(\"\\n\");\n      }\n    }\n  }\n  return 0;\n}", "label": "int\nmain(int argc, char *argv[])\n{\n  int rank;\n  int size;\n  int i = 1;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    printf(\"*** Will print ten rows of dots.\\n\");  \n\n  }\n  printf(\"Hello, world, I am %d of %d\\n\", rank, size);\n\n  for (i = 1; i < (int)1e7; i++) {\n    int buf;\n    MPI_Status status;\n\n    buf = i;\n    if (rank == 0) {\n      \n\n      MPI_Send(&buf, 1, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD);\n    }\n\n    \n\n    MPI_Recv(&buf,\n             1,\n             MPI_INT,\n             (rank - 1 + size) % size,\n             0,\n             MPI_COMM_WORLD,\n             &status);\n    if (i != buf) {\n      fprintf(stderr, \"****** INCORRECT RESULT:  %d\\n\", i);\n      exit(1);\n    }\n    if (rank != 0) {\n      \n\n      MPI_Send(&buf, 1, MPI_INT, (rank + 1) % size, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n      if (i % (int)1e5 == 0) {\n        printf(\".\"); fflush(stdout);\n      }\n      if (i % (int)5e6 == 0) {\n        printf(\"\\n\");\n      }\n    }\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "hestela_688", "code": "int main(int argc, char* argv[])\n{\n    int align_size, rank, nprocs; \n    int pairs;\n\n\n    align_size = getpagesize();\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    memset(s_buf, 0, MAX_MSG_SIZE);\n    memset(r_buf, 0, MAX_MSG_SIZE);\n\n\n    pairs = nprocs/2;\n\n    if(rank == 0) {\n        fprintf(stdout, HEADER);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH, \"Latency (us)\");\n        fflush(stdout);\n    }\n\n\n    multi_latency(rank, pairs);\n    \n\n\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char* argv[])\n{\n    int align_size, rank, nprocs; \n    int pairs;\n\n    MPI_Init(&argc, &argv);\n\n    align_size = getpagesize();\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    memset(s_buf, 0, MAX_MSG_SIZE);\n    memset(r_buf, 0, MAX_MSG_SIZE);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    pairs = nprocs/2;\n\n    if(rank == 0) {\n        fprintf(stdout, HEADER);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH, \"Latency (us)\");\n        fflush(stdout);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    multi_latency(rank, pairs);\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "htapiagroup_689", "code": "int main(int argc, char *argv[])\r\n{\r\n\tint rank, size, dato=0;\r\n\tchar host[20];\r\n\tMPI_Status estado;\r\n\n\n\n\t\n\n\tgethostname(host, 20);\r\n\tif(rank > 0 && rank < (size-1))\r\n\t{\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado \n\n\t\t); \r\n\t\tprintf\r\n\t\t(\r\n\t\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\tdato=run_kernel(dato,rank);\r\n\t}else if(rank == size-1){\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado); \n\n\t\tprintf\r\n\t\t(\r\n\t\t\t\"\\nProcess[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\t\r\n\t\tdato=run_kernel(dato,rank);\r\n\t}else if(rank == 0){ \r\n\tprintf\r\n\t(\r\n\t\"\\nProceso[ %d ] desde %s : Enviando Dato = %d \\n\",\r\n\trank,\r\n\thost,\r\n\tdato\r\n\t);\r\n\tMPI_Send\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\trank+1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD \n\n\t); \r\n\tMPI_Recv\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\tsize-1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD, \n\n\t\t&estado \n\n\t);\r\n\tprintf\r\n\t(\r\n\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d \\n\\n\",\r\n\t\trank,\r\n\t\thost,\r\n\t\tdato\r\n\t);\r\n}\r\n\treturn 0;\r\n} \r\n", "label": "int main(int argc, char *argv[])\r\n{\r\n\tint rank, size, dato=0;\r\n\tchar host[20];\r\n\tMPI_Status estado;\r\n\tMPI_Init (&argc, &argv); \n\n\tMPI_Comm_size (MPI_COMM_WORLD, &size);\n\n\tMPI_Comm_rank (MPI_COMM_WORLD, &rank); \n\n\t\n\n\tgethostname(host, 20);\r\n\tif(rank > 0 && rank < (size-1))\r\n\t{\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado \n\n\t\t); \r\n\t\tprintf\r\n\t\t(\r\n\t\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\tdato=run_kernel(dato,rank);\r\n\t\tMPI_Send(&dato, 1 ,MPI_INT ,rank+1 , 0 ,MPI_COMM_WORLD); \r\n\t}else if(rank == size-1){\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado); \n\n\t\tprintf\r\n\t\t(\r\n\t\t\t\"\\nProcess[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\t\r\n\t\tdato=run_kernel(dato,rank);\r\n\t\tMPI_Send(&dato, 1 ,MPI_INT ,0 , 0 ,MPI_COMM_WORLD);\r\n\t}else if(rank == 0){ \r\n\tprintf\r\n\t(\r\n\t\"\\nProceso[ %d ] desde %s : Enviando Dato = %d \\n\",\r\n\trank,\r\n\thost,\r\n\tdato\r\n\t);\r\n\tMPI_Send\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\trank+1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD \n\n\t); \r\n\tMPI_Recv\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\tsize-1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD, \n\n\t\t&estado \n\n\t);\r\n\tprintf\r\n\t(\r\n\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d \\n\\n\",\r\n\t\trank,\r\n\t\thost,\r\n\t\tdato\r\n\t);\r\n}\r\n\tMPI_Finalize(); \r\n\treturn 0;\r\n} \r\n"}
{"program": "dionesf_690", "code": "main(int argc, char *argv[])\n{\n    int i, numtasks, rank, rc;\n    double global_pi = 0, local_pi = 0;\n    double n_iteracoes;\n\n    MPI_Status Stat;\n\n\n    if(rank == 0)\n        n_iteracoes = N_iteracoes;\n\n\n    \n\n    rc =\n\n    int numeros_por_task = n_iteracoes/numtasks;\n\n    int n0 = rank*numeros_por_task;\n    int n = (rank+1)*numeros_por_task - 1;\n\n\n    \n\n    for(i=n0;i<=n;i++)\n    {\n        if(i%2 == 0)\n            local_pi += 1.0/(2.0*i + 1.0);\n        else\n            local_pi -= 1.0/(2.0*i + 1.0);\n\n    }\n\n    printf(\"rank %d local_pi = %.10f\\n\", rank, local_pi);\n\n\n    \n\n    rc =\n\n    if(rank == 0)\n    {\n        \n\n        global_pi = global_pi * 4;\n        printf(\"pi = %.10f\\n\",global_pi);\n\n    }\n\n}", "label": "main(int argc, char *argv[])\n{\n    int i, numtasks, rank, rc;\n    double global_pi = 0, local_pi = 0;\n    double n_iteracoes;\n\n    MPI_Status Stat;\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n    if(rank == 0)\n        n_iteracoes = N_iteracoes;\n\n\n    \n\n    rc = MPI_Bcast(&n_iteracoes,1,MPI_DOUBLE,0,MPI_COMM_WORLD);\n\n    int numeros_por_task = n_iteracoes/numtasks;\n\n    int n0 = rank*numeros_por_task;\n    int n = (rank+1)*numeros_por_task - 1;\n\n\n    \n\n    for(i=n0;i<=n;i++)\n    {\n        if(i%2 == 0)\n            local_pi += 1.0/(2.0*i + 1.0);\n        else\n            local_pi -= 1.0/(2.0*i + 1.0);\n\n    }\n\n    printf(\"rank %d local_pi = %.10f\\n\", rank, local_pi);\n\n\n    \n\n    rc = MPI_Reduce(&local_pi, &global_pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0)\n    {\n        \n\n        global_pi = global_pi * 4;\n        printf(\"pi = %.10f\\n\",global_pi);\n\n    }\n\n    MPI_Finalize();\n}"}
{"program": "callmetaste_691", "code": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,nbsols,mysolnum,*a,*b,fail;\n   double *c,startwtime,endwtime,wtime,*time;\n   MPI_Status status;\n \n   adainit();\n\n   if(myid ==0)\n   {\n      time = (double*) calloc(numprocs, sizeof(double));\n      startwtime =\n   }\n\n   dimension_broadcast(myid,&n);\n   monomials_broadcast(myid,n);\n   copy_broadcast(myid); \n   start_system_broadcast(myid,n,&nbsols);\n\n   if(myid == 0)\n   {\n      fail = _ada_use_c2phc(19,a,b,c);  \n\n      printf(\"\\nSee the output file for results...\\n\\n\");\n   }\n\n   solutions_distribute(myid,nbsols,n,numprocs,&mysolnum);\n\n   if(myid != 0) \n   {\n      startwtime =\n      fail = _ada_use_c2phc(16,a,b,c);  \n\n      if(v) printf(\"Node %d done.\\n\", myid);\n      if(v) printf(\"Solutions computed by node %d:\\n\", myid);\n      if(v) fail = _ada_use_c2phc(17,a,b,c);  \n\n      fail = _ada_use_c2phc(5,a,b,c);        \n\n      \n \n      \n\n      \n\n      endwtime =\n   }\n   solutions_collect(myid,nbsols,n,numprocs,mysolnum);\n   if(myid==0)\n   {\n      fail = _ada_use_c2phc(31,a,b,c);       \n\n      fail = _ada_use_c2phc(37,a,b,c);       \n\n   }\n\n   if(myid == 0) endwtime =\n   wtime = endwtime-startwtime;\n   if(myid == 0) print_time(time,numprocs);\n\n   adafinal();\n\n   return 0;\n}", "label": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,nbsols,mysolnum,*a,*b,fail;\n   double *c,startwtime,endwtime,wtime,*time;\n   MPI_Status status;\n \n   adainit();\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n   if(myid ==0)\n   {\n      time = (double*) calloc(numprocs, sizeof(double));\n      startwtime = MPI_Wtime();\n   }\n\n   dimension_broadcast(myid,&n);\n   monomials_broadcast(myid,n);\n   copy_broadcast(myid); \n   start_system_broadcast(myid,n,&nbsols);\n\n   if(myid == 0)\n   {\n      fail = _ada_use_c2phc(19,a,b,c);  \n\n      printf(\"\\nSee the output file for results...\\n\\n\");\n   }\n\n   MPI_Bcast(&nbsols,1,MPI_INT,0,MPI_COMM_WORLD);\n   solutions_distribute(myid,nbsols,n,numprocs,&mysolnum);\n\n   if(myid != 0) \n   {\n      startwtime = MPI_Wtime();\n      fail = _ada_use_c2phc(16,a,b,c);  \n\n      if(v) printf(\"Node %d done.\\n\", myid);\n      if(v) printf(\"Solutions computed by node %d:\\n\", myid);\n      if(v) fail = _ada_use_c2phc(17,a,b,c);  \n\n      fail = _ada_use_c2phc(5,a,b,c);        \n\n      \n \n      \n\n      \n\n      endwtime = MPI_Wtime();\n   }\n   solutions_collect(myid,nbsols,n,numprocs,mysolnum);\n   if(myid==0)\n   {\n      fail = _ada_use_c2phc(31,a,b,c);       \n\n      fail = _ada_use_c2phc(37,a,b,c);       \n\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n   if(myid == 0) endwtime = MPI_Wtime();\n   wtime = endwtime-startwtime;\n   MPI_Gather(&wtime,1,MPI_DOUBLE,time,1,MPI_DOUBLE,0,MPI_COMM_WORLD);\n   if(myid == 0) print_time(time,numprocs);\n\n   MPI_Finalize();\n   adafinal();\n\n   return 0;\n}"}
{"program": "syftalent_692", "code": "int main( int argc, char *argv[] )\n{\n    int          rank;\n    MPI_Datatype my_int;\n\n\n    \n\n    CAS_CHECK_TYPE(signed char,         MPI_SIGNED_CHAR,        MPI_SUCCESS);\n    CAS_CHECK_TYPE(short,               MPI_SHORT,              MPI_SUCCESS);\n    CAS_CHECK_TYPE(int,                 MPI_INT,                MPI_SUCCESS);\n    CAS_CHECK_TYPE(long,                MPI_LONG,               MPI_SUCCESS);\n    CAS_CHECK_TYPE(long long,           MPI_LONG_LONG,          MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned char,       MPI_UNSIGNED_CHAR,      MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned short,      MPI_UNSIGNED_SHORT,     MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned,            MPI_UNSIGNED,           MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned long,       MPI_UNSIGNED_LONG,      MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned long long,  MPI_UNSIGNED_LONG_LONG, MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(MPI_Aint,            MPI_AINT,               MPI_SUCCESS);\n    CAS_CHECK_TYPE(MPI_Offset,          MPI_OFFSET,             MPI_SUCCESS);\n    CAS_CHECK_TYPE(MPI_Count,           MPI_COUNT,              MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(char,                MPI_BYTE,               MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(char,                MPI_C_BOOL,             MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(int,                 my_int,                 MPI_ERR_TYPE);\n\n    \n\n    CAS_CHECK_TYPE(char,                MPI_CHAR,               MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(float,               MPI_FLOAT,              MPI_ERR_TYPE);\n    CAS_CHECK_TYPE(double,              MPI_DOUBLE,             MPI_ERR_TYPE);\n#ifdef HAVE_LONG_DOUBLE\n    if (MPI_LONG_DOUBLE != MPI_DATATYPE_NULL) {\n        CAS_CHECK_TYPE(long double,         MPI_LONG_DOUBLE,        MPI_ERR_TYPE);\n    }\n#endif\n\n    if (rank == 0) printf(\" No Errors\\n\");\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int          rank;\n    MPI_Datatype my_int;\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\n    \n\n    CAS_CHECK_TYPE(signed char,         MPI_SIGNED_CHAR,        MPI_SUCCESS);\n    CAS_CHECK_TYPE(short,               MPI_SHORT,              MPI_SUCCESS);\n    CAS_CHECK_TYPE(int,                 MPI_INT,                MPI_SUCCESS);\n    CAS_CHECK_TYPE(long,                MPI_LONG,               MPI_SUCCESS);\n    CAS_CHECK_TYPE(long long,           MPI_LONG_LONG,          MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned char,       MPI_UNSIGNED_CHAR,      MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned short,      MPI_UNSIGNED_SHORT,     MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned,            MPI_UNSIGNED,           MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned long,       MPI_UNSIGNED_LONG,      MPI_SUCCESS);\n    CAS_CHECK_TYPE(unsigned long long,  MPI_UNSIGNED_LONG_LONG, MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(MPI_Aint,            MPI_AINT,               MPI_SUCCESS);\n    CAS_CHECK_TYPE(MPI_Offset,          MPI_OFFSET,             MPI_SUCCESS);\n    CAS_CHECK_TYPE(MPI_Count,           MPI_COUNT,              MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(char,                MPI_BYTE,               MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(char,                MPI_C_BOOL,             MPI_SUCCESS);\n\n    \n\n    MPI_Type_contiguous(sizeof(int), MPI_BYTE, &my_int);\n    MPI_Type_commit(&my_int);\n    CAS_CHECK_TYPE(int,                 my_int,                 MPI_ERR_TYPE);\n    MPI_Type_free(&my_int);\n\n    \n\n    CAS_CHECK_TYPE(char,                MPI_CHAR,               MPI_SUCCESS);\n\n    \n\n    CAS_CHECK_TYPE(float,               MPI_FLOAT,              MPI_ERR_TYPE);\n    CAS_CHECK_TYPE(double,              MPI_DOUBLE,             MPI_ERR_TYPE);\n#ifdef HAVE_LONG_DOUBLE\n    if (MPI_LONG_DOUBLE != MPI_DATATYPE_NULL) {\n        CAS_CHECK_TYPE(long double,         MPI_LONG_DOUBLE,        MPI_ERR_TYPE);\n    }\n#endif\n\n    if (rank == 0) printf(\" No Errors\\n\");\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mnv104_693", "code": "int main( int argc, char *argv[] ) \n{\n    struct config_t config;\n\n    int my_rank, ring_size;\n    \n\n    if ( parse_args( argc, argv, &config ) < 0 ) {\n        return -1;\n    }\n\n    if ( init( config ) < 0 ) {\n        return -1;\n    }\n\n    do_test( config.no_writes, config.write_size, ring_size, my_rank );  \n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] ) \n{\n    struct config_t config;\n\n    int my_rank, ring_size;\n    \n    MPI_Init( &argc, &argv );\n    MPI_Comm_size( MPI_COMM_WORLD, &ring_size );\n    MPI_Comm_rank( MPI_COMM_WORLD, &my_rank );\n\n    if ( parse_args( argc, argv, &config ) < 0 ) {\n        MPI_Finalize();\n        return -1;\n    }\n\n    if ( init( config ) < 0 ) {\n        MPI_Finalize();\n        return -1;\n    }\n\n    do_test( config.no_writes, config.write_size, ring_size, my_rank );  \n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_695", "code": "int main(int argc, char *argv[])\n{\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tStream*\t\t\tstream;\n\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\n\tstream = Journal_Register( \"info\", \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\n\tif ( rank == procToWatch ) {\n\t\tNamedObject_Register* reg = NamedObject_Register_New();\n\t\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"a\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"b\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"c\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"d\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"e\" ) );\n\t\n\t\tPrint( reg, stream );\n\n\t\tJournal_Printf( stream, \"Index of \\\"b\\\": %d\\n\", NamedObject_Register_GetIndex( reg, \"b\" ) );\n\t\tPrint( NamedObject_Register_GetByName( reg, \"d\" ), stream );\n\t\tPrint( NamedObject_Register_GetByIndex( reg, 2 ), stream );\n\t}\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main(int argc, char *argv[])\n{\n\tint\t\t\trank;\n\tint\t\t\tprocCount;\n\tint\t\t\tprocToWatch;\n\tStream*\t\t\tstream;\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tBaseFoundation_Init( &argc, &argv );\n\n\tstream = Journal_Register( \"info\", \"myStream\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\n\tif ( rank == procToWatch ) {\n\t\tNamedObject_Register* reg = NamedObject_Register_New();\n\t\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"a\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"b\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"c\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"d\" ) );\n\t\tNamedObject_Register_Add( reg, TestObject_New( \"e\" ) );\n\t\n\t\tPrint( reg, stream );\n\n\t\tJournal_Printf( stream, \"Index of \\\"b\\\": %d\\n\", NamedObject_Register_GetIndex( reg, \"b\" ) );\n\t\tPrint( NamedObject_Register_GetByName( reg, \"d\" ), stream );\n\t\tPrint( NamedObject_Register_GetByIndex( reg, 2 ), stream );\n\t}\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "luizirber_697", "code": "int main(int argc, char **argv) {\n  int rank, procs;\n  long long int my_sum = 0;\n  long long int total_sum = 0;\n  int vector[VECTOR_SIZE];\n  int chunk_size;\n\n  MPI_Status status;\n\n  chunk_size = calc_chunk_for_proc(rank, VECTOR_SIZE, procs);\n  if (rank == 0) {\n    init_vector(vector);\n    distribute_vector(vector, procs);\n  } else {\n    receive_vector(vector, chunk_size);\n  }\n\n  my_sum = sum_vector(vector, chunk_size);\n\n\n  \n\n\n\n  return(0);\n\n}", "label": "int main(int argc, char **argv) {\n  int rank, procs;\n  long long int my_sum = 0;\n  long long int total_sum = 0;\n  int vector[VECTOR_SIZE];\n  int chunk_size;\n\n  MPI_Status status;\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  chunk_size = calc_chunk_for_proc(rank, VECTOR_SIZE, procs);\n  if (rank == 0) {\n    init_vector(vector);\n    distribute_vector(vector, procs);\n  } else {\n    receive_vector(vector, chunk_size);\n  }\n\n  my_sum = sum_vector(vector, chunk_size);\n\n  MPI_Reduce(&my_sum, &total_sum, 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  \n\n\n  MPI_Finalize();\n\n  return(0);\n\n}"}
{"program": "pf-aics-riken_700", "code": "int\nmain(int argc, char **argv)\n{\n    static char b[8 * 1024];\n    int cc;\n    int nprocs, rank;\n\n    struct kmr_fefs_stripe stripe;\n\n    if (rank == 0) {\n\tprintf(\"print ost of shared directory on each rank.\\n\");\n\tfflush(0);\n    }\n    usleep(200 * 1000);\n\n    for (int i = 0; i < nprocs; i++) {\n\tif (rank == i) {\n\t    kmr_readin(\"/proc/tofu/position\", b, sizeof(b));\n\t    printf(\"rank=%d position=%s\\n\", rank, b);\n\n\t    cc = kmr_fefs_get_stripe(\".\", \"a.out\", &stripe, 0);\n\t    assert(cc == 0);\n\t    printf(\"[%04d] rankdirfile=./a.out stripe_count=%d\\n\",\n\t\t   rank, stripe.count);\n\t    for (int j = 0; j < stripe.count; j++) {\n\t\tprintf(\"[%04d] idx=%d\\n\", rank, stripe.obdidx[j]);\n\t    }\n\t    printf(\"\\n\");\n\n\t    cc = kmr_fefs_get_stripe(\"..\", 0, &stripe, 0);\n\t    assert(cc == 0);\n\t    printf(\"[%04d] sharedir=.. stripe_count=%d\\n\",\n\t\t   rank, stripe.count);\n\t    for (int j = 0; j < stripe.count; j++) {\n\t\tprintf(\"[%04d] idx=%d\\n\", rank, stripe.obdidx[j]);\n\t    }\n\t    printf(\"\\n\");\n\t}\n\tfflush(0);\n\tusleep(200 * 1000);\n    }\n\n    if (rank == 0) {\n\tint fd = open(\"../aho\", O_WRONLY, O_CREAT, 0);\n\tassert(fd != -1);\n\tclose(fd);\n\n\tcc = kmr_fefs_get_stripe(\"..\", \"aho\", &stripe, 0);\n\tassert(cc == 0);\n\tprintf(\"[%04d] sharedir=../aho stripe_count=%d\\n\",\n\t       rank, stripe.count);\n\tfor (int j = 0; j < stripe.count; j++) {\n\t    printf(\"[%04d] idx=%d\\n\", rank, stripe.obdidx[j]);\n\t}\n\tprintf(\"\\n\");\n    }\n\n    return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n    static char b[8 * 1024];\n    int cc;\n    MPI_Init(&argc, &argv);\n    int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    struct kmr_fefs_stripe stripe;\n\n    if (rank == 0) {\n\tprintf(\"print ost of shared directory on each rank.\\n\");\n\tfflush(0);\n    }\n    usleep(200 * 1000);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    for (int i = 0; i < nprocs; i++) {\n\tif (rank == i) {\n\t    kmr_readin(\"/proc/tofu/position\", b, sizeof(b));\n\t    printf(\"rank=%d position=%s\\n\", rank, b);\n\n\t    cc = kmr_fefs_get_stripe(\".\", \"a.out\", &stripe, 0);\n\t    assert(cc == 0);\n\t    printf(\"[%04d] rankdirfile=./a.out stripe_count=%d\\n\",\n\t\t   rank, stripe.count);\n\t    for (int j = 0; j < stripe.count; j++) {\n\t\tprintf(\"[%04d] idx=%d\\n\", rank, stripe.obdidx[j]);\n\t    }\n\t    printf(\"\\n\");\n\n\t    cc = kmr_fefs_get_stripe(\"..\", 0, &stripe, 0);\n\t    assert(cc == 0);\n\t    printf(\"[%04d] sharedir=.. stripe_count=%d\\n\",\n\t\t   rank, stripe.count);\n\t    for (int j = 0; j < stripe.count; j++) {\n\t\tprintf(\"[%04d] idx=%d\\n\", rank, stripe.obdidx[j]);\n\t    }\n\t    printf(\"\\n\");\n\t}\n\tfflush(0);\n\tusleep(200 * 1000);\n\tMPI_Barrier(MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n\tint fd = open(\"../aho\", O_WRONLY, O_CREAT, 0);\n\tassert(fd != -1);\n\tclose(fd);\n\n\tcc = kmr_fefs_get_stripe(\"..\", \"aho\", &stripe, 0);\n\tassert(cc == 0);\n\tprintf(\"[%04d] sharedir=../aho stripe_count=%d\\n\",\n\t       rank, stripe.count);\n\tfor (int j = 0; j < stripe.count; j++) {\n\t    printf(\"[%04d] idx=%d\\n\", rank, stripe.obdidx[j]);\n\t}\n\tprintf(\"\\n\");\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "arjona00_701", "code": "int main(int argc, char** argv) {\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: avg num_elements_per_proc\\n\");\n    exit(1);\n  }\n\n  int num_elements_per_proc = atoi(argv[1]);\n  \n\n  srand(time(NULL));\n\n\n  int world_rank;\n  int world_size;\n\n  \n\n  \n\n  \n\n  float *rand_nums = NULL;\n  if (world_rank == 0) {\n    rand_nums = create_rand_nums(num_elements_per_proc * world_size);\n  }\n\n  \n\n  \n\n  float *sub_rand_nums = (float *)malloc(sizeof(float) * num_elements_per_proc);\n  assert(sub_rand_nums != NULL);\n\n  \n\n  \n\n\n  \n\n  float sub_avg = compute_avg(sub_rand_nums, num_elements_per_proc);\n \n  \n\n  float *sub_avgs = (float *)malloc(sizeof(float) * world_size);\n  assert(sub_avgs != NULL);\n\n  \n\n  \n\n  \n\n  \n\n  float avg = compute_avg(sub_avgs, world_size);\n  printf(\"Avg of all elements from proc %d is %f\\n\", world_rank, avg);\n\n  \n\n  if (world_rank == 0) {\n    free(rand_nums);\n  }\n  free(sub_avgs);\n  free(sub_rand_nums);\n \n}", "label": "int main(int argc, char** argv) {\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: avg num_elements_per_proc\\n\");\n    exit(1);\n  }\n\n  int num_elements_per_proc = atoi(argv[1]);\n  \n\n  srand(time(NULL));\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  \n\n  \n\n  float *rand_nums = NULL;\n  if (world_rank == 0) {\n    rand_nums = create_rand_nums(num_elements_per_proc * world_size);\n  }\n\n  \n\n  \n\n  float *sub_rand_nums = (float *)malloc(sizeof(float) * num_elements_per_proc);\n  assert(sub_rand_nums != NULL);\n\n  \n\n  \n\n  MPI_Scatter(rand_nums, num_elements_per_proc, MPI_FLOAT, sub_rand_nums,\n              num_elements_per_proc, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n  \n\n  float sub_avg = compute_avg(sub_rand_nums, num_elements_per_proc);\n \n  \n\n  float *sub_avgs = (float *)malloc(sizeof(float) * world_size);\n  assert(sub_avgs != NULL);\n  MPI_Allgather(&sub_avg, 1, MPI_FLOAT, sub_avgs, 1, MPI_FLOAT, MPI_COMM_WORLD);\n\n  \n\n  \n\n  \n\n  \n\n  float avg = compute_avg(sub_avgs, world_size);\n  printf(\"Avg of all elements from proc %d is %f\\n\", world_rank, avg);\n\n  \n\n  if (world_rank == 0) {\n    free(rand_nums);\n  }\n  free(sub_avgs);\n  free(sub_rand_nums);\n \n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}"}
{"program": "lorenzgerber_702", "code": "int main(int argc, char *argv[]) {\n  int my_rank, comm_sz;\n  int n, local_n, local_dotp_sum = 0, scalar, result_dot;\n  int* sendcounts;\n  int* displs;\n  int* local_vec1;\n  int* local_vec2;\n  int* vector1;\n  int* vector2;\n\n  \n\n  srand(time(NULL));\n\n  \n\n  if(my_rank==0 && argc > 1){\n\n    if(strcmp(argv[1], \"r\") == 0){\n      printf(\"using random data, vector length = %d\\n\", 100*comm_sz);\n      n = 100*comm_sz;\n      vector1 = (int *) malloc(100*comm_sz * sizeof(int));\n      vector2 = (int *) malloc(100*comm_sz * sizeof(int));\n\n      for(int i = 0; i < n;i++){\n\tvector1[i] = rand() % 1000;\n\tvector2[i] = rand() % 1000;\n      }\n      scalar = rand() % 1000;\n    }\n    \n  } else if (my_rank==0){\n     \n    printf(\"enter vector length\\n\");\n    scanf(\"%d\", &n);\n    printf(\"enter integer vector 1\\n\");\n\n    vector1 = (int *) malloc(n * sizeof(int));\n    vector2 = (int *) malloc(n * sizeof(int));\n\n    for(int i = 0; i < n; i++){\n      scanf(\"%d\", &vector1[i]);\n    }\n\n    printf(\"enter integer vector 2\\n\");\n\n    for(int i = 0; i < n; i++){\n      scanf(\"%d\", &vector2[i]);\n    }\n\n    printf(\"enter integer scalar\\n\");\n    scanf(\"%d\", &scalar);\n  }\n\n  \n\n\n  \n\n  sendcounts = (int *) malloc(comm_sz * sizeof(int));\n  displs = (int *) calloc(comm_sz,  sizeof(int));\n\n  for(int i = 0; i < comm_sz;i++){\n    if(n % comm_sz > i){\n      sendcounts[i] = (n/comm_sz) + 1;\n    } else {\n      sendcounts[i] = (n/comm_sz);\n    }\n\n  }\n\n  for(int i = 1; i <comm_sz; i++){\n    displs[i] = displs[i-1]+sendcounts[i];   \n  }\n\n \n  local_n = sendcounts[my_rank];\n\n  local_vec1 = (int*) malloc(local_n * sizeof(int));\n  local_vec2 = (int*) malloc(local_n * sizeof(int));\n  \n\n  \n  \n\n  \n\n  for(int i = 0;  i< local_n; i++){\n    local_vec2[i]*=local_vec1[i];\n  }\n\n  \n\n  for(int i = 0; i< local_n; i++){\n    local_vec1[i]*=scalar;\n  }\n\n  \n\n  for(int i = 0; i < local_n; i++){\n    local_dotp_sum += local_vec2[i];    \n  } \n \n  \n\n\n  \n\n  if(my_rank == 0){\n    printf(\"dot product = %d\\n\", result_dot);\n\n    printf(\"vector-scalar product = \");\n    for(int i = 0; i < n;i++){\n      printf(\"%d \", vector1[i]);\n    }\n    printf(\"\\n\");\n  }\n\n  \n\n  if(my_rank==0){\n    free(vector1);\n    free(vector2);\n  }\n  \n  free(local_vec1);\n  free(local_vec2);\n\n\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  int my_rank, comm_sz;\n  int n, local_n, local_dotp_sum = 0, scalar, result_dot;\n  int* sendcounts;\n  int* displs;\n  int* local_vec1;\n  int* local_vec2;\n  int* vector1;\n  int* vector2;\n\n  \n\n  MPI_Init(NULL, NULL);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n  srand(time(NULL));\n\n  \n\n  if(my_rank==0 && argc > 1){\n\n    if(strcmp(argv[1], \"r\") == 0){\n      printf(\"using random data, vector length = %d\\n\", 100*comm_sz);\n      n = 100*comm_sz;\n      vector1 = (int *) malloc(100*comm_sz * sizeof(int));\n      vector2 = (int *) malloc(100*comm_sz * sizeof(int));\n\n      for(int i = 0; i < n;i++){\n\tvector1[i] = rand() % 1000;\n\tvector2[i] = rand() % 1000;\n      }\n      scalar = rand() % 1000;\n    }\n    \n  } else if (my_rank==0){\n     \n    printf(\"enter vector length\\n\");\n    scanf(\"%d\", &n);\n    printf(\"enter integer vector 1\\n\");\n\n    vector1 = (int *) malloc(n * sizeof(int));\n    vector2 = (int *) malloc(n * sizeof(int));\n\n    for(int i = 0; i < n; i++){\n      scanf(\"%d\", &vector1[i]);\n    }\n\n    printf(\"enter integer vector 2\\n\");\n\n    for(int i = 0; i < n; i++){\n      scanf(\"%d\", &vector2[i]);\n    }\n\n    printf(\"enter integer scalar\\n\");\n    scanf(\"%d\", &scalar);\n  }\n\n  \n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  \n\n  sendcounts = (int *) malloc(comm_sz * sizeof(int));\n  displs = (int *) calloc(comm_sz,  sizeof(int));\n\n  for(int i = 0; i < comm_sz;i++){\n    if(n % comm_sz > i){\n      sendcounts[i] = (n/comm_sz) + 1;\n    } else {\n      sendcounts[i] = (n/comm_sz);\n    }\n\n  }\n\n  for(int i = 1; i <comm_sz; i++){\n    displs[i] = displs[i-1]+sendcounts[i];   \n  }\n\n \n  local_n = sendcounts[my_rank];\n\n  local_vec1 = (int*) malloc(local_n * sizeof(int));\n  local_vec2 = (int*) malloc(local_n * sizeof(int));\n  \n  MPI_Bcast(&scalar, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Scatterv(vector1, sendcounts, displs, MPI_INT, local_vec1, sendcounts[my_rank], MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Scatterv(vector2, sendcounts, displs, MPI_INT, local_vec2, sendcounts[my_rank], MPI_INT, 0, MPI_COMM_WORLD);\n\n  \n  \n\n  \n\n  for(int i = 0;  i< local_n; i++){\n    local_vec2[i]*=local_vec1[i];\n  }\n\n  \n\n  for(int i = 0; i< local_n; i++){\n    local_vec1[i]*=scalar;\n  }\n\n  \n\n  for(int i = 0; i < local_n; i++){\n    local_dotp_sum += local_vec2[i];    \n  } \n \n  \n\n  MPI_Gatherv(local_vec1, sendcounts[my_rank], MPI_INT, vector1, sendcounts, displs, MPI_INT, 0, MPI_COMM_WORLD); \n  MPI_Reduce(&local_dotp_sum, &result_dot, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  \n\n  if(my_rank == 0){\n    printf(\"dot product = %d\\n\", result_dot);\n\n    printf(\"vector-scalar product = \");\n    for(int i = 0; i < n;i++){\n      printf(\"%d \", vector1[i]);\n    }\n    printf(\"\\n\");\n  }\n\n  \n\n  if(my_rank==0){\n    free(vector1);\n    free(vector2);\n  }\n  \n  free(local_vec1);\n  free(local_vec2);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "markfasheh_703", "code": "int main(int argc, char *argv[])\n{\n\tint ret;\n\tchar *which;\n\n\tret =\n\tif (ret != MPI_SUCCESS) {\n\t\tfprintf(stderr, \"MPI_Init failed: %d\\n\", ret);\n\t\texit(1);\n\t}\n\n\tprog = strrchr(argv[0], '/');\n\tif (prog == NULL)\n\t\tprog = argv[0];\n\telse\n\t\tprog++;\n\n\tif (parse_opts(argc, argv))\n\t\tusage();\n\n\tif (gethostname(hostname, HOSTNAME_SIZE) < 0) {\n\t\tfprintf(stderr, \"gethostname failed: %s\\n\",\n\t\t\tstrerror(errno));\n\t\texit(1);\n\t}\n\n\tret =\n        if (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_rank failed: %d\\n\", ret);\n\n\tret =\n\tif (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_size failed: %d\\n\", ret);\n\n\tif (num_procs != 2)\n\t\tabort_printf(\"Need two processes for this test.\\n\");\n\n        printf(\"%s: rank: %d, procs: %d, path1 \\\"%s\\\" path2 \\\"%s\\\"\\n\",\n\t       hostname, rank, num_procs, path1, path2);\n\n\tif (rank == 0)\n\t\twhich = path1;\n\telse\n\t\twhich = path2;\n\ttest_1node_1file(which);\n\n\tdo_barrier();\n\n\ttest_2nodes_1file(path1, rank == 0);\n\n\tdo_barrier();\n\n\ttest_deadlocks(path1, path2, rank == 0);\n\n        return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint ret;\n\tchar *which;\n\n\tret = MPI_Init(&argc, &argv);\n\tif (ret != MPI_SUCCESS) {\n\t\tfprintf(stderr, \"MPI_Init failed: %d\\n\", ret);\n\t\texit(1);\n\t}\n\n\tprog = strrchr(argv[0], '/');\n\tif (prog == NULL)\n\t\tprog = argv[0];\n\telse\n\t\tprog++;\n\n\tif (parse_opts(argc, argv))\n\t\tusage();\n\n\tif (gethostname(hostname, HOSTNAME_SIZE) < 0) {\n\t\tfprintf(stderr, \"gethostname failed: %s\\n\",\n\t\t\tstrerror(errno));\n\t\texit(1);\n\t}\n\n\tret = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        if (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_rank failed: %d\\n\", ret);\n\n\tret = MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\tif (ret != MPI_SUCCESS)\n\t\tabort_printf(\"MPI_Comm_size failed: %d\\n\", ret);\n\n\tif (num_procs != 2)\n\t\tabort_printf(\"Need two processes for this test.\\n\");\n\n        printf(\"%s: rank: %d, procs: %d, path1 \\\"%s\\\" path2 \\\"%s\\\"\\n\",\n\t       hostname, rank, num_procs, path1, path2);\n\n\tif (rank == 0)\n\t\twhich = path1;\n\telse\n\t\twhich = path2;\n\ttest_1node_1file(which);\n\n\tdo_barrier();\n\n\ttest_2nodes_1file(path1, rank == 0);\n\n\tdo_barrier();\n\n\ttest_deadlocks(path1, path2, rank == 0);\n\n        MPI_Finalize();\n        return 0;\n}"}
{"program": "autarchprinceps_705", "code": "int main (int argc, char **argv) {\n\tint maxiter, err;\n\tdouble xmin, ymin, xmax, ymax;\n\tdouble dx, dy;\n\tdouble t_start, t_end;\n\n\tidx_t part[X_RESOLUTION];\n\n\t\n\n\terr =\n\tassert(err == MPI_SUCCESS);\n\terr =\n\tassert(err == MPI_SUCCESS);\n\terr =\n\tassert(err == MPI_SUCCESS);\n\n\t\n\n\txmin = -1.5;\n\tymin = -1.5;\n\txmax = 1.5;\n\tymax = 1.5;\n\tmaxiter = 100000;\n\n\t\n\n\tdx = (xmax - xmin) / X_RESOLUTION;\n\tdy = (ymax - ymin) / Y_RESOLUTION;\n\n\tMPI_Status status;\n\n\tif(size > 2) {\n\t\tif(rank == 0) {\n\t\t\tidx_t task_times[X_RESOLUTION];\n\t\t\tmandelbrot_simulate(maxiter, dx, dy, xmin, ymin, task_times);\n\t\t\tgraph_distribution(size, task_times, part);\n\n\t\t\tMPI_Request async[size - 1];\n\t\t\tfor(int i = 1; i < size; i++) {\n\t\t\t}\n\t\t\tfor(int i = 0; i < size - 1; i++) {\n\t\t\t\terr =\n\t\t\t\tassert(err == MPI_SUCCESS);\n\t\t\t}\n\t\t} else {\n\t\t\terr =\n\t\t\tassert(err == MPI_SUCCESS);\n\t\t}\n\t}\n\t\n\n\t\n\t\n\n\t\n\n\n\t\n\n\tt_start = gettime();\n\n\tmandelbrot(maxiter, dx, dy, xmin, ymin, part);\n\n\t\n\n\tt_end = gettime();\n\n\t\n\n\n\tif(rank == 0) {\n\t\tprintf (\"calculation took %.2f s on %d+1 processors\\n\", t_end - t_start, size-1);\n\t\tprintf(\"checksum = %lu\\n\", checksum);\n\t}\n\t\n\t\n\n\terr =\n\tassert(err == MPI_SUCCESS);\n\t\n\treturn EXIT_SUCCESS;\n}\n", "label": "int main (int argc, char **argv) {\n\tint maxiter, err;\n\tdouble xmin, ymin, xmax, ymax;\n\tdouble dx, dy;\n\tdouble t_start, t_end;\n\n\tidx_t part[X_RESOLUTION];\n\n\t\n\n\terr = MPI_Init (&argc, &argv);\n\tassert(err == MPI_SUCCESS);\n\terr = MPI_Comm_size (MPI_COMM_WORLD, &size);\n\tassert(err == MPI_SUCCESS);\n\terr = MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\tassert(err == MPI_SUCCESS);\n\n\t\n\n\txmin = -1.5;\n\tymin = -1.5;\n\txmax = 1.5;\n\tymax = 1.5;\n\tmaxiter = 100000;\n\n\t\n\n\tdx = (xmax - xmin) / X_RESOLUTION;\n\tdy = (ymax - ymin) / Y_RESOLUTION;\n\n\tMPI_Status status;\n\n\tif(size > 2) {\n\t\tif(rank == 0) {\n\t\t\tidx_t task_times[X_RESOLUTION];\n\t\t\tmandelbrot_simulate(maxiter, dx, dy, xmin, ymin, task_times);\n\t\t\tgraph_distribution(size, task_times, part);\n\n\t\t\tMPI_Request async[size - 1];\n\t\t\tfor(int i = 1; i < size; i++) {\n\t\t\t\tMPI_Isend(part, X_RESOLUTION, MPI_INT, i, 42, MPI_COMM_WORLD, &async[i - 1]);\n\t\t\t}\n\t\t\tfor(int i = 0; i < size - 1; i++) {\n\t\t\t\terr = MPI_Wait(&async[i], &status);\n\t\t\t\tassert(err == MPI_SUCCESS);\n\t\t\t}\n\t\t} else {\n\t\t\terr = MPI_Recv(part, X_RESOLUTION, MPI_INT, 0, 42, MPI_COMM_WORLD, &status);\n\t\t\tassert(err == MPI_SUCCESS);\n\t\t}\n\t}\n\t\n\n\t\n\t\n\n\t\n\n\n\t\n\n\tt_start = gettime();\n\n\tmandelbrot(maxiter, dx, dy, xmin, ymin, part);\n\n\t\n\n\tt_end = gettime();\n\n\t\n\n\n\tif(rank == 0) {\n\t\tprintf (\"calculation took %.2f s on %d+1 processors\\n\", t_end - t_start, size-1);\n\t\tprintf(\"checksum = %lu\\n\", checksum);\n\t}\n\t\n\t\n\n\terr = MPI_Finalize ();\n\tassert(err == MPI_SUCCESS);\n\t\n\treturn EXIT_SUCCESS;\n}\n"}
{"program": "bmi-forum_708", "code": "int main( int argc, char* argv[] )\n{\t\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\n\tvoid* bytesObj;\n\tvoid* bytesArray;\n\n\tStructA*\tobject;\n\tStructB*\tarray1d;\n\tStructC**\tarray2d;\n\tStructA***\tarray3d;\n\tStructB****\tarray4d;\n\tStructC*\tone2d;\n\tStructA*\tone3d;\n\tStructB*\tone4d;\n\tStructC**\tcomplex2d;\n\n\tIndex**\t\tsetup;\n\tStructA***\tcomplex3d;\n\n\tIndex x1 = 4;\n\tIndex y1[] = { 1, 2, 3, 4 };\n\n\tIndex x2 = 2;\n\tIndex y2[] = { 1, 1 };\n\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\n\tbytesObj = Memory_Alloc_Bytes( 5, \"Bytes\", \"Test1\" );\n\tbytesArray = Memory_Alloc_Array_Bytes( 3, 10, \"Bytes\", \"Test1\" );\n\n\tobject = Memory_Alloc( StructA, \"Test1\" );\n\tarray1d = Memory_Alloc_Array( StructB, 3, \"Test2\" );\n\tarray2d = Memory_Alloc_2DArray( StructC, 4, 5, \"Test2\" );\n\tarray3d = Memory_Alloc_3DArray( StructA, 2, 3, 4, \"Test2\" );\n\tarray4d = Memory_Alloc_4DArray_Unnamed( StructB, 5, 4, 3, 2 );\n\tone2d = Memory_Alloc_2DArrayAs1D_Unnamed( StructC, 4, 2 );\n\tone3d = Memory_Alloc_3DArrayAs1D_Unnamed( StructA, 2, 2, 3 );\n\tone4d = Memory_Alloc_4DArrayAs1D( StructB, 4, 2, 3, 5, \"Test1\" );\n\n\tcomplex2d = Memory_Alloc_2DComplex( StructC, x1, y1, \"Test1\" );\n\n\tsetup = Memory_Alloc_3DSetup( x2, y2 );\n\tsetup[0][0] = 2;\n\tsetup[1][0] = 3;\n\tcomplex3d = Memory_Alloc_3DComplex( StructA, x2, y2, setup, \"Test1\" );\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"Before Freeing memory\\n\" );\n\t\tMemory_Print();\n\t\tJournal_Printf( stream,  \"\\nTest search pointer function\\n\" );\n\t\tMemory_Print_Pointer( bytesObj );\n\t\tMemory_Print_Pointer( array1d );\n\t\tMemory_Print_Pointer( one3d );\n\t\tMemory_Print_Pointer( complex2d );\n\t\tJournal_Printf( stream,  \"\\n\" );\n\t}\n\n\t\n\n\n\tMemory_Free( bytesArray );\n\tMemory_Free_Type_Name( StructA, \"Test1\" );\n\tMemory_Free_Type( StructB );\n\t\n\tMemory_Free( array2d );\n\tMemory_Free( array3d );\n\tMemory_Free( one2d );\n\tMemory_Free( one3d );\n\tMemory_Free( complex2d );\n\tMemory_Free( setup );\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"After Memory Free\\n\" );\n\t\tMemory_Print();\n\t\tJournal_Printf( stream,  \"\\nTest search/print functions\\n\" );\n\t\tMemory_Print_Type_Name( StructB, \"Name_Test2\" );\n\t\tJournal_Printf( stream,  \"\\n\" );\n\t\tMemory_Print_Type( StructC );\n\t\tJournal_Printf( stream,  \"\\n\" );\n\n\t\tJournal_Printf( stream,  \"Test Memory Leak print.\\n\" );\n\t\tMemory_Print_Leak();\n\t\tJournal_Printf( stream,  \"\\n\" );\n\t}\n\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] )\n{\t\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\n\tvoid* bytesObj;\n\tvoid* bytesArray;\n\n\tStructA*\tobject;\n\tStructB*\tarray1d;\n\tStructC**\tarray2d;\n\tStructA***\tarray3d;\n\tStructB****\tarray4d;\n\tStructC*\tone2d;\n\tStructA*\tone3d;\n\tStructB*\tone4d;\n\tStructC**\tcomplex2d;\n\n\tIndex**\t\tsetup;\n\tStructA***\tcomplex3d;\n\n\tIndex x1 = 4;\n\tIndex y1[] = { 1, 2, 3, 4 };\n\n\tIndex x2 = 2;\n\tIndex y2[] = { 1, 1 };\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\n\tbytesObj = Memory_Alloc_Bytes( 5, \"Bytes\", \"Test1\" );\n\tbytesArray = Memory_Alloc_Array_Bytes( 3, 10, \"Bytes\", \"Test1\" );\n\n\tobject = Memory_Alloc( StructA, \"Test1\" );\n\tarray1d = Memory_Alloc_Array( StructB, 3, \"Test2\" );\n\tarray2d = Memory_Alloc_2DArray( StructC, 4, 5, \"Test2\" );\n\tarray3d = Memory_Alloc_3DArray( StructA, 2, 3, 4, \"Test2\" );\n\tarray4d = Memory_Alloc_4DArray_Unnamed( StructB, 5, 4, 3, 2 );\n\tone2d = Memory_Alloc_2DArrayAs1D_Unnamed( StructC, 4, 2 );\n\tone3d = Memory_Alloc_3DArrayAs1D_Unnamed( StructA, 2, 2, 3 );\n\tone4d = Memory_Alloc_4DArrayAs1D( StructB, 4, 2, 3, 5, \"Test1\" );\n\n\tcomplex2d = Memory_Alloc_2DComplex( StructC, x1, y1, \"Test1\" );\n\n\tsetup = Memory_Alloc_3DSetup( x2, y2 );\n\tsetup[0][0] = 2;\n\tsetup[1][0] = 3;\n\tcomplex3d = Memory_Alloc_3DComplex( StructA, x2, y2, setup, \"Test1\" );\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"Before Freeing memory\\n\" );\n\t\tMemory_Print();\n\t\tJournal_Printf( stream,  \"\\nTest search pointer function\\n\" );\n\t\tMemory_Print_Pointer( bytesObj );\n\t\tMemory_Print_Pointer( array1d );\n\t\tMemory_Print_Pointer( one3d );\n\t\tMemory_Print_Pointer( complex2d );\n\t\tJournal_Printf( stream,  \"\\n\" );\n\t}\n\n\t\n\n\n\tMemory_Free( bytesArray );\n\tMemory_Free_Type_Name( StructA, \"Test1\" );\n\tMemory_Free_Type( StructB );\n\t\n\tMemory_Free( array2d );\n\tMemory_Free( array3d );\n\tMemory_Free( one2d );\n\tMemory_Free( one3d );\n\tMemory_Free( complex2d );\n\tMemory_Free( setup );\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"After Memory Free\\n\" );\n\t\tMemory_Print();\n\t\tJournal_Printf( stream,  \"\\nTest search/print functions\\n\" );\n\t\tMemory_Print_Type_Name( StructB, \"Name_Test2\" );\n\t\tJournal_Printf( stream,  \"\\n\" );\n\t\tMemory_Print_Type( StructC );\n\t\tJournal_Printf( stream,  \"\\n\" );\n\n\t\tJournal_Printf( stream,  \"Test Memory Leak print.\\n\" );\n\t\tMemory_Print_Leak();\n\t\tJournal_Printf( stream,  \"\\n\" );\n\t}\n\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "libsmelt_709", "code": "int main(int argc, char **argv){\n\tint rank, size, root=0;\t\n \t\n\n\n    char outname[128];\n    if (argc == 2) {\n        sprintf(outname, \"reduction_mpisync_%d\", size);\n    } else {\n        sprintf(outname, \"reduction_mpi_%d\", size);\n    }\n    uint64_t *buf = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    sk_m_init(&mes, NVALUES, outname, buf);\n\n    char a = 0;\n    char global = 0;\n\tfor(int n=0; n<NITERS; n++){\n        \n\n        if (argc == 2) {\n            dissem_bar();\n            dissem_bar();\n        }\n\n        sk_m_restart_tsc(&mes);\n        sk_m_add(&mes);\n\t}\n\n    uint64_t *buf2 = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    if (rank == 0) {\n        sk_m_print(&mes);\n        for (int i = 1; i < size; i++) {\n            for (uint32_t j=0; j< NVALUES; j++) {\n                char name[100];\n                printf(\"sk_m_print(%d,%s) idx= %d tscdiff= %ld\\n\",\n                       i, outname, j, buf2[j]);\n            }   \n        }\n    } else {\n    }\n\n}", "label": "int main(int argc, char **argv){\n\tint rank, size, root=0;\t\n \t\n\tMPI_Init(&argc,&argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\tMPI_Comm_size(MPI_COMM_WORLD,&size);\n\n    char outname[128];\n    if (argc == 2) {\n        sprintf(outname, \"reduction_mpisync_%d\", size);\n    } else {\n        sprintf(outname, \"reduction_mpi_%d\", size);\n    }\n    uint64_t *buf = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    sk_m_init(&mes, NVALUES, outname, buf);\n\n    char a = 0;\n    char global = 0;\n\tfor(int n=0; n<NITERS; n++){\n        \n\n        if (argc == 2) {\n            dissem_bar();\n            dissem_bar();\n        }\n\n        sk_m_restart_tsc(&mes);\n        MPI_Reduce((void*) &a, (void*) &global, 1, MPI_BYTE, MPI_SUM, 0,\n                   MPI_COMM_WORLD);\n        sk_m_add(&mes);\n\t}\n\n    uint64_t *buf2 = (uint64_t*) malloc(sizeof(uint64_t)*NITERS);\n    if (rank == 0) {\n        sk_m_print(&mes);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(buf2, NVALUES, MPI_UINT64_T, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (uint32_t j=0; j< NVALUES; j++) {\n                char name[100];\n                printf(\"sk_m_print(%d,%s) idx= %d tscdiff= %ld\\n\",\n                       i, outname, j, buf2[j]);\n            }   \n        }\n    } else {\n        MPI_Send(buf, NVALUES,MPI_UINT64_T, 0, 0, MPI_COMM_WORLD);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n\tMPI_Finalize();\n}"}
{"program": "pf-aics-riken_711", "code": "int\nmain(int argc, char *argv[])\n{\n    int rank, size;\n    char line[LINELEN];\n    FILE *ifp, *ofp;\n    int points, mine, i, count, total_count;\n\n    srand((unsigned int)rank);\n\n    if (argc != 2) {\n        if (rank == 0) {\n            fprintf(stderr, \"specify an input file\\n\");\n        }\n    }\n\n    if (rank == 0) {\n        ifp = fopen(argv[1], \"r\");\n        if (fgets(line, sizeof(line), ifp) == NULL) {\n            fprintf(stderr, \"failed to read a file\\n\");\n        }\n        fclose(ifp);\n        points = atoi(line);\n    }\n\n    mine = points / size;\n    if (rank == size - 1) {\n        mine += points % size;\n    }\n\n    count = 0;\n    for (i = 0; i < mine; i++) {\n        float x = (float)rand() / ((float)RAND_MAX + 1.0F);\n        float y = (float)rand() / ((float)RAND_MAX + 1.0F);\n        if ( x * x + y * y < 1.0) {\n            count += 1;\n        }\n    }\n\n    if (rank == 0) {\n        char *ofilename = malloc(strlen(argv[1]) + 5);\n        strncpy(ofilename, argv[1], strlen(argv[1]) + 1);\n        strncat(ofilename, \".out\", 4);\n\n        ofp = fopen(ofilename, \"w\");\n        fprintf(ofp, \"%d/%d\\n\", total_count, points);\n        fclose(ofp);\n    }\n\n    return 0;\n}", "label": "int\nmain(int argc, char *argv[])\n{\n    int rank, size;\n    char line[LINELEN];\n    FILE *ifp, *ofp;\n    int points, mine, i, count, total_count;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    srand((unsigned int)rank);\n\n    if (argc != 2) {\n        if (rank == 0) {\n            fprintf(stderr, \"specify an input file\\n\");\n        }\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    if (rank == 0) {\n        ifp = fopen(argv[1], \"r\");\n        if (fgets(line, sizeof(line), ifp) == NULL) {\n            fprintf(stderr, \"failed to read a file\\n\");\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n        fclose(ifp);\n        points = atoi(line);\n    }\n\n    MPI_Bcast(&points, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    mine = points / size;\n    if (rank == size - 1) {\n        mine += points % size;\n    }\n\n    count = 0;\n    for (i = 0; i < mine; i++) {\n        float x = (float)rand() / ((float)RAND_MAX + 1.0F);\n        float y = (float)rand() / ((float)RAND_MAX + 1.0F);\n        if ( x * x + y * y < 1.0) {\n            count += 1;\n        }\n    }\n    MPI_Reduce(&count, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        char *ofilename = malloc(strlen(argv[1]) + 5);\n        strncpy(ofilename, argv[1], strlen(argv[1]) + 1);\n        strncat(ofilename, \".out\", 4);\n\n        ofp = fopen(ofilename, \"w\");\n        fprintf(ofp, \"%d/%d\\n\", total_count, points);\n        fclose(ofp);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "miguelzf_713", "code": "int main (int argc, char* argv[] )\n{\n\tscore_t** documents, **cabinets, **cabinetsums;\n\tdouble t1, t2, t3, t4;\n\tchar procname[64], *infname, *outfname;\n\tint nround, nchanged, i, namelen;\n\tint *assignments;\n\n\t\n\n\tdocuments = cabinets = cabinetsums = NULL;\n\tinfname = outfname = NULL;\n\tt1 = t2 = t3 = t4 = 0.0;\n\t\n\n\n\tsprintf(procname+namelen, \"-P%d\", rank);\n\tif (argc < 2)\n\t\treturn fprintf(stderr, \"Too few arguments!\\nUsage: %s\\n\", usage);\n\t}\n\n#ifdef NUM_THREADS\n\tomp_set_num_threads(NUM_THREADS);\n#endif\n\n\t#pragma omp parallel\n\t{\n\t\tnthreads = omp_get_num_threads();\n\t\tif (omp_get_thread_num() == 0 && ROOT)\n\t\t\tprintf(\"Startup MPI process %d of %d on %s Running with %d threads\\n\",\n\t\t\t\trank, nprocs, procname, nthreads);\n\t}\n\t\n\tt1 =\n\tif (ROOT)\t\n\n\t{\n\t\tint len = strlen(argv[1]);\n\t\tinfname = argv[1];\n\t\tif (strcmp(infname+len-3, \".in\"))\n\t\t\treturn fprintf(stderr, \"Invalid input file name! Must use extension '.in'\\n\");\n\t\n\t\toutfname = malloc(len+3);\n\t\tstrcpy(outfname, infname);\n\t\tstrcpy(outfname+len-2, \"out\");\n\n\t\tnum_cabinets = 0;\n\t\tif (argc > 2)\n\t\t\tif ((num_cabinets = atoi(argv[2])) <= 0)\n\t\t\t\treturn fprintf(stderr, \"Not a valid No. of Cabinets!\\nUsage: %s\\n\", usage);\n\t\n\t\tdocuments = read_and_send_input(infname);\n\n\t\tdprintf(\"[Process %d] In %s: %d documents, %d cabinets, %d subjects\\n\", \n\t\t\t\trank, procname, num_documents, num_cabinets, num_subjects);\n\t}\n\telse\n\t\tdocuments = receive_input();\n\n\n\tt2 =\n\n\tcabinets = create_score_matrix(num_cabinets, nsubjects1);\t\n\n\tcabinetsums = create_score_matrix(num_cabinets, nsubjects1);\n\tcompute_centroids(documents, cabinets, cabinetsums);\n\t\t\n\tfor (nround = 0; 1; nround++)\n\t{\n\t\t\n\n\n\n\t\tnchanged = reassign_documents(documents, cabinets, cabinetsums);\n\n\t\tif (!DISTR && nchanged == 0)\t\n\n\t\t\tbreak;\n\n\t\tif (update_centroids(cabinets, cabinetsums, nchanged) == 0)\n\t\t\tbreak;\n\t}\n\n\tdprintf(\"[P%d] Finished\\n\", rank);\n\t\n\t\n\n\tassignments = malloc(nprocs*(blocksize+1)*sizeof(int));\n\tfor (i = 0; i < blocksize; i++)\n\t\tassignments[i] = (int) DOC_CABINET(documents[i]);\n\n\tif (DISTR)\n\t{\n\n\t\tif (!ROOT)\n\t\t\treturn 0;\n\t\t}\n\t}\n\t\n\tt3 =\n\tprint_result(assignments, outfname);\n\tt4 =\n\n\tprintf(\"=== Finished %s ===\\n\", infname);\n\tprintf(\"Distribut: %.3lf secs\\n\", t2-t1);\n\n\n\tprintf(\"Computing: %.3lf secs\\n\", t3-t2);\n\tprintf(\"Writing  : %.3lf secs\\n\", t4-t3);\n\tprintf(\"Total    : %.3lf secs\\n\", t4-t1);\t\n\treturn 0;\n}", "label": "int main (int argc, char* argv[] )\n{\n\tscore_t** documents, **cabinets, **cabinetsums;\n\tdouble t1, t2, t3, t4;\n\tchar procname[64], *infname, *outfname;\n\tint nround, nchanged, i, namelen;\n\tint *assignments;\n\n\t\n\n\tdocuments = cabinets = cabinetsums = NULL;\n\tinfname = outfname = NULL;\n\tt1 = t2 = t3 = t4 = 0.0;\n\t\n\tMPI_Init(&argc, &argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tMPI_Get_processor_name(procname, &namelen);\n\n\tsprintf(procname+namelen, \"-P%d\", rank);\n\tif (argc < 2)\n\t{\tMPI_Finalize();\n\t\treturn fprintf(stderr, \"Too few arguments!\\nUsage: %s\\n\", usage);\n\t}\n\n#ifdef NUM_THREADS\n\tomp_set_num_threads(NUM_THREADS);\n#endif\n\n\t#pragma omp parallel\n\t{\n\t\tnthreads = omp_get_num_threads();\n\t\tif (omp_get_thread_num() == 0 && ROOT)\n\t\t\tprintf(\"Startup MPI process %d of %d on %s Running with %d threads\\n\",\n\t\t\t\trank, nprocs, procname, nthreads);\n\t}\n\t\n\tt1 = MPI_Wtime();\n\tif (ROOT)\t\n\n\t{\n\t\tint len = strlen(argv[1]);\n\t\tinfname = argv[1];\n\t\tif (strcmp(infname+len-3, \".in\"))\n\t\t\treturn fprintf(stderr, \"Invalid input file name! Must use extension '.in'\\n\");\n\t\n\t\toutfname = malloc(len+3);\n\t\tstrcpy(outfname, infname);\n\t\tstrcpy(outfname+len-2, \"out\");\n\n\t\tnum_cabinets = 0;\n\t\tif (argc > 2)\n\t\t\tif ((num_cabinets = atoi(argv[2])) <= 0)\n\t\t\t\treturn fprintf(stderr, \"Not a valid No. of Cabinets!\\nUsage: %s\\n\", usage);\n\t\n\t\tdocuments = read_and_send_input(infname);\n\n\t\tdprintf(\"[Process %d] In %s: %d documents, %d cabinets, %d subjects\\n\", \n\t\t\t\trank, procname, num_documents, num_cabinets, num_subjects);\n\t}\n\telse\n\t\tdocuments = receive_input();\n\n\tMPI_Barrier(MPI_COMM_WORLD);\t\n\n\tt2 = MPI_Wtime();\n\n\tcabinets = create_score_matrix(num_cabinets, nsubjects1);\t\n\n\tcabinetsums = create_score_matrix(num_cabinets, nsubjects1);\n\tcompute_centroids(documents, cabinets, cabinetsums);\n\t\t\n\tfor (nround = 0; 1; nround++)\n\t{\n\t\t\n\n\n\n\t\tnchanged = reassign_documents(documents, cabinets, cabinetsums);\n\n\t\tif (!DISTR && nchanged == 0)\t\n\n\t\t\tbreak;\n\n\t\tif (update_centroids(cabinets, cabinetsums, nchanged) == 0)\n\t\t\tbreak;\n\t}\n\n\tdprintf(\"[P%d] Finished\\n\", rank);\n\t\n\t\n\n\tassignments = malloc(nprocs*(blocksize+1)*sizeof(int));\n\tfor (i = 0; i < blocksize; i++)\n\t\tassignments[i] = (int) DOC_CABINET(documents[i]);\n\n\tif (DISTR)\n\t{\n\t\tMPI_Gather(ROOT? MPI_IN_PLACE : assignments, blocksize, MPI_INT, assignments, blocksize, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t\tif (!ROOT)\n\t\t{\tMPI_Finalize();\n\t\t\treturn 0;\n\t\t}\n\t}\n\t\n\tt3 = MPI_Wtime();\n\tprint_result(assignments, outfname);\n\tt4 = MPI_Wtime();\n\n\tprintf(\"=== Finished %s ===\\n\", infname);\n\tprintf(\"Distribut: %.3lf secs\\n\", t2-t1);\n\n\n\tprintf(\"Computing: %.3lf secs\\n\", t3-t2);\n\tprintf(\"Writing  : %.3lf secs\\n\", t4-t3);\n\tprintf(\"Total    : %.3lf secs\\n\", t4-t1);\t\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "gonzalomcruz_715", "code": "int main(int argc, char *argv[]) {\n    int rank, size, step_count = 0;\n\n    const int MAX_STEPS = 100;\n\n    \n\n\n    \n\n\n    if (size != 2) {\n        fprintf(stderr, \"Number of processes must be 2 in order to run this example\\n\");\n    }\n\n    while (step_count < MAX_STEPS) {\n        if (rank % 2 == 0) {\n            step_count ++;\n            printf(\"[%i] Sent and incremented step count %i to process 1\\n\",\n                    rank, step_count);\n        } else if (rank % 2 == 1) {\n            printf(\"[%i] Received step count %i from process 0\\n\",\n                    rank, step_count);\n        }\n    }\n\n    \n\n\n    return 0;\n}\n", "label": "int main(int argc, char *argv[]) {\n    int rank, size, step_count = 0;\n\n    const int MAX_STEPS = 100;\n\n    \n\n    MPI_Init (&argc, &argv);\n\n    \n\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n    MPI_Comm_size (MPI_COMM_WORLD, &size);\n\n    if (size != 2) {\n        fprintf(stderr, \"Number of processes must be 2 in order to run this example\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    while (step_count < MAX_STEPS) {\n        if (rank % 2 == 0) {\n            step_count ++;\n            MPI_Send(&step_count, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n            printf(\"[%i] Sent and incremented step count %i to process 1\\n\",\n                    rank, step_count);\n        } else if (rank % 2 == 1) {\n            MPI_Recv(&step_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            printf(\"[%i] Received step count %i from process 0\\n\",\n                    rank, step_count);\n        }\n    }\n\n    \n\n    MPI_Finalize();\n\n    return 0;\n}\n"}
{"program": "grondo_717", "code": "int main(int argc, char * argv[])\n{\n\tint size, rank,buf;\n\n\n\tbuf = rank;\t\n\n\n\tpass_its_neighbor(rank, size, &buf);\n\n\treturn 0;\n}", "label": "int main(int argc, char * argv[])\n{\n\tint size, rank,buf;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tbuf = rank;\t\n\n\n\tpass_its_neighbor(rank, size, &buf);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "syftalent_718", "code": "int main(int argc, char **argv)\n{\n\n    run_test(MPI_LOCK_EXCLUSIVE, 0);\n    run_test(MPI_LOCK_EXCLUSIVE, MPI_MODE_NOCHECK);\n    run_test(MPI_LOCK_SHARED, 0);\n    run_test(MPI_LOCK_SHARED, MPI_MODE_NOCHECK);\n\n\n    if (rank == 0)\n        printf(\" No Errors\\n\");\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    MPI_Init(&argc, &argv);\n\n    run_test(MPI_LOCK_EXCLUSIVE, 0);\n    run_test(MPI_LOCK_EXCLUSIVE, MPI_MODE_NOCHECK);\n    run_test(MPI_LOCK_SHARED, 0);\n    run_test(MPI_LOCK_SHARED, MPI_MODE_NOCHECK);\n\n    MPI_Finalize();\n\n    if (rank == 0)\n        printf(\" No Errors\\n\");\n\n    return 0;\n}"}
{"program": "telemed-duth_722", "code": "int  main(int argc, char *argv[]) {\n\tint i, err;\n\tchar command[30];\n\tint numNodi, rank; \n\tint enable = atoi (argv [1]); \n\tMPI_Status Stat; \n\tif (DEBUG == 1) {\n\t\tprintf (\"Numero di nodi = %d \\n\", numNodi);\n\t\tprintf (\"enable = %d\\n\", enable);\n\t\tfflush (stdout); \n\t}\n\tif (rank == 0) {\n\t\t\n\n\t\tsprintf(command,\"php -f docdoc.php %d %s %s %s\",(rank+1),numNodi,argv[1],argv[2]);\n\t\tprintf(\"Executing %s...\\n\\n\", command); fflush (stdout); \n\t\tsystem(command);\n\t}\n\telse {\n\t\tsprintf(command,\"php -f docdoc.php %d %s %s %s\",(rank+1),numNodi,argv[1],argv[2]);\n\t\tprintf(\"Executing %s...\\n\\n\", command);\n\t\tsystem(command);\n\t}\n}", "label": "int  main(int argc, char *argv[]) {\n\tint i, err;\n\tchar command[30];\n\tint numNodi, rank; \n\tint enable = atoi (argv [1]); \n\tMPI_Status Stat; \n\tMPI_Init(&argc, &argv); \n\tMPI_Comm_size(MPI_COMM_WORLD, &numNodi);\n\tif (DEBUG == 1) {\n\t\tprintf (\"Numero di nodi = %d \\n\", numNodi);\n\t\tprintf (\"enable = %d\\n\", enable);\n\t\tfflush (stdout); \n\t}\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tif (rank == 0) {\n\t\t\n\n\t\tsprintf(command,\"php -f docdoc.php %d %s %s %s\",(rank+1),numNodi,argv[1],argv[2]);\n\t\tprintf(\"Executing %s...\\n\\n\", command); fflush (stdout); \n\t\tsystem(command);\n\t}\n\telse {\n\t\tsprintf(command,\"php -f docdoc.php %d %s %s %s\",(rank+1),numNodi,argv[1],argv[2]);\n\t\tprintf(\"Executing %s...\\n\\n\", command);\n\t\tsystem(command);\n\t}\n\tMPI_Finalize();\n}"}
{"program": "gnu3ra_725", "code": "int main( int argc, char *argv[] )\n{\n    int wsize, wrank;\n    int source, dest, i;\n    int buf1[10], buf2[10], buf3[10];\n    MPI_Request r[3];\n    volatile int hold = 0; \n    MPI_Comm dupcomm;\n    MPI_Status status;\n\n\n    \n    \n\n    source = (wrank + 1) % wsize;\n    dest   = (wrank + wsize - 1) % wsize;\n\n\n    for (i=0; i<3; i++) {\n    }\n\n    \n    while (hold) ;\n\n\n    for (i=0; i<3; i++) {\n    }\n\n    \n    \n    \n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int wsize, wrank;\n    int source, dest, i;\n    int buf1[10], buf2[10], buf3[10];\n    MPI_Request r[3];\n    volatile int hold = 0; \n    MPI_Comm dupcomm;\n    MPI_Status status;\n\n    MPI_Init( &argc, &argv );\n\n    MPI_Comm_size( MPI_COMM_WORLD, &wsize );\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n    \n    \n\n    source = (wrank + 1) % wsize;\n    dest   = (wrank + wsize - 1) % wsize;\n\n    MPI_Comm_dup( MPI_COMM_WORLD, &dupcomm );\n    MPI_Comm_set_name( dupcomm, \"Dup of comm world\" );\n\n    for (i=0; i<3; i++) {\n\tMPI_Irecv( MPI_BOTTOM, 0, MPI_INT, source, i + 100, MPI_COMM_WORLD, \n\t\t   &r[i] );\n    }\n\n    MPI_Send( buf2, 8, MPI_INT, dest, 1, MPI_COMM_WORLD );\n    MPI_Send( buf3, 4, MPI_INT, dest, 2, dupcomm );\n    \n    while (hold) ;\n\n    MPI_Recv( buf1, 10, MPI_INT, source, 1, MPI_COMM_WORLD, &status );\n    MPI_Recv( buf1, 10, MPI_INT, source, 2, dupcomm, &status );\n\n    for (i=0; i<3; i++) {\n\tMPI_Cancel( &r[i] );\n    }\n\n    MPI_Comm_free( &dupcomm );\n    \n    \n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "mnakao_728", "code": "int main(int argc, char **argv){\n  int i, me, target;\n  unsigned int size;\n  double t, t_max;\n  MPI_Win win;\n\n  \n  target = 1 - me;\n  \n  init_buf(send_buf, me);\n\n  if(me==0) print_items();\n\n  for(size=1;size<MAX_SIZE+1;size*=2){\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n\tt = wtime();\n\n      if(me == 0){\n\n\twhile(send_buf[0] == '0' || send_buf[size-1] == '0'){ }\n\tsend_buf[0] = '0'; send_buf[size-1] = '0';\n      }\n      else {\n\twhile(send_buf[0] == '1' || send_buf[size-1] == '1'){ }\n\tsend_buf[0] = '1'; send_buf[size-1] = '1';\n\n      }\n    } \n\n\n    t = wtime() - t;\n    if(me == 0)\n      print_results(size, t_max);\n  }\n\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int i, me, target;\n  unsigned int size;\n  double t, t_max;\n  MPI_Win win;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Win_create(&send_buf, sizeof(char)*MAX_SIZE, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n  \n  target = 1 - me;\n  MPI_Win_lock_all(0, win);\n  \n  init_buf(send_buf, me);\n\n  if(me==0) print_items();\n\n  for(size=1;size<MAX_SIZE+1;size*=2){\n    MPI_Barrier(MPI_COMM_WORLD);\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n\tt = wtime();\n\n      if(me == 0){\n\tMPI_Put(send_buf, size, MPI_CHAR, target, 0, size, MPI_CHAR, win);\n\tMPI_Win_flush_local(target, win);\n\n\twhile(send_buf[0] == '0' || send_buf[size-1] == '0'){ MPI_Win_flush(me, win); }\n\tsend_buf[0] = '0'; send_buf[size-1] = '0';\n      }\n      else {\n\twhile(send_buf[0] == '1' || send_buf[size-1] == '1'){ MPI_Win_flush(me, win); }\n\tsend_buf[0] = '1'; send_buf[size-1] = '1';\n\n\tMPI_Put(send_buf, size, MPI_CHAR, target, 0, size, MPI_CHAR, win);\n\tMPI_Win_flush_local(target, win);\n      }\n    } \n\n\n    t = wtime() - t;\n    MPI_Reduce(&t, &t_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n    if(me == 0)\n      print_results(size, t_max);\n  }\n\n  MPI_Win_unlock_all(win);\n  MPI_Win_free(&win);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "sg0_729", "code": "int main(int argc, char **argv)\n{\n  int rank, nprocs;\n  int g_A, dims[DIM]={SIZE,SIZE}; \n  int lo[DIM]={SIZE-SIZE,SIZE-SIZE}, hi[DIM]={SIZE-1,SIZE-1}, ld=SIZE;\n  int val=5;\n\n#if defined(USE_ELEMENTAL)\n  \n\n  ElInitialize( &argc, &argv );\n  ElMPICommRank( MPI_COMM_WORLD, &rank );\n  ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n  \n\n  ElGlobalArraysConstruct_i( &eliga );\n  \n\n  ElGlobalArraysInitialize_i( eliga );\n#else\n\n\n  MA_init(C_DBL, 1000, 1000);\n\n  GA_Initialize();\n#endif\n\n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysCreate_i( eliga, DIM, dims, \"array_A\", NULL, &g_A );\n#else\n  g_A = NGA_Create(C_INT, DIM, dims, \"array_A\", NULL);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysFill_i( eliga, g_A, &val );\n#else\n  GA_Fill(g_A, &val);\n#endif\n\n  if (rank == 0) printf (\"Initially: \\n\");\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysPrint_i( eliga, g_A );\n#else \n  GA_Print(g_A);\n#endif\n\n  int subscript[2];\n  subscript[0] = 2;\n  subscript[1] = 2;\n  long inc = 4;\n  long prev = -99;\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysReadIncrement_i( eliga, g_A, subscript, inc, (ElInt *)&prev );\n#else\n  prev = NGA_Read_inc(g_A, subscript, inc);\n#endif\n\n  if (rank == 0) printf (\"After read increment: \\n\");\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysPrint_i( eliga, g_A );\n#else \n  GA_Print(g_A);\n#endif\n\n  printf (\"[%d] Previous value at position {%d, %d}: %d\\n\", \n\t  rank, subscript[0], subscript[1], prev);\n \n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysSync_i( eliga );\n#else\n  GA_Sync();\n#endif\n\n  if(rank==0)\n    printf(\"Test Completed \\n\");\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDestroy_i( eliga, g_A );\n#else\n  GA_Destroy(g_A);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysTerminate_i( eliga );\n  \n\n  ElGlobalArraysDestruct_i( eliga );\n  ElFinalize();\n#else\n  GA_Terminate();\n#endif\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int rank, nprocs;\n  int g_A, dims[DIM]={SIZE,SIZE}; \n  int lo[DIM]={SIZE-SIZE,SIZE-SIZE}, hi[DIM]={SIZE-1,SIZE-1}, ld=SIZE;\n  int val=5;\n\n#if defined(USE_ELEMENTAL)\n  \n\n  ElInitialize( &argc, &argv );\n  ElMPICommRank( MPI_COMM_WORLD, &rank );\n  ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n  \n\n  ElGlobalArraysConstruct_i( &eliga );\n  \n\n  ElGlobalArraysInitialize_i( eliga );\n#else\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  MA_init(C_DBL, 1000, 1000);\n\n  GA_Initialize();\n#endif\n\n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysCreate_i( eliga, DIM, dims, \"array_A\", NULL, &g_A );\n#else\n  g_A = NGA_Create(C_INT, DIM, dims, \"array_A\", NULL);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysFill_i( eliga, g_A, &val );\n#else\n  GA_Fill(g_A, &val);\n#endif\n\n  if (rank == 0) printf (\"Initially: \\n\");\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysPrint_i( eliga, g_A );\n#else \n  GA_Print(g_A);\n#endif\n\n  int subscript[2];\n  subscript[0] = 2;\n  subscript[1] = 2;\n  long inc = 4;\n  long prev = -99;\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysReadIncrement_i( eliga, g_A, subscript, inc, (ElInt *)&prev );\n#else\n  prev = NGA_Read_inc(g_A, subscript, inc);\n#endif\n\n  if (rank == 0) printf (\"After read increment: \\n\");\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysPrint_i( eliga, g_A );\n#else \n  GA_Print(g_A);\n#endif\n\n  printf (\"[%d] Previous value at position {%d, %d}: %d\\n\", \n\t  rank, subscript[0], subscript[1], prev);\n \n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysSync_i( eliga );\n#else\n  GA_Sync();\n#endif\n\n  if(rank==0)\n    printf(\"Test Completed \\n\");\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDestroy_i( eliga, g_A );\n#else\n  GA_Destroy(g_A);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysTerminate_i( eliga );\n  \n\n  ElGlobalArraysDestruct_i( eliga );\n  ElFinalize();\n#else\n  GA_Terminate();\n  MPI_Finalize();\n#endif\n\n  return 0;\n}"}
{"program": "syftalent_730", "code": "int main(int argc, char **argv)\n{\n    int rank, size, err, errclass;\n    int sendbuf[1] = { 42 };\n    int *recvbuf;\n\n\n    if (size < 3) {\n        fprintf( stderr, \"Must run with at least 3 processes\\n\" );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    recvbuf = (int *)malloc(size*sizeof(int));\n\n    err =\n\n    if (rank == 0) {\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n        if (errclass == MPIX_ERR_PROC_FAILED) {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        } else {\n            fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        }\n#else\n        if (err) {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        } else {\n            fprintf(stderr, \"Program reported MPI_SUCCESS, but an error code was expected.\\n\");\n        }\n#endif\n    }\n\n    free(recvbuf);\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, err, errclass;\n    int sendbuf[1] = { 42 };\n    int *recvbuf;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n\n    if (size < 3) {\n        fprintf( stderr, \"Must run with at least 3 processes\\n\" );\n        MPI_Abort( MPI_COMM_WORLD, 1 );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    recvbuf = (int *)malloc(size*sizeof(int));\n\n    err = MPI_Gather(sendbuf, 1, MPI_INT, recvbuf, size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n        MPI_Error_class(err, &errclass);\n        if (errclass == MPIX_ERR_PROC_FAILED) {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        } else {\n            fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        }\n#else\n        if (err) {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        } else {\n            fprintf(stderr, \"Program reported MPI_SUCCESS, but an error code was expected.\\n\");\n        }\n#endif\n    }\n\n    free(recvbuf);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "annavicente_731", "code": "int main(int argc, char **argv) {\r\n    int size, rank;\r\n\r\n\r\n    int *globaldata=NULL;\r\n    int localdata;\r\n\r\n    if (rank == 0) {\r\n        globaldata = malloc(size * sizeof(int) );\r\n        for (int i=0; i<size; i++)\r\n            globaldata[i] = 2*i+1;\r\n\r\n        printf(\"Processor %d has data: \", rank);\r\n        for (int i=0; i<size; i++)\r\n            printf(\"%d \", globaldata[i]);\r\n        printf(\"\\n\");\r\n    }\r\n\r\n\r\n    printf(\"Processor %d has data %d\\n\", rank, localdata);\r\n    localdata *= 2;\r\n    printf(\"Processor %d doubling the data, now has %d\\n\", rank, localdata);\r\n\r\n\r\n    if (rank == 0) {\r\n        printf(\"Processor %d has data: \", rank);\r\n        for (int i=0; i<size; i++)\r\n            printf(\"%d \", globaldata[i]);\r\n        printf(\"\\n\");\r\n    }\r\n\r\n    if (rank == 0)\r\n        free(globaldata);\r\n\r\n    return 0;\r\n}", "label": "int main(int argc, char **argv) {\r\n    int size, rank;\r\n\r\n    MPI_Init(&argc, &argv);\r\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\r\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n\r\n    int *globaldata=NULL;\r\n    int localdata;\r\n\r\n    if (rank == 0) {\r\n        globaldata = malloc(size * sizeof(int) );\r\n        for (int i=0; i<size; i++)\r\n            globaldata[i] = 2*i+1;\r\n\r\n        printf(\"Processor %d has data: \", rank);\r\n        for (int i=0; i<size; i++)\r\n            printf(\"%d \", globaldata[i]);\r\n        printf(\"\\n\");\r\n    }\r\n\r\n    MPI_Scatter(globaldata, 1, MPI_INT, &localdata, 1, MPI_INT, 0, MPI_COMM_WORLD);\r\n\r\n    printf(\"Processor %d has data %d\\n\", rank, localdata);\r\n    localdata *= 2;\r\n    printf(\"Processor %d doubling the data, now has %d\\n\", rank, localdata);\r\n\r\n    MPI_Gather(&localdata, 1, MPI_INT, globaldata, 1, MPI_INT, 0, MPI_COMM_WORLD);\r\n\r\n    if (rank == 0) {\r\n        printf(\"Processor %d has data: \", rank);\r\n        for (int i=0; i<size; i++)\r\n            printf(\"%d \", globaldata[i]);\r\n        printf(\"\\n\");\r\n    }\r\n\r\n    if (rank == 0)\r\n        free(globaldata);\r\n\r\n    MPI_Finalize();\r\n    return 0;\r\n}"}
{"program": "fintler_732", "code": "int main(int argc, char **argv)\n{\n\tint myid=-1;\n\tint size=0;\n\tint i;\n\tdouble t1=0, t2=0;\n\tint msg[10];\n\tint result[10];\n\n\n\tprintf(\"myid=%d\\n\", myid);\n\tif (myid==0) {\n\t\tprintf(\"comm size =%d\\n\", size);\n\t}\n\n\tfor (i=0; i<10; i++)\n\t\tmsg[i]=(i+1)*(i+1);\n\n\tif (myid==0)\n\n\tfor (i=0; i<NITER; i++) {\n\t}\n\n\tif (myid==0) \n\n\tif (myid==0) {\n\t\tprintf(\"allreduce cost = %lf seconds.\\n\", (t2-t1)/NITER);\n\t}\n\n\tdumpMsg(result, 10);\n\n}", "label": "int main(int argc, char **argv)\n{\n\tint myid=-1;\n\tint size=0;\n\tint i;\n\tdouble t1=0, t2=0;\n\tint msg[10];\n\tint result[10];\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tprintf(\"myid=%d\\n\", myid);\n\tif (myid==0) {\n\t\tprintf(\"comm size =%d\\n\", size);\n\t}\n\n\tfor (i=0; i<10; i++)\n\t\tmsg[i]=(i+1)*(i+1);\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif (myid==0)\n\t\tt1=MPI_Wtime();\n\n\tfor (i=0; i<NITER; i++) {\n\t\tMPI_Allreduce((char *)msg,(char *)result,10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif (myid==0) \n\t\tt2=MPI_Wtime();\n\n\tif (myid==0) {\n\t\tprintf(\"allreduce cost = %lf seconds.\\n\", (t2-t1)/NITER);\n\t}\n\n\tdumpMsg(result, 10);\n\n\tMPI_Finalize();\n}"}
{"program": "ghisvail_735", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2r(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2r(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "eddelbuettel_736", "code": "void main(int argc, char **argv) {\n    PGAContext *ctx;\n\n\n    \n\n    strcpy(Name, \"David M. Levine, Philip L. Hallstrom, David M. Noelle, \"\n                 \"Brian P. Walenz\");\n\n    ctx = PGACreate(&argc, argv, PGA_DATATYPE_CHARACTER, strlen(Name),\n\t\t    PGA_MAXIMIZE);\n    \n    PGASetRandomSeed(ctx, 42);\n    \n    PGASetUserFunction(ctx, PGA_USERFUNCTION_INITSTRING, (void *)N_InitString);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_MUTATION,   (void *)N_Mutation);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_CROSSOVER,  (void *)N_Crossover);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_DUPLICATE,  (void *)N_Duplicate);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_STOPCOND,   (void *)N_StopCond);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_PRINTSTRING,(void *)N_PrintString);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_ENDOFGEN,   (void *)N_EndOfGen);\n\n    \n\n    PGASetPrintFrequencyValue(ctx, 10000);\n    PGASetPopSize(ctx, 100);\n    PGASetNumReplaceValue(ctx, 90);\n    PGASetPopReplaceType(ctx, PGA_POPREPL_BEST);\n    PGASetNoDuplicatesFlag(ctx, PGA_TRUE);\n    PGASetMaxGAIterValue(ctx, 100);\n    \n    PGASetUp(ctx);\n    PGARun(ctx, EvalName);\n    PGADestroy(ctx);\n\n}", "label": "void main(int argc, char **argv) {\n    PGAContext *ctx;\n\n    MPI_Init(&argc, &argv);\n\n    \n\n    strcpy(Name, \"David M. Levine, Philip L. Hallstrom, David M. Noelle, \"\n                 \"Brian P. Walenz\");\n\n    ctx = PGACreate(&argc, argv, PGA_DATATYPE_CHARACTER, strlen(Name),\n\t\t    PGA_MAXIMIZE);\n    \n    PGASetRandomSeed(ctx, 42);\n    \n    PGASetUserFunction(ctx, PGA_USERFUNCTION_INITSTRING, (void *)N_InitString);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_MUTATION,   (void *)N_Mutation);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_CROSSOVER,  (void *)N_Crossover);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_DUPLICATE,  (void *)N_Duplicate);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_STOPCOND,   (void *)N_StopCond);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_PRINTSTRING,(void *)N_PrintString);\n    PGASetUserFunction(ctx, PGA_USERFUNCTION_ENDOFGEN,   (void *)N_EndOfGen);\n\n    \n\n    PGASetPrintFrequencyValue(ctx, 10000);\n    PGASetPopSize(ctx, 100);\n    PGASetNumReplaceValue(ctx, 90);\n    PGASetPopReplaceType(ctx, PGA_POPREPL_BEST);\n    PGASetNoDuplicatesFlag(ctx, PGA_TRUE);\n    PGASetMaxGAIterValue(ctx, 100);\n    \n    PGASetUp(ctx);\n    PGARun(ctx, EvalName);\n    PGADestroy(ctx);\n\n    MPI_Finalize();\n}"}
{"program": "superhit0_737", "code": "int main(int argc, char** argv){\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: n size of arrays\\n\");\n    exit(1);\n  }\n\n  int n;\n  n=atoi(argv[1]);\n  srand(time(NULL));\n\n  int world_rank;\n  int world_size;\n  int *st = NULL;\n  int *arr=NULL;\n  int *tree=NULL;\n  int *ans=NULL;\n  int size=0;\n  if(world_rank == 0){\n    int x = (int)(ceil(log2(n)));\n    int max_size = 2*(int)pow(2, x) - 1;\n    arr=create_rand_nums(n);\n    st=constructST(arr, n, 0);\n    tree=constructST(arr, n, 1);\n    ans=malloc(sizeof(int)*n);\n    size=max_size;\n  }else{\n    int x = (int)(ceil(log2(n)));\n    int max_size = 2*(int)pow(2, x) - 1;\n    size=max_size;\n    st = malloc(sizeof(int)*max_size);\n    tree = malloc(sizeof(int)*max_size);\n  }\n\n\n\n\n  bottomUpUpdate(tree,st,0,n-1,0,world_rank);\n  topDownUpdate(tree,st,0,n-1,0,world_rank,ans);\n\n  if(world_rank==0){\n    printf(\"Array: \\n\");\n    for(int i=0;i<n;i++){\n      printf(\"%d \",arr[i]);\n    }\n    printf(\"\\nSum: \\n\");\n    for(int i=1;i<n;i++){\n    }\n    for(int i=0;i<n;i++){\n      printf(\"%d \",ans[i]);\n    }\n    printf(\"\\n\");\n  }\n\n}", "label": "int main(int argc, char** argv){\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: n size of arrays\\n\");\n    exit(1);\n  }\n\n  int n;\n  n=atoi(argv[1]);\n  srand(time(NULL));\n  MPI_Init(NULL, NULL);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  int *st = NULL;\n  int *arr=NULL;\n  int *tree=NULL;\n  int *ans=NULL;\n  int size=0;\n  if(world_rank == 0){\n    int x = (int)(ceil(log2(n)));\n    int max_size = 2*(int)pow(2, x) - 1;\n    arr=create_rand_nums(n);\n    st=constructST(arr, n, 0);\n    tree=constructST(arr, n, 1);\n    ans=malloc(sizeof(int)*n);\n    size=max_size;\n  }else{\n    int x = (int)(ceil(log2(n)));\n    int max_size = 2*(int)pow(2, x) - 1;\n    size=max_size;\n    st = malloc(sizeof(int)*max_size);\n    tree = malloc(sizeof(int)*max_size);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  MPI_Bcast(tree, size, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Bcast(st, size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  bottomUpUpdate(tree,st,0,n-1,0,world_rank);\n  topDownUpdate(tree,st,0,n-1,0,world_rank,ans);\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if(world_rank==0){\n    printf(\"Array: \\n\");\n    for(int i=0;i<n;i++){\n      printf(\"%d \",arr[i]);\n    }\n    printf(\"\\nSum: \\n\");\n    for(int i=1;i<n;i++){\n      MPI_Recv(&ans[i],1,MPI_INT,i,i,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n    }\n    for(int i=0;i<n;i++){\n      printf(\"%d \",ans[i]);\n    }\n    printf(\"\\n\");\n  }\n\n  MPI_Finalize();\n}"}
{"program": "qingu_738", "code": "int main(int argc, char *argv[])\n{\n    int          rank, nproc;\n    int          errors = 0, all_errors = 0;\n    int          buf, my_buf;\n    MPI_Win      win;\n\n\n\n\n\n\n    \n\n    CHECK_ERR(MPI_Get(&my_buf, 1, MPI_INT, 0, 0, 1, MPI_INT, win));\n\n\n\n    if (rank == 0 && all_errors == 0) printf(\" No Errors\\n\");\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int          rank, nproc;\n    int          errors = 0, all_errors = 0;\n    int          buf, my_buf;\n    MPI_Win      win;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    MPI_Win_create(&buf, sizeof(int), sizeof(int),\n                    MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n    MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);\n\n    MPI_Win_fence(0, win);\n\n    MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);\n    MPI_Get(&my_buf, 1, MPI_INT, 0, 0, 1, MPI_INT, win);\n    MPI_Win_unlock(0, win);\n\n    \n\n    CHECK_ERR(MPI_Get(&my_buf, 1, MPI_INT, 0, 0, 1, MPI_INT, win));\n\n    MPI_Win_fence(0, win);\n    MPI_Win_free(&win);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0) printf(\" No Errors\\n\");\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "qingu_739", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = indexed_contig_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_contig_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_first_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_first_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_middle_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_middle_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_last_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_last_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_contig_leading_zero_test();\n    if (err && verbose) fprintf(stderr,\n                                \"%d errors in indexed_contig_leading_zero_test.\\n\",\n                                err);\n    errs += err;\n\n    err = indexed_same_lengths();\n    if (err && verbose) fprintf(stderr,\n                                \"%d errors in indexed_contig_leading_zero_test.\\n\",\n                                err);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = indexed_contig_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_contig_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_first_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_first_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_middle_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_middle_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_zeroblock_last_test();\n    if (err && verbose) fprintf(stderr,\n\t\t\t\t\"%d errors in indexed_zeroblock_last_test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    err = indexed_contig_leading_zero_test();\n    if (err && verbose) fprintf(stderr,\n                                \"%d errors in indexed_contig_leading_zero_test.\\n\",\n                                err);\n    errs += err;\n\n    err = indexed_same_lengths();\n    if (err && verbose) fprintf(stderr,\n                                \"%d errors in indexed_contig_leading_zero_test.\\n\",\n                                err);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "PawseySupercomputing_740", "code": "int main(int argc, char *argv[])\n{\n  int rank, thread;\n  cpu_set_t coremask;\n  char clbuf[7 * CPU_SETSIZE], hnbuf[64];\n\n  memset(clbuf, 0, sizeof(clbuf));\n  memset(hnbuf, 0, sizeof(hnbuf));\n  (void)gethostname(hnbuf, sizeof(hnbuf));\n  #pragma omp parallel private(thread, coremask, clbuf)\n  {\n    thread = omp_get_thread_num();\n    (void)sched_getaffinity(0, sizeof(coremask), &coremask);\n    cpuset_to_cstr(&coremask, clbuf);\n    #pragma omp barrier\n    printf(\"Hello from rank %d, thread %d, on %s. (core affinity = %s)\\n\",\n            rank, thread, hnbuf, clbuf);\n  }\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  int rank, thread;\n  cpu_set_t coremask;\n  char clbuf[7 * CPU_SETSIZE], hnbuf[64];\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  memset(clbuf, 0, sizeof(clbuf));\n  memset(hnbuf, 0, sizeof(hnbuf));\n  (void)gethostname(hnbuf, sizeof(hnbuf));\n  #pragma omp parallel private(thread, coremask, clbuf)\n  {\n    thread = omp_get_thread_num();\n    (void)sched_getaffinity(0, sizeof(coremask), &coremask);\n    cpuset_to_cstr(&coremask, clbuf);\n    #pragma omp barrier\n    printf(\"Hello from rank %d, thread %d, on %s. (core affinity = %s)\\n\",\n            rank, thread, hnbuf, clbuf);\n  }\n  MPI_Finalize();\n  return(0);\n}"}
{"program": "bmi-forum_741", "code": "int main( int argc, char* argv[] ) {\n\tDictionary*\tdictionary;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\tMPI_Comm\tCommWorld;\n\t\n\t\n\n\t\n\tBase_Init( &argc, &argv );\n\tDiscretisationGeometry_Init( &argc, &argv );\n\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\n\tprintf( \"+++ 1D Tests +++\\n\\n\" );\n\tTest_TestTopologyOfSize( dictionary, 3, 1, 1 );\n\tprintf( \"+++ 2D Tests +++\\n\\n\" );\n\tTest_TestTopologyOfSize( dictionary, 3, 3, 1 );\n\tprintf( \"+++ 3D Tests +++\\n\\n\" );\n\tTest_TestTopologyOfSize( dictionary, 3, 3, 3 );\n\n\tStg_Class_Delete( dictionary );\n\t\t\n\tDiscretisationGeometry_Finalise();\n\tBase_Finalise();\n\t\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tDictionary*\tdictionary;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\tMPI_Comm\tCommWorld;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBase_Init( &argc, &argv );\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\t\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\n\tprintf( \"+++ 1D Tests +++\\n\\n\" );\n\tTest_TestTopologyOfSize( dictionary, 3, 1, 1 );\n\tprintf( \"+++ 2D Tests +++\\n\\n\" );\n\tTest_TestTopologyOfSize( dictionary, 3, 3, 1 );\n\tprintf( \"+++ 3D Tests +++\\n\\n\" );\n\tTest_TestTopologyOfSize( dictionary, 3, 3, 3 );\n\n\tStg_Class_Delete( dictionary );\n\t\t\n\tDiscretisationGeometry_Finalise();\n\tBase_Finalise();\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "syftalent_745", "code": "int main(int argc, char **argv)\n{\n\n    int nerrors=0, i;\n    int rank, size;\n\n#define NR_TYPES 3\n    MPI_Offset expected_sizes[NR_TYPES] = {1024UL*1024UL*2400UL,\n\t2346319872,\n\t2346319872};\n    MPI_Datatype types[NR_TYPES];\n\n    \n\n    types[0] = make_largexfer_type_struct(expected_sizes[0]);\n    \n\n    types[1] = make_largexfer_type_struct(expected_sizes[1]);\n    \n\n    types[2] = make_largexfer_type_hindexed(expected_sizes[2]);\n\n    for (i=0; i<NR_TYPES; i++) {\n\tif (types[i] != MPI_DATATYPE_NULL) {\n\t\tnerrors += testtype(types[i], expected_sizes[i]);\n\t}\n    }\n\n    if (rank == 0) {\n\tif (nerrors) {\n\t    printf(\"found %d errors\\n\", nerrors);\n\t} else {\n\t    printf(\" No errors\\n\");\n\t}\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n    int nerrors=0, i;\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n#define NR_TYPES 3\n    MPI_Offset expected_sizes[NR_TYPES] = {1024UL*1024UL*2400UL,\n\t2346319872,\n\t2346319872};\n    MPI_Datatype types[NR_TYPES];\n\n    \n\n    types[0] = make_largexfer_type_struct(expected_sizes[0]);\n    \n\n    types[1] = make_largexfer_type_struct(expected_sizes[1]);\n    \n\n    types[2] = make_largexfer_type_hindexed(expected_sizes[2]);\n\n    for (i=0; i<NR_TYPES; i++) {\n\tif (types[i] != MPI_DATATYPE_NULL) {\n\t\tnerrors += testtype(types[i], expected_sizes[i]);\n\t \tMPI_Type_free(&(types[i]));\n\t}\n    }\n\n    MPI_Finalize();\n    if (rank == 0) {\n\tif (nerrors) {\n\t    printf(\"found %d errors\\n\", nerrors);\n\t} else {\n\t    printf(\" No errors\\n\");\n\t}\n    }\n\n    return 0;\n}"}
{"program": "bmi-forum_746", "code": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tStream*\t\tstream;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\t\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tConditionFunction*\tcf;\n\n\t\tcf = ConditionFunction_New(func, \"quadratic\");\n\t\tPrint(cf, stream);\n\n\t\tConditionFunction_Apply(cf, 4, 2, NULL, NULL);\n\n\t\tStg_Class_Delete(cf);\n\t}\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tStream*\t\tstream;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_size( MPI_COMM_WORLD, &procCount );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\t\n\tstream = Journal_Register (Info_Type, \"myStream\");\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tConditionFunction*\tcf;\n\n\t\tcf = ConditionFunction_New(func, \"quadratic\");\n\t\tPrint(cf, stream);\n\n\t\tConditionFunction_Apply(cf, 4, 2, NULL, NULL);\n\n\t\tStg_Class_Delete(cf);\n\t}\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "edponce_747", "code": "int main(int argc, char **argv)\n{\n  int i;           \n\n  int err;         \n\n  args_t args;     \n\n  iomap_t iomap;   \n\n  hits_t hits;     \n\n  double start, finish;\n  mpi_t mpi;\n\n  \n\n  \n  \n\n  start =\n \n  \n\n  for(i = 0; i < mpi.procCnt; i++)\n  {\n    if( mpi.procRank == i )\n    {\n      err = parseCmdline(argc, argv, &args, &mpi);\n      if( err != 0 )\n      {\n        fprintf(stderr, \"Error: failed parsing command line options\\n\\n\");\n        return CFGERROR;\n      }\n    }\n  }\n\n#ifdef BCAST_INFILES\n  \n\n  err = distributeInputFiles(&args, &mpi);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed distributing input files\\n\\n\");\n    return ERROR;\n  }\n#endif\n\n  \n\n  err = openQueryFile(args.qf, &iomap);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed opening query file\\n\\n\");\n    return ERROR;\n  }\n\n  \n\n  \n\n  err = setOffs(&iomap, &mpi);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed to set offsets\\n\\n\");\n    free(iomap.fileOffs);\n    fclose(iomap.qfd);\n    return ERROR;\n  }\n \n  \n\n  memset(&hits, 0, sizeof(hits_t));\n  hits.pipeProg = args.pipeProg;\n  err = loadBlastTable(args.btable, &hits);\n  if( err != 0 )   \n  {\n    fprintf(stderr, \"Error: failed loading BLAST table file\\n\\n\");\n    free(iomap.fileOffs);\n    fclose(iomap.qfd);\n    return ERROR;\n  }\n\n  \n\n  \n\n  err = partQueryFile(&args, &iomap, &hits, &mpi);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed extracting sequences\\n\\n\");\n    free(iomap.fileOffs);\n    freeHitsMemory(&hits);\n    fclose(iomap.qfd);\n    return ERROR;\n  }\n   \n  \n\n  free(iomap.fileOffs);\n  freeHitsMemory(&hits);\n  fclose(iomap.qfd);\n\n  \n\n  finish =\n  if( mpi.procRank == 0 )\n    fprintf(stdout, \"Total wall time = %f\\n\\n\", finish - start);\n  \n  \n\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int i;           \n\n  int err;         \n\n  args_t args;     \n\n  iomap_t iomap;   \n\n  hits_t hits;     \n\n  double start, finish;\n  mpi_t mpi;\n\n  \n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_dup(MPI_COMM_WORLD, &mpi.MPI_MY_WORLD);\n  MPI_Comm_size(mpi.MPI_MY_WORLD, &mpi.procCnt);\n  MPI_Comm_rank(mpi.MPI_MY_WORLD, &mpi.procRank);\n  MPI_Get_processor_name(mpi.procName, &mpi.nameLen);\n  \n  \n\n  start = MPI_Wtime();\n \n  \n\n  for(i = 0; i < mpi.procCnt; i++)\n  {\n    MPI_Barrier(mpi.MPI_MY_WORLD);\n    if( mpi.procRank == i )\n    {\n      err = parseCmdline(argc, argv, &args, &mpi);\n      if( err != 0 )\n      {\n        fprintf(stderr, \"Error: failed parsing command line options\\n\\n\");\n        MPI_Comm_free(&mpi.MPI_MY_WORLD);\n        MPI_Finalize();\n        return CFGERROR;\n      }\n    }\n  }\n\n#ifdef BCAST_INFILES\n  \n\n  err = distributeInputFiles(&args, &mpi);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed distributing input files\\n\\n\");\n    MPI_Comm_free(&mpi.MPI_MY_WORLD);\n    MPI_Finalize();\n    return ERROR;\n  }\n  MPI_Barrier(mpi.MPI_MY_WORLD);\n#endif\n\n  \n\n  err = openQueryFile(args.qf, &iomap);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed opening query file\\n\\n\");\n    MPI_Comm_free(&mpi.MPI_MY_WORLD);\n    MPI_Finalize();\n    return ERROR;\n  }\n\n  \n\n  \n\n  err = setOffs(&iomap, &mpi);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed to set offsets\\n\\n\");\n    free(iomap.fileOffs);\n    fclose(iomap.qfd);\n    MPI_Comm_free(&mpi.MPI_MY_WORLD);\n    MPI_Finalize();\n    return ERROR;\n  }\n \n  \n\n  memset(&hits, 0, sizeof(hits_t));\n  hits.pipeProg = args.pipeProg;\n  err = loadBlastTable(args.btable, &hits);\n  if( err != 0 )   \n  {\n    fprintf(stderr, \"Error: failed loading BLAST table file\\n\\n\");\n    free(iomap.fileOffs);\n    fclose(iomap.qfd);\n    MPI_Comm_free(&mpi.MPI_MY_WORLD);\n    MPI_Finalize();\n    return ERROR;\n  }\n\n  \n\n  \n\n  err = partQueryFile(&args, &iomap, &hits, &mpi);\n  if( err != 0 )\n  {\n    fprintf(stderr, \"Error: failed extracting sequences\\n\\n\");\n    free(iomap.fileOffs);\n    freeHitsMemory(&hits);\n    fclose(iomap.qfd);\n    MPI_Comm_free(&mpi.MPI_MY_WORLD);\n    MPI_Finalize();\n    return ERROR;\n  }\n   \n  \n\n  free(iomap.fileOffs);\n  freeHitsMemory(&hits);\n  fclose(iomap.qfd);\n\n  \n\n  MPI_Barrier(mpi.MPI_MY_WORLD);\n  finish = MPI_Wtime();\n  if( mpi.procRank == 0 )\n    fprintf(stdout, \"Total wall time = %f\\n\\n\", finish - start);\n  \n  \n\n  MPI_Comm_free(&mpi.MPI_MY_WORLD);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "NLeSC_748", "code": "int main(int argc, char *argv[])\n{\n    MPI_Group world;\n    MPI_Group reverse;\n \n    MPI_Comm c1;\n\n    int  namelen, rank, size, newrank, newsize, color, key, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    int rev[1][3];\n\n\n\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (size != 16) {\n        fprintf(stderr, \"Need 16 processes for this test\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_Group failed!\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_rank failed!\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_size failed!\\n\");\n        return 1;\n    }\n\n    fprintf(stderr, \"I am process %d of %d in world group\\n\", newrank, newsize);\n\n    rev[0][0] = 15;\n    rev[0][1] = 0;\n    rev[0][2] = -1;\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_range_incl failed!\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_rank failed!\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_size failed!\\n\");\n        return 1;\n    }\n\n    fprintf(stderr, \"I am process %d of %d in reverse group\\n\", newrank, newsize);\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_create failed!\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_size failed!\\n\");\n        return 1;\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_rank failed!\\n\");\n        return 1;\n    }\n\n    fprintf(stderr, \"I am process %d of %d in reverse comm\\n\", newrank, newsize);\n\n    fprintf(stderr, \"Done!\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Group world;\n    MPI_Group reverse;\n \n    MPI_Comm c1;\n\n    int  namelen, rank, size, newrank, newsize, color, key, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    int rev[1][3];\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (size != 16) {\n        fprintf(stderr, \"Need 16 processes for this test\\n\");\n        return 1;\n    }\n\n    error = MPI_Comm_group(MPI_COMM_WORLD, &world);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_Group failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    error = MPI_Group_rank(world, &newrank);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_rank failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    error = MPI_Group_size(world, &newsize);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_size failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    fprintf(stderr, \"I am process %d of %d in world group\\n\", newrank, newsize);\n\n    rev[0][0] = 15;\n    rev[0][1] = 0;\n    rev[0][2] = -1;\n\n    error = MPI_Group_range_incl(world, 1, rev, &reverse);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_range_incl failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    error = MPI_Group_rank(reverse, &newrank);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_rank failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    error = MPI_Group_size(reverse, &newsize);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Group_size failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    fprintf(stderr, \"I am process %d of %d in reverse group\\n\", newrank, newsize);\n\n    error = MPI_Comm_create(MPI_COMM_WORLD, reverse, &c1);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_create failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    error = MPI_Comm_size(c1, &newsize);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_size failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    error = MPI_Comm_rank(c1, &newrank);\n\n    if (error != MPI_SUCCESS) {\n\tfprintf(stderr, \"MPI_Comm_rank failed!\\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    fprintf(stderr, \"I am process %d of %d in reverse comm\\n\", newrank, newsize);\n\n    fprintf(stderr, \"Done!\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "tcsiwula_750", "code": "int main(int argc, char* argv[]) {\n   int p, my_rank;\n   MPI_Comm comm;\n   int result, in_val;\n\n   comm = MPI_COMM_WORLD;\n\n   if (my_rank == 0) {\n      printf(\"Enter an int\\n\");\n      scanf(\"%d\", &in_val);\n   }\n\n   result = Bcast(in_val, my_rank, p, comm);\n\n   printf(\"Proc %d > result = %d\\n\", my_rank, result); \n\n   return 0;\n}", "label": "int main(int argc, char* argv[]) {\n   int p, my_rank;\n   MPI_Comm comm;\n   int result, in_val;\n\n   MPI_Init(&argc, &argv);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &p);\n   MPI_Comm_rank(comm, &my_rank);\n\n   if (my_rank == 0) {\n      printf(\"Enter an int\\n\");\n      scanf(\"%d\", &in_val);\n   }\n\n   result = Bcast(in_val, my_rank, p, comm);\n\n   printf(\"Proc %d > result = %d\\n\", my_rank, result); \n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "mkurnosov_751", "code": "int main(int argc, char **argv)\n{\n    int i;\n\n    \n\n    mpiperf_is_measure_started = 0;\n\n    set_default_options();\n\n    \n\n    mpiperf_cmdline = (char *)malloc(sizeof(*mpiperf_cmdline) *\n                                     MPIPERF_CMDLINE_MAX);\n    if (mpiperf_cmdline) {\n        strcpy(mpiperf_cmdline, argv[0]);\n        for (i = 1; i < argc; i++) {\n            strcat(mpiperf_cmdline, \" \");\n            strcat(mpiperf_cmdline, argv[i]);\n        }\n    }\n\n    if (parse_options(argc, argv) == MPIPERF_FAILURE) {\n        exit(EXIT_FAILURE);\n    }\n\n    mpiperf_initialize();\n    if (mpiperf_collbench) {\n        run_collbench(mpiperf_collbench);\n    } else if (mpiperf_pt2ptbench) {\n        run_pt2ptbench(mpiperf_pt2ptbench);\n    } else if (mpiperf_nbcbench) {\n        run_nbcbench(mpiperf_nbcbench);\n    }\n    mpiperf_finalize();\n\n    free(mpiperf_cmdline);\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char **argv)\n{\n    int i;\n\n    \n\n    mpiperf_is_measure_started = 0;\n\n    MPI_Init(&argc, &argv);\n    set_default_options();\n\n    \n\n    mpiperf_cmdline = (char *)malloc(sizeof(*mpiperf_cmdline) *\n                                     MPIPERF_CMDLINE_MAX);\n    if (mpiperf_cmdline) {\n        strcpy(mpiperf_cmdline, argv[0]);\n        for (i = 1; i < argc; i++) {\n            strcat(mpiperf_cmdline, \" \");\n            strcat(mpiperf_cmdline, argv[i]);\n        }\n    }\n\n    if (parse_options(argc, argv) == MPIPERF_FAILURE) {\n        MPI_Finalize();\n        exit(EXIT_FAILURE);\n    }\n\n    mpiperf_initialize();\n    if (mpiperf_collbench) {\n        run_collbench(mpiperf_collbench);\n    } else if (mpiperf_pt2ptbench) {\n        run_pt2ptbench(mpiperf_pt2ptbench);\n    } else if (mpiperf_nbcbench) {\n        run_nbcbench(mpiperf_nbcbench);\n    }\n    mpiperf_finalize();\n\n    free(mpiperf_cmdline);\n    MPI_Finalize();\n    return EXIT_SUCCESS;\n}"}
{"program": "bmi-forum_752", "code": "int main( int argc, char* argv[] )\n{\t\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\t\n\tint i;\n\n\tvoid* bytesObj;\n\tvoid* bytesArray;\n\n\tdouble*\tarray1d;\n\n\tdouble** array2d;\n\tdouble*** array3d;\n\n\tvoid* fromReallocObj;\n\tchar* fromReallocArray;\n\tchar** fromRealloc2DArray;\n\tchar*** fromRealloc3DArray;\n\tchar* fromRealloc2DAs1D;\n\tchar* fromRealloc3DAs1D;\n\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) \n\t{\n\t\tJournal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\t}\n\n\n\t\n\n\tbytesObj = Memory_Alloc_Bytes( 6, \"Bytes\", \"Alloced\" );\n\tbytesArray = Memory_Alloc_Array_Bytes( 2, 10, \"Bytes\", \"Alloced\" );\t\n\n\n\tarray1d = Memory_Alloc_Array( double, 3, \"Alloced\" );\n\tarray2d = Memory_Alloc_2DArray( double, 1, 3, \"Alloced\" );\t\t\n\n\tarray3d = Memory_Alloc_3DArray( double, 1, 2, 3, \"Alloced\" );\t\t\n\n\t\n\t\n\n\tstrcpy( (char*)bytesObj, \"hello\" );\n\t\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tarray1d[i] = (double)i;\n\t}\n\t\n\t\n\tif ( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream, \"Allocation results.\\n\" );\n\t\tMemory_Print();\n\t}\n\t\n\t\n\t\n\n\tbytesObj = Memory_Realloc( bytesObj, 10 );\t\t\t\t\n\n\tbytesArray = Memory_Realloc_Array_Bytes( bytesArray, 5, 10 );\t\t\n\n\t\n\tarray1d = Memory_Realloc_Array( array1d, double, 5 );\t\t\t\n\n\tarray2d = Memory_Realloc_2DArraySafe( array2d, double, 1, 3, 3, 3 );\n\tarray3d = Memory_Realloc_3DArraySafe( array3d, double, 1, 2, 3, 4, 4, 4 );\n\t\n\t\n\n\tJournal_Printf( stream, \"%s\\n\", (char*)bytesObj );\n\t\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tJournal_Printf( stream, \"%lf \", array1d[i] );\n\t}\n\tJournal_Printf( stream, \"\\n\" );\n\n\t\n\t\n\n\tfromReallocObj = Memory_Realloc( NULL, 50 );\n\tfromReallocArray = Memory_Realloc_Array( NULL, char, 50 );\n\tfromRealloc2DArray = Memory_Realloc_2DArray( NULL, char, 4, 4 );\n\tfromRealloc3DArray = Memory_Realloc_3DArray( NULL, char, 3, 3, 3 );\n\tfromRealloc2DAs1D = Memory_Realloc_2DArrayAs1D( NULL, char, 0, 0, 4, 4 );\n\tfromRealloc3DAs1D = Memory_Realloc_3DArrayAs1D( NULL, char, 0, 0, 0, 3, 3, 3 );\n\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"\\nReallocaiton results.\\n\" );\n\t\tMemory_Print();\n\n\t\tJournal_Printf( stream, \"\\nNew pointers from realloc:\\n\" );\n\t\tMemory_Print_Pointer( fromReallocObj );\n\t\tMemory_Print_Pointer( fromReallocArray );\n\t\tMemory_Print_Pointer( fromRealloc2DArray );\n\t\tMemory_Print_Pointer( fromRealloc3DArray );\n\t\tMemory_Print_Pointer( fromRealloc2DAs1D );\n\t\tMemory_Print_Pointer( fromRealloc3DAs1D );\t\n\t}\n\n\tMemory_Free( bytesObj );\n\tMemory_Free( bytesArray );\n\tMemory_Free( array1d );\n\tMemory_Free( array2d );\n\tMemory_Free( array3d );\n\tMemory_Free( fromReallocObj );\n\tMemory_Free( fromReallocArray );\n\tMemory_Free( fromRealloc2DArray );\n\tMemory_Free( fromRealloc3DArray );\n\tMemory_Free( fromRealloc2DAs1D );\n\tMemory_Free( fromRealloc3DAs1D );\n\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"\\nFree results\\n\" );\n\t\tMemory_Print();\n\t\tMemory_Print_Leak();\n\t}\n\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] )\n{\t\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\t\n\tint i;\n\n\tvoid* bytesObj;\n\tvoid* bytesArray;\n\n\tdouble*\tarray1d;\n\n\tdouble** array2d;\n\tdouble*** array3d;\n\n\tvoid* fromReallocObj;\n\tchar* fromReallocArray;\n\tchar** fromRealloc2DArray;\n\tchar*** fromRealloc3DArray;\n\tchar* fromRealloc2DAs1D;\n\tchar* fromRealloc3DAs1D;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) \n\t{\n\t\tJournal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\t}\n\n\n\t\n\n\tbytesObj = Memory_Alloc_Bytes( 6, \"Bytes\", \"Alloced\" );\n\tbytesArray = Memory_Alloc_Array_Bytes( 2, 10, \"Bytes\", \"Alloced\" );\t\n\n\n\tarray1d = Memory_Alloc_Array( double, 3, \"Alloced\" );\n\tarray2d = Memory_Alloc_2DArray( double, 1, 3, \"Alloced\" );\t\t\n\n\tarray3d = Memory_Alloc_3DArray( double, 1, 2, 3, \"Alloced\" );\t\t\n\n\t\n\t\n\n\tstrcpy( (char*)bytesObj, \"hello\" );\n\t\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tarray1d[i] = (double)i;\n\t}\n\t\n\t\n\tif ( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream, \"Allocation results.\\n\" );\n\t\tMemory_Print();\n\t}\n\t\n\t\n\t\n\n\tbytesObj = Memory_Realloc( bytesObj, 10 );\t\t\t\t\n\n\tbytesArray = Memory_Realloc_Array_Bytes( bytesArray, 5, 10 );\t\t\n\n\t\n\tarray1d = Memory_Realloc_Array( array1d, double, 5 );\t\t\t\n\n\tarray2d = Memory_Realloc_2DArraySafe( array2d, double, 1, 3, 3, 3 );\n\tarray3d = Memory_Realloc_3DArraySafe( array3d, double, 1, 2, 3, 4, 4, 4 );\n\t\n\t\n\n\tJournal_Printf( stream, \"%s\\n\", (char*)bytesObj );\n\t\n\tfor ( i = 0; i < 3; ++i )\n\t{\n\t\tJournal_Printf( stream, \"%lf \", array1d[i] );\n\t}\n\tJournal_Printf( stream, \"\\n\" );\n\n\t\n\t\n\n\tfromReallocObj = Memory_Realloc( NULL, 50 );\n\tfromReallocArray = Memory_Realloc_Array( NULL, char, 50 );\n\tfromRealloc2DArray = Memory_Realloc_2DArray( NULL, char, 4, 4 );\n\tfromRealloc3DArray = Memory_Realloc_3DArray( NULL, char, 3, 3, 3 );\n\tfromRealloc2DAs1D = Memory_Realloc_2DArrayAs1D( NULL, char, 0, 0, 4, 4 );\n\tfromRealloc3DAs1D = Memory_Realloc_3DArrayAs1D( NULL, char, 0, 0, 0, 3, 3, 3 );\n\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"\\nReallocaiton results.\\n\" );\n\t\tMemory_Print();\n\n\t\tJournal_Printf( stream, \"\\nNew pointers from realloc:\\n\" );\n\t\tMemory_Print_Pointer( fromReallocObj );\n\t\tMemory_Print_Pointer( fromReallocArray );\n\t\tMemory_Print_Pointer( fromRealloc2DArray );\n\t\tMemory_Print_Pointer( fromRealloc3DArray );\n\t\tMemory_Print_Pointer( fromRealloc2DAs1D );\n\t\tMemory_Print_Pointer( fromRealloc3DAs1D );\t\n\t}\n\n\tMemory_Free( bytesObj );\n\tMemory_Free( bytesArray );\n\tMemory_Free( array1d );\n\tMemory_Free( array2d );\n\tMemory_Free( array3d );\n\tMemory_Free( fromReallocObj );\n\tMemory_Free( fromReallocArray );\n\tMemory_Free( fromRealloc2DArray );\n\tMemory_Free( fromRealloc3DArray );\n\tMemory_Free( fromRealloc2DAs1D );\n\tMemory_Free( fromRealloc3DAs1D );\n\n\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"\\nFree results\\n\" );\n\t\tMemory_Print();\n\t\tMemory_Print_Leak();\n\t}\n\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "aarestad_753", "code": "int main(int argc, char* argv[]) {\n    int iter;\n    int in, out, i, iters, max, ix, iy, ranks[1], done, temp;\n    double x, y, Pi, error, epsilon;\n    int numprocs, myid, server, totalin, totalout, workerid;\n    int rands[CHUNKSIZE], request;\n    MPI_Comm world, workers;\n    MPI_Group world_group, worker_group;\n    MPI_Status status;\n\n    world = MPI_COMM_WORLD;\n    server = numprocs - 1;\n\n    if (myid == 0) {\n        sscanf(argv[1], \"%lf\", &epsilon);\n    }\n\n    ranks[0] = server;\n\n    if (myid == server) {\n        FILE* randfile = fopen(\"/dev/urandom\", \"rb\");\n        int tmp = 0;\n        do {\n            if (request) {\n                for (i = 0; i < CHUNKSIZE; i++) {\n                    fread(&tmp, sizeof(int), 1, randfile);\n                    rands[i] = abs(tmp);\n                }\n            }\n        } while (request > 0);\n        fclose(randfile);\n    } else {\n        request = 1;\n        done = in = out = 0;\n        max = INT_MAX;\n        iter = 0;\n        while (!done) {\n            iter++;\n            request = 1;\n            for (i = 0; i < CHUNKSIZE; ) {\n                x = (((double) rands[i++]) / max) * 2 - 1;\n                y = (((double) rands[i++]) / max) * 2 - 1;\n                if (x*x + y*y < 1.0) {\n                    ++in;\n                } else {\n                    ++out;\n                }\n            }\n            Pi = (4.0 * totalin) / (totalin + totalout);\n            error = fabs(Pi - PI_EXACT);\n            done = (error < epsilon || (totalin + totalout) > MAXPOINTS);\n            request = done ? 0 : 1;\n            if (myid == 0) {\n                printf(\"\\rpi = %23.20f\", Pi);\n            } else {\n                if (request) {\n                }\n            }\n        }\n    }\n\n    if (myid == 0) {\n        printf(\"\\npoints: %d\\nin: %d, out: %d\\n\",\n               totalin + totalout, totalin, totalout);\n    }\n\n    \n\n}", "label": "int main(int argc, char* argv[]) {\n    int iter;\n    int in, out, i, iters, max, ix, iy, ranks[1], done, temp;\n    double x, y, Pi, error, epsilon;\n    int numprocs, myid, server, totalin, totalout, workerid;\n    int rands[CHUNKSIZE], request;\n    MPI_Comm world, workers;\n    MPI_Group world_group, worker_group;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    world = MPI_COMM_WORLD;\n    MPI_Comm_size(world, &numprocs);\n    MPI_Comm_rank(world, &myid);\n    server = numprocs - 1;\n\n    if (myid == 0) {\n        sscanf(argv[1], \"%lf\", &epsilon);\n    }\n    MPI_Bcast(&epsilon, 1, MPI_DOUBLE, 0, world);\n\n    MPI_Comm_group(world, &world_group);\n    ranks[0] = server;\n    MPI_Group_excl(world_group, 1, ranks, &worker_group);\n    MPI_Comm_create(world, worker_group, &workers);\n    MPI_Group_free(&worker_group);\n\n    if (myid == server) {\n        FILE* randfile = fopen(\"/dev/urandom\", \"rb\");\n        int tmp = 0;\n        do {\n            MPI_Recv(&request, 1, MPI_INT, MPI_ANY_SOURCE, REQUEST,\n                     world, &status);\n            if (request) {\n                for (i = 0; i < CHUNKSIZE; i++) {\n                    fread(&tmp, sizeof(int), 1, randfile);\n                    rands[i] = abs(tmp);\n                }\n                MPI_Send(rands, CHUNKSIZE, MPI_INT, status.MPI_SOURCE,\n                         REPLY, world);\n            }\n        } while (request > 0);\n        fclose(randfile);\n    } else {\n        request = 1;\n        done = in = out = 0;\n        max = INT_MAX;\n        MPI_Send(&request, 1, MPI_INT, server, REQUEST, world);\n        MPI_Comm_rank(workers, &workerid);\n        iter = 0;\n        while (!done) {\n            iter++;\n            request = 1;\n            MPI_Recv(rands, CHUNKSIZE, MPI_INT, server, REPLY, world, &status);\n            for (i = 0; i < CHUNKSIZE; ) {\n                x = (((double) rands[i++]) / max) * 2 - 1;\n                y = (((double) rands[i++]) / max) * 2 - 1;\n                if (x*x + y*y < 1.0) {\n                    ++in;\n                } else {\n                    ++out;\n                }\n            }\n            MPI_Allreduce(&in, &totalin, 1, MPI_INT, MPI_SUM, workers);\n            MPI_Allreduce(&out, &totalout, 1, MPI_INT, MPI_SUM, workers);\n            Pi = (4.0 * totalin) / (totalin + totalout);\n            error = fabs(Pi - PI_EXACT);\n            done = (error < epsilon || (totalin + totalout) > MAXPOINTS);\n            request = done ? 0 : 1;\n            if (myid == 0) {\n                printf(\"\\rpi = %23.20f\", Pi);\n                MPI_Send(&request, 1, MPI_INT, server, REQUEST, world);\n            } else {\n                if (request) {\n                    MPI_Send(&request, 1, MPI_INT, server, REQUEST, world);\n                }\n            }\n        }\n    }\n\n    if (myid == 0) {\n        printf(\"\\npoints: %d\\nin: %d, out: %d\\n\",\n               totalin + totalout, totalin, totalout);\n    }\n\n    \n\n    MPI_Finalize();\n}"}
{"program": "mpip_756", "code": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_dft(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_dft(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_complex_3d(ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_757", "code": "int main(int argc, char *argv[])\n{\n    int size, rank, i, *excl;\n    MPI_Group world_group, even_group;\n    MPI_Comm even_comm;\n\n\n\n    if (size % 2) {\n        fprintf(stderr, \"this program requires a multiple of 2 number of processes\\n\");\n    }\n\n    excl = malloc((size / 2) * sizeof(int));\n    assert(excl);\n\n    \n\n    for (i = 0; i < size / 2; i++)\n        excl[i] = (2 * i) + 1;\n\n    \n\n\n#if !defined(USE_STRICT_MPI) && defined(MPICH)\n    if (rank % 2 == 0) {\n        \n\n    }\n#endif \n\n\n\n    if (rank == 0)\n        printf(\" No errors\\n\");\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int size, rank, i, *excl;\n    MPI_Group world_group, even_group;\n    MPI_Comm even_comm;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (size % 2) {\n        fprintf(stderr, \"this program requires a multiple of 2 number of processes\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    excl = malloc((size / 2) * sizeof(int));\n    assert(excl);\n\n    \n\n    for (i = 0; i < size / 2; i++)\n        excl[i] = (2 * i) + 1;\n\n    \n\n    MPI_Comm_group(MPI_COMM_WORLD, &world_group);\n    MPI_Group_excl(world_group, size / 2, excl, &even_group);\n    MPI_Group_free(&world_group);\n\n#if !defined(USE_STRICT_MPI) && defined(MPICH)\n    if (rank % 2 == 0) {\n        \n\n        MPI_Comm_create_group(MPI_COMM_WORLD, even_group, 0, &even_comm);\n        MPI_Barrier(even_comm);\n        MPI_Comm_free(&even_comm);\n    }\n#endif \n\n\n    MPI_Group_free(&even_group);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0)\n        printf(\" No errors\\n\");\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "slitvinov_758", "code": "int main(int argc, char *argv[]) {\n\n    if (rank == SEND)\n        send();\n    else {\n        recv();\n        cnt();\n        dump();\n    }\n    \n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(COMM, &rank);\n\n    if (rank == SEND)\n        send();\n    else {\n        recv();\n        cnt();\n        dump();\n    }\n    \n    MPI_Finalize();\n    return 0;\n}"}
{"program": "gnu3ra_761", "code": "int main(int argc, char **argv)\n{\n    int mpi_err, errs = 0;\n    MPI_Datatype type;\n\n    parse_args(argc, argv);\n\n    \n\n\n    type = MPI_INT;\n    mpi_err =\n    if (mpi_err != MPI_SUCCESS) {\n\tif (verbose) {\n\t    fprintf(stderr, \"MPI_Type_commit of MPI_INT failed.\\n\");\n\t}\n\terrs++;\n    }\n\n    type = MPI_FLOAT_INT;\n    mpi_err =\n    if (mpi_err != MPI_SUCCESS) {\n\tif (verbose) {\n\t    fprintf(stderr, \"MPI_Type_commit of MPI_FLOAT_INT failed.\\n\");\n\t}\n\terrs++;\n    }\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int mpi_err, errs = 0;\n    MPI_Datatype type;\n\n    MPI_Init(&argc, &argv);\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    type = MPI_INT;\n    mpi_err = MPI_Type_commit(&type);\n    if (mpi_err != MPI_SUCCESS) {\n\tif (verbose) {\n\t    fprintf(stderr, \"MPI_Type_commit of MPI_INT failed.\\n\");\n\t}\n\terrs++;\n    }\n\n    type = MPI_FLOAT_INT;\n    mpi_err = MPI_Type_commit(&type);\n    if (mpi_err != MPI_SUCCESS) {\n\tif (verbose) {\n\t    fprintf(stderr, \"MPI_Type_commit of MPI_FLOAT_INT failed.\\n\");\n\t}\n\terrs++;\n    }\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "pscedu_762", "code": "int\nmain(int argc, char *argv[])\n{\n#ifdef HAVE_LIBPTHREAD\n\tGROUP_t          *group;\n\tint rc, i;\n#elif defined(HAVE_MPI)\n\tint mype = 0;\n#endif\n\tstruct stat sbuf;\n\tint c, fd;\n\n\tTOTAL_PES = 0;\n\n\tprogname = argv[0];\n\tpfl_init();\n\tif (argc == 1) {\n\t\t\n\n\t\tfstat(STDIN_FILENO, &sbuf);\n\t\tif (S_ISCHR(sbuf.st_mode)) {\n\t\t\tfprintf(stderr, \"Use either -i or stdin redirection to specify configuration.\\n\");\n\t\t\tusage();\n\t\t}\n\t}\n\n\twhile (((c = getopt(argc, argv, \"dDi:o:\")) != -1))\n\t\tswitch (c) {\n\t\tcase 'D':\n\t\t\tfio_lexyacc_debug = 1;\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\tfio_global_debug = 1;\n\t\t\tbreak;\n\t\tcase 'i':\n\t\t\tfd = open(optarg, O_RDONLY);\n\t\t\tif (fd < 0)\n\t\t\t\terr(1, \"%s\", optarg);\n\t\t\tif (dup2(fd, STDIN_FILENO) == -1)\n\t\t\t\terr(1, \"dup2\");\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\tstderr_redirect     = 1;\n\t\t\tstderr_fnam_prefix  = pfl_strdup(optarg);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t}\n\targc -= optind;\n\tif (argc)\n\t\tusage();\n\n\trun_yacc();\n\tdump_groups();\n\n#ifdef HAVE_MPI\n\t\n\n\t\n\n\tinit_barriers(mype);\n\tworker(&mype);\n\n#elif defined(HAVE_LIBPTHREAD)\n\tinit_barriers(0);\n\tpsclist_for_each_entry(group, &groupList, group_lentry) {\n\t\tif (group->files_per_dir < group->num_pes) {\n\t\t\tfprintf(stderr, \"# of files per directory (%d) \"\n\t\t\t    \"should be no less than # of PEs (%d), a \"\n\t\t\t    \"multiple is preferred.\\n\",\n\t\t\t    group->files_per_dir, group->num_pes);\n\t\t\tcontinue;\n\t\t}\n\t\t\n\n\t\tfor (i = 0; i < group->num_pes; i++) {\n\t\t\tgroup->threads[i].mype = i + TOTAL_PES;\n\n\t\t\tBDEBUG(\"about to create %d\\n\", group->threads[i].mype);\n\n\t\t\trc = pthread_create(&group->threads[i].thread_id, NULL,\n\t\t\t\tworker, &group->threads[i].mype);\n\t\t\tASSERT(!rc);\n\t\t}\n\n\t\tTOTAL_PES += group->num_pes;\n\n\t\tfor (i = 0; i < group->num_pes; i++)\n\t\t\trc = pthread_join(group->threads[i].thread_id, NULL);\n\t}\n#endif\n\n\texit(0);\n}", "label": "int\nmain(int argc, char *argv[])\n{\n#ifdef HAVE_LIBPTHREAD\n\tGROUP_t          *group;\n\tint rc, i;\n#elif defined(HAVE_MPI)\n\tint mype = 0;\n\tMPI_Init(&argc, &argv);\n#endif\n\tstruct stat sbuf;\n\tint c, fd;\n\n\tTOTAL_PES = 0;\n\n\tprogname = argv[0];\n\tpfl_init();\n\tif (argc == 1) {\n\t\t\n\n\t\tfstat(STDIN_FILENO, &sbuf);\n\t\tif (S_ISCHR(sbuf.st_mode)) {\n\t\t\tfprintf(stderr, \"Use either -i or stdin redirection to specify configuration.\\n\");\n\t\t\tusage();\n\t\t}\n\t}\n\n\twhile (((c = getopt(argc, argv, \"dDi:o:\")) != -1))\n\t\tswitch (c) {\n\t\tcase 'D':\n\t\t\tfio_lexyacc_debug = 1;\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\tfio_global_debug = 1;\n\t\t\tbreak;\n\t\tcase 'i':\n\t\t\tfd = open(optarg, O_RDONLY);\n\t\t\tif (fd < 0)\n\t\t\t\terr(1, \"%s\", optarg);\n\t\t\tif (dup2(fd, STDIN_FILENO) == -1)\n\t\t\t\terr(1, \"dup2\");\n\t\t\tbreak;\n\t\tcase 'o':\n\t\t\tstderr_redirect     = 1;\n\t\t\tstderr_fnam_prefix  = pfl_strdup(optarg);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tusage();\n\t\t}\n\targc -= optind;\n\tif (argc)\n\t\tusage();\n\n\trun_yacc();\n\tdump_groups();\n\n#ifdef HAVE_MPI\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &TOTAL_PES);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mype);\n\t\n\n\tinit_barriers(mype);\n\tworker(&mype);\n\tMPI_Finalize();\n\n#elif defined(HAVE_LIBPTHREAD)\n\tinit_barriers(0);\n\tpsclist_for_each_entry(group, &groupList, group_lentry) {\n\t\tif (group->files_per_dir < group->num_pes) {\n\t\t\tfprintf(stderr, \"# of files per directory (%d) \"\n\t\t\t    \"should be no less than # of PEs (%d), a \"\n\t\t\t    \"multiple is preferred.\\n\",\n\t\t\t    group->files_per_dir, group->num_pes);\n\t\t\tcontinue;\n\t\t}\n\t\t\n\n\t\tfor (i = 0; i < group->num_pes; i++) {\n\t\t\tgroup->threads[i].mype = i + TOTAL_PES;\n\n\t\t\tBDEBUG(\"about to create %d\\n\", group->threads[i].mype);\n\n\t\t\trc = pthread_create(&group->threads[i].thread_id, NULL,\n\t\t\t\tworker, &group->threads[i].mype);\n\t\t\tASSERT(!rc);\n\t\t}\n\n\t\tTOTAL_PES += group->num_pes;\n\n\t\tfor (i = 0; i < group->num_pes; i++)\n\t\t\trc = pthread_join(group->threads[i].thread_id, NULL);\n\t}\n#endif\n\n\texit(0);\n}"}
{"program": "brian-o_763", "code": "int main( int argc, char *argv[])\n{\n\n    int i;\n    int id; \n\n    int p;  \n\n\n   int value;  \n\n\n      \n\n\n     value=0;  \n\n\n\n     if ( id == 0 )\n      {  \n\n       printf (\"This is node %d \\n\", id);\n     fflush (stdout);\n         value=1;\n\n  \n        value=2;\n\n\n       printf (\"This is node %d with value %d\\n\", id,value);\n     fflush (stdout);\n\n    \n\n\n      }\n\n     if ( id == 1 )\n      {  \n\n       printf (\"This is node %d \\n\", id);\n     fflush (stdout);\n        sleep(2);\n       printf (\"This is node %d with value %d\\n\", id,value);\n     fflush (stdout);\n\n\n      }\n\n     if ( id == 2 )\n      {  \n\n       printf (\"This is node %d \\n\", id);\n     fflush (stdout);\n       printf (\"This is node %d with value %d\\n\", id,value);\n     fflush (stdout);\n\n\n      }\n \n\n\n     printf(\"Process %d is done \\n\", id);\n \n     fflush (stdout);\n\n     return 0;\n\n\n\n}", "label": "int main( int argc, char *argv[])\n{\n\n    int i;\n    int id; \n\n    int p;  \n\n\n   int value;  \n\n\n      \n     MPI_Init(&argc, &argv);\n\n     MPI_Comm_rank ( MPI_COMM_WORLD, &id);\n     MPI_Comm_size ( MPI_COMM_WORLD, &p );\n\n     value=0;  \n\n\n\n     if ( id == 0 )\n      {  \n\n       printf (\"This is node %d \\n\", id);\n     fflush (stdout);\n         value=1;\n\n        MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  \n        value=2;\n\n        MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n       printf (\"This is node %d with value %d\\n\", id,value);\n     fflush (stdout);\n\n    \n\n\n      }\n\n     if ( id == 1 )\n      {  \n\n       printf (\"This is node %d \\n\", id);\n     fflush (stdout);\n        sleep(2);\n        MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);\n       printf (\"This is node %d with value %d\\n\", id,value);\n     fflush (stdout);\n\n\n      }\n\n     if ( id == 2 )\n      {  \n\n       printf (\"This is node %d \\n\", id);\n     fflush (stdout);\n        MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);\n       printf (\"This is node %d with value %d\\n\", id,value);\n     fflush (stdout);\n\n\n      }\n \n\n\n     printf(\"Process %d is done \\n\", id);\n \n     fflush (stdout);\n\n     MPI_Finalize ();\n     return 0;\n\n\n\n}"}
{"program": "MaoRodriguesJ_764", "code": "int main(int argc, char **argv) {\n\tunsigned int pontos;\n\tunsigned int pontos_no_circulo;\n\n\tint buffer[1];\n\tint rank, size;\n\tMPI_Status st;\n\n\tif(argc != 2){\n\t\tprintf(\"Uso:\\n\");\n\t\tprintf(\"\\t%s <numero de pontos a serem sorteados>\\n\", argv[0]);\n\t\treturn 1;\n\t}\n\n\tpontos = atoi(argv[1]);\n\n\tunsigned int min = (rank*pontos)/size;\n\tunsigned int max = ((rank + 1) * pontos)/size;\n\n\t\n\n\t\n\n\t\n\n\tpontos_no_circulo = compute_pi(rank, min, max);\n\n\tbuffer[0] = pontos_no_circulo;\n\tif(rank != 0)\n\tif(rank == 0) {\n\t\tfor(int i = 0; i < size-1; i++) {\n\t\t\tpontos_no_circulo += buffer[0];\n\t\t}\n\n\t\t\n\n\t\tprintf(\"Pi = %.040f\\n\", ((double)pontos_no_circulo/(double)pontos)*4);\n\t}\n\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv) {\n\tunsigned int pontos;\n\tunsigned int pontos_no_circulo;\n\n\tint buffer[1];\n\tint rank, size;\n\tMPI_Status st;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif(argc != 2){\n\t\tprintf(\"Uso:\\n\");\n\t\tprintf(\"\\t%s <numero de pontos a serem sorteados>\\n\", argv[0]);\n\t\treturn 1;\n\t}\n\n\tpontos = atoi(argv[1]);\n\n\tunsigned int min = (rank*pontos)/size;\n\tunsigned int max = ((rank + 1) * pontos)/size;\n\n\t\n\n\t\n\n\t\n\n\tpontos_no_circulo = compute_pi(rank, min, max);\n\n\tbuffer[0] = pontos_no_circulo;\n\tif(rank != 0)\n\t\tMPI_Send(&buffer, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\tif(rank == 0) {\n\t\tfor(int i = 0; i < size-1; i++) {\n\t\t\tMPI_Recv(&buffer, 2, MPI_INT, MPI_ANY_SOURCE,\n\t\t\t\t\t MPI_ANY_TAG, MPI_COMM_WORLD, &st);\n\t\t\tpontos_no_circulo += buffer[0];\n\t\t}\n\n\t\t\n\n\t\tprintf(\"Pi = %.040f\\n\", ((double)pontos_no_circulo/(double)pontos)*4);\n\t}\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "PhilippParis_765", "code": "int main(int argc, char* argv[]) {\n    int my_rank, nprocs, p;\n    int i;\n    reprompib_st_opts_t opts;\n    int master_rank;\n    MPI_Status stat;\n\n    double *all_frequencies;\n    double *all_wtime_times;\n    double *frequencies;\n    double *wtime_times;\n    double time_msg[2];\n\n    int step;\n    int n_wait_steps = 11;\n    double wait_time_s = 1;\n\n    \n\n    init_globals();\n    master_rank = get_master_rank();\n\n    parse_test_options(&opts, argc, argv);\n\n    n_wait_steps = opts.n_rep + 1;\n\n\n\n    frequencies = (double*) calloc(n_wait_steps, sizeof(double));\n\n    if (my_rank == master_rank) {\n\n\n        all_frequencies = (double*) calloc(nprocs * n_wait_steps,\n                sizeof(double));\n    }\n\n    for (step = 0; step < n_wait_steps; step++) {\n        uint64_t timerfreq = 0;\n        HRT_INIT(0 \n, timerfreq);\n\n        frequencies[step] = (double) timerfreq;\n\n\n\n\n\n        struct timespec sleep_time;\n        sleep_time.tv_sec = 0;\n        sleep_time.tv_nsec = wait_time_s * 1e9;\n\n        nanosleep(&sleep_time, NULL);\n    }\n\n    if (my_rank == master_rank) {\n        printf(\"wait_time_s p freq\\n\");\n    }\n\n    \n\n\n\n\n    \n\n\n    if (my_rank == master_rank) {\n        for (p = 0; p < nprocs; p++) {\n            for (step = 0; step < n_wait_steps; step++) {\n                printf(\"%14.9f %3d %14.9f\\n\", step * wait_time_s, p,\n                \n\n                        all_frequencies[p * n_wait_steps + step]);\n            }\n        }\n    }\n\n\n\n    free(frequencies);\n    if (my_rank == master_rank) {\n        free(all_frequencies);\n\n\n\n    }\n\n    reprompib_free_parameters(&opts);\n\n    return 0;\n}", "label": "int main(int argc, char* argv[]) {\n    int my_rank, nprocs, p;\n    int i;\n    reprompib_st_opts_t opts;\n    int master_rank;\n    MPI_Status stat;\n\n    double *all_frequencies;\n    double *all_wtime_times;\n    double *frequencies;\n    double *wtime_times;\n    double time_msg[2];\n\n    int step;\n    int n_wait_steps = 11;\n    double wait_time_s = 1;\n\n    \n\n    MPI_Init(&argc, &argv);\n    init_globals();\n    master_rank = get_master_rank();\n\n    parse_test_options(&opts, argc, argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    n_wait_steps = opts.n_rep + 1;\n\n\n\n    frequencies = (double*) calloc(n_wait_steps, sizeof(double));\n\n    if (my_rank == master_rank) {\n\n\n        all_frequencies = (double*) calloc(nprocs * n_wait_steps,\n                sizeof(double));\n    }\n\n    for (step = 0; step < n_wait_steps; step++) {\n        uint64_t timerfreq = 0;\n        HRT_INIT(0 \n, timerfreq);\n\n        frequencies[step] = (double) timerfreq;\n\n\n\n\n\n        struct timespec sleep_time;\n        sleep_time.tv_sec = 0;\n        sleep_time.tv_nsec = wait_time_s * 1e9;\n\n        nanosleep(&sleep_time, NULL);\n    }\n\n    if (my_rank == master_rank) {\n        printf(\"wait_time_s p freq\\n\");\n    }\n\n    \n\n    MPI_Gather(frequencies, n_wait_steps, MPI_DOUBLE, all_frequencies,\n            n_wait_steps, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\n\n    \n\n\n    if (my_rank == master_rank) {\n        for (p = 0; p < nprocs; p++) {\n            for (step = 0; step < n_wait_steps; step++) {\n                printf(\"%14.9f %3d %14.9f\\n\", step * wait_time_s, p,\n                \n\n                        all_frequencies[p * n_wait_steps + step]);\n            }\n        }\n    }\n\n\n\n    free(frequencies);\n    if (my_rank == master_rank) {\n        free(all_frequencies);\n\n\n\n    }\n\n    MPI_Finalize();\n    reprompib_free_parameters(&opts);\n\n    return 0;\n}"}
{"program": "joao-lima_767", "code": "int main(int argc, char **argv)\n{\n\tint rank, size;\n\tint n;\n\tint ret;\n\tunsigned A[N];\n\tunsigned X[N];\n\tstarpu_data_handle_t data_A[N];\n\tstarpu_data_handle_t data_X[N];\n\n\n\tif (size < 3)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 3 processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tA[n] = (n+1)*10;\n\t\tX[n] = n+1;\n\t}\n\n\tFPRINTF_MPI(stderr, \"A = \");\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tFPRINTF(stderr, \"%u \", A[n]);\n\t}\n\tFPRINTF(stderr, \"\\n\");\n\tFPRINTF_MPI(stderr, \"X = \");\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tFPRINTF(stderr, \"%u \", X[n]);\n\t}\n\tFPRINTF(stderr, \"\\n\");\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tif (rank == n%2)\n\t\t\tstarpu_variable_data_register(&data_A[n], STARPU_MAIN_RAM, (uintptr_t)&A[n], sizeof(unsigned));\n\t\telse\n\t\t\tstarpu_variable_data_register(&data_A[n], -1, (uintptr_t)NULL, sizeof(unsigned));\n\t\tstarpu_mpi_data_register(data_A[n], n+100, n%2);\n\t\tFPRINTF_MPI(stderr, \"Registering A[%d] to %p with tag %d and node %d\\n\", n,data_A[n], n+100, n%2);\n\t}\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tif (rank == 2)\n\t\t\tstarpu_variable_data_register(&data_X[n], STARPU_MAIN_RAM, (uintptr_t)&X[n], sizeof(unsigned));\n\t\telse\n\t\t\tstarpu_variable_data_register(&data_X[n], -1, (uintptr_t)NULL, sizeof(unsigned));\n\t\tstarpu_mpi_data_register(data_X[n], n+200, 2);\n\t\tFPRINTF_MPI(stderr, \"Registering X[%d] to %p with tag %d and node %d\\n\", n, data_X[n], n+200, 2);\n\t}\n\n\tfor(n = 0; n < N-1; n++)\n\t{\n\t\tret = starpu_mpi_task_insert(MPI_COMM_WORLD, &mycodelet,\n\t\t\t\t\t     STARPU_R, data_A[n],\n\t\t\t\t\t     STARPU_R, data_X[n],\n\t\t\t\t\t     STARPU_RW, data_X[N-1],\n\t\t\t\t\t     STARPU_EXECUTE_ON_DATA, data_A[n],\n\t\t\t\t\t     0);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_task_insert\");\n\t}\n\n\tFPRINTF(stderr, \"Waiting ...\\n\");\n\tstarpu_task_wait_for_all();\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tstarpu_data_unregister(data_A[n]);\n\t\tstarpu_data_unregister(data_X[n]);\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tFPRINTF(stdout, \"[%d] X[%d]=%u\\n\", rank, N-1, X[N-1]);\n\n\tif (rank == 2)\n\t{\n\t\tSTARPU_ASSERT_MSG(X[N-1]==144, \"Error when calculating X[N-1]=%u\\n\", X[N-1]);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tint rank, size;\n\tint n;\n\tint ret;\n\tunsigned A[N];\n\tunsigned X[N];\n\tstarpu_data_handle_t data_A[N];\n\tstarpu_data_handle_t data_X[N];\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 3)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 3 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tA[n] = (n+1)*10;\n\t\tX[n] = n+1;\n\t}\n\n\tFPRINTF_MPI(stderr, \"A = \");\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tFPRINTF(stderr, \"%u \", A[n]);\n\t}\n\tFPRINTF(stderr, \"\\n\");\n\tFPRINTF_MPI(stderr, \"X = \");\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tFPRINTF(stderr, \"%u \", X[n]);\n\t}\n\tFPRINTF(stderr, \"\\n\");\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tif (rank == n%2)\n\t\t\tstarpu_variable_data_register(&data_A[n], STARPU_MAIN_RAM, (uintptr_t)&A[n], sizeof(unsigned));\n\t\telse\n\t\t\tstarpu_variable_data_register(&data_A[n], -1, (uintptr_t)NULL, sizeof(unsigned));\n\t\tstarpu_mpi_data_register(data_A[n], n+100, n%2);\n\t\tFPRINTF_MPI(stderr, \"Registering A[%d] to %p with tag %d and node %d\\n\", n,data_A[n], n+100, n%2);\n\t}\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tif (rank == 2)\n\t\t\tstarpu_variable_data_register(&data_X[n], STARPU_MAIN_RAM, (uintptr_t)&X[n], sizeof(unsigned));\n\t\telse\n\t\t\tstarpu_variable_data_register(&data_X[n], -1, (uintptr_t)NULL, sizeof(unsigned));\n\t\tstarpu_mpi_data_register(data_X[n], n+200, 2);\n\t\tFPRINTF_MPI(stderr, \"Registering X[%d] to %p with tag %d and node %d\\n\", n, data_X[n], n+200, 2);\n\t}\n\n\tfor(n = 0; n < N-1; n++)\n\t{\n\t\tret = starpu_mpi_task_insert(MPI_COMM_WORLD, &mycodelet,\n\t\t\t\t\t     STARPU_R, data_A[n],\n\t\t\t\t\t     STARPU_R, data_X[n],\n\t\t\t\t\t     STARPU_RW, data_X[N-1],\n\t\t\t\t\t     STARPU_EXECUTE_ON_DATA, data_A[n],\n\t\t\t\t\t     0);\n\t\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_task_insert\");\n\t}\n\n\tFPRINTF(stderr, \"Waiting ...\\n\");\n\tstarpu_task_wait_for_all();\n\n\tfor(n = 0; n < N; n++)\n\t{\n\t\tstarpu_data_unregister(data_A[n]);\n\t\tstarpu_data_unregister(data_X[n]);\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tFPRINTF(stdout, \"[%d] X[%d]=%u\\n\", rank, N-1, X[N-1]);\n\n\tif (rank == 2)\n\t{\n\t\tSTARPU_ASSERT_MSG(X[N-1]==144, \"Error when calculating X[N-1]=%u\\n\", X[N-1]);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "syftalent_768", "code": "int main(int argc, char **argv)\n{\n    int rank, size, err, errclass;\n    char buf[100000];\n    MPI_Request request;\n\n    if (size < 2) {\n        fprintf( stderr, \"Must run with at least 2 processes\\n\" );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =\n        if (err)\n            fprintf(stderr, \"MPI_Isend returned error\\n\");\n\n        err =\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n        if ((err) && (errclass != MPIX_ERR_PROC_FAILED)) {\n            fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        } else {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        }\n#else\n        printf(\" No Errors\\n\");\n        fflush(stdout);\n#endif\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, err, errclass;\n    char buf[100000];\n    MPI_Request request;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size < 2) {\n        fprintf( stderr, \"Must run with at least 2 processes\\n\" );\n        MPI_Abort( MPI_COMM_WORLD, 1 );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n        err = MPI_Isend(buf, 100000, MPI_CHAR, 1, 0, MPI_COMM_WORLD, &request);\n        if (err)\n            fprintf(stderr, \"MPI_Isend returned error\\n\");\n\n        err = MPI_Wait(&request, MPI_STATUS_IGNORE);\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n        MPI_Error_class(err, &errclass);\n        if ((err) && (errclass != MPIX_ERR_PROC_FAILED)) {\n            fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        } else {\n            printf(\" No Errors\\n\");\n            fflush(stdout);\n        }\n#else\n        printf(\" No Errors\\n\");\n        fflush(stdout);\n#endif\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ghisvail_769", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "joeladams_777", "code": "int main(int argc, char** argv) {\n    int id = -1, numProcesses = -1, length = -1;\n    char myHostName[MPI_MAX_PROCESSOR_NAME] = {'\\0'};\n\n\n    sendReceivePrint(id, numProcesses, myHostName, \"BEFORE\");\n\n\n\n\n    sendReceivePrint(id, numProcesses, myHostName, \"AFTER\");\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int id = -1, numProcesses = -1, length = -1;\n    char myHostName[MPI_MAX_PROCESSOR_NAME] = {'\\0'};\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n    MPI_Get_processor_name (myHostName, &length);\n\n    sendReceivePrint(id, numProcesses, myHostName, \"BEFORE\");\n\n\n\n\n    sendReceivePrint(id, numProcesses, myHostName, \"AFTER\");\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "joao-lima_781", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\tstarpu_data_handle_t handle;\n\tint var;\n\n\n\tif (size<3)\n\t{\n\t\tFPRINTF(stderr, \"We need more than 2 processes.\\n\");\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tif (rank == 0)\n\t{\n\t\tint n;\n\t\tfor(n=1 ; n<size ; n++)\n\t\t{\n\t\t\tMPI_Status status;\n\n\t\t\tFPRINTF_MPI(stderr, \"receiving from node %d\\n\", n);\n\t\t\tstarpu_variable_data_register(&handle, STARPU_MAIN_RAM, (uintptr_t)&var, sizeof(var));\n\t\t\tstarpu_mpi_recv(handle, n, 42, MPI_COMM_WORLD, &status);\n\t\t\tstarpu_data_acquire(handle, STARPU_R);\n\t\t\tSTARPU_ASSERT_MSG(var == n, \"Received incorrect value <%d> from node <%d>\\n\", var, n);\n\t\t\tFPRINTF_MPI(stderr, \"received <%d> from node %d\\n\", var, n);\n\t\t\tstarpu_data_release(handle);\n\t\t\tstarpu_data_unregister(handle);\n\t\t}\n\t}\n\telse\n\t{\n\t\tFPRINTF_MPI(stderr, \"sending to node %d\\n\", 0);\n\t\tvar = rank;\n\t\tstarpu_variable_data_register(&handle, STARPU_MAIN_RAM, (uintptr_t)&var, sizeof(var));\n\t\tstarpu_mpi_send(handle, 0, 42, MPI_COMM_WORLD);\n\t\tstarpu_data_unregister(handle);\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\tstarpu_data_handle_t handle;\n\tint var;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size<3)\n\t{\n\t\tFPRINTF(stderr, \"We need more than 2 processes.\\n\");\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tif (rank == 0)\n\t{\n\t\tint n;\n\t\tfor(n=1 ; n<size ; n++)\n\t\t{\n\t\t\tMPI_Status status;\n\n\t\t\tFPRINTF_MPI(stderr, \"receiving from node %d\\n\", n);\n\t\t\tstarpu_variable_data_register(&handle, STARPU_MAIN_RAM, (uintptr_t)&var, sizeof(var));\n\t\t\tstarpu_mpi_recv(handle, n, 42, MPI_COMM_WORLD, &status);\n\t\t\tstarpu_data_acquire(handle, STARPU_R);\n\t\t\tSTARPU_ASSERT_MSG(var == n, \"Received incorrect value <%d> from node <%d>\\n\", var, n);\n\t\t\tFPRINTF_MPI(stderr, \"received <%d> from node %d\\n\", var, n);\n\t\t\tstarpu_data_release(handle);\n\t\t\tstarpu_data_unregister(handle);\n\t\t}\n\t}\n\telse\n\t{\n\t\tFPRINTF_MPI(stderr, \"sending to node %d\\n\", 0);\n\t\tvar = rank;\n\t\tstarpu_variable_data_register(&handle, STARPU_MAIN_RAM, (uintptr_t)&var, sizeof(var));\n\t\tstarpu_mpi_send(handle, 0, 42, MPI_COMM_WORLD);\n\t\tstarpu_data_unregister(handle);\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "alucas_782", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\tstarpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(unsigned));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tunsigned last_loop = nloops - 1;\n\tunsigned last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (!((loop == 0) && (rank == 0)))\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(token_handle, (rank+size-1)%size, tag, MPI_COMM_WORLD, &status);\n\t\t}\n\t\telse {\n\t\t\ttoken = 0;\n\t\t\tfprintf(stdout, \"Start with token value %d\\n\", token);\n\t\t}\n\n\t\tincrement_token();\n\t\t\n\t\tif (!((loop == last_loop) && (rank == last_rank)))\n\t\t{\n\t\t\tstarpu_mpi_send(token_handle, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tfprintf(stdout, \"Finished : token value %d\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\tstarpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(unsigned));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tunsigned last_loop = nloops - 1;\n\tunsigned last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (!((loop == 0) && (rank == 0)))\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(token_handle, (rank+size-1)%size, tag, MPI_COMM_WORLD, &status);\n\t\t}\n\t\telse {\n\t\t\ttoken = 0;\n\t\t\tfprintf(stdout, \"Start with token value %d\\n\", token);\n\t\t}\n\n\t\tincrement_token();\n\t\t\n\t\tif (!((loop == last_loop) && (rank == last_rank)))\n\t\t{\n\t\t\tstarpu_mpi_send(token_handle, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tfprintf(stdout, \"Finished : token value %d\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n"}
{"program": "KWARC_784", "code": "int main(int argc, char *argv[]) {\n\tint myrank;\n\n\tprintf(\"%2d - started\\n\", myrank);\n\n\tif (myrank == 0) {   \n\n\t\tint number_of_processes = ITEMS_TO_BE_SENT;\n\t\tITEMS_TO_BE_SENT--;    \n\n\t\tif(argc == 1)\n\t\t\tftw(\".\", parse, 1);\n\t\telse\n\t\t\tftw(argv[1], parse, 1);\n\t\tkill_slaves();\n\t\tsleep(1);  \n\n\t\tmerge_files(number_of_processes);\n\t\tfprintf(stderr, \"Finished merging\\n\");\n\t} else {\n\t\trun_slave(myrank);\n\t}\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tint myrank;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\tprintf(\"%2d - started\\n\", myrank);\n\n\tif (myrank == 0) {   \n\n\t\tMPI_Comm_size(MPI_COMM_WORLD, &ITEMS_TO_BE_SENT);\n\t\tint number_of_processes = ITEMS_TO_BE_SENT;\n\t\tITEMS_TO_BE_SENT--;    \n\n\t\tif(argc == 1)\n\t\t\tftw(\".\", parse, 1);\n\t\telse\n\t\t\tftw(argv[1], parse, 1);\n\t\tkill_slaves();\n\t\tsleep(1);  \n\n\t\tmerge_files(number_of_processes);\n\t\tfprintf(stderr, \"Finished merging\\n\");\n\t} else {\n\t\trun_slave(myrank);\n\t}\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "ursache_786", "code": "int main(int argc, char** argv) \n{\n\tint dim_n = DIM_N;\n\tint dim_m = DIM_M;\n\tint dim_k = DIM_K;\n\t\n\n\tif (argc == 4)\n\t{\n\t\tdim_n = atoi(argv[1]);\n\t\tdim_m = atoi(argv[2]);\n\t\tdim_k = atoi(argv[3]);\n\t}\n\tint local_k;\n\t\n\n#ifdef MPI\n\tint num_tasks;\n\tint my_rank;\n\tlocal_k = dim_k/num_tasks; \n#endif\n\t\n\n\tdouble* restrict storage1 = (double*)malloc(dim_n*dim_m*dim_k*sizeof(double));\n\tdouble* restrict storage2 = (double*)malloc(dim_n*dim_m*dim_k*sizeof(double));\n\n\tprintf(\"3x3 stencil: M=%d N=%d K=%d global.\\n\", dim_n, dim_m, dim_k);\n\n\tprintf(\"array size = %f MB\\n\", (double) dim_n*dim_m*dim_k*sizeof(double)/1024/1024);\n\n\tdouble alloctime = -myseconds();\n\tinit (storage1, dim_n, dim_m, dim_k);\n\tinit (storage2, dim_n, dim_m, dim_k);\n\talloctime += myseconds();\n\tprintf(\"Allocation time = %f s.\\n\\n\", alloctime);\n\n\tint n_iters = 0;\n\tint nrep    = NREP;\n\tint ii;\n\n\tdouble * st_read  = storage2;\n\tdouble * st_write = storage1;\n\n\tdouble phi;\n\tdouble norminf, norml2;\n\tdouble time = - myseconds();\n\n\t\n\n\tdo \n\t{\n\t\t\n\n\t\tdouble *tmp = st_read;\n\t\tst_read     = st_write;\n\t\tst_write    = tmp;\n\t\t\n\n\t\t++n_iters;\n\t\t\n\n\t\tdouble ftime = -myseconds();\n\t\t\n\n\t\tunsigned long long c0 = cycles();\n\t\t\n\n\t\tfor (ii = 0; ii < nrep; ++ii)\n\t\t{\n\t\t\tlaplacian(st_read, st_write, dim_m, dim_n, dim_k);\n\t\t}\n\t\t\n\n\t\tunsigned long long c1 = cycles();\n\t\t\n\n\t\tunsigned long long cycles = (c1 - c0)/nrep;\n\t\t\n\n\t\t\n\n\t\tftime += myseconds();\n\t\tftime /= nrep;\n\t\t\n\n\t\t\n\n\t\t\n\n\t\tnorminf = maxNorm(st_write, st_read, dim_n*dim_m*dim_k);\n\t\tnorml2   = l2Norm(st_write, st_read, dim_n*dim_m*dim_k);\n\t\tdouble flops = (dim_m - 2)*(dim_n - 2)*(dim_k - 2)*6.;\n\t\t\n\n\t\tprintf(\"iter = %d, %f s. %ld c., linf = %g l2 = %g, %d flops, %ld c, %f F/c, %f GF/s\\n\", n_iters, ftime, cycles, norminf, norml2, (long long int) flops, flops/cycles, (double) flops/ftime/1e9); \n\t} while (norminf > EPSI);\n\ttime += myseconds();\n\t\n\n\tprintf(\"\\n# iter= %d time= %g residual= %g\\n\", n_iters, time, norml2); \n\t\n\n\tfree(storage1);\n\tfree(storage2);\n\t\n\n#ifdef MPI\n#endif\n\treturn 0;\n}", "label": "int main(int argc, char** argv) \n{\n\tint dim_n = DIM_N;\n\tint dim_m = DIM_M;\n\tint dim_k = DIM_K;\n\t\n\n\tif (argc == 4)\n\t{\n\t\tdim_n = atoi(argv[1]);\n\t\tdim_m = atoi(argv[2]);\n\t\tdim_k = atoi(argv[3]);\n\t}\n\tint local_k;\n\t\n\n#ifdef MPI\n\tMPI_Init(&argc, &argv);\n\tint num_tasks;\n\tint my_rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_tasks);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\tlocal_k = dim_k/num_tasks; \n#endif\n\t\n\n\tdouble* restrict storage1 = (double*)malloc(dim_n*dim_m*dim_k*sizeof(double));\n\tdouble* restrict storage2 = (double*)malloc(dim_n*dim_m*dim_k*sizeof(double));\n\n\tprintf(\"3x3 stencil: M=%d N=%d K=%d global.\\n\", dim_n, dim_m, dim_k);\n\n\tprintf(\"array size = %f MB\\n\", (double) dim_n*dim_m*dim_k*sizeof(double)/1024/1024);\n\n\tdouble alloctime = -myseconds();\n\tinit (storage1, dim_n, dim_m, dim_k);\n\tinit (storage2, dim_n, dim_m, dim_k);\n\talloctime += myseconds();\n\tprintf(\"Allocation time = %f s.\\n\\n\", alloctime);\n\n\tint n_iters = 0;\n\tint nrep    = NREP;\n\tint ii;\n\n\tdouble * st_read  = storage2;\n\tdouble * st_write = storage1;\n\n\tdouble phi;\n\tdouble norminf, norml2;\n\tdouble time = - myseconds();\n\n\t\n\n\tdo \n\t{\n\t\t\n\n\t\tdouble *tmp = st_read;\n\t\tst_read     = st_write;\n\t\tst_write    = tmp;\n\t\t\n\n\t\t++n_iters;\n\t\t\n\n\t\tdouble ftime = -myseconds();\n\t\t\n\n\t\tunsigned long long c0 = cycles();\n\t\t\n\n\t\tfor (ii = 0; ii < nrep; ++ii)\n\t\t{\n\t\t\tlaplacian(st_read, st_write, dim_m, dim_n, dim_k);\n\t\t}\n\t\t\n\n\t\tunsigned long long c1 = cycles();\n\t\t\n\n\t\tunsigned long long cycles = (c1 - c0)/nrep;\n\t\t\n\n\t\t\n\n\t\tftime += myseconds();\n\t\tftime /= nrep;\n\t\t\n\n\t\t\n\n\t\t\n\n\t\tnorminf = maxNorm(st_write, st_read, dim_n*dim_m*dim_k);\n\t\tnorml2   = l2Norm(st_write, st_read, dim_n*dim_m*dim_k);\n\t\tdouble flops = (dim_m - 2)*(dim_n - 2)*(dim_k - 2)*6.;\n\t\t\n\n\t\tprintf(\"iter = %d, %f s. %ld c., linf = %g l2 = %g, %d flops, %ld c, %f F/c, %f GF/s\\n\", n_iters, ftime, cycles, norminf, norml2, (long long int) flops, flops/cycles, (double) flops/ftime/1e9); \n\t} while (norminf > EPSI);\n\ttime += myseconds();\n\t\n\n\tprintf(\"\\n# iter= %d time= %g residual= %g\\n\", n_iters, time, norml2); \n\t\n\n\tfree(storage1);\n\tfree(storage2);\n\t\n\n#ifdef MPI\n\tMPI_Finalize();\n#endif\n\treturn 0;\n}"}
{"program": "d-meiser_787", "code": "int main(int argn, char **argv) {\n#ifdef BL_WITH_MPI\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  double nbar = 1.0e2;\n  int maxNumPtcls = 100 * nbar;\n  int internalStateSize = 2;\n  double complex initialState[2];\n  initialState[0] = 0;\n  initialState[1] = 1;\n\n  struct BLSimulationState simulationState;\n  blEnsembleCreate(maxNumPtcls, internalStateSize,\n      &simulationState.ensemble);\n  simulationState.ensemble.ptclWeight = 100.0;\n  simulationState.fieldState.q = 1.0;\n  simulationState.fieldState.p = 0.0;\n\n  double lambda = 1.0e-6;\n  struct BlBox simulationBox =\n    {-0.5 * lambda, 0.5 * lambda, 1.0e-4, 1.0e-4, 1.0e-4, 1.0e-4};\n  double vbar[3] = {0};\n  double deltaV[3] = {0};\n  struct BLParticleSource* src =\n    blParticleSourceUniformCreate(simulationBox, 1.0e4, vbar, deltaV,\n                                  internalStateSize, initialState, 0);\n\n  int n = blParticleSourceGetNumParticles(src);\n  blEnsembleCreateSpace(n, &simulationState.ensemble);\n  blParticleSourceCreateParticles(src,\n      simulationState.ensemble.x,\n      simulationState.ensemble.y,\n      simulationState.ensemble.z,\n      simulationState.ensemble.vx,\n      simulationState.ensemble.vy,\n      simulationState.ensemble.vz,\n      internalStateSize,\n      simulationState.ensemble.internalState);\n\n  double dipoleMatrixElement = 1.0e-31;\n  struct BLDipoleOperator *dipoleOperator =\n    blDipoleOperatorTLACreate(dipoleMatrixElement);\n\n  double waist = 1.0e-4;\n  double length = 1.0e-2;\n  struct BLModeFunction *modeFunction =\n    blModeFunctionSimplifiedGaussianCreate(waist, lambda, length);\n\n  double veff = waist * waist * length;\n  double omega =\n    sqrt(2.0 * M_PI * SPEED_OF_LIGHT /\n        (lambda * 2.0 * EPSILON_0 * veff * H_BAR)) *\n    dipoleMatrixElement;\n\n  struct BLUpdate *atomFieldInteraction = blAtomFieldInteractionCreate(\n      simulationState.ensemble.maxNumPtcls,\n      simulationState.ensemble.internalStateSize,\n      dipoleOperator, modeFunction);\n\n  double dt = 1.0e-3 / omega;\n  printf(\"# dt == %le\\n\", dt);\n  int i;\n  int dumpPeriodicity = 10;\n  struct BLDiagnostics *diagnostics =\n    blDiagnosticsFieldStateCreate(dumpPeriodicity, 0);\n  diagnostics = blDiagnosticsInternalStateCreate(dumpPeriodicity,\n                                                 \"internal_state\", diagnostics);\n  int numSteps = 100;\n  for (i = 0; i < numSteps; ++i) {\n    blDiagnosticsProcess(diagnostics, i, &simulationState);\n    blUpdateTakeStep(atomFieldInteraction, i * dt, dt, &simulationState);\n  }\n\n  blUpdateDestroy(atomFieldInteraction);\n  blDiagnosticsDestroy(diagnostics);\n  blModeFunctionDestroy(modeFunction);\n  blDipoleOperatorDestroy(dipoleOperator);\n  blEnsembleDestroy(&simulationState.ensemble);\n  blParticleSourceDestroy(src);\n\n#ifdef BL_WITH_MPI\n#endif\n  return BL_SUCCESS;\n}", "label": "int main(int argn, char **argv) {\n#ifdef BL_WITH_MPI\n  MPI_Init(&argn, &argv);\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n  double nbar = 1.0e2;\n  int maxNumPtcls = 100 * nbar;\n  int internalStateSize = 2;\n  double complex initialState[2];\n  initialState[0] = 0;\n  initialState[1] = 1;\n\n  struct BLSimulationState simulationState;\n  blEnsembleCreate(maxNumPtcls, internalStateSize,\n      &simulationState.ensemble);\n  simulationState.ensemble.ptclWeight = 100.0;\n  simulationState.fieldState.q = 1.0;\n  simulationState.fieldState.p = 0.0;\n\n  double lambda = 1.0e-6;\n  struct BlBox simulationBox =\n    {-0.5 * lambda, 0.5 * lambda, 1.0e-4, 1.0e-4, 1.0e-4, 1.0e-4};\n  double vbar[3] = {0};\n  double deltaV[3] = {0};\n  struct BLParticleSource* src =\n    blParticleSourceUniformCreate(simulationBox, 1.0e4, vbar, deltaV,\n                                  internalStateSize, initialState, 0);\n\n  int n = blParticleSourceGetNumParticles(src);\n  blEnsembleCreateSpace(n, &simulationState.ensemble);\n  blParticleSourceCreateParticles(src,\n      simulationState.ensemble.x,\n      simulationState.ensemble.y,\n      simulationState.ensemble.z,\n      simulationState.ensemble.vx,\n      simulationState.ensemble.vy,\n      simulationState.ensemble.vz,\n      internalStateSize,\n      simulationState.ensemble.internalState);\n\n  double dipoleMatrixElement = 1.0e-31;\n  struct BLDipoleOperator *dipoleOperator =\n    blDipoleOperatorTLACreate(dipoleMatrixElement);\n\n  double waist = 1.0e-4;\n  double length = 1.0e-2;\n  struct BLModeFunction *modeFunction =\n    blModeFunctionSimplifiedGaussianCreate(waist, lambda, length);\n\n  double veff = waist * waist * length;\n  double omega =\n    sqrt(2.0 * M_PI * SPEED_OF_LIGHT /\n        (lambda * 2.0 * EPSILON_0 * veff * H_BAR)) *\n    dipoleMatrixElement;\n\n  struct BLUpdate *atomFieldInteraction = blAtomFieldInteractionCreate(\n      simulationState.ensemble.maxNumPtcls,\n      simulationState.ensemble.internalStateSize,\n      dipoleOperator, modeFunction);\n\n  double dt = 1.0e-3 / omega;\n  printf(\"# dt == %le\\n\", dt);\n  int i;\n  int dumpPeriodicity = 10;\n  struct BLDiagnostics *diagnostics =\n    blDiagnosticsFieldStateCreate(dumpPeriodicity, 0);\n  diagnostics = blDiagnosticsInternalStateCreate(dumpPeriodicity,\n                                                 \"internal_state\", diagnostics);\n  int numSteps = 100;\n  for (i = 0; i < numSteps; ++i) {\n    blDiagnosticsProcess(diagnostics, i, &simulationState);\n    blUpdateTakeStep(atomFieldInteraction, i * dt, dt, &simulationState);\n  }\n\n  blUpdateDestroy(atomFieldInteraction);\n  blDiagnosticsDestroy(diagnostics);\n  blModeFunctionDestroy(modeFunction);\n  blDipoleOperatorDestroy(dipoleOperator);\n  blEnsembleDestroy(&simulationState.ensemble);\n  blParticleSourceDestroy(src);\n\n#ifdef BL_WITH_MPI\n  MPI_Finalize();\n#endif\n  return BL_SUCCESS;\n}"}
{"program": "bsc-performance-tools_790", "code": "int main(int argc, char *argv[])\n{\n\tint v;\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint v;\n\tMPI_Init (&argc, &argv);\n\tMPI_Bcast (&v, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "tcsiwula_791", "code": "int main(int argc, char** argv) {\n    int         my_rank;   \n\n    int         p;         \n\n    double      a;         \n\n    double      b;         \n\n    int         n;         \n\n    double      h;         \n\n    double      local_a;   \n\n    double      local_b;   \n\n    int         local_n;   \n\n                           \n\n    double      area;      \n\n    double      total = 0; \n\n    int         source;    \n\n    int         dest = 0;  \n\n    int         tag = 0;\n    MPI_Status  status;\n    double      start, finish, par_elapsed;\n\n    \n\n\n    \n\n\n    \n\n\n    Get_data(p, my_rank, &a, &b, &n);\n\n    start =\n    h = (b-a)/n;    \n\n    local_n = n/p;  \n\n\n    \n\n    local_a = a + my_rank*local_n*h;\n    local_b = local_a + local_n*h;\n    area = Trap(local_a, local_b, local_n, h);\n\n    \n\n    if (my_rank == 0) {\n        total = area;\n        for (source = 1; source < p; source++) {\n            total = total + area;\n        }\n    } else {  \n    }\n    finish =\n\n    par_elapsed = finish - start;\n\n    par_elapsed = Get_max_time(par_elapsed, my_rank, p);\n\n    \n\n    if (my_rank == 0) {\n        printf(\"With n = %d trapezoids, our estimate\\n\",\n            n);\n        printf(\"of the area from %f to %f = %23.16e\\n\",\n            a, b, total);\n        printf(\"Parallel elapsed time = %e seconds\\n\", par_elapsed);\n    }\n\n    \n\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int         my_rank;   \n\n    int         p;         \n\n    double      a;         \n\n    double      b;         \n\n    int         n;         \n\n    double      h;         \n\n    double      local_a;   \n\n    double      local_b;   \n\n    int         local_n;   \n\n                           \n\n    double      area;      \n\n    double      total = 0; \n\n    int         source;    \n\n    int         dest = 0;  \n\n    int         tag = 0;\n    MPI_Status  status;\n    double      start, finish, par_elapsed;\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    Get_data(p, my_rank, &a, &b, &n);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    start = MPI_Wtime();\n    h = (b-a)/n;    \n\n    local_n = n/p;  \n\n\n    \n\n    local_a = a + my_rank*local_n*h;\n    local_b = local_a + local_n*h;\n    area = Trap(local_a, local_b, local_n, h);\n\n    \n\n    if (my_rank == 0) {\n        total = area;\n        for (source = 1; source < p; source++) {\n            MPI_Recv(&area, 1, MPI_DOUBLE, source, tag,\n                MPI_COMM_WORLD, &status);\n            total = total + area;\n        }\n    } else {  \n        MPI_Send(&area, 1, MPI_DOUBLE, dest,\n            tag, MPI_COMM_WORLD);\n    }\n    finish = MPI_Wtime();\n\n    par_elapsed = finish - start;\n\n    par_elapsed = Get_max_time(par_elapsed, my_rank, p);\n\n    \n\n    if (my_rank == 0) {\n        printf(\"With n = %d trapezoids, our estimate\\n\",\n            n);\n        printf(\"of the area from %f to %f = %23.16e\\n\",\n            a, b, total);\n        printf(\"Parallel elapsed time = %e seconds\\n\", par_elapsed);\n    }\n\n    \n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "auag92_792", "code": "void main(int argc, char *argv[]) {\n  numworkers = numtasks - 1;\n\n  allocate_rows();\n  allocate_memory();\n\n  char fname1[100], fname2[100];\n  int  m = sprintf(fname1,\"velocities\");\n  int  n = sprintf(fname2,\"rho\");\n\n  if (taskid == MASTER){\n    printf(\"Hi, I am the Master node.\\n\");\n    for (t = 1; t <= tsteps; t++) {\n      if(t%savet == 0){\n        receivefrmworker(u, Mx);\n        receivefrmworker(v, Mx);\n        printf(\"%d\\n\",t);\n        write2file1(t,Mx,u,v,fname1);\n        \n\n      }\n    }\n  }else {\n    printf(\"Hi, I am worker node no. %d, with no. of rows = %d. start = %d, end = %d\\n\", taskid, rows, start, end);\n    init();\n    for (t = 1; t <= tsteps; t++) {\n      collision_step();\n      streaming_step();\n      \n\n      \n\n      calculate_rho();\n      calculate_velocities();\n      if(t%savet == 0){\n        sendtomaster(taskid, u, Mx);\n        sendtomaster(taskid, v, Mx);\n      }\n    }\n  }\n  free_memory();\n}", "label": "void main(int argc, char *argv[]) {\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n  MPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n  numworkers = numtasks - 1;\n\n  allocate_rows();\n  allocate_memory();\n\n  char fname1[100], fname2[100];\n  int  m = sprintf(fname1,\"velocities\");\n  int  n = sprintf(fname2,\"rho\");\n\n  if (taskid == MASTER){\n    printf(\"Hi, I am the Master node.\\n\");\n    for (t = 1; t <= tsteps; t++) {\n      if(t%savet == 0){\n        receivefrmworker(u, Mx);\n        receivefrmworker(v, Mx);\n        printf(\"%d\\n\",t);\n        write2file1(t,Mx,u,v,fname1);\n        \n\n      }\n    }\n  }else {\n    printf(\"Hi, I am worker node no. %d, with no. of rows = %d. start = %d, end = %d\\n\", taskid, rows, start, end);\n    init();\n    for (t = 1; t <= tsteps; t++) {\n      collision_step();\n      streaming_step();\n      \n\n      \n\n      calculate_rho();\n      calculate_velocities();\n      if(t%savet == 0){\n        sendtomaster(taskid, u, Mx);\n        sendtomaster(taskid, v, Mx);\n      }\n    }\n  }\n  free_memory();\n  MPI_Finalize();\n}"}
{"program": "rahlk_794", "code": "main (int argc, char **argv)\n{\n  int nprocs = -1;\n  int rank = -1;\n  float data = 0.0;\n  int tag = 30;\n  char processor_name[128];\n  int namelen = 128;\n\n  \n\n  printf (\"Initializing (%d of %d)\\n\", rank, nprocs);\n  printf (\"(%d) is alive on %s\\n\", rank, processor_name);\n  fflush (stdout);\n  {\n    int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n    data = rank;\n    printf (\"(%d) sent data %f\\n\", rank, data);\n    fflush (stdout);\n  }\n  {\n    int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n    MPI_Status status;\n    printf (\"(%d) got data %f\\n\", rank, data);\n    fflush (stdout);\n  }\n  printf (\"(%d) Finished normally\\n\", rank);\n}", "label": "main (int argc, char **argv)\n{\n  int nprocs = -1;\n  int rank = -1;\n  float data = 0.0;\n  int tag = 30;\n  char processor_name[128];\n  int namelen = 128;\n\n  \n\n  MPI_Init (NULL, NULL);\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  printf (\"Initializing (%d of %d)\\n\", rank, nprocs);\n  MPI_Get_processor_name (processor_name, &namelen);\n  printf (\"(%d) is alive on %s\\n\", rank, processor_name);\n  fflush (stdout);\n  {\n    int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n    data = rank;\n    MPI_Send (&data, 1, MPI_FLOAT, dest, tag, MPI_COMM_WORLD);\n    printf (\"(%d) sent data %f\\n\", rank, data);\n    fflush (stdout);\n  }\n  {\n    int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n    MPI_Status status;\n    MPI_Recv (&data, 1, MPI_FLOAT, src, tag, MPI_COMM_WORLD, &status);\n    printf (\"(%d) got data %f\\n\", rank, data);\n    fflush (stdout);\n  }\n  MPI_Finalize ();\n  printf (\"(%d) Finished normally\\n\", rank);\n}"}
{"program": "haohaibo_798", "code": "int main(int argc, char * argv[])\n{\n    int rank, numtasks; \n    int sendcount, recvcount, source;\n    float sendbuf[SIZE][SIZE] = {\n        {1.0, 2.0, 3.0, 4.0},\n        {5.0, 6.0, 7.0, 8.0},\n        {9.0, 10.0, 11.0, 12.0},\n        {13.0, 14.0, 15.0, 16.0}\n    };\n    float recvbuf[SIZE];\n\n\n\n    if(numtasks == SIZE) {\n        source = 1;\n        sendcount = SIZE;\n        recvcount = SIZE;\n\n        printf(\"rank = %d Results: %f %f %f %f\\n\", rank, recvbuf[0], recvbuf[1], recvbuf[2], recvbuf[3]);\n    }else{\n        printf(\"Must specify %d processors. Terminating.\\n\",SIZE);\n    }\n\n\n    return 0;\n}\n\n", "label": "int main(int argc, char * argv[])\n{\n    int rank, numtasks; \n    int sendcount, recvcount, source;\n    float sendbuf[SIZE][SIZE] = {\n        {1.0, 2.0, 3.0, 4.0},\n        {5.0, 6.0, 7.0, 8.0},\n        {9.0, 10.0, 11.0, 12.0},\n        {13.0, 14.0, 15.0, 16.0}\n    };\n    float recvbuf[SIZE];\n\n    MPI_Init(&argc, &argv); \n\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(numtasks == SIZE) {\n        source = 1;\n        sendcount = SIZE;\n        recvcount = SIZE;\n        MPI_Scatter(sendbuf, sendcount, MPI_FLOAT, recvbuf, recvcount,\n                MPI_FLOAT, source, MPI_COMM_WORLD);\n\n        printf(\"rank = %d Results: %f %f %f %f\\n\", rank, recvbuf[0], recvbuf[1], recvbuf[2], recvbuf[3]);\n    }else{\n        printf(\"Must specify %d processors. Terminating.\\n\",SIZE);\n    }\n\n    MPI_Finalize(); \n\n    return 0;\n}\n\n"}
{"program": "ebaty_799", "code": "int main(int argc, char *argv[]) {\n\tcreateDataType();\n\n\tif ( argc != 4 ) {\n\t\tprintf(\"this program argument format: ./exe [FileHeader] [FileHooter] [ElementSize]\\n\");\n\t\treturn 1;\n\t}else {\n\t\tfileHeader = argv[1];\n\t\tfileHooter = argv[2];\n\t\tfileSize = atoi(argv[3]);\n\t}\n\n\tomp_set_num_threads(2);\n\tint arraySize = (fileSize / nodeSize) + ( fileSize % nodeSize > 0 ? 1 : 0 );\n\n\tmatter m[arraySize];\n\tinitMatter(nodeRank * arraySize, m, arraySize);\n\n\tmatter buf[2][arraySize];\n\n\tdouble time =\n\n\tint i, j, buf_j, k;\n\tREP(k, STEP) {\n\t\tprintf(\"STEP: %d, nodeRank: %d\\n\", k, nodeRank);\n\n\t\tREP(i, arraySize) buf[0][i] = m[i];\n\t\tREP(i, nodeSize) {\n\t\t\tint bufIndex = i & 1;\n\t\t\t\n\n\t\t\t\n\n\t\t\t\n\n\t\t\t\n\n\t\t\t\t\t\tif ( i < nodeSize-1 ) {\n\t\t\t\t\t\t\tMPI_Status status;\n\t\t\t\t\t\t\tint sendRank = (nodeRank - (i + 1) + nodeSize) % nodeSize;\n\t\t\t\t\t\t\tint recvRank = (nodeRank + (i + 1) ) % nodeSize;\n\t\t\t\t\t\t}\n\t\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\tdouble xx, yy, zz, r, rr, gm;\n\t\t\t\t\t#pragma omp parallel for private(xx, yy, zz, r, rr, gm, j, buf_j)\n\t\t\t\t\tREP(j, arraySize) REP(buf_j, arraySize) {\n\t\t\t\t\t\tif ( (i != 0) || (j != buf_j) ) {\n\t\t\t\t\t\t\txx = m[j].x - buf[bufIndex][buf_j].x; xx *= xx;\n\t\t\t\t\t\t\tyy = m[j].y - buf[bufIndex][buf_j].y; yy *= yy;\n\t\t\t\t\t\t\tzz = m[j].z - buf[bufIndex][buf_j].z; zz *= zz;\n\n\t\t\t\t\t\t\tr = 1.0 / sqrt(xx + yy + zz);\n\t\t\t\t\t\t\trr = r * r;\n\n\t\t\t\t\t\t\tgm = G * (m[j].m * rr);\n\t\t\t\t\t\t\tm[j].vx += gm * ((buf[bufIndex][buf_j].x - m[j].x) * r);\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tm[j].vy += gm * ((buf[bufIndex][buf_j].y - m[j].y) * r);\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tm[j].vz += gm * ((buf[bufIndex][buf_j].z - m[j].z) * r);\t\t\t\t\t\t\t\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\n\n\t\t\t\n\n\t\t}\n\t\tREP(i, arraySize) {\n\t\t\tm[i].x += m[i].vx * DT;\n\t\t\tm[i].y += m[i].vy * DT;\n\t\t\tm[i].z += m[i].vz * DT;\n\t\t}\n\t}\n\n\ttime = MPI_Wtime() - time;\n\t\n\tprintMatter(m, nodeRank, arraySize);\n\tprintf(\"%s, %s, %d\\n\", __FUNCTION__, __FILE__, __LINE__);\n\n\n\tif ( nodeRank == 0 ) printf(\"time: %lf\\n\", time);\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nodeSize);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &nodeRank);\n\tcreateDataType();\n\n\tif ( argc != 4 ) {\n\t\tprintf(\"this program argument format: ./exe [FileHeader] [FileHooter] [ElementSize]\\n\");\n\t\treturn 1;\n\t}else {\n\t\tfileHeader = argv[1];\n\t\tfileHooter = argv[2];\n\t\tfileSize = atoi(argv[3]);\n\t}\n\n\tomp_set_num_threads(2);\n\tint arraySize = (fileSize / nodeSize) + ( fileSize % nodeSize > 0 ? 1 : 0 );\n\n\tmatter m[arraySize];\n\tinitMatter(nodeRank * arraySize, m, arraySize);\n\n\tmatter buf[2][arraySize];\n\n\tdouble time = MPI_Wtime();\n\n\tint i, j, buf_j, k;\n\tREP(k, STEP) {\n\t\tprintf(\"STEP: %d, nodeRank: %d\\n\", k, nodeRank);\n\n\t\tREP(i, arraySize) buf[0][i] = m[i];\n\t\tREP(i, nodeSize) {\n\t\t\tint bufIndex = i & 1;\n\t\t\t\n\n\t\t\t\n\n\t\t\t\n\n\t\t\t\n\n\t\t\t\t\t\tif ( i < nodeSize-1 ) {\n\t\t\t\t\t\t\tMPI_Status status;\n\t\t\t\t\t\t\tint sendRank = (nodeRank - (i + 1) + nodeSize) % nodeSize;\n\t\t\t\t\t\t\tint recvRank = (nodeRank + (i + 1) ) % nodeSize;\n\t\t\t\t\t\t\tMPI_Sendrecv(\t\t\t\t\t\t\t\t\t m, arraySize, mpi_matter_type, sendRank, 0,\n\t\t\t\t\t\t\t\t\t\t\t\t\t buf[(bufIndex+1)&1], arraySize, mpi_matter_type, recvRank, 0, MPI_COMM_WORLD, &status);\n\t\t\t\t\t\t}\n\t\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\n\n\t\t\t\t\tdouble xx, yy, zz, r, rr, gm;\n\t\t\t\t\t#pragma omp parallel for private(xx, yy, zz, r, rr, gm, j, buf_j)\n\t\t\t\t\tREP(j, arraySize) REP(buf_j, arraySize) {\n\t\t\t\t\t\tif ( (i != 0) || (j != buf_j) ) {\n\t\t\t\t\t\t\txx = m[j].x - buf[bufIndex][buf_j].x; xx *= xx;\n\t\t\t\t\t\t\tyy = m[j].y - buf[bufIndex][buf_j].y; yy *= yy;\n\t\t\t\t\t\t\tzz = m[j].z - buf[bufIndex][buf_j].z; zz *= zz;\n\n\t\t\t\t\t\t\tr = 1.0 / sqrt(xx + yy + zz);\n\t\t\t\t\t\t\trr = r * r;\n\n\t\t\t\t\t\t\tgm = G * (m[j].m * rr);\n\t\t\t\t\t\t\tm[j].vx += gm * ((buf[bufIndex][buf_j].x - m[j].x) * r);\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tm[j].vy += gm * ((buf[bufIndex][buf_j].y - m[j].y) * r);\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tm[j].vz += gm * ((buf[bufIndex][buf_j].z - m[j].z) * r);\t\t\t\t\t\t\t\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\n\n\t\t\t\n\n\t\t}\n\t\tREP(i, arraySize) {\n\t\t\tm[i].x += m[i].vx * DT;\n\t\t\tm[i].y += m[i].vy * DT;\n\t\t\tm[i].z += m[i].vz * DT;\n\t\t}\n\t}\n\n\ttime = MPI_Wtime() - time;\n\t\n\tprintMatter(m, nodeRank, arraySize);\n\tprintf(\"%s, %s, %d\\n\", __FUNCTION__, __FILE__, __LINE__);\n\n\tMPI_Type_free(&mpi_matter_type);\n\tMPI_Finalize();\n\n\tif ( nodeRank == 0 ) printf(\"time: %lf\\n\", time);\n\n\treturn 0;\n}"}
{"program": "airqinc_800", "code": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n\n\n  int world_rank, world_size;\n\n  \n\n  float *rands = NULL;\n  if (world_rank == 0) {\n    rands = create_rands(ELEMENTS_PER_PROC * world_size, MAX_RANDOM_VAL, world_rank);\n  }\n\n  \n\n  float *buff_rands = (float *)malloc(sizeof(float) * ELEMENTS_PER_PROC);\n\n  \n\n  dumpData ( world_rank , world_size , ELEMENTS_PER_PROC , buff_rands , \" Data \" , 1);\n\n  \n\n  float sub_avg = calc_avg(buff_rands, ELEMENTS_PER_PROC);\n\n  \n\n  float *avgs = NULL;\n  \n\n  avgs = (float *)malloc(sizeof(float) * world_size);\n  \n\n\n  float avg = calc_avg(avgs, world_size);\n  printf(\" Process %d: Avg of all elements is %f\\n\", world_rank, avg);\n\n  if (world_rank == 0) {\n    free(rands);\n  }\n  free(buff_rands);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank, world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  float *rands = NULL;\n  if (world_rank == 0) {\n    rands = create_rands(ELEMENTS_PER_PROC * world_size, MAX_RANDOM_VAL, world_rank);\n  }\n\n  \n\n  float *buff_rands = (float *)malloc(sizeof(float) * ELEMENTS_PER_PROC);\n\n  \n\n  MPI_Scatter(rands, ELEMENTS_PER_PROC, MPI_FLOAT, buff_rands,\n              ELEMENTS_PER_PROC, MPI_FLOAT, 0, MPI_COMM_WORLD);\n  dumpData ( world_rank , world_size , ELEMENTS_PER_PROC , buff_rands , \" Data \" , 1);\n\n  \n\n  float sub_avg = calc_avg(buff_rands, ELEMENTS_PER_PROC);\n\n  \n\n  float *avgs = NULL;\n  \n\n  avgs = (float *)malloc(sizeof(float) * world_size);\n  \n\n  MPI_Allgather(&sub_avg, 1, MPI_FLOAT, avgs, 1, MPI_FLOAT, MPI_COMM_WORLD);\n\n  float avg = calc_avg(avgs, world_size);\n  printf(\" Process %d: Avg of all elements is %f\\n\", world_rank, avg);\n\n  if (world_rank == 0) {\n    free(rands);\n  }\n  free(buff_rands);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gyaikhom_801", "code": "int main(int argc, char *argv[])\n{\n\tint i;\n\tdouble start;\n\n  \tbc_init(BC_ERR);\n\tstart = bc_gettime_usec();\n\n#if SEQUENTIAL\n\tif (bc_rank == 0) {\n\t\tint j;\n\n\t\tgaussian_seq();\n\t\t\n\t\tprintf(\"Ux = y\\n\\nMatrix U:\\n\");\n\t\tfor (i = 0; i < dim; i++) {\n\t\t\tfor (j = 0; j < dim; j++)\n\t\t\t\tprintf(\"%8.5f \", A[i][j]);\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\tprintf(\"\\nVector y:\\n\");\n\t\tfor (i = 0; i < dim; i++)\n\t\t\tprintf(\"[%d] %8.5f\\n\", bc_rank, A[i][y]);\n\t}\n#else\n\tif (bc_size != dim) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: %d processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c %d gauss_replicate\\n\\n\",\n\t\t\t\t   dim, dim);\n\t\tbc_final();\n\t\treturn -1;\n\t}\n\n#if WITH_MEM_COPY\n\tgaussian_par_mcpy();\n#else\n\tgaussian_par_nmcpy();\n#endif\n\n \tfor (i = 0; i < dim; i++)\n \t\tprintf(\"U[%d, %d] = %8.5f\\n\", bc_rank, i, A[bc_rank][i]);\n\tprintf(\"y[%d] = %8.5f\\n\", bc_rank, A[bc_rank][y]);\n\n#endif\n\n#if SEQUENTIAL\n\tif (bc_rank == 0)\n\t\tprintf(\"\\nTime: %f\\n\", bc_gettime_usec() - start);\n#else\n\tprintf(\"Time[%d]: %f usecs\\n\\n\", bc_rank, bc_gettime_usec() - start);\n#endif\n  \tbc_final();\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint i;\n\tdouble start;\n\n  \tMPI_Init(&argc, &argv);\n  \tbc_init(BC_ERR);\n\tstart = bc_gettime_usec();\n\n#if SEQUENTIAL\n\tif (bc_rank == 0) {\n\t\tint j;\n\n\t\tgaussian_seq();\n\t\t\n\t\tprintf(\"Ux = y\\n\\nMatrix U:\\n\");\n\t\tfor (i = 0; i < dim; i++) {\n\t\t\tfor (j = 0; j < dim; j++)\n\t\t\t\tprintf(\"%8.5f \", A[i][j]);\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\tprintf(\"\\nVector y:\\n\");\n\t\tfor (i = 0; i < dim; i++)\n\t\t\tprintf(\"[%d] %8.5f\\n\", bc_rank, A[i][y]);\n\t}\n#else\n\tif (bc_size != dim) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: %d processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c %d gauss_replicate\\n\\n\",\n\t\t\t\t   dim, dim);\n\t\tbc_final();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n#if WITH_MEM_COPY\n\tgaussian_par_mcpy();\n#else\n\tgaussian_par_nmcpy();\n#endif\n\n \tfor (i = 0; i < dim; i++)\n \t\tprintf(\"U[%d, %d] = %8.5f\\n\", bc_rank, i, A[bc_rank][i]);\n\tprintf(\"y[%d] = %8.5f\\n\", bc_rank, A[bc_rank][y]);\n\n#endif\n\n#if SEQUENTIAL\n\tif (bc_rank == 0)\n\t\tprintf(\"\\nTime: %f\\n\", bc_gettime_usec() - start);\n#else\n\tprintf(\"Time[%d]: %f usecs\\n\\n\", bc_rank, bc_gettime_usec() - start);\n#endif\n  \tbc_final();\n  \tMPI_Finalize();\n\n    return 0;\n}"}
{"program": "isislab-unisa_802", "code": "int main(int argc, char** argv) {\n  \n\n  \n\n  \n\n\n  \n\n  int world_size;\n\n  \n\n  int world_rank;\n\n  \n\n  char processor_name[MPI_MAX_PROCESSOR_NAME];\n  int name_len;\n\n  \n\n  example_printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n         processor_name, world_rank, world_size);\n\n  \n\n}", "label": "int main(int argc, char** argv) {\n  \n\n  \n\n  \n\n  MPI_Init(NULL, NULL);\n\n  \n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  \n\n  char processor_name[MPI_MAX_PROCESSOR_NAME];\n  int name_len;\n  MPI_Get_processor_name(processor_name, &name_len);\n\n  \n\n  example_printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n         processor_name, world_rank, world_size);\n\n  \n\n  MPI_Finalize();\n}"}
{"program": "healpy_803", "code": "int main(int argc, const char **argv)\n  {\n#ifdef USE_MPI\n#else\n  mytask=0; ntasks=1;\n#endif\n\n  UTIL_ASSERT(argc>=2,\"need at least one command line argument\");\n\n  if (strcmp(argv[1],\"acctest\")==0)\n    sharp_acctest();\n  else if (strcmp(argv[1],\"test\")==0)\n    sharp_test(argc,argv);\n  else\n    UTIL_FAIL(\"unknown command\");\n\n#ifdef USE_MPI\n#endif\n  return 0;\n  }", "label": "int main(int argc, const char **argv)\n  {\n#ifdef USE_MPI\n  MPI_Init(NULL,NULL);\n  MPI_Comm_size(MPI_COMM_WORLD,&ntasks);\n  MPI_Comm_rank(MPI_COMM_WORLD,&mytask);\n#else\n  mytask=0; ntasks=1;\n#endif\n\n  UTIL_ASSERT(argc>=2,\"need at least one command line argument\");\n\n  if (strcmp(argv[1],\"acctest\")==0)\n    sharp_acctest();\n  else if (strcmp(argv[1],\"test\")==0)\n    sharp_test(argc,argv);\n  else\n    UTIL_FAIL(\"unknown command\");\n\n#ifdef USE_MPI\n  MPI_Finalize();\n#endif\n  return 0;\n  }"}
{"program": "mozuzaeram_804", "code": "int main(int argc, char **argv)\n{\n\tint total_threads;\n\tint myid;\n\tmpi_central_barrier_test(myid);\n}", "label": "int main(int argc, char **argv)\n{\n\tint total_threads;\n\tint myid;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &total_threads);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\tmpi_central_barrier_test(myid);\n\tMPI_Finalize();\n}"}
{"program": "betoesquivel_806", "code": "int\nmain (int argc, char **argv)\n{\n  \n\n\n  char *opnames[] = {\"ebr\", \"ibr\", \"mser\", \"sfop\"};\n  int nopnames = 4;\n  char *imsets[] = {\"bark\", \"bikes\", \"boat\", \"graf\", \"leuv\", \"trees\", \"ubc\",\n\t\t    \"wall\"};\n  int nimsets = 8;\n  int i1, i2, i3, i4;\n  double mc;\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    mc = mean_coverage1 (opnames[i1], imsets, nimsets);\n    if (rank == 0) {\n      printf (\"%s: %f\\n\", opnames[i1], mc);\n    }\n  }\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    for (i2 = i1+1; i2 < nopnames; i2++) {\n      mc = mean_coverage2 (opnames[i1], opnames[i2], imsets, nimsets);\n      if (rank == 0) {\n        printf (\"%s + %s: %f\\n\", opnames[i1], opnames[i2], mc);\n      }\n    }\n  }\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    for (i2 = i1+1; i2 < nopnames; i2++) {\n      for (i3 = i2+1; i3 < nopnames; i3++) {\n        mc = mean_coverage3 (opnames[i1], opnames[i2], opnames[i3],\n\t\t\t     imsets, nimsets);\n        if (rank == 0) {\n          printf (\"%s + %s + %s: %f\\n\", opnames[i1], opnames[i2],\n              opnames[i3], mc);\n        }\n      }\n    }\n  }\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    for (i2 = i1+1; i2 < nopnames; i2++) {\n      for (i3 = i2+1; i3 < nopnames; i3++) {\n        for (i4 = i3+1; i4 < nopnames; i4++) {\n          mc = mean_coverage4 (opnames[i1], opnames[i2], opnames[i3],\n                   opnames[i4],  imsets, nimsets);\n          if (rank == 0) {\n            printf (\"%s + %s + %s + %s: %f\\n\", opnames[i1], opnames[i2],\n            opnames[i3], opnames[i4], mc);\n          }\n        }\n      }\n    }\n  }\n\n  \n\n\n  return EXIT_SUCCESS;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  \n\n  MPI_Init(NULL, NULL);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  char *opnames[] = {\"ebr\", \"ibr\", \"mser\", \"sfop\"};\n  int nopnames = 4;\n  char *imsets[] = {\"bark\", \"bikes\", \"boat\", \"graf\", \"leuv\", \"trees\", \"ubc\",\n\t\t    \"wall\"};\n  int nimsets = 8;\n  int i1, i2, i3, i4;\n  double mc;\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    mc = mean_coverage1 (opnames[i1], imsets, nimsets);\n    if (rank == 0) {\n      printf (\"%s: %f\\n\", opnames[i1], mc);\n    }\n  }\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    for (i2 = i1+1; i2 < nopnames; i2++) {\n      mc = mean_coverage2 (opnames[i1], opnames[i2], imsets, nimsets);\n      if (rank == 0) {\n        printf (\"%s + %s: %f\\n\", opnames[i1], opnames[i2], mc);\n      }\n    }\n  }\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    for (i2 = i1+1; i2 < nopnames; i2++) {\n      for (i3 = i2+1; i3 < nopnames; i3++) {\n        mc = mean_coverage3 (opnames[i1], opnames[i2], opnames[i3],\n\t\t\t     imsets, nimsets);\n        if (rank == 0) {\n          printf (\"%s + %s + %s: %f\\n\", opnames[i1], opnames[i2],\n              opnames[i3], mc);\n        }\n      }\n    }\n  }\n\n  \n\n  for (i1 = 0; i1 < nopnames; i1++) {\n    for (i2 = i1+1; i2 < nopnames; i2++) {\n      for (i3 = i2+1; i3 < nopnames; i3++) {\n        for (i4 = i3+1; i4 < nopnames; i4++) {\n          mc = mean_coverage4 (opnames[i1], opnames[i2], opnames[i3],\n                   opnames[i4],  imsets, nimsets);\n          if (rank == 0) {\n            printf (\"%s + %s + %s + %s: %f\\n\", opnames[i1], opnames[i2],\n            opnames[i3], opnames[i4], mc);\n          }\n        }\n      }\n    }\n  }\n\n  \n\n  MPI_Finalize();\n\n  return EXIT_SUCCESS;\n}"}
{"program": "egentry_810", "code": "int main(int argc, char *argv[])\n{\n\n\tint proc_rank, proc_size;\n\tconst int root_proc = 0;\n\n\t\n\n\t\n\n\n\tif (argc > 1)\n\t{\n\t\tif (strcmp(argv[1], \"comp\")==0)\n\t\t{\n\t\t\tint num_timesteps = 1024;\n\t\t\tif (argc > 2)\n\t\t\t{\n\t\t\t\tchar *end;\n\t\t\t\tnum_timesteps = strtol(argv[2], &end, 10);\n\t\t\t}\n\t\t\tdouble comp_time = get_comp_time(proc_rank, num_timesteps);\n\t\t\tdouble comp_time_avg = get_filtered_average(comp_time, proc_rank, proc_size, root_proc);\n\n\t\t\tif (proc_rank == root_proc)\n\t\t\t{\n\t\t\t\tchar filename[] = \"data/performance_constants.dat\";\n\t\t\t\tFILE *file_pointer = fopen(filename, \"a\");\n\t\t\t\tfprintf(file_pointer, \"Computation: t = %e [s] / true grid point / timestep \\n\", comp_time_avg );\n\t\t\t\tfclose(file_pointer);\n\t\t\t}\n\n\t\t}\n\t\tif (strcmp(argv[1], \"comm\")==0)\n\t\t{\n\t\t\tlong size;\n\t\t\tsize = 1;\n\t\t\tdouble comm_time_single = get_comm_time(proc_rank, proc_size, size);\n\t\t\tdouble comm_time_single_avg = get_filtered_average(comm_time_single, proc_rank, proc_size, root_proc);\n\n\n\n\t\t\tsize = 4194304;\n\t\t\tif (argc > 2)\n\t\t\t{\n\t\t\t\tchar *end;\n\t\t\t\tsize = strtol(argv[2], &end, 10);\n\t\t\t}\n\t\t\tdouble comm_time_long = get_comm_time(proc_rank, proc_size, size);\n\t\t\tdouble comm_time_long_avg = get_filtered_average(comm_time_long, proc_rank, proc_size, root_proc);\n\t\t\tcomm_time_long_avg = comm_time_long_avg / size;\n\n\t\t\tif (proc_rank == root_proc)\n\t\t\t{\n\t\t\t\tchar filename[] = \"data/performance_constants.dat\";\n\t\t\t\tFILE *file_pointer = fopen(filename, \"a\");\n\t\t\t\tfprintf(file_pointer, \"Communication: t_start = %e [s], t_length = %e [s] / SHORT variable \\n\", comm_time_single_avg, comm_time_long_avg );\n\t\t\t\tfclose(file_pointer);\n\t\t\t}\n\n\t\t}\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\n\tint proc_rank, proc_size;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank( MPI_COMM_WORLD, &proc_rank);\n\tMPI_Comm_size( MPI_COMM_WORLD, &proc_size);\n\tconst int root_proc = 0;\n\n\t\n\n\t\n\n\n\tif (argc > 1)\n\t{\n\t\tif (strcmp(argv[1], \"comp\")==0)\n\t\t{\n\t\t\tint num_timesteps = 1024;\n\t\t\tif (argc > 2)\n\t\t\t{\n\t\t\t\tchar *end;\n\t\t\t\tnum_timesteps = strtol(argv[2], &end, 10);\n\t\t\t}\n\t\t\tdouble comp_time = get_comp_time(proc_rank, num_timesteps);\n\t\t\tdouble comp_time_avg = get_filtered_average(comp_time, proc_rank, proc_size, root_proc);\n\n\t\t\tif (proc_rank == root_proc)\n\t\t\t{\n\t\t\t\tchar filename[] = \"data/performance_constants.dat\";\n\t\t\t\tFILE *file_pointer = fopen(filename, \"a\");\n\t\t\t\tfprintf(file_pointer, \"Computation: t = %e [s] / true grid point / timestep \\n\", comp_time_avg );\n\t\t\t\tfclose(file_pointer);\n\t\t\t}\n\n\t\t}\n\t\tif (strcmp(argv[1], \"comm\")==0)\n\t\t{\n\t\t\tlong size;\n\t\t\tsize = 1;\n\t\t\tdouble comm_time_single = get_comm_time(proc_rank, proc_size, size);\n\t\t\tdouble comm_time_single_avg = get_filtered_average(comm_time_single, proc_rank, proc_size, root_proc);\n\n\n\n\t\t\tsize = 4194304;\n\t\t\tif (argc > 2)\n\t\t\t{\n\t\t\t\tchar *end;\n\t\t\t\tsize = strtol(argv[2], &end, 10);\n\t\t\t}\n\t\t\tdouble comm_time_long = get_comm_time(proc_rank, proc_size, size);\n\t\t\tdouble comm_time_long_avg = get_filtered_average(comm_time_long, proc_rank, proc_size, root_proc);\n\t\t\tcomm_time_long_avg = comm_time_long_avg / size;\n\n\t\t\tif (proc_rank == root_proc)\n\t\t\t{\n\t\t\t\tchar filename[] = \"data/performance_constants.dat\";\n\t\t\t\tFILE *file_pointer = fopen(filename, \"a\");\n\t\t\t\tfprintf(file_pointer, \"Communication: t_start = %e [s], t_length = %e [s] / SHORT variable \\n\", comm_time_single_avg, comm_time_long_avg );\n\t\t\t\tfclose(file_pointer);\n\t\t\t}\n\n\t\t}\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "OnRampOrg_813", "code": "int main(int argc, char* argv[])\n{\n    int rank, size, len;\n    char processor[MPI_MAX_PROCESSOR_NAME];\n    char *name = NULL;\n\n    \n\n\n    \n\n    if( argc > 1 ) {\n        name = strdup(argv[1]);\n    }\n    else {\n        name = strdup(\"World\");\n    }\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n    printf(\"Hello, this will result in deterministic output!\");\n\n    \n\n\n    \n\n    if( NULL != name ) {\n        free(name);\n        name = NULL;\n    }\n\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    int rank, size, len;\n    char processor[MPI_MAX_PROCESSOR_NAME];\n    char *name = NULL;\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    if( argc > 1 ) {\n        name = strdup(argv[1]);\n    }\n    else {\n        name = strdup(\"World\");\n    }\n\n    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    MPI_Get_processor_name(processor, &len);\n\n    \n\n    printf(\"Hello, this will result in deterministic output!\");\n\n    \n\n    MPI_Finalize();\n\n    \n\n    if( NULL != name ) {\n        free(name);\n        name = NULL;\n    }\n\n    return 0;\n}"}
{"program": "JulianKunkel_815", "code": "int main(int argc, char ** argv){\n  if (argc != 3){\n      printf(\"Synopsis: %s <source> <target>\\n\", argv[0]);\n      printf(\"Source and/or target can reside on an XPD\\n\");\n      exit(1);\n  }\n\n\n  char * source = argv[1];\n  char * target = argv[2];\n  const int srcXPD = fileIsOnXPD(source);\n  const int tgtXPD = fileIsOnXPD(target);\n  size_t processed;\n  int ret;\n\n  const size_t size = filesize(source);\n  printf(\"Copy %s (%s) size: %lld to %s (%s)\\n\", source, srcXPD ? \"XPD\" : \"local\", (long long) size, target, tgtXPD ? \"XPD\" : \"local\");\n\n  MPI_File fr, fw;\n  ret =\n  assert(ret == MPI_SUCCESS);\n  ret =\n  assert(ret == MPI_SUCCESS);\n\n  char * buff = (char*) malloc(BLOCK_SIZE);\n\n  for(processed = 0; processed < size; processed += BLOCK_SIZE ){\n    MPI_Status status;\n    size_t transfer_size = BLOCK_SIZE;\n    if (processed + BLOCK_SIZE > size){\n      transfer_size = size - processed;\n    }\n    ret =\n    assert(ret == MPI_SUCCESS);\n    printf(\"%zu \\n\", transfer_size);\n    ret =\n    assert(ret == MPI_SUCCESS);\n  }\n  free(buff);\n  ret =\n  assert(ret == MPI_SUCCESS);\n  ret =\n  assert(ret == MPI_SUCCESS);\n\n\n  return 0;\n}", "label": "int main(int argc, char ** argv){\n  if (argc != 3){\n      printf(\"Synopsis: %s <source> <target>\\n\", argv[0]);\n      printf(\"Source and/or target can reside on an XPD\\n\");\n      exit(1);\n  }\n\n  MPI_Init(& argc, & argv);\n\n  char * source = argv[1];\n  char * target = argv[2];\n  const int srcXPD = fileIsOnXPD(source);\n  const int tgtXPD = fileIsOnXPD(target);\n  size_t processed;\n  int ret;\n\n  const size_t size = filesize(source);\n  printf(\"Copy %s (%s) size: %lld to %s (%s)\\n\", source, srcXPD ? \"XPD\" : \"local\", (long long) size, target, tgtXPD ? \"XPD\" : \"local\");\n\n  MPI_File fr, fw;\n  ret = MPI_File_open(MPI_COMM_WORLD, source, MPI_MODE_RDONLY, MPI_INFO_NULL, & fr);\n  assert(ret == MPI_SUCCESS);\n  MPI_File_delete(target, MPI_INFO_NULL);\n  ret = MPI_File_open(MPI_COMM_WORLD, target, MPI_MODE_WRONLY|MPI_MODE_CREATE, MPI_INFO_NULL, & fw);\n  assert(ret == MPI_SUCCESS);\n\n  char * buff = (char*) malloc(BLOCK_SIZE);\n\n  for(processed = 0; processed < size; processed += BLOCK_SIZE ){\n    MPI_Status status;\n    size_t transfer_size = BLOCK_SIZE;\n    if (processed + BLOCK_SIZE > size){\n      transfer_size = size - processed;\n    }\n    ret = MPI_File_read_at(fr, processed, buff, transfer_size, MPI_BYTE, & status);\n    assert(ret == MPI_SUCCESS);\n    printf(\"%zu \\n\", transfer_size);\n    ret = MPI_File_write_at(fw, processed, buff, transfer_size, MPI_BYTE, & status);\n    assert(ret == MPI_SUCCESS);\n  }\n  free(buff);\n  ret = MPI_File_close(& fr);\n  assert(ret == MPI_SUCCESS);\n  ret = MPI_File_close(& fw);\n  assert(ret == MPI_SUCCESS);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "mpip_816", "code": "int main(int argc, char **argv){\n  int np[2], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M;\n  double x_max[3];\n  \n  \n\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=1;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2]= %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %td real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] = %td x %td processes (change with -pnfft_np * *)\\n\", np[0], np[1]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD);\n\n  \n\n  pnfft_cleanup();\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M;\n  double x_max[3];\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=1;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2]= %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %td real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] = %td x %td processes (change with -pnfft_np * *)\\n\", np[0], np[1]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD);\n\n  \n\n  pnfft_cleanup();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gyaikhom_818", "code": "int main(int argc, char *argv[])\n{\n\tcomplex_t coeff;\n\tint nproc;\n\n#if TEST1\n\tcomplex_t page201_1[2] = {{2.0, 0.0},\n\t\t\t\t\t\t\t  {3.0, 0.0}};\n\tnproc = 2;\n#endif\n#if TEST2\n \tcomplex_t page201_2[4] = {{1.0, 0.0},\n\t\t\t\t\t\t\t  {2.0, 0.0},\n\t\t\t\t\t\t\t  {4.0, 0.0},\n\t\t\t\t\t\t\t  {3.0, 0.0}};\n\tnproc = 4;\n#endif\n#if TEST3\n \tcomplex_t page204[8] = {{-1.0, 0.0},\n\t\t\t\t\t\t\t{ 5.0, 0.0},\n\t\t\t\t\t\t\t{-4.0, 0.0},\n\t\t\t\t\t\t\t{ 2.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0}};\n\tnproc = 8;\n#endif\n#if TEST4\n\tcomplex_t page205[8] = {{2.0, 0.0},\n\t\t\t\t\t\t\t{3.0, 0.0},\n\t\t\t\t\t\t\t{2.0, 0.0},\n\t\t\t\t\t\t\t{1.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0}};\n\tnproc = 8;\n#endif\n\n\tbc_init(BC_ERR);\n\n\tif (bc_size < nproc) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: %d processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c %d fft\\n\\n\",\n\t\t\t\t   nproc, nproc);\n\t\tbc_final();\n\t\treturn -1;\n\t}\n\n\n#if TEST1\n\t\n\n\tif (bc_rank < 2) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 1: page 201, example 1\\n\");\n\t\tpermute(2, page201_1);\n\t\tcoeff.r = page201_1[bc_rank].r;\n\t\tcoeff.i = page201_1[bc_rank].i;\n\t\tfft(2, 2, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n#if TEST2\n\t\n\n\tif (bc_rank < 4) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 2: page 201, example 2\\n\");\n\t\tpermute(4, page201_2);\n\t\tcoeff.r = page201_2[bc_rank].r;\n\t\tcoeff.i = page201_2[bc_rank].i; \n\t\tfft(4, 4, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n#if TEST3\n\t\n\n\tif (bc_rank < 8) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 3: page 204\\n\");\n\t\tpermute(8, page204);\n\t\tcoeff.r = page204[bc_rank].r;\n\t\tcoeff.i = page204[bc_rank].i; \n\t\tfft(8, 8, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n#if TEST4\n\t\n\n\tif (bc_rank < 8) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 4: page 205\\n\");\n\t\tpermute(8, page205);\n\t\tcoeff.r = page205[bc_rank].r;\n\t\tcoeff.i = page205[bc_rank].i; \n\t\tfft(8, 8, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n\n\tbc_final();\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tcomplex_t coeff;\n\tint nproc;\n\n#if TEST1\n\tcomplex_t page201_1[2] = {{2.0, 0.0},\n\t\t\t\t\t\t\t  {3.0, 0.0}};\n\tnproc = 2;\n#endif\n#if TEST2\n \tcomplex_t page201_2[4] = {{1.0, 0.0},\n\t\t\t\t\t\t\t  {2.0, 0.0},\n\t\t\t\t\t\t\t  {4.0, 0.0},\n\t\t\t\t\t\t\t  {3.0, 0.0}};\n\tnproc = 4;\n#endif\n#if TEST3\n \tcomplex_t page204[8] = {{-1.0, 0.0},\n\t\t\t\t\t\t\t{ 5.0, 0.0},\n\t\t\t\t\t\t\t{-4.0, 0.0},\n\t\t\t\t\t\t\t{ 2.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0},\n\t\t\t\t\t\t\t{ 0.0, 0.0}};\n\tnproc = 8;\n#endif\n#if TEST4\n\tcomplex_t page205[8] = {{2.0, 0.0},\n\t\t\t\t\t\t\t{3.0, 0.0},\n\t\t\t\t\t\t\t{2.0, 0.0},\n\t\t\t\t\t\t\t{1.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0},\n\t\t\t\t\t\t\t{0.0, 0.0}};\n\tnproc = 8;\n#endif\n\n\tMPI_Init(&argc, &argv);\n\tbc_init(BC_ERR);\n\n\tif (bc_size < nproc) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: %d processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c %d fft\\n\\n\",\n\t\t\t\t   nproc, nproc);\n\t\tbc_final();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n\n#if TEST1\n\t\n\n\tif (bc_rank < 2) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 1: page 201, example 1\\n\");\n\t\tpermute(2, page201_1);\n\t\tcoeff.r = page201_1[bc_rank].r;\n\t\tcoeff.i = page201_1[bc_rank].i;\n\t\tfft(2, 2, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n#if TEST2\n\t\n\n\tif (bc_rank < 4) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 2: page 201, example 2\\n\");\n\t\tpermute(4, page201_2);\n\t\tcoeff.r = page201_2[bc_rank].r;\n\t\tcoeff.i = page201_2[bc_rank].i; \n\t\tfft(4, 4, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n#if TEST3\n\t\n\n\tif (bc_rank < 8) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 3: page 204\\n\");\n\t\tpermute(8, page204);\n\t\tcoeff.r = page204[bc_rank].r;\n\t\tcoeff.i = page204[bc_rank].i; \n\t\tfft(8, 8, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n#if TEST4\n\t\n\n\tif (bc_rank < 8) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"Test 4: page 205\\n\");\n\t\tpermute(8, page205);\n\t\tcoeff.r = page205[bc_rank].r;\n\t\tcoeff.i = page205[bc_rank].i; \n\t\tfft(8, 8, &coeff);\n\t\tprintf(\"[%2d] %5.2f %5.2fi\\n\", bc_rank, coeff.r, coeff.i);\n\t}\n#endif\n\n\tbc_final();\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "swift-lang_820", "code": "int main(int argc, char* argv[])\n{\n  int mpi_size, mpi_rank;\n\n\n\n  startup();\n\n  \n\n  \n\n  \n\n\n  char* data;\n  int size;\n\n  if (argc != 3)\n  {\n    print_help();\n    return 1;\n  }\n\n  if (mpi_rank == 0)\n  {\n    read_input(argv[1], &data, &size);\n    printf(\"size: %i\\n\", size);\n  }\n  else\n  {\n    MPI_Status status;\n\n    printf(\"size: %i\\n\", size);\n\n    data = malloc(size);\n    assert(data);\n\n    write_output(argv[2], data, size);\n  }\n  free(data);\n\n  return EXIT_SUCCESS;\n}", "label": "int main(int argc, char* argv[])\n{\n  int mpi_size, mpi_rank;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  startup();\n\n  \n\n  \n\n  \n\n\n  char* data;\n  int size;\n\n  if (argc != 3)\n  {\n    print_help();\n    return 1;\n  }\n\n  if (mpi_rank == 0)\n  {\n    read_input(argv[1], &data, &size);\n    printf(\"size: %i\\n\", size);\n    MPI_Send(&size, 1, MPI_INT,\n             1, 1, MPI_COMM_WORLD);\n    MPI_Send(data, size, MPI_CHAR,\n             1, 1, MPI_COMM_WORLD);\n  }\n  else\n  {\n    MPI_Status status;\n\n    MPI_Recv(&size, 1, MPI_INT,\n             0, 1, MPI_COMM_WORLD, &status);\n    printf(\"size: %i\\n\", size);\n\n    data = malloc(size);\n    assert(data);\n    MPI_Recv(data, size, MPI_CHAR,\n             0, 1, MPI_COMM_WORLD, &status);\n\n    write_output(argv[2], data, size);\n  }\n  free(data);\n\n  MPI_Finalize();\n  return EXIT_SUCCESS;\n}"}
{"program": "lorenzgerber_821", "code": "int main(void) {\n   double* local_A;\n   double* local_x;\n   double* local_y;\n   int m, local_m, n, local_n;\n   int my_rank, comm_sz;\n   MPI_Comm comm;\n\n   comm = MPI_COMM_WORLD;\n\n   Get_dims(&m, &local_m, &n, &local_n, my_rank, comm_sz, comm);\n   Allocate_arrays(&local_A, &local_x, &local_y, local_m, n, local_n, comm);\n   Read_matrix(\"A\", local_A, m, local_m, n, my_rank, comm);\n#  ifdef DEBUG\n   Print_matrix(\"A\", local_A, m, local_m, n, my_rank, comm);\n#  endif\n   Read_vector(\"x\", local_x, n, local_n, my_rank, comm);\n#  ifdef DEBUG\n   Print_vector(\"x\", local_x, n, local_n, my_rank, comm);\n#  endif\n\n   Mat_vect_mult(local_A, local_x, local_y, local_m, n, local_n, comm);\n\n   Print_vector(\"y\", local_y, m, local_m, my_rank, comm);\n\n   free(local_A);\n   free(local_x);\n   free(local_y);\n   return 0;\n}", "label": "int main(void) {\n   double* local_A;\n   double* local_x;\n   double* local_y;\n   int m, local_m, n, local_n;\n   int my_rank, comm_sz;\n   MPI_Comm comm;\n\n   MPI_Init(NULL, NULL);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &comm_sz);\n   MPI_Comm_rank(comm, &my_rank);\n\n   Get_dims(&m, &local_m, &n, &local_n, my_rank, comm_sz, comm);\n   Allocate_arrays(&local_A, &local_x, &local_y, local_m, n, local_n, comm);\n   Read_matrix(\"A\", local_A, m, local_m, n, my_rank, comm);\n#  ifdef DEBUG\n   Print_matrix(\"A\", local_A, m, local_m, n, my_rank, comm);\n#  endif\n   Read_vector(\"x\", local_x, n, local_n, my_rank, comm);\n#  ifdef DEBUG\n   Print_vector(\"x\", local_x, n, local_n, my_rank, comm);\n#  endif\n\n   Mat_vect_mult(local_A, local_x, local_y, local_m, n, local_n, comm);\n\n   Print_vector(\"y\", local_y, m, local_m, my_rank, comm);\n\n   free(local_A);\n   free(local_x);\n   free(local_y);\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "gnu3ra_822", "code": "int main( int argc, char *argv[] )\n{\n    MPI_Datatype newtype;\n    int b[1], d[1];\n\n\n    \n\n\n\n\n    printf( \" No Errors\\n\" );\n\n    \n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    MPI_Datatype newtype;\n    int b[1], d[1];\n\n    MPI_Init( 0, 0 );\n\n    \n\n    MPI_Type_hvector( 0, 1, 10, MPI_DOUBLE, &newtype );\n    MPI_Type_commit( &newtype );\n    MPI_Type_free( &newtype );\n\n    MPI_Type_indexed( 0, b, d, MPI_DOUBLE, &newtype );\n    MPI_Type_commit( &newtype );\n\n    MPI_Sendrecv( b, 1, newtype, 0, 0, \n\t\t  d, 0, newtype, 0, 0, \n\t\t  MPI_COMM_WORLD, MPI_STATUS_IGNORE );\n\n    printf( \" No Errors\\n\" );\n\n    MPI_Type_free( &newtype );\n    \n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_825", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tnumProcessors;\n\tint\t\t\tprocToWatch;\n\t\n\t\n\n\t\n\t\n\n\n\tif( !StGermainAll_Init( &argc, &argv ) ) {\n\t\tfprintf( stderr, \"Error initialising StGermain, exiting.\\n\" );\n\t\texit( EXIT_FAILURE );\n\t}\n\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) printf( \"Watching rank: %i\\n\", rank );\n\t\n\tStGermainAll_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\tCommWorld;\n\tint\t\t\trank;\n\tint\t\t\tnumProcessors;\n\tint\t\t\tprocToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\t\n\n\n\tif( !StGermainAll_Init( &argc, &argv ) ) {\n\t\tfprintf( stderr, \"Error initialising StGermain, exiting.\\n\" );\n\t\texit( EXIT_FAILURE );\n\t}\n\tMPI_Barrier( CommWorld ); \n\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) printf( \"Watching rank: %i\\n\", rank );\n\t\n\tStGermainAll_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "lapesd_826", "code": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing AFIO CSTREAM test...\\n\", stdout);\n  ctx_init ();\n\n  if (ctx.num_procs > MAX_PROCESSES)\n    {\n      MPI_Comm new_comm;\n      MPI_Group grp, new_grp;\n      int range[3] =\n\t{ 0, MAX_PROCESSES - 1, 1 };\n      ctx.comm = new_comm;\n    }\n\n  if (ctx.comm != MPI_COMM_NULL)\n    {\n      const iore_afio_vtable_t *backend = afio_pool[IORE_AFIO_CSTREAM];\n      iore_file_t file =\n\t{ };\n      file.name = TEST_FILE_NAME;\n\n      fprintf (stdout, \"[Task %d] Test Offset WR 01: ...\\n\", ctx.task_id);\n      if (test_wr_oset01 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Offset WR 02: ...\\n\", ctx.task_id);\n      if (test_wr_oset02 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Offset WR 03: ...\\n\", ctx.task_id);\n      if (test_wr_oset03 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Offset WR 04: ...\\n\", ctx.task_id);\n      if (test_wr_oset04 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Dataset WR 01: ...\\n\", ctx.task_id);\n      if (test_wr_dset01 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Dataset WR 02: ...\\n\", ctx.task_id);\n      if (test_wr_dset02 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Dataset WR 03: ...\\n\", ctx.task_id);\n      if (test_wr_dset03 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test Dataset WR 04: ...\\n\", ctx.task_id);\n      if (test_wr_dset04 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      fprintf (stdout, \"[Task %d] Test AFIO to string: ...\\n\", ctx.task_id);\n      if (test_to_str ())\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n    }\n\n  fputs (\"Finalizing AFIO CSTREAM test.\\n\", stdout);\n}", "label": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing AFIO CSTREAM test...\\n\", stdout);\n  MPI_Init (&argc, &argv);\n  ctx_init ();\n\n  if (ctx.num_procs > MAX_PROCESSES)\n    {\n      MPI_Comm new_comm;\n      MPI_Group grp, new_grp;\n      int range[3] =\n\t{ 0, MAX_PROCESSES - 1, 1 };\n      MPI_Comm_group (MPI_COMM_WORLD, &grp);\n      MPI_Group_range_incl (grp, 1, &range, &new_grp);\n      MPI_Comm_create (MPI_COMM_WORLD, new_grp, &new_comm);\n      ctx.comm = new_comm;\n    }\n\n  if (ctx.comm != MPI_COMM_NULL)\n    {\n      const iore_afio_vtable_t *backend = afio_pool[IORE_AFIO_CSTREAM];\n      iore_file_t file =\n\t{ };\n      file.name = TEST_FILE_NAME;\n\n      fprintf (stdout, \"[Task %d] Test Offset WR 01: ...\\n\", ctx.task_id);\n      if (test_wr_oset01 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Offset WR 02: ...\\n\", ctx.task_id);\n      if (test_wr_oset02 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Offset WR 03: ...\\n\", ctx.task_id);\n      if (test_wr_oset03 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Offset WR 04: ...\\n\", ctx.task_id);\n      if (test_wr_oset04 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Dataset WR 01: ...\\n\", ctx.task_id);\n      if (test_wr_dset01 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Dataset WR 02: ...\\n\", ctx.task_id);\n      if (test_wr_dset02 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Dataset WR 03: ...\\n\", ctx.task_id);\n      if (test_wr_dset03 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test Dataset WR 04: ...\\n\", ctx.task_id);\n      if (test_wr_dset04 (backend, file))\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n\n      MPI_Barrier (ctx.comm);\n      fprintf (stdout, \"[Task %d] Test AFIO to string: ...\\n\", ctx.task_id);\n      if (test_to_str ())\n\tfprintf (stdout, \"[Task %d] FAIL!\\n\", ctx.task_id);\n      else\n\tfprintf (stdout, \"[Task %d] SUCCESS!\\n\", ctx.task_id);\n    }\n\n  MPI_Finalize ();\n  fputs (\"Finalizing AFIO CSTREAM test.\\n\", stdout);\n}"}
{"program": "gnu3ra_831", "code": "int main( int argc, char **argv )\n{\n    int              rank, size, i,j;\n    int              table[MAX_PROCESSES][MAX_PROCESSES];\n    int              errors=0;\n    int              participants;\n    int              displs[MAX_PROCESSES];\n    int              recv_counts[MAX_PROCESSES];\n\n\n    \n\n    if ( size > MAX_PROCESSES ) participants = MAX_PROCESSES;\n    else              participants = size;\n    \n\n    if (MAX_PROCESSES % participants) {\n\tfprintf( stderr, \"Number of processors must divide %d\\n\",\n\t\tMAX_PROCESSES );\n\t}\n    if ( (rank < participants) ) {\n\n      \n\n      int block_size = MAX_PROCESSES / participants;\n      int begin_row  = rank * block_size;\n      int end_row    = (rank+1) * block_size;\n      int send_count = block_size * MAX_PROCESSES;\n      \n      \n\n      for (i=0; i<participants; i++) {\n\tdispls[i]      = i * block_size * MAX_PROCESSES;\n\trecv_counts[i] = send_count;\n      }\n\n      \n\n      for (i=begin_row; i<end_row ;i++)\n\tfor (j=0; j<MAX_PROCESSES; j++)\n\t  table[i][j] = rank + 10;\n      \n      \n\n      \n\n      for (i=0; i<participants; i++) {\n        void *sendbuf = (i == rank ? MPI_IN_PLACE : &table[begin_row][0]);\n      }\n\n\n      \n\n      for (i=0; i<MAX_PROCESSES;i++) \n\tif ( (table[i][0] - table[i][MAX_PROCESSES-1] !=0) ) \n\t  errors++;\n      for (i=0; i<MAX_PROCESSES;i++) {\n\t  for (j=0; j<MAX_PROCESSES;j++) {\n\t      if (table[i][j] != (i/block_size) + 10) errors++;\n\t      }\n\t  }\n      if (errors) {\n\t  \n\n\t  for (i=0; i<MAX_PROCESSES;i++) {\n\t      printf(\"\\n\");\n\t      for (j=0; j<MAX_PROCESSES; j++)\n\t\t  printf(\"  %d\",table[i][j]);\n\t      }\n\t  printf(\"\\n\");\n\t  }\n    } \n\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0)  \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}", "label": "int main( int argc, char **argv )\n{\n    int              rank, size, i,j;\n    int              table[MAX_PROCESSES][MAX_PROCESSES];\n    int              errors=0;\n    int              participants;\n    int              displs[MAX_PROCESSES];\n    int              recv_counts[MAX_PROCESSES];\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n\n    \n\n    if ( size > MAX_PROCESSES ) participants = MAX_PROCESSES;\n    else              participants = size;\n    \n\n    if (MAX_PROCESSES % participants) {\n\tfprintf( stderr, \"Number of processors must divide %d\\n\",\n\t\tMAX_PROCESSES );\n\tMPI_Abort( MPI_COMM_WORLD, 1 );\n\t}\n    if ( (rank < participants) ) {\n\n      \n\n      int block_size = MAX_PROCESSES / participants;\n      int begin_row  = rank * block_size;\n      int end_row    = (rank+1) * block_size;\n      int send_count = block_size * MAX_PROCESSES;\n      \n      \n\n      for (i=0; i<participants; i++) {\n\tdispls[i]      = i * block_size * MAX_PROCESSES;\n\trecv_counts[i] = send_count;\n      }\n\n      \n\n      for (i=begin_row; i<end_row ;i++)\n\tfor (j=0; j<MAX_PROCESSES; j++)\n\t  table[i][j] = rank + 10;\n      \n      \n\n      \n\n      for (i=0; i<participants; i++) {\n        void *sendbuf = (i == rank ? MPI_IN_PLACE : &table[begin_row][0]);\n        MPI_Gatherv(sendbuf,      send_count, MPI_INT,\n\t\t    &table[0][0], recv_counts, displs, MPI_INT, \n\t\t    i, MPI_COMM_WORLD);\n      }\n\n\n      \n\n      for (i=0; i<MAX_PROCESSES;i++) \n\tif ( (table[i][0] - table[i][MAX_PROCESSES-1] !=0) ) \n\t  errors++;\n      for (i=0; i<MAX_PROCESSES;i++) {\n\t  for (j=0; j<MAX_PROCESSES;j++) {\n\t      if (table[i][j] != (i/block_size) + 10) errors++;\n\t      }\n\t  }\n      if (errors) {\n\t  \n\n\t  for (i=0; i<MAX_PROCESSES;i++) {\n\t      printf(\"\\n\");\n\t      for (j=0; j<MAX_PROCESSES; j++)\n\t\t  printf(\"  %d\",table[i][j]);\n\t      }\n\t  printf(\"\\n\");\n\t  }\n    } \n\n    MPI_Finalize();\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0)  \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}"}
{"program": "mskovacic_833", "code": "int main(int argc, char *argv[])\r\n{\r\n\tint done = 0, n, myid, numprocs, i;\r\n\tdouble PI25DT = 3.141592653589793238462643;\r\n\tdouble mypi, pi;\r\n\tdouble startwtime, endwtime;\r\n\tint  namelen;\r\n\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\r\n\tint jobs, start, jobsdone, procid, jobsgiven;\r\n\tMPI_Status status;\r\n\r\n\r\n\tprintf(\"Proces %d na racunalu %s\\n\", myid, processor_name); fflush(stdout);\r\n\r\n\tn = 0;\r\n\tjobs = (numprocs - 1) * JOBS_PER_PROC;\r\n\twhile (1)\r\n\t{\tif (myid == MASTER)\r\n\t\t{\tprintf(\"Upisite broj elemenata reda: (0 za kraj) \"); fflush(stdout);\r\n\t\t\tscanf(\"%d\",&n);\r\n\t\t\tstartwtime =\r\n\t\t}\r\n\r\n\t\tif (n == 0)\t\n\n\t\t\tbreak;\r\n\r\n\t\tmypi = 0;\r\n\t\tif(myid == MASTER)\t\n\n\t\t{\tjobsdone = 0;\r\n\t\t\twhile(jobsdone < jobs)\r\n\t\t\t{\t\n\n\t\t\t\tjobsdone++;\r\n\t\t\t\t\n\n\t\t\t}\r\n\t\t\t\n\n\t\t\tjobsdone = -1;\r\n\t\t\tfor(i = 0; i<numprocs-1; i++)\r\n\t\t\t}\r\n\t\t}\r\n\t\telse\t\n\n\t\t{\tjobsgiven = 0;\r\n\t\t\twhile(1)\r\n\t\t\t{\t\n\n\t\t\t\t\n\n\t\t\t\tif(start < 0)\r\n\t\t\t\t\tbreak;\t\n\n\t\t\t\tjobsgiven++;\r\n\t\t\t\tmypi += dosinglejob(start, jobs, n);\r\n\t\t\t}\r\n\t\t\tprintf(\"Proces %d napravio %d (%.3g%%) poslova! (%s\\b)\\n\", \r\n\t\t\t\tmyid, jobsgiven, 100*(double)(jobsgiven)/jobs, processor_name);\r\n\t\t\tfflush(stdout);\r\n\t\t}\r\n\r\n\t\t\n\n\r\n\t\tif (myid == MASTER)\r\n\t\t{\tprintf(\"\\nPi je otprilike %.16f, pogreska je %.16f\\n\", pi, fabs(pi - PI25DT));\r\n\t\t\tendwtime =\r\n\t\t\tprintf(\"Proteklo vrijeme = %f\\n\\n\", endwtime-startwtime);\r\n\t\t}\r\n\t}\r\n\treturn 0;\r\n}", "label": "int main(int argc, char *argv[])\r\n{\r\n\tint done = 0, n, myid, numprocs, i;\r\n\tdouble PI25DT = 3.141592653589793238462643;\r\n\tdouble mypi, pi;\r\n\tdouble startwtime, endwtime;\r\n\tint  namelen;\r\n\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\r\n\tint jobs, start, jobsdone, procid, jobsgiven;\r\n\tMPI_Status status;\r\n\r\n\tMPI_Init(&argc,&argv);\r\n\tMPI_Comm_size(MPI_COMM_WORLD,&numprocs);\r\n\tMPI_Comm_rank(MPI_COMM_WORLD,&myid);\r\n\tMPI_Get_processor_name(processor_name,&namelen);\r\n\r\n\tprintf(\"Proces %d na racunalu %s\\n\", myid, processor_name); fflush(stdout);\r\n\r\n\tn = 0;\r\n\tjobs = (numprocs - 1) * JOBS_PER_PROC;\r\n\tMPI_Barrier(MPI_COMM_WORLD);\r\n\twhile (1)\r\n\t{\tif (myid == MASTER)\r\n\t\t{\tprintf(\"Upisite broj elemenata reda: (0 za kraj) \"); fflush(stdout);\r\n\t\t\tscanf(\"%d\",&n);\r\n\t\t\tstartwtime = MPI_Wtime();\r\n\t\t}\r\n\r\n\t\tMPI_Bcast(&n, 1, MPI_INT, MASTER, MPI_COMM_WORLD);\r\n\t\tif (n == 0)\t\n\n\t\t\tbreak;\r\n\r\n\t\tmypi = 0;\r\n\t\tif(myid == MASTER)\t\n\n\t\t{\tjobsdone = 0;\r\n\t\t\twhile(jobsdone < jobs)\r\n\t\t\t{\t\n\n\t\t\t\tMPI_Recv(&procid, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\r\n\t\t\t\tjobsdone++;\r\n\t\t\t\t\n\n\t\t\t\tMPI_Send(&jobsdone, 1, MPI_INT, procid, 99, MPI_COMM_WORLD);\r\n\t\t\t}\r\n\t\t\t\n\n\t\t\tjobsdone = -1;\r\n\t\t\tfor(i = 0; i<numprocs-1; i++)\r\n\t\t\t{\tMPI_Recv(&procid, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\r\n\t\t\t\tMPI_Send(&jobsdone, 1, MPI_INT, procid, 99, MPI_COMM_WORLD);\r\n\t\t\t}\r\n\t\t}\r\n\t\telse\t\n\n\t\t{\tjobsgiven = 0;\r\n\t\t\twhile(1)\r\n\t\t\t{\t\n\n\t\t\t\tMPI_Send(&myid, 1, MPI_INT, MASTER, 99, MPI_COMM_WORLD);\r\n\t\t\t\t\n\n\t\t\t\tMPI_Recv(&start, 1, MPI_INT, MASTER, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\r\n\t\t\t\tif(start < 0)\r\n\t\t\t\t\tbreak;\t\n\n\t\t\t\tjobsgiven++;\r\n\t\t\t\tmypi += dosinglejob(start, jobs, n);\r\n\t\t\t}\r\n\t\t\tprintf(\"Proces %d napravio %d (%.3g%%) poslova! (%s\\b)\\n\", \r\n\t\t\t\tmyid, jobsgiven, 100*(double)(jobsgiven)/jobs, processor_name);\r\n\t\t\tfflush(stdout);\r\n\t\t}\r\n\r\n\t\t\n\n\t\tMPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, MASTER, MPI_COMM_WORLD);\r\n\t\tMPI_Barrier(MPI_COMM_WORLD);\r\n\r\n\t\tif (myid == MASTER)\r\n\t\t{\tprintf(\"\\nPi je otprilike %.16f, pogreska je %.16f\\n\", pi, fabs(pi - PI25DT));\r\n\t\t\tendwtime = MPI_Wtime();\r\n\t\t\tprintf(\"Proteklo vrijeme = %f\\n\\n\", endwtime-startwtime);\r\n\t\t}\r\n\t}\r\n\tMPI_Finalize();\r\n\treturn 0;\r\n}"}
{"program": "JuantAldea_834", "code": "void sparse_rows_version(int argc, char *argv[])\n{\n    window win;\n    int max_iters = atoi(argv[3]);\n    win.pixels_height = atoi(argv[2]);\n    win.pixels_width = win.pixels_height;\n    win.x_start = -2;\n    win.x_len = 0.8f + 2.0f;\n    win.y_start = -1.5;\n    win.y_len = 1.5 + 1.5;\n\n    int com_rank, com_size;\n\n\n    uchar *chunk = NULL;\n    uchar *buffer = NULL;\n\n    printf(\"Rank: %d, %d %d\\n\", com_rank,\n           com_rank * win.pixels_height / com_size,\n           win.pixels_height / com_size);\n\n    chunk =\n        sparse_rows_mandelbrot(win, com_rank, win.pixels_height / com_size,\n                               com_size, max_iters);\n\n    int buffer_size = win.pixels_width * win.pixels_height / com_size;\n\n    if (com_rank == 0) {\n        buffer =\n            (uchar *) malloc(sizeof(uchar) * win.pixels_height *\n                             win.pixels_width);\n    }\n\n    char path[20];\n    sprintf(path, \"%d.pgm\", com_rank);\n\n    write_pgm(path, win.pixels_height, win.pixels_width / com_size, 255,\n              chunk);\n\n    free(chunk);\n\n    if (com_rank == 0) {\n        uchar *ordered_im =\n            sparse_rows_image_reconstruction(com_size, win.pixels_height,\n                                             win.pixels_width, buffer);\n        char path[100];\n        sprintf(path, \"mandelbrot_%d_%d.pgm\", win.pixels_height,\n                max_iters);\n        write_pgm(path, win.pixels_height, win.pixels_width, 255,\n                  ordered_im);\n        free(ordered_im);\n        free(buffer);\n    }\n\n}", "label": "void sparse_rows_version(int argc, char *argv[])\n{\n    window win;\n    int max_iters = atoi(argv[3]);\n    win.pixels_height = atoi(argv[2]);\n    win.pixels_width = win.pixels_height;\n    win.x_start = -2;\n    win.x_len = 0.8f + 2.0f;\n    win.y_start = -1.5;\n    win.y_len = 1.5 + 1.5;\n\n    int com_rank, com_size;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &com_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &com_size);\n\n    uchar *chunk = NULL;\n    uchar *buffer = NULL;\n\n    printf(\"Rank: %d, %d %d\\n\", com_rank,\n           com_rank * win.pixels_height / com_size,\n           win.pixels_height / com_size);\n\n    chunk =\n        sparse_rows_mandelbrot(win, com_rank, win.pixels_height / com_size,\n                               com_size, max_iters);\n\n    int buffer_size = win.pixels_width * win.pixels_height / com_size;\n\n    if (com_rank == 0) {\n        buffer =\n            (uchar *) malloc(sizeof(uchar) * win.pixels_height *\n                             win.pixels_width);\n    }\n\n    MPI_Gather(chunk, buffer_size, MPI_UNSIGNED_CHAR, buffer, buffer_size,\n               MPI_UNSIGNED_CHAR, 0, MPI_COMM_WORLD);\n    char path[20];\n    sprintf(path, \"%d.pgm\", com_rank);\n\n    write_pgm(path, win.pixels_height, win.pixels_width / com_size, 255,\n              chunk);\n\n    free(chunk);\n\n    if (com_rank == 0) {\n        uchar *ordered_im =\n            sparse_rows_image_reconstruction(com_size, win.pixels_height,\n                                             win.pixels_width, buffer);\n        char path[100];\n        sprintf(path, \"mandelbrot_%d_%d.pgm\", win.pixels_height,\n                max_iters);\n        write_pgm(path, win.pixels_height, win.pixels_width, 255,\n                  ordered_im);\n        free(ordered_im);\n        free(buffer);\n    }\n\n    MPI_Finalize();\n}"}
{"program": "gnu3ra_837", "code": "int main(int argc, char **argv) {\n  int me, nproc, i;\n  int gsize, *glist;\n  MPI_Comm group;\n\n\n\n  gsize = nproc/2 + (nproc % 2);\n  glist = malloc(gsize*sizeof(int));\n\n  for (i = 0; i < nproc; i += 2)\n    glist[i/2] = i;\n\n\n  if (me % 2 == 0) {\n    int    gme, gnproc;\n    double t1, t2;\n\n    t1 =\n    pgroup_create(gsize, glist, &group);\n    t2 =\n\n\n    if (verbose) printf(\"[%d] Group rank = %d, size = %d time = %f sec\\n\", me, gme, gnproc, t2-t1);\n\n    pgroup_free(&group);\n  }\n\n  free(glist);\n\n  if (me == 0)\n    printf(\" No Errors\\n\");\n\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  int me, nproc, i;\n  int gsize, *glist;\n  MPI_Comm group;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  gsize = nproc/2 + (nproc % 2);\n  glist = malloc(gsize*sizeof(int));\n\n  for (i = 0; i < nproc; i += 2)\n    glist[i/2] = i;\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (me % 2 == 0) {\n    int    gme, gnproc;\n    double t1, t2;\n\n    t1 = MPI_Wtime();\n    pgroup_create(gsize, glist, &group);\n    t2 = MPI_Wtime();\n\n    MPI_Barrier(group);\n\n    MPI_Comm_rank(group, &gme);\n    MPI_Comm_size(group, &gnproc);\n    if (verbose) printf(\"[%d] Group rank = %d, size = %d time = %f sec\\n\", me, gme, gnproc, t2-t1);\n\n    pgroup_free(&group);\n  }\n\n  free(glist);\n\n  if (me == 0)\n    printf(\" No Errors\\n\");\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "d-meiser_838", "code": "int main(int argn, char **argv) {\n  int result = 0;\n#ifdef BL_WITH_MPI\n  int status;\n  status =\n  assert(status == MPI_SUCCESS);\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n\n  int myRank;\n  int numRanks;\n#ifdef BL_WITH_MPI\n  status =\n  assert(status == MPI_SUCCESS);\n  status =\n  assert(status == MPI_SUCCESS);\n#else\n  myRank = 0;\n#endif\n\n  double x = 1.0;\n  double y = 0;\n\n  BL_MPI_Request req = blAddAllBegin(&x, &y, 1);\n  blAddAllEnd(req, &x, &y, 1);\n\n  assert(fabs(y - numRanks * x) < 1.0e-13);\n\n#ifdef BL_WITH_MPI\n  status =\n  assert(status == MPI_SUCCESS);\n#endif\n  return result;\n}", "label": "int main(int argn, char **argv) {\n  int result = 0;\n#ifdef BL_WITH_MPI\n  int status;\n  status = MPI_Init(&argn, &argv);\n  assert(status == MPI_SUCCESS);\n#else\n  BL_UNUSED(argn);\n  BL_UNUSED(argv);\n#endif\n\n  int myRank;\n  int numRanks;\n#ifdef BL_WITH_MPI\n  status = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  assert(status == MPI_SUCCESS);\n  status = MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  assert(status == MPI_SUCCESS);\n#else\n  myRank = 0;\n#endif\n\n  double x = 1.0;\n  double y = 0;\n\n  BL_MPI_Request req = blAddAllBegin(&x, &y, 1);\n  blAddAllEnd(req, &x, &y, 1);\n\n  assert(fabs(y - numRanks * x) < 1.0e-13);\n\n#ifdef BL_WITH_MPI\n  status = MPI_Finalize();\n  assert(status == MPI_SUCCESS);\n#endif\n  return result;\n}"}
{"program": "auag92_842", "code": "void main(int argc, char *argv[]) {\n\n\n  numworkers = numtasks-1;\n\n\n  if (taskid == MASTER) {\n    comp = (double *)malloc(MESHX*sizeof(double));\n\n    initialize();\n\n    averow  = MESHX/numworkers;\n    extra   = MESHX%numworkers;\n\n    offset = 0;\n\n     for (rank=1; rank <= (numworkers); rank++) {\n      rows = (rank <= extra) ? averow+1 : averow;\n\n      left_node  = rank - 1;\n      right_node = rank + 1;\n\n      if(rank == 1) {\n        left_node  = NONE;\n      }\n\n      if(rank == (numworkers)) {\n        right_node = NONE;\n      }\n\n      dest = rank;\n\n\n      offset = offset + rows;\n    }\n\n    for (t=1; t < ntimesteps; t++) {\n      if (t%saveT == 0) {\n        receivefrmworker();\n        writetofile(t);\n      }\n    }\n    free(comp);\n  }\n\n  if(taskid != MASTER) {\n    source  = MASTER;\n    msgtype = BEGIN;\n\n\n    start = 1;\n    if ((taskid == 1) || (taskid == numworkers)) {\n      comp = (double *)malloc((rows+1)*sizeof(double));\n\n      if (taskid == 1) {\n      } else {\n      }\n      end   = rows-1;\n    } else {\n      comp = (double *)malloc((rows+2)*sizeof(double));\n      end = rows;\n    }\n\n    long t;\n\n    for (t=1; t < ntimesteps; t++) {\n      mpiexchange(taskid);\n      solverloop(comp);\n      apply_boundary_conditions(taskid);\n      if (t%saveT == 0) {\n        sendtomaster(taskid);\n      }\n    }\n    free(comp);\n  }\n\n\n}", "label": "void main(int argc, char *argv[]) {\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n  MPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n\n  numworkers = numtasks-1;\n\n\n  if (taskid == MASTER) {\n    comp = (double *)malloc(MESHX*sizeof(double));\n\n    initialize();\n\n    averow  = MESHX/numworkers;\n    extra   = MESHX%numworkers;\n\n    offset = 0;\n\n     for (rank=1; rank <= (numworkers); rank++) {\n      rows = (rank <= extra) ? averow+1 : averow;\n\n      left_node  = rank - 1;\n      right_node = rank + 1;\n\n      if(rank == 1) {\n        left_node  = NONE;\n      }\n\n      if(rank == (numworkers)) {\n        right_node = NONE;\n      }\n\n      dest = rank;\n\n      MPI_Send(&offset,       1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&rows,         1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&left_node,    1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&right_node,   1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&comp[offset], rows, MPI_DOUBLE,      dest, BEGIN, MPI_COMM_WORLD);\n\n      offset = offset + rows;\n    }\n\n    for (t=1; t < ntimesteps; t++) {\n      if (t%saveT == 0) {\n        receivefrmworker();\n        writetofile(t);\n      }\n    }\n    free(comp);\n  }\n\n  if(taskid != MASTER) {\n    source  = MASTER;\n    msgtype = BEGIN;\n    MPI_Recv(&offset,     1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&rows,       1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&left_node,  1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&right_node, 1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n\n\n    start = 1;\n    if ((taskid == 1) || (taskid == numworkers)) {\n      comp = (double *)malloc((rows+1)*sizeof(double));\n\n      if (taskid == 1) {\n        MPI_Recv(&comp[0], rows, MPI_DOUBLE, source, msgtype, MPI_COMM_WORLD, &status);\n      } else {\n        MPI_Recv(&comp[1], rows, MPI_DOUBLE, source, msgtype, MPI_COMM_WORLD, &status);\n      }\n      end   = rows-1;\n    } else {\n      comp = (double *)malloc((rows+2)*sizeof(double));\n      MPI_Recv(&comp[1], rows, MPI_DOUBLE, source, msgtype, MPI_COMM_WORLD, &status);\n      end = rows;\n    }\n\n    long t;\n\n    for (t=1; t < ntimesteps; t++) {\n      mpiexchange(taskid);\n      solverloop(comp);\n      apply_boundary_conditions(taskid);\n      if (t%saveT == 0) {\n        sendtomaster(taskid);\n      }\n    }\n    free(comp);\n  }\n\n\n  MPI_Finalize();\n}"}
{"program": "eaubanel_843", "code": "int main(int argc, char **argv){\n\tint n; \n\n\tint ngen; \n\n\tchar **grid; \n\n\tchar **newGrid; \n\n\tint id; \n\n\tint p; \n\n\tFILE *f; \n\n\tdouble time;\n\n\tMPI_Status status;\n  MPI_Request req_send_up, req_send_down;\n\n\tif(argc <2){\n\t\tif(!id) fprintf(stderr,\"usage: %s n [seed]\\n\", argv[0]);\n\t\treturn 1;\n\t}\n\tn = strtol(argv[1], NULL, 10);\n\tif(n%p){\n\t\tif(!id) fprintf(stderr,\"n must be divisible by number of processes\\n\");\n\t\treturn 1;\n\t}\n\tint m = n/p; \n\n\t\n\tint seed;\n\tif(!id){\n\t\tif(3 == argc)\n\t\t\tseed = strtol(argv[2], NULL, 10);\n\t\telse\n\t\t\tseed = -1;\n\t\tgrid = initialize(seed, n);\n\t} else{\n\t\t grid = allocate2D(m+2, n);\n\t}\n\tif(grid == NULL){\n\t\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for grid\\n\");\n\t}\n\n\n\tif(!id)\n\t\tnewGrid = allocate2D(n+1, n);\n\telse\n\t\tnewGrid = allocate2D(m+2, n);\n\tif(newGrid == NULL){\n\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for newGrid\\n\");\n\t\treturn 1;\n\t}\n\tif(!id){\n\t\tf = fopen(\"gameOfLifeMPI.txt\",\"w\");\n\t\tdisplay(f, grid, n);\n\t\tprintf(\"enter number of generations: \");\n\t\tfflush(stdout);\n\t\tscanf(\"%d\", &ngen);\n\t}\n\n\tint nbDown = (id+1)%p; \n\n\tint nbUp = (id-1+p)%p; \n\n\ttime =\n\tfor(int k=0; k<ngen; k++){\n\t\t\n\n\n\t\tupdateGrid(grid, newGrid, 2, m-1, m, n);\n\n\n\t\tupdateGrid(grid, newGrid, 1, 1, m, n);\n\t\tupdateGrid(grid, newGrid, m, m, m, n);\n\t\tchar ** temp = grid;\n\t\tgrid = newGrid;\n\t\tnewGrid = temp;\n\n\t\t#ifdef DISPLAY\n\t\tif(!id && k % DISP_FREQ == 0){\n\t\t\n\t\t\tdisplay(f, grid, n);\n\t\t}\n\t\t#endif\n\t}\n\ttime +=\n\tdouble ptime;\n\tif(!id)\n\t\tprintf(\"time in seconds: %f\\n\\n\", ptime);\n\tif(!id)\n\t\tdisplay(f, grid, n);\n\treturn 0;\n}", "label": "int main(int argc, char **argv){\n\tint n; \n\n\tint ngen; \n\n\tchar **grid; \n\n\tchar **newGrid; \n\n\tint id; \n\n\tint p; \n\n\tFILE *f; \n\n\tdouble time;\n\n\tMPI_Status status;\n  MPI_Request req_send_up, req_send_down;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &id);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\n\tif(argc <2){\n\t\tif(!id) fprintf(stderr,\"usage: %s n [seed]\\n\", argv[0]);\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\tn = strtol(argv[1], NULL, 10);\n\tif(n%p){\n\t\tif(!id) fprintf(stderr,\"n must be divisible by number of processes\\n\");\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\tint m = n/p; \n\n\t\n\tint seed;\n\tif(!id){\n\t\tif(3 == argc)\n\t\t\tseed = strtol(argv[2], NULL, 10);\n\t\telse\n\t\t\tseed = -1;\n\t\tgrid = initialize(seed, n);\n\t} else{\n\t\t grid = allocate2D(m+2, n);\n\t}\n\tif(grid == NULL){\n\t\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for grid\\n\");\n\t\t\tMPI_Finalize();\n\t}\n\n\tMPI_Scatter(&grid[1][0], m*n, MPI_CHAR, &grid[1][0], m*n, MPI_CHAR,\n\t\t\t\t\t\t\t0, MPI_COMM_WORLD);\n\n\tif(!id)\n\t\tnewGrid = allocate2D(n+1, n);\n\telse\n\t\tnewGrid = allocate2D(m+2, n);\n\tif(newGrid == NULL){\n\t\tif(!id) fprintf(stderr,\"couldn't allocate memory for newGrid\\n\");\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\tif(!id){\n\t\tf = fopen(\"gameOfLifeMPI.txt\",\"w\");\n\t\tdisplay(f, grid, n);\n\t\tprintf(\"enter number of generations: \");\n\t\tfflush(stdout);\n\t\tscanf(\"%d\", &ngen);\n\t}\n\tMPI_Bcast(&ngen, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tint nbDown = (id+1)%p; \n\n\tint nbUp = (id-1+p)%p; \n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\ttime = -MPI_Wtime();\n\tfor(int k=0; k<ngen; k++){\n\t\t\n\n\t\tMPI_Isend(&grid[m][0], n, MPI_CHAR, nbDown, 1, \n\t\t\t\t\t\tMPI_COMM_WORLD, &req_send_down);\n\t\tMPI_Isend(&grid[1][0], n, MPI_CHAR, nbUp, 2, \n\t\t\t\t\t\tMPI_COMM_WORLD, &req_send_up);\n\n\t\tupdateGrid(grid, newGrid, 2, m-1, m, n);\n\n\t\tMPI_Recv(&grid[m+1][0], n, MPI_CHAR, nbDown, 2, \n\t\t\t\t\t\tMPI_COMM_WORLD, &status);\n\t\tMPI_Recv(&grid[0][0], n, MPI_CHAR, nbUp, 1, \n\t\t\t\t\t\tMPI_COMM_WORLD, &status);\n\n\t\tupdateGrid(grid, newGrid, 1, 1, m, n);\n\t\tupdateGrid(grid, newGrid, m, m, m, n);\n\t\tchar ** temp = grid;\n\t\tgrid = newGrid;\n\t\tnewGrid = temp;\n\n\t\t#ifdef DISPLAY\n\t\tif(!id && k % DISP_FREQ == 0){\n\t\t\tMPI_Gather(&grid[1][0], m*n, MPI_CHAR, &grid[1][0], m*n, MPI_CHAR,\n\t\t\t\t\t\t\t\t0, MPI_COMM_WORLD);\n\t\t\n\t\t\tdisplay(f, grid, n);\n\t\t}\n\t\t#endif\n\t}\n\ttime += MPI_Wtime();\n\tdouble ptime;\n\tMPI_Reduce(&time, &ptime, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n\tif(!id)\n\t\tprintf(\"time in seconds: %f\\n\\n\", ptime);\n\tMPI_Gather(&grid[1][0], m*n, MPI_CHAR, &grid[1][0], m*n, MPI_CHAR,\n\t\t\t\t\t\t0, MPI_COMM_WORLD);\n\tif(!id)\n\t\tdisplay(f, grid, n);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "ElofssonLab_845", "code": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS     *go      = NULL;\t\n\n  struct cfg_s     cfg;                 \n\n  int              status  = eslOK;\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.qfile      = NULL;\n  cfg.dbfile     = NULL;\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.qfile, &cfg.dbfile);    \n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS     *go      = NULL;\t\n\n  struct cfg_s     cfg;                 \n\n  int              status  = eslOK;\n\n  \n\n  impl_Init();\n\n  \n\n  cfg.qfile      = NULL;\n  cfg.dbfile     = NULL;\n  cfg.do_mpi     = FALSE;\t           \n\n  cfg.nproc      = 0;\t\t           \n\n  cfg.my_rank    = 0;\t\t           \n\n\n  \n\n  p7_FLogsumInit();\t\t\n\n  process_commandline(argc, argv, &go, &cfg.qfile, &cfg.dbfile);    \n\n  \n\n#ifdef HAVE_MPI\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\")) pause();\n\n  if (esl_opt_GetBoolean(go, \"--mpi\")) \n    {\n      cfg.do_mpi     = TRUE;\n      MPI_Init(&argc, &argv);\n      MPI_Comm_rank(MPI_COMM_WORLD, &(cfg.my_rank));\n      MPI_Comm_size(MPI_COMM_WORLD, &(cfg.nproc));\n\n      if (cfg.my_rank > 0)  status = mpi_worker(go, &cfg);\n      else \t\t    status = mpi_master(go, &cfg);\n\n      MPI_Finalize();\n    }\n  else\n#endif \n\n    {\n      status = serial_master(go, &cfg);\n    }\n\n  esl_getopts_Destroy(go);\n\n  return status;\n}"}
{"program": "ghisvail_846", "code": "int main(int argc, char **argv){\n  int np[2], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M;\n  double f_hat_sum, x_max[3];\n  pnfft_complex *f1, *f2;\n  \n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=1;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2]= %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %td real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] = %td x %td processes (change with -pnfft_np * *)\\n\", np[0], np[1]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD,\n      &f1, &f_hat_sum);\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m+2, x_max, window_flag, np, MPI_COMM_WORLD,\n      &f2, &f_hat_sum);\n\n  \n\n  compare_f(f1, f2, local_M, f_hat_sum, \"* Results in\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_free(f1); pnfft_free(f2);\n  pnfft_cleanup();\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M;\n  double f_hat_sum, x_max[3];\n  pnfft_complex *f1, *f2;\n  \n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=1;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2]= %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %td real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] = %td x %td processes (change with -pnfft_np * *)\\n\", np[0], np[1]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD,\n      &f1, &f_hat_sum);\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m+2, x_max, window_flag, np, MPI_COMM_WORLD,\n      &f2, &f_hat_sum);\n\n  \n\n  compare_f(f1, f2, local_M, f_hat_sum, \"* Results in\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_free(f1); pnfft_free(f2);\n  pnfft_cleanup();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "aleph7_847", "code": "int main(int argc, char **argv)\n{\n    int mpi_size, mpi_rank;\t\t\t\t\n\n\n#ifndef H5_HAVE_WIN32_API\n    \n\n    HDsetbuf(stderr, NULL);\n    HDsetbuf(stdout, NULL);\n#endif\n\n\n    dim0 = ROW_FACTOR*mpi_size;\n    dim1 = COL_FACTOR*mpi_size;\n\n    if (MAINPROCESS){\n\tprintf(\"===================================\\n\");\n\tprintf(\"Shape Same Tests Start\\n\");\n        printf(\"\texpress_test = %d.\\n\", GetTestExpress());\n\tprintf(\"===================================\\n\");\n    }\n\n    \n\n    if (H5dont_atexit() < 0){\n\tprintf(\"%d: Failed to turn off atexit processing. Continue.\\n\", mpi_rank);\n    };\n    H5open();\n    h5_show_hostname();\n\n    \n\n    TestInit(argv[0], usage, parse_options);\n\n    \n\n#if 1\n    AddTest(\"sscontig1\", sscontig1, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig2\", sscontig2, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig3\", sscontig3, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sscontig4\", sscontig4, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, chunked datasets\", PARATESTFILE);\n#endif\n\n    \n\n    AddTest(\"sschecker1\", sschecker1, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker2\", sschecker2, NULL,\n\t\"Shape Same, checker hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker3\", sschecker3, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sschecker4\", sschecker4, NULL,\n\t\"Shape Same, checker hyperslab, col IO, chunked datasets\", PARATESTFILE);\n\n    \n\n    TestInfo(argv[0]);\n\n    \n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n\n    \n\n    TestParseCmdLine(argc, argv);\n\n    if (dxfer_coll_type == DXFER_INDEPENDENT_IO && MAINPROCESS){\n\tprintf(\"===================================\\n\"\n\t       \"   Using Independent I/O with file set view to replace collective I/O \\n\"\n\t       \"===================================\\n\");\n    }\n\n\n    \n\n    PerformTests();\n\n    \n\n\n    \n\n    if (MAINPROCESS && GetTestSummary())\n        TestSummary();\n\n    \n\n    h5_clean_files(FILENAME, fapl);\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    {\n        int temp;\n\tnerrors=temp;\n    }\n\n    if (MAINPROCESS){\t\t\n\n\tprintf(\"===================================\\n\");\n\tif (nerrors)\n\t    printf(\"***Shape Same tests detected %d errors***\\n\", nerrors);\n\telse\n\t    printf(\"Shape Same tests finished with no errors\\n\");\n\tprintf(\"===================================\\n\");\n    }\n\n\n    \n\n    return(nerrors!=0);\n}", "label": "int main(int argc, char **argv)\n{\n    int mpi_size, mpi_rank;\t\t\t\t\n\n\n#ifndef H5_HAVE_WIN32_API\n    \n\n    HDsetbuf(stderr, NULL);\n    HDsetbuf(stdout, NULL);\n#endif\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    dim0 = ROW_FACTOR*mpi_size;\n    dim1 = COL_FACTOR*mpi_size;\n\n    if (MAINPROCESS){\n\tprintf(\"===================================\\n\");\n\tprintf(\"Shape Same Tests Start\\n\");\n        printf(\"\texpress_test = %d.\\n\", GetTestExpress());\n\tprintf(\"===================================\\n\");\n    }\n\n    \n\n    if (H5dont_atexit() < 0){\n\tprintf(\"%d: Failed to turn off atexit processing. Continue.\\n\", mpi_rank);\n    };\n    H5open();\n    h5_show_hostname();\n\n    \n\n    TestInit(argv[0], usage, parse_options);\n\n    \n\n#if 1\n    AddTest(\"sscontig1\", sscontig1, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig2\", sscontig2, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig3\", sscontig3, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sscontig4\", sscontig4, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, chunked datasets\", PARATESTFILE);\n#endif\n\n    \n\n    AddTest(\"sschecker1\", sschecker1, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker2\", sschecker2, NULL,\n\t\"Shape Same, checker hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker3\", sschecker3, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sschecker4\", sschecker4, NULL,\n\t\"Shape Same, checker hyperslab, col IO, chunked datasets\", PARATESTFILE);\n\n    \n\n    TestInfo(argv[0]);\n\n    \n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n\n    \n\n    TestParseCmdLine(argc, argv);\n\n    if (dxfer_coll_type == DXFER_INDEPENDENT_IO && MAINPROCESS){\n\tprintf(\"===================================\\n\"\n\t       \"   Using Independent I/O with file set view to replace collective I/O \\n\"\n\t       \"===================================\\n\");\n    }\n\n\n    \n\n    PerformTests();\n\n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    if (MAINPROCESS && GetTestSummary())\n        TestSummary();\n\n    \n\n    h5_clean_files(FILENAME, fapl);\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    {\n        int temp;\n        MPI_Allreduce(&nerrors, &temp, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\tnerrors=temp;\n    }\n\n    if (MAINPROCESS){\t\t\n\n\tprintf(\"===================================\\n\");\n\tif (nerrors)\n\t    printf(\"***Shape Same tests detected %d errors***\\n\", nerrors);\n\telse\n\t    printf(\"Shape Same tests finished with no errors\\n\");\n\tprintf(\"===================================\\n\");\n    }\n\n    MPI_Finalize();\n\n    \n\n    return(nerrors!=0);\n}"}
{"program": "ANL-CESAR_848", "code": "int main( int argc, char* argv[] )\n{\n\t\n\n\t\n\n\t\n\n\tint version = 20;\n\tint mype = 0;\n\tdouble omp_start, omp_end;\n\tint nprocs = 1;\n\tunsigned long long verification;\n\n\t#ifdef MPI\n\tMPI_Status stat;\n\t#endif\n\n\t\n\n\tInputs in = read_CLI( argc, argv );\n\n\t\n\n\tif( mype == 0 )\n\t\tprint_inputs( in, nprocs, version );\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\tSimulationData SD;\n\n\t\n\n\t\n\n\tif( in.binary_mode == READ )\n\t\tSD = binary_read(in);\n\telse\n\t\tSD = grid_init_do_not_profile( in, mype );\n\n\t\n\n\t\n\n\tif( in.binary_mode == WRITE && mype == 0 )\n\t\tbinary_write(in, SD);\n\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\n\n\t\n\n\tomp_start = get_time();\n\n\tdouble sim_runtime;\n\n\t\n\n\tif( in.simulation_method == EVENT_BASED )\n\t{\n\t\tif( in.kernel_id == 0 )\n\t\t\tverification = run_event_based_simulation(in, SD, mype, &sim_runtime);\n\t\telse\n\t\t{\n\t\t\tprintf(\"Error: No kernel ID %d found!\\n\", in.kernel_id);\n\t\t\texit(1);\n\t\t}\n\t}\n\telse\n\t{\n\t\tprintf(\"History-based simulation not implemented in OpenMP offload code. Instead,\\nuse the event-based method with \\\"-m event\\\" argument.\\n\");\n\t\texit(1);\n\t}\n\n\tif( mype == 0)\t\n\t{\t\n\t\tprintf(\"\\n\" );\n\t\tprintf(\"Simulation complete.\\n\" );\n\t}\n\n\t\n\n\tomp_end = get_time();\n\n\t\n\n\t\n\n\t\n\n\n\t\n\n\tverification = verification % 999983;\n\n\t\n\n\tint is_invalid_result = print_results( in, mype, omp_end-omp_start, nprocs, verification, sim_runtime );\n\n\t#ifdef MPI\n\t#endif\n\n\treturn is_invalid_result;\n}", "label": "int main( int argc, char* argv[] )\n{\n\t\n\n\t\n\n\t\n\n\tint version = 20;\n\tint mype = 0;\n\tdouble omp_start, omp_end;\n\tint nprocs = 1;\n\tunsigned long long verification;\n\n\t#ifdef MPI\n\tMPI_Status stat;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mype);\n\t#endif\n\n\t\n\n\tInputs in = read_CLI( argc, argv );\n\n\t\n\n\tif( mype == 0 )\n\t\tprint_inputs( in, nprocs, version );\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\tSimulationData SD;\n\n\t\n\n\t\n\n\tif( in.binary_mode == READ )\n\t\tSD = binary_read(in);\n\telse\n\t\tSD = grid_init_do_not_profile( in, mype );\n\n\t\n\n\t\n\n\tif( in.binary_mode == WRITE && mype == 0 )\n\t\tbinary_write(in, SD);\n\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\t\n\n\n\n\t\n\n\tomp_start = get_time();\n\n\tdouble sim_runtime;\n\n\t\n\n\tif( in.simulation_method == EVENT_BASED )\n\t{\n\t\tif( in.kernel_id == 0 )\n\t\t\tverification = run_event_based_simulation(in, SD, mype, &sim_runtime);\n\t\telse\n\t\t{\n\t\t\tprintf(\"Error: No kernel ID %d found!\\n\", in.kernel_id);\n\t\t\texit(1);\n\t\t}\n\t}\n\telse\n\t{\n\t\tprintf(\"History-based simulation not implemented in OpenMP offload code. Instead,\\nuse the event-based method with \\\"-m event\\\" argument.\\n\");\n\t\texit(1);\n\t}\n\n\tif( mype == 0)\t\n\t{\t\n\t\tprintf(\"\\n\" );\n\t\tprintf(\"Simulation complete.\\n\" );\n\t}\n\n\t\n\n\tomp_end = get_time();\n\n\t\n\n\t\n\n\t\n\n\n\t\n\n\tverification = verification % 999983;\n\n\t\n\n\tint is_invalid_result = print_results( in, mype, omp_end-omp_start, nprocs, verification, sim_runtime );\n\n\t#ifdef MPI\n\tMPI_Finalize();\n\t#endif\n\n\treturn is_invalid_result;\n}"}
{"program": "ianliu_850", "code": "int main(int argc, char *argv[])\n{\n\tint rank, size;\n\n\tchar *debug = getenv(\"SPITS_DEBUG_SLEEP\");\n\tif (debug) {\n\t\tint amount = atoi(debug);\n\t\tpid_t pid = getpid();\n\t\tprintf(\"Rank %d at pid %d\\n\", rank, pid);\n\t\tsleep(amount);\n\t}\n\n\tchar *loglvl = getenv(\"SPITS_LOG_LEVEL\");\n\tif (loglvl)\n\t\tLOG_LEVEL = atoi(loglvl);\n\n\tchar *nthreads = getenv(\"SPITS_NUM_THREADS\");\n\tif (nthreads)\n\t\tNTHREADS = atoi(nthreads);\n\n\tchar *fifosz = getenv(\"SPITS_TMCACHE_SIZE\");\n\tif (fifosz)\n\t\tFIFOSZ = atoi(fifosz);\n\n\tif (rank == 0 && LOG_LEVEL >= 1)\n\t\tprintf(\"Welcome to spitz \" SPITZ_VERSION \"\\n\");\n\n\tif (rank == 0 && argc < 2) {\n\t\tfprintf(stderr, \"Usage: SO_PATH\\n\");\n\t\treturn EXIT_FAILURE;\n\t}\n\n\tnworkers = (size - 2) * NTHREADS;\n\n\tchar *so = argv[1];\n\n\t\n\n\targc -= 2;\n\targv += 2;\n\n\tif (rank == JOB_MANAGER)\n\t\tstart_master_process(argc, argv, so);\n\telse\n\t\tstart_slave_processes(argc, argv);\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint rank, size;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tchar *debug = getenv(\"SPITS_DEBUG_SLEEP\");\n\tif (debug) {\n\t\tint amount = atoi(debug);\n\t\tpid_t pid = getpid();\n\t\tprintf(\"Rank %d at pid %d\\n\", rank, pid);\n\t\tsleep(amount);\n\t}\n\n\tchar *loglvl = getenv(\"SPITS_LOG_LEVEL\");\n\tif (loglvl)\n\t\tLOG_LEVEL = atoi(loglvl);\n\n\tchar *nthreads = getenv(\"SPITS_NUM_THREADS\");\n\tif (nthreads)\n\t\tNTHREADS = atoi(nthreads);\n\n\tchar *fifosz = getenv(\"SPITS_TMCACHE_SIZE\");\n\tif (fifosz)\n\t\tFIFOSZ = atoi(fifosz);\n\n\tif (rank == 0 && LOG_LEVEL >= 1)\n\t\tprintf(\"Welcome to spitz \" SPITZ_VERSION \"\\n\");\n\n\tif (rank == 0 && argc < 2) {\n\t\tfprintf(stderr, \"Usage: SO_PATH\\n\");\n\t\treturn EXIT_FAILURE;\n\t}\n\n\tnworkers = (size - 2) * NTHREADS;\n\n\tchar *so = argv[1];\n\n\t\n\n\targc -= 2;\n\targv += 2;\n\n\tif (rank == JOB_MANAGER)\n\t\tstart_master_process(argc, argv, so);\n\telse\n\t\tstart_slave_processes(argc, argv);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "mkurnosov_851", "code": "int main(int argc, char **argv)\n{\n    int rank, commsize;    \n    create_mpitypes();\n\n    \n\n    if (rank > 0)\n        run_handler();\n    else\n        run_store();\n\n    free_mpitypes();    \n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, commsize;    \n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &commsize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    create_mpitypes();\n\n    \n\n    if (rank > 0)\n        run_handler();\n    else\n        run_store();\n\n    free_mpitypes();    \n    MPI_Finalize();\n    return 0;\n}"}
{"program": "MengbinZhu_852", "code": "int\nmain(int argc, char **argv)\n{\n#ifdef H5_HAVE_PARALLEL\n    int mpi_size, mpi_rank;\t\t\t\t\n\n#endif \n\n\n    if(parse_options(argc, argv) != 0) {\n       usage();\n       return 0;\n    }\n\n#ifdef H5_HAVE_PARALLEL\n    if(facc_type == FACC_MPIO) {\n    }\n#endif \n\n\n#ifdef H5_HAVE_PARALLEL\n    if (facc_type == FACC_DEFAULT || (facc_type != FACC_DEFAULT && MAINPROCESS))\n#endif \n\n        fprintf(stderr, \"\\t\\tPerformance result of metadata for datasets and attributes\\n\\n\");\n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n#ifdef H5_HAVE_PARALLEL\n    if(facc_type == FACC_MPIO)\n        H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n#endif \n\n\n    nerrors += create_dspace() < 0 \t?1:0;\n\n    if((RUN_TEST & TEST_1) || !RUN_TEST)\n        nerrors += create_attrs_1() < 0 \t?1:0;\n    if((RUN_TEST & TEST_2) || !RUN_TEST)\n        nerrors += create_attrs_2() < 0 \t?1:0;\n    if(((RUN_TEST & TEST_3) || !RUN_TEST) && BATCH_ATTRS && NUM_ATTRS)\n        nerrors += create_attrs_3() < 0 \t?1:0;\n\n    if (H5Sclose(space) < 0) goto error;\n    if (H5Sclose(small_space) < 0) goto error;\n\n    h5_cleanup(FILENAME, fapl);\n\n#ifdef H5_HAVE_PARALLEL\n    if(facc_type == FACC_MPIO)\n        \n\n#endif \n\n\n    if (nerrors) goto error;\n#ifdef H5_HAVE_PARALLEL\n    if (facc_type != FACC_DEFAULT && MAINPROCESS)\n#endif \n\n        printf(\"All metadata performance tests passed.\\n\");\n\n    return 0;\n\n error:\n    nerrors = MAX(1, nerrors);\n#ifdef H5_HAVE_PARALLEL\n    if (facc_type != FACC_DEFAULT && MAINPROCESS)\n#endif \n\n        printf(\"***** %d PERFORMANCE TEST%s FAILED! *****\\n\",\n\t   nerrors, 1 == nerrors ? \"\" : \"S\");\n\n    return 1;\n}", "label": "int\nmain(int argc, char **argv)\n{\n#ifdef H5_HAVE_PARALLEL\n    int mpi_size, mpi_rank;\t\t\t\t\n\n#endif \n\n\n    if(parse_options(argc, argv) != 0) {\n       usage();\n       return 0;\n    }\n\n#ifdef H5_HAVE_PARALLEL\n    if(facc_type == FACC_MPIO) {\n        MPI_Init(&argc, &argv);\n        MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n        MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    }\n#endif \n\n\n#ifdef H5_HAVE_PARALLEL\n    if (facc_type == FACC_DEFAULT || (facc_type != FACC_DEFAULT && MAINPROCESS))\n#endif \n\n        fprintf(stderr, \"\\t\\tPerformance result of metadata for datasets and attributes\\n\\n\");\n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n#ifdef H5_HAVE_PARALLEL\n    if(facc_type == FACC_MPIO)\n        H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n#endif \n\n\n    nerrors += create_dspace() < 0 \t?1:0;\n\n    if((RUN_TEST & TEST_1) || !RUN_TEST)\n        nerrors += create_attrs_1() < 0 \t?1:0;\n    if((RUN_TEST & TEST_2) || !RUN_TEST)\n        nerrors += create_attrs_2() < 0 \t?1:0;\n    if(((RUN_TEST & TEST_3) || !RUN_TEST) && BATCH_ATTRS && NUM_ATTRS)\n        nerrors += create_attrs_3() < 0 \t?1:0;\n\n    if (H5Sclose(space) < 0) goto error;\n    if (H5Sclose(small_space) < 0) goto error;\n\n    h5_cleanup(FILENAME, fapl);\n\n#ifdef H5_HAVE_PARALLEL\n    if(facc_type == FACC_MPIO)\n        \n\n        MPI_Finalize();\n#endif \n\n\n    if (nerrors) goto error;\n#ifdef H5_HAVE_PARALLEL\n    if (facc_type != FACC_DEFAULT && MAINPROCESS)\n#endif \n\n        printf(\"All metadata performance tests passed.\\n\");\n\n    return 0;\n\n error:\n    nerrors = MAX(1, nerrors);\n#ifdef H5_HAVE_PARALLEL\n    if (facc_type != FACC_DEFAULT && MAINPROCESS)\n#endif \n\n        printf(\"***** %d PERFORMANCE TEST%s FAILED! *****\\n\",\n\t   nerrors, 1 == nerrors ? \"\" : \"S\");\n\n    return 1;\n}"}
{"program": "gnu3ra_854", "code": "int main( int argc, char *argv[] )\n{\n    int done = 0, n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime=0.0, endwtime;\n    int  namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n    fprintf(stderr,\"Process %d on %s\\n\",\n\t    myid, processor_name);\n\n    n = 0;\n    while (!done)\n    {\n        if (myid == 0)\n        {\n\n\n\t    if (n==0) n=100; else n=0;\n\n\t    startwtime =\n        }\n        if (n == 0)\n            done = 1;\n        else\n        {\n            h   = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs)\n            {\n                x = h * ((double)i - 0.5);\n                sum += f(x);\n            }\n            mypi = h * sum;\n\n\n            if (myid == 0)\n\t    {\n                printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n                       pi, fabs(pi - PI25DT));\n\t\tendwtime =\n\t\tprintf(\"wall clock time = %f\\n\",\n\t\t       endwtime-startwtime);\t       \n\t    }\n        }\n    }\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int done = 0, n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime=0.0, endwtime;\n    int  namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    MPI_Get_processor_name(processor_name,&namelen);\n\n    fprintf(stderr,\"Process %d on %s\\n\",\n\t    myid, processor_name);\n\n    n = 0;\n    while (!done)\n    {\n        if (myid == 0)\n        {\n\n\n\t    if (n==0) n=100; else n=0;\n\n\t    startwtime = MPI_Wtime();\n        }\n        MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        if (n == 0)\n            done = 1;\n        else\n        {\n            h   = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs)\n            {\n                x = h * ((double)i - 0.5);\n                sum += f(x);\n            }\n            mypi = h * sum;\n\n            MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n            if (myid == 0)\n\t    {\n                printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n                       pi, fabs(pi - PI25DT));\n\t\tendwtime = MPI_Wtime();\n\t\tprintf(\"wall clock time = %f\\n\",\n\t\t       endwtime-startwtime);\t       \n\t    }\n        }\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "mpip_855", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_3d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      in);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      in);\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_3d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      in);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      in);\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "pyrovski_856", "code": "int main(int argc, char *argv[]) {\n  int rank, numranks,i;\n  EPIK_TOPOL * topology;\n  char buf[1024];\n\n\n  elg_pform_init();\n  topology = EPIK_Pform_hw_topol();\n  if ( rank==0 ) {\n    printf(\"platform    : %s\\n\", elg_pform_name());\n    printf(\"Topology name: %s\\n\", (const char *)EPIK_Cart_get_toponame(topology));\n    printf(\"nodes x CPUs: %u x %u\\n\", elg_pform_num_nodes(), elg_pform_num_cpus());\n    if ( topology ) {\n      if(topology->num_dims>0) {\n\tfor(i=0;i<topology->num_dims;i++) {\n\t  printf(\"topology    : \");\n\t  printf(\"%u%c \",topology->dim_sizes[i], (topology->periods[i] ? '*' : ' '));\n\t} \n\tprintf(\"\\n\");\n      } else { printf(\"No dimensions set!\"); }\n    }\n  }\n\n  if ( topology ) {\n\n    sprintf(buf, \"%s [%lu] R%05d <\", elg_pform_node_name(), elg_pform_node_id(), rank);\n    for(i=0; i<topology->num_dims;i++) {\n      sprintf(buf,\"%5u\",topology->coords[i]);\n    } \n    if(i>0) sprintf(buf,\">\\n\"); else sprintf(buf,\"\\n\");\n\n  } else {\n    sprintf(buf, \"%s [%lu] R%05d\", elg_pform_node_name(), elg_pform_node_id(),\n\t    rank);\n  }\n  puts(buf);\n\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  int rank, numranks,i;\n  EPIK_TOPOL * topology;\n  char buf[1024];\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &numranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  elg_pform_init();\n  topology = EPIK_Pform_hw_topol();\n  if ( rank==0 ) {\n    printf(\"platform    : %s\\n\", elg_pform_name());\n    printf(\"Topology name: %s\\n\", (const char *)EPIK_Cart_get_toponame(topology));\n    printf(\"nodes x CPUs: %u x %u\\n\", elg_pform_num_nodes(), elg_pform_num_cpus());\n    if ( topology ) {\n      if(topology->num_dims>0) {\n\tfor(i=0;i<topology->num_dims;i++) {\n\t  printf(\"topology    : \");\n\t  printf(\"%u%c \",topology->dim_sizes[i], (topology->periods[i] ? '*' : ' '));\n\t} \n\tprintf(\"\\n\");\n      } else { printf(\"No dimensions set!\"); }\n    }\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if ( topology ) {\n\n    sprintf(buf, \"%s [%lu] R%05d <\", elg_pform_node_name(), elg_pform_node_id(), rank);\n    for(i=0; i<topology->num_dims;i++) {\n      sprintf(buf,\"%5u\",topology->coords[i]);\n    } \n    if(i>0) sprintf(buf,\">\\n\"); else sprintf(buf,\"\\n\");\n\n  } else {\n    sprintf(buf, \"%s [%lu] R%05d\", elg_pform_node_name(), elg_pform_node_id(),\n\t    rank);\n  }\n  puts(buf);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "cicinsain_857", "code": "int\nmain( int argc, char **argv ) {\n    double *delta;              \n\n    proc_id = -1;\n\n#ifdef MPI\n    \n\n\n\n\n\n    stop_tune_count = STOP_TUNE_CNT;    \n\n    proc_id = myid;\n\n    printf( \"#%d: Starting Simulated Annealing\\n\", proc_id );\n#else\n    printf( \"Starting Simulated Annealing\\n\" );\n#endif\n\n\n\n    \n\n    cpu_start = ( struct tms * ) malloc( sizeof( struct tms ) );        \n\n    cpu_finish = ( struct tms * ) malloc( sizeof( struct tms ) );\n    times( cpu_start );\n\n#ifdef MPI\n    start =      \n\n#else\n    start = time( NULL );       \n\n#endif\n    \n\n    \n\n    Initialize( argc, argv );\n    \n\n    \n\n    if( ( bench != 1 ) && ( ( equil != 1 ) || ( 1.0 / S > equil_param.end_T ) ) ) {\n        Loop(  );\n    }\n\n    \n\n    int fixloopcounter = 0;\n\n    if( equil == 1 ) {\n        printf( \"%d: Starting fixloop %d\\n\", proc_id, fixloopcounter );\n        FixTLoop(  );\n        fixloopcounter++;\n    }\n\n    \n\n\n    if( time_flag ) {\n        delta = GetTimes(  );   \n\n#ifdef MPI\n        if( myid == 0 )\n#endif\n            WriteTimes( delta );        \n\n        free( delta );\n    }\n\n    \n\n\n#ifdef MPI\n\n#endif\n\n    return 0;\n\n}", "label": "int\nmain( int argc, char **argv ) {\n    double *delta;              \n\n    proc_id = -1;\n\n#ifdef MPI\n    \n\n    MPI_Init( &argc, &argv );   \n\n    MPI_Comm_size( MPI_COMM_WORLD, &nnodes );   \n\n    MPI_Comm_rank( MPI_COMM_WORLD, &myid );     \n\n\n    stop_tune_count = STOP_TUNE_CNT;    \n\n    proc_id = myid;\n\n    printf( \"#%d: Starting Simulated Annealing\\n\", proc_id );\n#else\n    printf( \"Starting Simulated Annealing\\n\" );\n#endif\n\n\n\n    \n\n    cpu_start = ( struct tms * ) malloc( sizeof( struct tms ) );        \n\n    cpu_finish = ( struct tms * ) malloc( sizeof( struct tms ) );\n    times( cpu_start );\n\n#ifdef MPI\n    start = MPI_Wtime(  );      \n\n#else\n    start = time( NULL );       \n\n#endif\n    \n\n    \n\n    Initialize( argc, argv );\n    \n\n    \n\n    if( ( bench != 1 ) && ( ( equil != 1 ) || ( 1.0 / S > equil_param.end_T ) ) ) {\n        Loop(  );\n    }\n\n    \n\n    int fixloopcounter = 0;\n\n    if( equil == 1 ) {\n        printf( \"%d: Starting fixloop %d\\n\", proc_id, fixloopcounter );\n        FixTLoop(  );\n        fixloopcounter++;\n    }\n\n    \n\n\n    if( time_flag ) {\n        delta = GetTimes(  );   \n\n#ifdef MPI\n        if( myid == 0 )\n#endif\n            WriteTimes( delta );        \n\n        free( delta );\n    }\n\n    \n\n\n#ifdef MPI\n    MPI_Finalize(  );           \n\n#endif\n\n    return 0;\n\n}"}
{"program": "ghisvail_858", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "raulperula_860", "code": "int main (int argc,char*argv[])\n{\n\tint numtasks, taskid, len, buffer, root, count;\n\tchar hostname[MPI_MAX_PROCESSOR_NAME];\n\n\t\n\n\tprintf (\"Task %d on %s starting...\\n\", taskid, hostname);\n\n\tbuffer = 23;\n\troot = 0;\n\n\t\n\n\t\n\n\t\n\n\tcount = 1;\n\t\n\n\n\tif(taskid == root)\n\t   printf(\"Root: Number of MPI tasks is: %d \\n\", numtasks);\n\n\t\n}", "label": "int main (int argc,char*argv[])\n{\n\tint numtasks, taskid, len, buffer, root, count;\n\tchar hostname[MPI_MAX_PROCESSOR_NAME];\n\n\tMPI_Init(&argc,&argv);\n\t\n\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n\tMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n\tMPI_Get_processor_name(hostname,&len);\n\n\tprintf (\"Task %d on %s starting...\\n\", taskid, hostname);\n\n\tbuffer = 23;\n\troot = 0;\n\n\t\n\n\t\n\n\t\n\n\tcount = 1;\n\t\n\n\n\tif(taskid == root)\n\t   printf(\"Root: Number of MPI tasks is: %d \\n\", numtasks);\n\n\tMPI_Bcast(&buffer, count, MPI_INT, root, MPI_COMM_WORLD);\n\t\n\tMPI_Finalize();\n}"}
{"program": "bmi-forum_862", "code": "int main( int argc, char* argv[] ) {\n\tDictionary*\tdictionary;\n\tMPI_Comm\tCommWorld;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\tBool            periodic[3] = { False, False, False };\n\tunsigned int    ii, jj, kk;\n\tStream*         stream;\n\tIJK             ijkSize;\n\t\n\t\n\n\t\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\n\t\n\tstream = Journal_Register( Info_Type, argv[0] );\n\tStream_Enable( stream, True );\n\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\n\tJournal_Printf( stream, \"+++++++++++++ 1D Tests +++++++++++++\\n\\n\" );\n\tijkSize[I_AXIS] = 1;\n\tijkSize[J_AXIS] = 1;\n\tijkSize[K_AXIS] = 1;\n\tperiodic[I_AXIS] = False;\n\tperiodic[J_AXIS] = False;\n\tperiodic[K_AXIS] = False;\n\tfor ( ijkSize[I_AXIS] = 1; ijkSize[I_AXIS] <= 3; ijkSize[I_AXIS]++ ) {\n\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\tperiodic[I_AXIS] = ii;\n\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t}\t\n\t}\n\tijkSize[I_AXIS] = 1;\n\tperiodic[I_AXIS] = True;\t\n\n\tfor ( ijkSize[J_AXIS] = 2; ijkSize[J_AXIS] <= 3; ijkSize[J_AXIS]++ ) {\n\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\tperiodic[J_AXIS] = ii;\n\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t}\t\n\t}\n\n\tJournal_Printf( stream, \"+++++++++++++ 2D Tests +++++++++++++\\n\\n\" );\n\tijkSize[I_AXIS] = 1;\n\tijkSize[J_AXIS] = 1;\n\tijkSize[K_AXIS] = 1;\n\tperiodic[I_AXIS] = False;\n\tperiodic[J_AXIS] = False;\n\tperiodic[K_AXIS] = False;\n\tfor ( ijkSize[J_AXIS] = 2; ijkSize[J_AXIS] <= 3; ijkSize[J_AXIS]++ ) {\n\t\tfor ( ijkSize[I_AXIS] = 2; ijkSize[I_AXIS] <= 3; ijkSize[I_AXIS]++ ) {\n\t\t\tfor ( jj=False; jj < 2; jj++ ) {\n\t\t\t\tperiodic[J_AXIS] = jj;\n\t\t\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\t\t\tperiodic[I_AXIS] = ii;\n\t\t\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tJournal_Printf( stream, \"+++++++++++++ 3D Tests +++++++++++++\\n\\n\" );\n\tijkSize[I_AXIS] = 3;\n\tijkSize[J_AXIS] = 3;\n\tijkSize[K_AXIS] = 3;\n\tperiodic[I_AXIS] = False;\n\tperiodic[J_AXIS] = False;\n\tperiodic[K_AXIS] = False;\n\tfor ( kk=False; kk < 2; kk++ ) {\n\t\tperiodic[K_AXIS] = kk;\n\n\t\tfor ( jj=False; jj < 2; jj++ ) {\n\t\t\tperiodic[J_AXIS] = jj;\n\n\t\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\t\tperiodic[I_AXIS] = ii;\n\t\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t\t}\n\t\t}\n\t}\t\n\n\tStg_Class_Delete( dictionary );\n\t\t\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn 0;\n}", "label": "int main( int argc, char* argv[] ) {\n\tDictionary*\tdictionary;\n\tMPI_Comm\tCommWorld;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\tBool            periodic[3] = { False, False, False };\n\tunsigned int    ii, jj, kk;\n\tStream*         stream;\n\tIJK             ijkSize;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\t\n\tstream = Journal_Register( Info_Type, argv[0] );\n\tStream_Enable( stream, True );\n\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\n\tJournal_Printf( stream, \"+++++++++++++ 1D Tests +++++++++++++\\n\\n\" );\n\tijkSize[I_AXIS] = 1;\n\tijkSize[J_AXIS] = 1;\n\tijkSize[K_AXIS] = 1;\n\tperiodic[I_AXIS] = False;\n\tperiodic[J_AXIS] = False;\n\tperiodic[K_AXIS] = False;\n\tfor ( ijkSize[I_AXIS] = 1; ijkSize[I_AXIS] <= 3; ijkSize[I_AXIS]++ ) {\n\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\tperiodic[I_AXIS] = ii;\n\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t}\t\n\t}\n\tijkSize[I_AXIS] = 1;\n\tperiodic[I_AXIS] = True;\t\n\n\tfor ( ijkSize[J_AXIS] = 2; ijkSize[J_AXIS] <= 3; ijkSize[J_AXIS]++ ) {\n\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\tperiodic[J_AXIS] = ii;\n\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t}\t\n\t}\n\n\tJournal_Printf( stream, \"+++++++++++++ 2D Tests +++++++++++++\\n\\n\" );\n\tijkSize[I_AXIS] = 1;\n\tijkSize[J_AXIS] = 1;\n\tijkSize[K_AXIS] = 1;\n\tperiodic[I_AXIS] = False;\n\tperiodic[J_AXIS] = False;\n\tperiodic[K_AXIS] = False;\n\tfor ( ijkSize[J_AXIS] = 2; ijkSize[J_AXIS] <= 3; ijkSize[J_AXIS]++ ) {\n\t\tfor ( ijkSize[I_AXIS] = 2; ijkSize[I_AXIS] <= 3; ijkSize[I_AXIS]++ ) {\n\t\t\tfor ( jj=False; jj < 2; jj++ ) {\n\t\t\t\tperiodic[J_AXIS] = jj;\n\t\t\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\t\t\tperiodic[I_AXIS] = ii;\n\t\t\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tJournal_Printf( stream, \"+++++++++++++ 3D Tests +++++++++++++\\n\\n\" );\n\tijkSize[I_AXIS] = 3;\n\tijkSize[J_AXIS] = 3;\n\tijkSize[K_AXIS] = 3;\n\tperiodic[I_AXIS] = False;\n\tperiodic[J_AXIS] = False;\n\tperiodic[K_AXIS] = False;\n\tfor ( kk=False; kk < 2; kk++ ) {\n\t\tperiodic[K_AXIS] = kk;\n\n\t\tfor ( jj=False; jj < 2; jj++ ) {\n\t\t\tperiodic[J_AXIS] = jj;\n\n\t\t\tfor ( ii=False; ii < 2; ii++ ) {\n\t\t\t\tperiodic[I_AXIS] = ii;\n\t\t\t\tTest_TestTopologyOfSize( dictionary, ijkSize, periodic, stream );\n\t\t\t}\n\t\t}\n\t}\t\n\n\tStg_Class_Delete( dictionary );\n\t\t\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "bjpop_863", "code": "int main(int argc, char *argv[])\n{\n    int myid, numprocs, i, j;\n    int size, align_size;\n    char *s_buf, *r_buf;\n    double t_start = 0.0, t_end = 0.0, t = 0.0;\n\n\n    align_size = getpagesize();\n    assert(align_size <= MAX_ALIGNMENT);\n\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    if(numprocs != 2) {\n        if(myid == 0) {\n            fprintf(stderr, \"This test requires exactly two processes\\n\");\n        }\n\n\n        return EXIT_FAILURE;\n    }\n\n    if(myid == 0) {\n        fprintf(stdout, \"# %s v%s\\n\", BENCHMARK, PACKAGE_VERSION);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH,\n                \"Bandwidth (MB/s)\");\n        fflush(stdout);\n    }\n\n    \n\n    for(size = 1; size <= MAX_MSG_SIZE; size *= 2) {\n        \n\n        for(i = 0; i < size; i++) {\n            s_buf[i] = 'a';\n            r_buf[i] = 'b';\n        }\n\n        if(size > large_message_size) {\n            loop = loop_large;\n            skip = skip_large;\n            window_size = window_size_large;\n        }\n\n        if(myid == 0) {\n            for(i = 0; i < loop + skip; i++) {\n                if(i == skip) {\n                    t_start =\n                }\n\n                for(j = 0; j < window_size; j++) {\n                }\n\n            }\n\n            t_end =\n            t = t_end - t_start;\n        }\n\n        else if(myid == 1) {\n            for(i = 0; i < loop + skip; i++) {\n                for(j = 0; j < window_size; j++) {\n                }\n\n            }\n        }\n\n        if(myid == 0) {\n            double tmp = size / 1e6 * loop * window_size;\n\n            fprintf(stdout, \"%-*d%*.*f\\n\", 10, size, FIELD_WIDTH,\n                    FLOAT_PRECISION, tmp / t);\n            fflush(stdout);\n        }\n    }\n\n\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char *argv[])\n{\n    int myid, numprocs, i, j;\n    int size, align_size;\n    char *s_buf, *r_buf;\n    double t_start = 0.0, t_end = 0.0, t = 0.0;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\n    align_size = getpagesize();\n    assert(align_size <= MAX_ALIGNMENT);\n\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    if(numprocs != 2) {\n        if(myid == 0) {\n            fprintf(stderr, \"This test requires exactly two processes\\n\");\n        }\n\n        MPI_Finalize();\n\n        return EXIT_FAILURE;\n    }\n\n    if(myid == 0) {\n        fprintf(stdout, \"# %s v%s\\n\", BENCHMARK, PACKAGE_VERSION);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH,\n                \"Bandwidth (MB/s)\");\n        fflush(stdout);\n    }\n\n    \n\n    for(size = 1; size <= MAX_MSG_SIZE; size *= 2) {\n        \n\n        for(i = 0; i < size; i++) {\n            s_buf[i] = 'a';\n            r_buf[i] = 'b';\n        }\n\n        if(size > large_message_size) {\n            loop = loop_large;\n            skip = skip_large;\n            window_size = window_size_large;\n        }\n\n        if(myid == 0) {\n            for(i = 0; i < loop + skip; i++) {\n                if(i == skip) {\n                    t_start = MPI_Wtime();\n                }\n\n                for(j = 0; j < window_size; j++) {\n                    MPI_Isend(s_buf, size, MPI_CHAR, 1, 100, MPI_COMM_WORLD,\n                            request + j);\n                }\n\n                MPI_Waitall(window_size, request, reqstat);\n                MPI_Recv(r_buf, 4, MPI_CHAR, 1, 101, MPI_COMM_WORLD,\n                        &reqstat[0]);\n            }\n\n            t_end = MPI_Wtime();\n            t = t_end - t_start;\n        }\n\n        else if(myid == 1) {\n            for(i = 0; i < loop + skip; i++) {\n                for(j = 0; j < window_size; j++) {\n                    MPI_Irecv(r_buf, size, MPI_CHAR, 0, 100, MPI_COMM_WORLD,\n                            request + j);\n                }\n\n                MPI_Waitall(window_size, request, reqstat);\n                MPI_Send(s_buf, 4, MPI_CHAR, 0, 101, MPI_COMM_WORLD);\n            }\n        }\n\n        if(myid == 0) {\n            double tmp = size / 1e6 * loop * window_size;\n\n            fprintf(stdout, \"%-*d%*.*f\\n\", 10, size, FIELD_WIDTH,\n                    FLOAT_PRECISION, tmp / t);\n            fflush(stdout);\n        }\n    }\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "yuwen41200_864", "code": "int main(int argc, char *argv[]) {\n\tlong long num, start, end, part;\n\tdouble width, x, sum, sum_par = 0;\n\tint rank, size;\n\n\n\tif (rank == 0)\n\t\tsscanf(argv[1], \"%llu\", &num);\n\n\n\tpart = (num - 1) / size;\n\tstart = 1 + rank * part;\n\tend = (rank != size - 1) ? (start + part) : num;\n\twidth = PI / num;\n\n\tfor (long long n = start; n < end ; n++) {\n\t\tx = (n - 0.5) * width;\n\t\tsum_par += sin(x);\n\t}\n\n\tif (rank == 0) {\n\t\tx = (num - 0.5) * width;\n\t\tsum_par += sin(x);\n\t}\n\n\n\tif (rank == 0)\n\t\tprintf(\"The total area is: %f.\\n\", (float) (sum * width));\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[]) {\n\tlong long num, start, end, part;\n\tdouble width, x, sum, sum_par = 0;\n\tint rank, size;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (rank == 0)\n\t\tsscanf(argv[1], \"%llu\", &num);\n\n\tMPI_Bcast(&num, 1, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n\n\tpart = (num - 1) / size;\n\tstart = 1 + rank * part;\n\tend = (rank != size - 1) ? (start + part) : num;\n\twidth = PI / num;\n\n\tfor (long long n = start; n < end ; n++) {\n\t\tx = (n - 0.5) * width;\n\t\tsum_par += sin(x);\n\t}\n\n\tif (rank == 0) {\n\t\tx = (num - 0.5) * width;\n\t\tsum_par += sin(x);\n\t}\n\n\tMPI_Reduce(&sum_par, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0)\n\t\tprintf(\"The total area is: %f.\\n\", (float) (sum * width));\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "recoli_865", "code": "int main( int argc, char *argv[] )\n   {\n\n\n      if ( argc != 4 )\n      {\n         printf(\"%s\\n\", \"Incorrect number of arguments (should be 3)!\");\n         printf(\"%s\\n\", \"1st argument: nFrame\");\n         printf(\"%s\\n\", \"2nd argument: nStart\");\n         printf(\"%s\\n\", \"3rd argument: temperature (in Kelvin)\");\n         exit(1);\n      }\n      \n      int nFrames, nStart;\n      nFrames = atoi(argv[1]);\n      nStart = atoi(argv[2]);\n      \n      double temperature;\n      temperature = atof(argv[3]); \n\n\n\n      char filename[10], command[256];\n      int master = 0;\n      int iproc, numprocs;\n      MPI_Status status;\n      time_t start_t, end_t;\n      int delta_time, delta_hour, delta_minute, delta_second;\n\n\n\n\n\n\n\n\n      if ( iproc == master )\n      {\n         printf ( \"*****************************************************\\n\" );\n         printf ( \"*  C program to compute surface tension of droplet  *\\n\" );\n         printf ( \"*           Xin Li, TheoChem & Biology, KTH         *\\n\" );\n         printf ( \"*               KTH, Stockholm, Sweden              *\\n\" );\n         printf ( \"*****************************************************\\n\" );\n\n         printf ( \"\\nThis program is based on the following papers:\\n\" );\n         printf ( \"   Thompson, et al., J. Chem. Phys., 1984, 81, 530-542.\\n\" );\n         printf ( \"   Brodskaya, et al., J. Colloid Interface Sci., 1996, 180, 86-97.\\n\" );\n         printf ( \"   Li, et al., J. Phys. Chem. Lett. 2010, 1, 769-773.\\n\" );\n\n         start_t = time(NULL);\n         printf ( \"\\nStep I: MPI execution\\n\" );\n         printf ( \"   There are %d frames; only the last %d frames will be used.\\n\", nFrames, nFrames-nStart );\n         printf ( \"   The temperature is %8.2f K\\n\", temperature );\n         printf ( \"<> Checking input files...\\n\" );\n         fflush(stdout);\n\n         system ( \"./check_input.x > check_input.log\" );\n         system ( \"cat check_input.log\" );\n\n         printf ( \"<> Job started at %s\", ctime(&start_t) );\n         printf ( \"   Number of processors: %d\\n\", numprocs );\n         printf ( \"   Running serial_calc_pres_dens.x on each processor...\\n\" );\n         fflush(stdout);\n      }\n\n\n      sprintf ( filename, \"%d\", iproc );\n\n\n      sprintf ( command, \"./serial_calc_pres_dens.x %d %d %f > data-%s.log\", \n                         nStart+(nFrames-nStart)/numprocs*(iproc+1), \n                         nStart+(nFrames-nStart)/numprocs*iproc, \n                         temperature, \n                         filename );\n      system ( command );\n\n\n\n\n      if ( iproc == master )\n      {\n         printf ( \"\\nStep II: Processing data\\n\" );\n         fflush(stdout);\n\n \n         sprintf ( command, \"./serial_process_data.x %d %f > surftens.dat\", \n                            numprocs, temperature );\n         system ( command );\n         system ( \"cat surftens.dat\" );\n\n         end_t = time(NULL);\n         printf ( \"\\n<> Job ended at %s\", ctime(&end_t) );\n\n         delta_time = (int)(difftime(end_t,start_t));\n         delta_hour = delta_time / 3600;\n         delta_minute = (delta_time - delta_hour*3600) / 60;\n         delta_second = delta_time - delta_hour*3600 - delta_minute*60;\n         printf ( \"The calculation used %d hours %d minutes %d seconds.\\n\", \n                  delta_hour, delta_minute, delta_second );\n         fflush(stdout);\n      }\n\n      return 0;\n\n   }", "label": "int main( int argc, char *argv[] )\n   {\n\n\n      if ( argc != 4 )\n      {\n         printf(\"%s\\n\", \"Incorrect number of arguments (should be 3)!\");\n         printf(\"%s\\n\", \"1st argument: nFrame\");\n         printf(\"%s\\n\", \"2nd argument: nStart\");\n         printf(\"%s\\n\", \"3rd argument: temperature (in Kelvin)\");\n         exit(1);\n      }\n      \n      int nFrames, nStart;\n      nFrames = atoi(argv[1]);\n      nStart = atoi(argv[2]);\n      \n      double temperature;\n      temperature = atof(argv[3]); \n\n\n\n      char filename[10], command[256];\n      int master = 0;\n      int iproc, numprocs;\n      MPI_Status status;\n      time_t start_t, end_t;\n      int delta_time, delta_hour, delta_minute, delta_second;\n\n\n      MPI_Init ( &argc, &argv );\n\n\n      MPI_Comm_size ( MPI_COMM_WORLD, &numprocs );\n\n\n      MPI_Comm_rank ( MPI_COMM_WORLD, &iproc );\n\n\n      if ( iproc == master )\n      {\n         printf ( \"*****************************************************\\n\" );\n         printf ( \"*  C program to compute surface tension of droplet  *\\n\" );\n         printf ( \"*           Xin Li, TheoChem & Biology, KTH         *\\n\" );\n         printf ( \"*               KTH, Stockholm, Sweden              *\\n\" );\n         printf ( \"*****************************************************\\n\" );\n\n         printf ( \"\\nThis program is based on the following papers:\\n\" );\n         printf ( \"   Thompson, et al., J. Chem. Phys., 1984, 81, 530-542.\\n\" );\n         printf ( \"   Brodskaya, et al., J. Colloid Interface Sci., 1996, 180, 86-97.\\n\" );\n         printf ( \"   Li, et al., J. Phys. Chem. Lett. 2010, 1, 769-773.\\n\" );\n\n         start_t = time(NULL);\n         printf ( \"\\nStep I: MPI execution\\n\" );\n         printf ( \"   There are %d frames; only the last %d frames will be used.\\n\", nFrames, nFrames-nStart );\n         printf ( \"   The temperature is %8.2f K\\n\", temperature );\n         printf ( \"<> Checking input files...\\n\" );\n         fflush(stdout);\n\n         system ( \"./check_input.x > check_input.log\" );\n         system ( \"cat check_input.log\" );\n\n         printf ( \"<> Job started at %s\", ctime(&start_t) );\n         printf ( \"   Number of processors: %d\\n\", numprocs );\n         printf ( \"   Running serial_calc_pres_dens.x on each processor...\\n\" );\n         fflush(stdout);\n      }\n\n\n      sprintf ( filename, \"%d\", iproc );\n\n\n      sprintf ( command, \"./serial_calc_pres_dens.x %d %d %f > data-%s.log\", \n                         nStart+(nFrames-nStart)/numprocs*(iproc+1), \n                         nStart+(nFrames-nStart)/numprocs*iproc, \n                         temperature, \n                         filename );\n      system ( command );\n\n\n      MPI_Finalize ( );\n\n\n      if ( iproc == master )\n      {\n         printf ( \"\\nStep II: Processing data\\n\" );\n         fflush(stdout);\n\n \n         sprintf ( command, \"./serial_process_data.x %d %f > surftens.dat\", \n                            numprocs, temperature );\n         system ( command );\n         system ( \"cat surftens.dat\" );\n\n         end_t = time(NULL);\n         printf ( \"\\n<> Job ended at %s\", ctime(&end_t) );\n\n         delta_time = (int)(difftime(end_t,start_t));\n         delta_hour = delta_time / 3600;\n         delta_minute = (delta_time - delta_hour*3600) / 60;\n         delta_second = delta_time - delta_hour*3600 - delta_minute*60;\n         printf ( \"The calculation used %d hours %d minutes %d seconds.\\n\", \n                  delta_hour, delta_minute, delta_second );\n         fflush(stdout);\n      }\n\n      return 0;\n\n   }"}
{"program": "ghisvail_866", "code": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_3d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  pfft_init();\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 2;\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[2] = 8;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_3d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_3d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_3d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_3d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= 3;\n\n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, data, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  pfft_free(data);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_3d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  MPI_Init(&argc, &argv);\n  pfft_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &rnk_self);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 2;\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[2] = 8;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_3d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_3d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_3d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_3d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= 3;\n\n  \n\n  MPI_Barrier(comm_cart_3d);\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, data, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(data);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "dantezhao_868", "code": "int main(void) {\n   char       greeting[MAX_STRING];  \n\n   int        comm_sz;               \n\n   int        my_rank;               \n\n\n   \n\n\n   \n\n\n   \n\n\n   if (my_rank != 0) { \n      \n\n      sprintf(greeting, \"Greetings from process %d of %d!\", \n            my_rank, comm_sz);\n      \n\n   } else {  \n      \n\n      printf(\"Hello! Greetings from process %d of %d!\\n\", my_rank, comm_sz);\n      for (int q = 1; q < comm_sz; q++) {\n         \n\n         \n\n         printf(\"%s\\n\", greeting);\n      } \n   }\n\n   \n\n\n   return 0;\n}", "label": "int main(void) {\n   char       greeting[MAX_STRING];  \n\n   int        comm_sz;               \n\n   int        my_rank;               \n\n\n   \n\n   MPI_Init(NULL, NULL); \n\n   \n\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_sz); \n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank); \n\n   if (my_rank != 0) { \n      \n\n      sprintf(greeting, \"Greetings from process %d of %d!\", \n            my_rank, comm_sz);\n      \n\n      MPI_Send(greeting, strlen(greeting)+1, MPI_CHAR, 0, 0,\n            MPI_COMM_WORLD); \n   } else {  \n      \n\n      printf(\"Hello! Greetings from process %d of %d!\\n\", my_rank, comm_sz);\n      for (int q = 1; q < comm_sz; q++) {\n         \n\n         MPI_Recv(greeting, MAX_STRING, MPI_CHAR, q,\n            0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         \n\n         printf(\"%s\\n\", greeting);\n      } \n   }\n\n   \n\n   MPI_Finalize(); \n\n   return 0;\n}"}
{"program": "byu-vv-lab_869", "code": "int main(int argc, char * argv[]) \n{ \n    int rank;\n    int procs;\n    int* sendBuf;\n    int* rcvBuf;\n\n    \n    if (rank == 0) {\n      sendBuf = (int*)malloc(sizeof(int)*procs);\n      rcvBuf = (int*)malloc(sizeof(int)*procs);\n      for(int i=0; i < procs; i++)\n\tsendBuf[i] = i;\n    }else{\n      sendBuf = (int*)malloc(sizeof(int));\n      rcvBuf = (int*)malloc(sizeof(int));\n    }\n\n#ifdef ROOT\n    if(rank != 2)\n    else\n#elif defined ORDER\n    if(rank%2)\n    else\n#else\n#endif\n    \n    if(rank != 0){\n      *sendBuf = *rcvBuf + rank;\n    }\n\n#ifdef ORDER\n    if(rank%2)\n    else\n#else\n#endif\n    \n    if(rank == 0){\n      for(int i=0; i<procs; i++){\n\tprintf(\"sendBuf[%d]=%d, rcvBuf[%d]=%d\\n\", i, sendBuf[i], i, rcvBuf[i]);\n\tassert(sendBuf[i]*2 == rcvBuf[i]);\n      }\n    }\n    free(sendBuf);\n    free(rcvBuf);\n    return 0; \n}", "label": "int main(int argc, char * argv[]) \n{ \n    int rank;\n    int procs;\n    int* sendBuf;\n    int* rcvBuf;\n\n    MPI_Init(&argc,&argv); \n    MPI_Comm_size(MPI_COMM_WORLD, &procs); \n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); \n    \n    if (rank == 0) {\n      sendBuf = (int*)malloc(sizeof(int)*procs);\n      rcvBuf = (int*)malloc(sizeof(int)*procs);\n      for(int i=0; i < procs; i++)\n\tsendBuf[i] = i;\n    }else{\n      sendBuf = (int*)malloc(sizeof(int));\n      rcvBuf = (int*)malloc(sizeof(int));\n    }\n\n#ifdef ROOT\n    if(rank != 2)\n        MPI_Scatter(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Scatter(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 2, MPI_COMM_WORLD);\n#elif defined ORDER\n    if(rank%2)\n        MPI_Scatter(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Gather(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n#else\n    MPI_Scatter(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n#endif\n    \n    if(rank != 0){\n      *sendBuf = *rcvBuf + rank;\n    }\n\n#ifdef ORDER\n    if(rank%2)\n        MPI_Gather(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Scatter(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n#else\n    MPI_Gather(sendBuf, 1, MPI_INT, rcvBuf, 1, MPI_INT, 0, MPI_COMM_WORLD);\n#endif\n    \n    if(rank == 0){\n      for(int i=0; i<procs; i++){\n\tprintf(\"sendBuf[%d]=%d, rcvBuf[%d]=%d\\n\", i, sendBuf[i], i, rcvBuf[i]);\n\tassert(sendBuf[i]*2 == rcvBuf[i]);\n      }\n    }\n    free(sendBuf);\n    free(rcvBuf);\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "jiangnanHugo_871", "code": "int main(int argc,char* argv[]){\n\tint *source,*sample;\n\tint size=1000,p;\n\tint myid,numprocs;\n\tint sample_len;\n\tint w;\n\tint * main_item,*items;\n\tMPI_Status status;\n\tp=numprocs-1;\n\tsample_len=size/p;\n\tw=sample_len/p;\n\t\n\tif(myid==0){\n\t\tsource=(int *)malloc(size*sizeof(int));\n\t\tgenerate_data(source,size);\n\t\tsend_data(source,p,sample_len);\n\t}else{\n\t\tsample=(int*)malloc(sample_len*sizeof(int));\n\t\treceive_data(sample,sample_len);\n\t}\n\tif(myid==0){\n\t\tint i=0;\n\t\tmain_item=fetch_represent(p);\n\t\t\n\n\t\tsend_main_item(main_item,p-1,p);\n\t}else{\n\t\titems=(int*)malloc((p-1)*sizeof(int));\n\t\tselect_represent(sample,sample_len,w,p);\n\t\n\n\t\treceive_main_item(items,p-1);\n\t}\n\tif(myid!=0){\n\t\tsplit(sample,sample_len,items,p-1);\n\t\tprintf(\"this is threads:%d\",myid);\n\t\t\n\n\t\tprintf(\"\\n\");\n\t}\n\tif(myid!=0){\n\t\tfetch_merge(numprocs);\n\t\t\n\t}\n\tif(myid==0){\n\t\tprintf(\"I am your admin\\n\");\n\t\tcollect_result(p);\n\t}\n\t\n}", "label": "int main(int argc,char* argv[]){\n\tint *source,*sample;\n\tint size=1000,p;\n\tint myid,numprocs;\n\tint sample_len;\n\tint w;\n\tint * main_item,*items;\n\tMPI_Status status;\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\tMPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n\tp=numprocs-1;\n\tsample_len=size/p;\n\tw=sample_len/p;\n\t\n\tif(myid==0){\n\t\tsource=(int *)malloc(size*sizeof(int));\n\t\tgenerate_data(source,size);\n\t\tsend_data(source,p,sample_len);\n\t}else{\n\t\tsample=(int*)malloc(sample_len*sizeof(int));\n\t\treceive_data(sample,sample_len);\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif(myid==0){\n\t\tint i=0;\n\t\tmain_item=fetch_represent(p);\n\t\t\n\n\t\tsend_main_item(main_item,p-1,p);\n\t}else{\n\t\titems=(int*)malloc((p-1)*sizeof(int));\n\t\tselect_represent(sample,sample_len,w,p);\n\t\n\n\t\treceive_main_item(items,p-1);\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif(myid!=0){\n\t\tsplit(sample,sample_len,items,p-1);\n\t\tprintf(\"this is threads:%d\",myid);\n\t\t\n\n\t\tprintf(\"\\n\");\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif(myid!=0){\n\t\tfetch_merge(numprocs);\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t\t\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD);\t\n\tif(myid==0){\n\t\tprintf(\"I am your admin\\n\");\n\t\tcollect_result(p);\n\t}\n\t\n\tMPI_Finalize();\n}"}
{"program": "jeffhammond_872", "code": "int main(int argc, char ** argv)\n{\n\n    int np, me;\n\n    int rlen = 0;\n    char pname[MPI_MAX_PROCESSOR_NAME+1] = {0};\n\n    printf(\"Hello from %d of %d processors (name=%s)\\n\", me, np, pname);\n\n    \n\n    MPI_Comm comm_shared = MPI_COMM_NULL;\n\n    int localnp, localme;\n\n    \n\n    MPI_Aint nbytes = (argc>1) ? atol(argv[1]) : 1000;\n    MPI_Win win_shared = MPI_WIN_NULL;\n    void * baseptr = NULL;\n\n    for (int i=0; i<localnp; i++) {\n        int rank = i;\n        MPI_Aint lsize = 0;\n        int ldisp = 0;\n        void * lbase = NULL;\n        printf(\"global %d of %d, local %d of %d, size=%zu, disp=%d, base=%p\\n\",\n                me, np, localme, localnp, lsize, ldisp, lbase);\n    }\n\n\n\n\n    return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n    MPI_Init(&argc, &argv);\n\n    int np, me;\n    MPI_Comm_size(MPI_COMM_WORLD,&np);\n    MPI_Comm_rank(MPI_COMM_WORLD,&me);\n\n    int rlen = 0;\n    char pname[MPI_MAX_PROCESSOR_NAME+1] = {0};\n    MPI_Get_processor_name(pname, &rlen);\n\n    printf(\"Hello from %d of %d processors (name=%s)\\n\", me, np, pname);\n\n    \n\n    MPI_Comm comm_shared = MPI_COMM_NULL;\n    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_shared);\n\n    int localnp, localme;\n    MPI_Comm_size(comm_shared,&localnp);\n    MPI_Comm_rank(comm_shared,&localme);\n\n    \n\n    MPI_Aint nbytes = (argc>1) ? atol(argv[1]) : 1000;\n    MPI_Win win_shared = MPI_WIN_NULL;\n    void * baseptr = NULL;\n    MPI_Win_allocate_shared(nbytes, 1, MPI_INFO_NULL, comm_shared, &baseptr, &win_shared);\n\n    for (int i=0; i<localnp; i++) {\n        int rank = i;\n        MPI_Aint lsize = 0;\n        int ldisp = 0;\n        void * lbase = NULL;\n        MPI_Win_shared_query(win_shared, rank, &lsize, &ldisp, &lbase);\n        printf(\"global %d of %d, local %d of %d, size=%zu, disp=%d, base=%p\\n\",\n                me, np, localme, localnp, lsize, ldisp, lbase);\n    }\n\n\n\n    MPI_Win_free(&win_shared);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ClaudioNahmad_875", "code": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    int rank, len, err, i;\n    int errs=0, toterrs;\n    char *filename;\n\n\n\n\n    if (!rank) {\n        i = 1;\n        while ((i < argc) && strcmp(\"-fname\", *argv)) {\n            i++;\n            argv++;\n        }\n        if (i >= argc) {\n            fprintf(stderr, \"\\n*#  Usage: excl -fname filename\\n\\n\");\n        }\n        argv++;\n        len = strlen(*argv);\n        filename = (char *) malloc(len+10);\n        strcpy(filename, *argv);\n    }\n    else {\n        filename = (char *) malloc(len+10);\n    }\n\n\n\n    \n\n    err =\n    if (err != MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open failed when it should have succeeded\\n\", rank);\n    }\n\n\n    \n\n    err =\n    if (err == MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open succeeded when it should have failed\\n\", rank);\n    }\n\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(filename);\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    int rank, len, err, i;\n    int errs=0, toterrs;\n    char *filename;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n\n    if (!rank) {\n        i = 1;\n        while ((i < argc) && strcmp(\"-fname\", *argv)) {\n            i++;\n            argv++;\n        }\n        if (i >= argc) {\n            fprintf(stderr, \"\\n*#  Usage: excl -fname filename\\n\\n\");\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n        argv++;\n        len = strlen(*argv);\n        filename = (char *) malloc(len+10);\n        strcpy(filename, *argv);\n        MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        filename = (char *) malloc(len+10);\n        MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n\n    if (!rank) MPI_File_delete(filename, MPI_INFO_NULL);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    err = MPI_File_open(MPI_COMM_WORLD, filename,\n         MPI_MODE_CREATE | MPI_MODE_EXCL | MPI_MODE_RDWR, MPI_INFO_NULL , &fh);\n    if (err != MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open failed when it should have succeeded\\n\", rank);\n    }\n    else MPI_File_close(&fh);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    err = MPI_File_open(MPI_COMM_WORLD, filename,\n         MPI_MODE_CREATE | MPI_MODE_EXCL | MPI_MODE_RDWR, MPI_INFO_NULL , &fh);\n    if (err == MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open succeeded when it should have failed\\n\", rank);\n    }\n\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(filename);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "syftalent_876", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = double_int_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in double_int test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = double_int_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in double_int test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "airqinc_878", "code": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n  const int ROWS_MULTIPLIER = 2;                                              \n\n  const int COLUMNS = 5;\n\n\n  int world_rank, world_size;\n\n  int rows = world_size * ROWS_MULTIPLIER;\n  int data_per_process = COLUMNS * ROWS_MULTIPLIER;                           \n\n  float *a = NULL;\n  float *b = NULL;\n  float *c = NULL;\n\n  float* ap = (float *) malloc(\n);              \n\n  float* bp = (float *) malloc(\n);\n  float* cp = (float *) malloc(\n);\n\n  if (world_rank == 0) {\n    a = create_rands_matrix(rows, COLUMNS, MAX_RANDOM_VAL, world_rank);\n    dumpMatrix(a, rows, COLUMNS, \"a\");\n    b = create_rands_matrix(rows, COLUMNS, MAX_RANDOM_VAL, world_rank + 1000);\n    dumpMatrix(b, rows, COLUMNS, \"b\");\n    c = (float *)malloc(\n);\n\n    \n\n    \n\n\n    for(int i = 0; i < \n; i++){\n      \n\n    }\n\n    \n\n\n    dumpMatrix(c, rows, COLUMNS, \"c = a + b\");\n  }\n  else{\n    \n\n    \n\n\n    for(int i = 0; i < \n; i++){\n      \n\n    }\n    \n\n\n  }\n\n  if (world_rank == 0) {\n    free(a);\n    free(b);\n    free(c);\n  }\n  free(ap);\n  free(bp);\n  free(cp);\n\n  return 0;\n}\n", "label": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n  const int ROWS_MULTIPLIER = 2;                                              \n\n  const int COLUMNS = 5;\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank, world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int rows = world_size * ROWS_MULTIPLIER;\n  int data_per_process = COLUMNS * ROWS_MULTIPLIER;                           \n\n  float *a = NULL;\n  float *b = NULL;\n  float *c = NULL;\n\n  float* ap = (float *) malloc(\n);              \n\n  float* bp = (float *) malloc(\n);\n  float* cp = (float *) malloc(\n);\n\n  if (world_rank == 0) {\n    a = create_rands_matrix(rows, COLUMNS, MAX_RANDOM_VAL, world_rank);\n    dumpMatrix(a, rows, COLUMNS, \"a\");\n    b = create_rands_matrix(rows, COLUMNS, MAX_RANDOM_VAL, world_rank + 1000);\n    dumpMatrix(b, rows, COLUMNS, \"b\");\n    c = (float *)malloc(\n);\n\n    \n\n    \n\n\n    for(int i = 0; i < \n; i++){\n      \n\n    }\n\n    \n\n\n    dumpMatrix(c, rows, COLUMNS, \"c = a + b\");\n  }\n  else{\n    \n\n    \n\n\n    for(int i = 0; i < \n; i++){\n      \n\n    }\n    \n\n\n  }\n\n  if (world_rank == 0) {\n    free(a);\n    free(b);\n    free(c);\n  }\n  free(ap);\n  free(bp);\n  free(cp);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n  return 0;\n}\n"}
{"program": "RafaelDexter_879", "code": "int main(int argc, char *argv[]) \n{\n  int numprocs,myid,ID;\n  int i,NN; \n  int *Num_Snd,*Num_Rcv;\n\n  double **SndA;\n  double **RcvA;\n  double TStime,TEtime;\n  double sum;\n\n  MPI_Status stat;\n  MPI_Request request;\n\n\n  NN = 3;\n\n  \n\n  \n  Num_Snd = (int*)malloc(sizeof(int)*numprocs);\n  Num_Rcv = (int*)malloc(sizeof(int)*numprocs);\n  \n  \n\n\n  for (ID=0; ID<numprocs; ID++){\n    Num_Snd[ID] = 0; \n    Num_Rcv[ID] = 0; \n  }\n\n  for (i=0; i<NN; i++){\n    printf(\"%3d\\n\",rnd(numprocs));  \n  }  \n\n\n\n  \n  \n\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n  \n\n\n  \n\n\n}", "label": "int main(int argc, char *argv[]) \n{\n  int numprocs,myid,ID;\n  int i,NN; \n  int *Num_Snd,*Num_Rcv;\n\n  double **SndA;\n  double **RcvA;\n  double TStime,TEtime;\n  double sum;\n\n  MPI_Status stat;\n  MPI_Request request;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n  NN = 3;\n\n  \n\n  \n  Num_Snd = (int*)malloc(sizeof(int)*numprocs);\n  Num_Rcv = (int*)malloc(sizeof(int)*numprocs);\n  \n  \n\n\n  for (ID=0; ID<numprocs; ID++){\n    Num_Snd[ID] = 0; \n    Num_Rcv[ID] = 0; \n  }\n\n  for (i=0; i<NN; i++){\n    printf(\"%3d\\n\",rnd(numprocs));  \n  }  \n\n\n\n  \n  \n\n\n\n\n\n\n\n  \n\n\n\n  \n\n\n  \n\n\n  \n\n\n  MPI_Finalize();\n}"}
{"program": "autarchprinceps_880", "code": "int main(int argc, char** argv) {\n\n\tresultset* log = malloc(sizeof(resultset));\n\tpp(0, 16384, 256, log);\n\tif(rank == 0) {\n\t\tprintRS(\"pp.0-2_14.csv\", log);\n\t}\n\tpp(16384, 16777216, 261888, log);\n\tif(rank == 0) {\n\t\tprintRS(\"pp.2_14-2_24.csv\", log);\n\t}\n\n}", "label": "int main(int argc, char** argv) {\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &n);\n\n\tresultset* log = malloc(sizeof(resultset));\n\tpp(0, 16384, 256, log);\n\tif(rank == 0) {\n\t\tprintRS(\"pp.0-2_14.csv\", log);\n\t}\n\tpp(16384, 16777216, 261888, log);\n\tif(rank == 0) {\n\t\tprintRS(\"pp.2_14-2_24.csv\", log);\n\t}\n\n\tMPI_Finalize();\n}"}
{"program": "sya-begleitmaterial_882", "code": "int main(int argc, char *argv[]) {\n\n  \n\n  \n\n  int myid,nprocs,length;\n  long int npts = 1e5;\n  long inside = 0;\n  long sum = 0;\n  long int i,mynpts;\n  double x, y, f;\n  char name[BUFSIZ];  \n  \n  \n\n  \n\n  \n\n  \n\n      \n\n  \n\n  if (myid == 0) {\n    mynpts = npts - (nprocs-1)*(npts/nprocs);\n  } else {\n    \n\n    mynpts = npts/nprocs;\n  }\n\n  \n\n  srand(time(NULL));\n  printf(\"Berechnung %d l\u00e4uft auf Knoten %s\\n\", myid, name);\n\n  \n\n  for (i=0; i<mynpts; i++) {\n    x = (double)rand()/RAND_MAX;\n    y = (double)rand()/RAND_MAX;\n    if ((x*x + y*y) <= 1) {\n        ++inside;\n    }\n  }\n\n  \n\n  \n  \n\n  \n\n  if (myid == 0) {\n    f = 4.0 * (double)sum/npts;\n    printf(\"PI calculated with %ld points = %f \\n\", npts, f);\n  }\n\n  \n\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n\n  \n\n  \n\n  int myid,nprocs,length;\n  long int npts = 1e5;\n  long inside = 0;\n  long sum = 0;\n  long int i,mynpts;\n  double x, y, f;\n  char name[BUFSIZ];  \n  \n  \n\n  MPI_Init(&argc,&argv);\n  \n\n  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n  \n\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n  \n\n  MPI_Get_processor_name(name, &length);\n      \n\n  \n\n  if (myid == 0) {\n    mynpts = npts - (nprocs-1)*(npts/nprocs);\n  } else {\n    \n\n    mynpts = npts/nprocs;\n  }\n\n  \n\n  srand(time(NULL));\n  printf(\"Berechnung %d l\u00e4uft auf Knoten %s\\n\", myid, name);\n\n  \n\n  for (i=0; i<mynpts; i++) {\n    x = (double)rand()/RAND_MAX;\n    y = (double)rand()/RAND_MAX;\n    if ((x*x + y*y) <= 1) {\n        ++inside;\n    }\n  }\n\n  \n\n  MPI_Reduce(&inside, &sum, 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  \n  \n\n  \n\n  if (myid == 0) {\n    f = 4.0 * (double)sum/npts;\n    printf(\"PI calculated with %ld points = %f \\n\", npts, f);\n  }\n\n  \n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "cenezaraujo_884", "code": "int main (int argc, char *argv[]) {\n   int rank, p, size = 8, tag = 0;\n   int left, right;\n   char send_buffer1[8], recv_buffer1[8];\n   char send_buffer2[8], recv_buffer2[8];\n   MPI_Status status;\n   MPI_Request send_request1, send_request2, recv_request1, recv_request2;\n\n   left = (rank-1 + p) % p;\n   right = (rank + 1) %p;\n\n   sprintf(send_buffer1, \"N:%d\\n\", rank);   \n   sprintf(send_buffer2, \"N:%d\\n\", rank);   \n\n   \n\n\n\n   printf(\"-------------------------\\n\");\n   printf(\"Processo %s\", send_buffer1);\n   printf(\"Vizinho Direita %s\", recv_buffer1);\n   printf(\"Vizinho Esquerda %s\", recv_buffer2);\n   printf(\"-------------------------\\n\");\n\n   return 0;\n}", "label": "int main (int argc, char *argv[]) {\n   int rank, p, size = 8, tag = 0;\n   int left, right;\n   char send_buffer1[8], recv_buffer1[8];\n   char send_buffer2[8], recv_buffer2[8];\n   MPI_Status status;\n   MPI_Request send_request1, send_request2, recv_request1, recv_request2;\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n   left = (rank-1 + p) % p;\n   right = (rank + 1) %p;\n\n   sprintf(send_buffer1, \"N:%d\\n\", rank);   \n   sprintf(send_buffer2, \"N:%d\\n\", rank);   \n\n   MPI_Isend(send_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD, &send_request1);\n   MPI_Irecv(recv_buffer1, size, MPI_CHAR, right, tag, MPI_COMM_WORLD, &recv_request1);\n   \n   MPI_Isend(send_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD, &send_request2);\n   MPI_Irecv(recv_buffer2, size, MPI_CHAR, left, tag, MPI_COMM_WORLD, &recv_request2);\n\n   MPI_Wait(&send_request1, &status);\n   MPI_Wait(&recv_request1, &status);\n   MPI_Wait(&send_request2, &status);\n   MPI_Wait(&recv_request2, &status);\n\n\n   printf(\"-------------------------\\n\");\n   printf(\"Processo %s\", send_buffer1);\n   printf(\"Vizinho Direita %s\", recv_buffer1);\n   printf(\"Vizinho Esquerda %s\", recv_buffer2);\n   printf(\"-------------------------\\n\");\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "likesmusik_886", "code": "int\nmain(int argc, char** argv) {\n    struct options options;\n    struct calculation_arguments arguments;\n    struct calculation_results results;\n\n    \n\n    options.rank = -1;\n    options.size = -1;\n\n    \n\n\n    \n\n    AskParams(&options, argc, argv);\n\n    \n\n    initVariables(&arguments, &results, &options);\n    allocateMatrices(&arguments);\n    initMatrices(&arguments, &options);\n\n    gettimeofday(&start_time, NULL); \n\n    if (options.method == METH_JACOBI) {\n        \n\n    } else {\n        \n\n    }\n    gettimeofday(&comp_time, NULL); \n\n\n    \n    \n\n    if (options.rank <= 0) {\n        displayStatistics(&arguments, &results, &options);\n    }\n\n    \n\n\n    \n\n\n    \n\n    freeMatrices(&arguments); \n\n\n    return 0;\n}", "label": "int\nmain(int argc, char** argv) {\n    struct options options;\n    struct calculation_arguments arguments;\n    struct calculation_results results;\n\n    \n\n    options.rank = -1;\n    options.size = -1;\n\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &(options.rank));\n    MPI_Comm_size(MPI_COMM_WORLD, &(options.size));\n\n    \n\n    AskParams(&options, argc, argv);\n\n    \n\n    initVariables(&arguments, &results, &options);\n    allocateMatrices(&arguments);\n    initMatrices(&arguments, &options);\n\n    gettimeofday(&start_time, NULL); \n\n    if (options.method == METH_JACOBI) {\n        \n\n        MPI_calculateJacobi(&arguments, &results, &options); \n    } else {\n        \n\n        MPI_calculateGaussSeidel(&arguments, &results, &options); \n    }\n    gettimeofday(&comp_time, NULL); \n\n\n    \n    \n\n    if (options.rank <= 0) {\n        displayStatistics(&arguments, &results, &options);\n    }\n\n    \n\n    MPI_DisplayMatrix(&arguments, &results, &options, options.rank, options.size, arguments.row_start + 1, arguments.row_end - 1);\n\n    \n\n    MPI_Finalize();\n\n    \n\n    freeMatrices(&arguments); \n\n\n    return 0;\n}"}
{"program": "mtg79_887", "code": "int main(int argc, char** argv)\n{\n    int n    = 200;            \n\n    double p = 0.05;           \n\n    const char* ifname = \"adj\"; \n\n    const char* ofname = \"dist\"; \n\n\n    \n\n    extern char* optarg;\n    const char* optstring = \"hn:d:p:o:i:\";\n    int c;\n    while ((c = getopt(argc, argv, optstring)) != -1) {\n        switch (c) {\n        case 'h':\n            fprintf(stderr, \"%s\", usage);\n            return -1;\n        case 'n': n = atoi(optarg); break;\n        case 'p': p = atof(optarg); break;\n        case 'o': ofname = optarg;  break;\n        case 'i': ifname = optarg;  break;\n        }\n    }\n\n    \n\n    int* l = gen_graph(n, p);\n    if (ifname)\n        write_matrix(ifname,  n, l);\n\n    \n\n    double t0 =\n    shortest_paths(n, l, argc, argv);\n    double t1 =\n    int myrank;\n    if(myrank==0) {\n\t\tint threads;\n\t\tprintf(\"== MPI with %d threads\\n\", threads);\n\t\tprintf(\"n:     %d\\n\", n);\n\t\tprintf(\"p:     %g\\n\", p);\n\t\tprintf(\"Time:  %g\\n\", t1-t0);\n\t\tprintf(\"Check: %X\\n\", fletcher16(l, n*n));\n\n\t\t\n\n\t\tif (ofname)\n\t\t\twrite_matrix(ofname, n, l);\n\t\t\n\t\t\n\n\t\tfree(l);\n    }\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n    int n    = 200;            \n\n    double p = 0.05;           \n\n    const char* ifname = \"adj\"; \n\n    const char* ofname = \"dist\"; \n\n\n    \n\n    extern char* optarg;\n    const char* optstring = \"hn:d:p:o:i:\";\n    int c;\n    while ((c = getopt(argc, argv, optstring)) != -1) {\n        switch (c) {\n        case 'h':\n            fprintf(stderr, \"%s\", usage);\n            return -1;\n        case 'n': n = atoi(optarg); break;\n        case 'p': p = atof(optarg); break;\n        case 'o': ofname = optarg;  break;\n        case 'i': ifname = optarg;  break;\n        }\n    }\n\n    \n\n    int* l = gen_graph(n, p);\n    if (ifname)\n        write_matrix(ifname,  n, l);\n\n    \n\n    MPI_Init(&argc, &argv);\n    double t0 = MPI_Wtime();\n    shortest_paths(n, l, argc, argv);\n    double t1 = MPI_Wtime();\n    int myrank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    if(myrank==0) {\n\t\tint threads;\n\t\tMPI_Comm_size(MPI_COMM_WORLD, &threads);  \n\t\tprintf(\"== MPI with %d threads\\n\", threads);\n\t\tprintf(\"n:     %d\\n\", n);\n\t\tprintf(\"p:     %g\\n\", p);\n\t\tprintf(\"Time:  %g\\n\", t1-t0);\n\t\tprintf(\"Check: %X\\n\", fletcher16(l, n*n));\n\n\t\t\n\n\t\tif (ofname)\n\t\t\twrite_matrix(ofname, n, l);\n\t\t\n\t\t\n\n\t\tfree(l);\n    }\n    MPI_Finalize();  \n    return 0;\n}"}
{"program": "pyrovski_888", "code": "int main(int argc, char* argv[]) {\n  int id=0, numprocs;\n  int color;\n  MPI_Comm local;\n  double t1, t2;\n\n  int provided = 0;\n#ifdef _OPENMP\n  const int required = MPI_THREAD_FUNNELED;\n  int ierr =\n  if (ierr != MPI_SUCCESS) {\n      printf(\"Abort: MPI_Init_thread unsuccessful: %s\\n\", strerror(errno));\n  }\n#else\n  int ierr =\n  if (ierr != MPI_SUCCESS) {\n      printf(\"Abort: MPI_Init unsuccessful: %s\\n\", strerror(errno));\n  }\n#endif\n\n\n  if (id == 0) {\n      int version, subversion;\n      printf(\"%d MPI-%d.%d#%d process(es)\", numprocs, version, subversion, provided);\n#ifdef _OPENMP\n      printf(\" with %d OpenMP-%u thread(s)/process on %d processors\",\n            omp_get_max_threads(), _OPENMP, omp_get_num_procs());\n      if (provided < required)\n          printf(\"\\nWarning: MPI_Init_thread only provided level %d<%d!\", provided, required);\n#endif\n      printf(\"\\n\");\n  } else sleep(1);\n\n  printf(\"%03d: ctest start\\n\", id);\n  color = (id >= numprocs/2);\n\n  t1 =\n  parallel(MPI_COMM_WORLD);\n  parallel(local);\n  parallel(MPI_COMM_WORLD);\n  t2 =\n\n  printf(\"%03d: ctest end (%12.9f)\\n\", id, (t2-t1));\n\n  return 0;\n}", "label": "int main(int argc, char* argv[]) {\n  int id=0, numprocs;\n  int color;\n  MPI_Comm local;\n  double t1, t2;\n\n  int provided = 0;\n#ifdef _OPENMP\n  const int required = MPI_THREAD_FUNNELED;\n  int ierr = MPI_Init_thread(&argc, &argv, required, &provided);\n  if (ierr != MPI_SUCCESS) {\n      printf(\"Abort: MPI_Init_thread unsuccessful: %s\\n\", strerror(errno));\n      MPI_Abort(MPI_COMM_WORLD, ENOSYS);\n  }\n#else\n  int ierr = MPI_Init(&argc, &argv);\n  if (ierr != MPI_SUCCESS) {\n      printf(\"Abort: MPI_Init unsuccessful: %s\\n\", strerror(errno));\n      MPI_Abort(MPI_COMM_WORLD, ENOSYS);\n  }\n#endif\n\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n\n  if (id == 0) {\n      int version, subversion;\n      MPI_Get_version(&version, &subversion);\n      printf(\"%d MPI-%d.%d#%d process(es)\", numprocs, version, subversion, provided);\n#ifdef _OPENMP\n      printf(\" with %d OpenMP-%u thread(s)/process on %d processors\",\n            omp_get_max_threads(), _OPENMP, omp_get_num_procs());\n      if (provided < required)\n          printf(\"\\nWarning: MPI_Init_thread only provided level %d<%d!\", provided, required);\n#endif\n      printf(\"\\n\");\n  } else sleep(1);\n\n  printf(\"%03d: ctest start\\n\", id);\n  color = (id >= numprocs/2);\n  MPI_Comm_split(MPI_COMM_WORLD, color, id, &local);\n\n  t1 = MPI_Wtime();\n  parallel(MPI_COMM_WORLD);\n  parallel(local);\n  parallel(MPI_COMM_WORLD);\n  t2 = MPI_Wtime();\n\n  MPI_Comm_free(&local);\n  MPI_Finalize();\n  printf(\"%03d: ctest end (%12.9f)\\n\", id, (t2-t1));\n\n  return 0;\n}"}
{"program": "gnu3ra_890", "code": "int main(int argc, char ** argv) {\n  int rank, nproc, i, j;\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCI mutex test with %d processes\\n\", nproc);\n\n  ARMCI_Create_mutexes(NUM_MUTEXES);\n\n  for (i = 0; i < nproc; i++)\n    for (j = 0; j < NUM_MUTEXES; j++) {\n      ARMCI_Lock(  j, (rank+i)%nproc);\n      ARMCI_Unlock(j, (rank+i)%nproc);\n    }\n\n  printf(\" + %3d done\\n\", rank);\n  fflush(NULL);\n\n  ARMCI_Destroy_mutexes();\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int rank, nproc, i, j;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCI mutex test with %d processes\\n\", nproc);\n\n  ARMCI_Create_mutexes(NUM_MUTEXES);\n\n  for (i = 0; i < nproc; i++)\n    for (j = 0; j < NUM_MUTEXES; j++) {\n      ARMCI_Lock(  j, (rank+i)%nproc);\n      ARMCI_Unlock(j, (rank+i)%nproc);\n    }\n\n  printf(\" + %3d done\\n\", rank);\n  fflush(NULL);\n\n  ARMCI_Destroy_mutexes();\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "afarah1_892", "code": "int\nmain(int argc, char **argv)\n{\n  int rank, size;\n  if (size != MASTER + 1) {\n    if (!rank)\n      fprintf(stderr, \"Expected -np %d\\n\", MASTER + 1);\n  }\n  char buffer[SYNCSIZE];\n  if (rank == MASTER) {\n    MPI_Request r, r2;\n    \n\n    \n\n    \n\n    \n\n    \n\n  } else if (rank == SLAVE1) {\n    MPI_Request r;\n    \n\n    \n\n    \n\n  } else if (rank == SLAVE2) {\n    \n\n  }\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  if (size != MASTER + 1) {\n    if (!rank)\n      fprintf(stderr, \"Expected -np %d\\n\", MASTER + 1);\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n  char buffer[SYNCSIZE];\n  if (rank == MASTER) {\n    MPI_Request r, r2;\n    \n\n    MPI_Send(buffer, SYNCSIZE, MPI_CHAR, SLAVE2, TAG, MPI_COMM_WORLD);\n    \n\n    MPI_Send(buffer, ASYNCSIZE, MPI_CHAR, SLAVE1, TAG, MPI_COMM_WORLD);\n    \n\n    MPI_Isend(buffer, SYNCSIZE, MPI_CHAR, SLAVE1, TAG, MPI_COMM_WORLD, &r);\n    \n\n    MPI_Isend(buffer, ASYNCSIZE, MPI_CHAR, SLAVE1, TAG, MPI_COMM_WORLD, &r2);\n    \n\n    MPI_Wait(&r, MPI_STATUS_IGNORE);\n  } else if (rank == SLAVE1) {\n    MPI_Request r;\n    \n\n    MPI_Irecv(buffer, ASYNCSIZE, MPI_CHAR, MASTER, TAG, MPI_COMM_WORLD, &r);\n    \n\n    MPI_Recv(buffer, SYNCSIZE, MPI_CHAR, MASTER, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(buffer, ASYNCSIZE, MPI_CHAR, MASTER, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    \n\n    MPI_Wait(&r, MPI_STATUS_IGNORE);\n  } else if (rank == SLAVE2) {\n    \n\n    MPI_Recv(buffer, SYNCSIZE, MPI_CHAR, MASTER, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  MPI_Finalize();\n}"}
{"program": "bmi-forum_893", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tDictionary* dictionary;\n\tTopology*\t\t\tnTopology;\n\tElementLayout*\t\t\teLayout;\n\tNodeLayout*\t\t\tnLayout;\n\tMeshDecomp*\t\t\tdecomp;\n\tMeshLayout*\t\t\tmeshLayout;\n\tMeshContext* meshContext;\n\t\n\t\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) printf( \"Watching rank: %i\\n\", rank );\n\t\n\tStGermain_Init( &argc, &argv );\n\tFD_Init( &argc, &argv );\n\t\n\t\n\n\ttestInfoStream =  Journal_Register( InfoStream_Type, \"testInfoStream\" );\n\n\t\n\n\tdictionary = Dictionary_New();\n\tdictionary->add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tdictionary->add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tdictionary->add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tdictionary->add( dictionary, \"minX\", Dictionary_Entry_Value_FromDouble( 0.0f ) );\n\tdictionary->add( dictionary, \"minY\", Dictionary_Entry_Value_FromDouble( 0.0f ) );\n\tdictionary->add( dictionary, \"minZ\", Dictionary_Entry_Value_FromDouble( 0.0f ) );\n\tdictionary->add( dictionary, \"maxX\", Dictionary_Entry_Value_FromDouble( 300.0f ) );\n\tdictionary->add( dictionary, \"maxY\", Dictionary_Entry_Value_FromDouble( 10.0f ) );\n\tdictionary->add( dictionary, \"maxZ\", Dictionary_Entry_Value_FromDouble( 300.0f ) );\n\t\n\t\n\n\tnTopology = (Topology*)IJK6Topology_New( \"IJKTopology\", dictionary );\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tdecomp = (MeshDecomp*)HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\tmeshLayout = MeshLayout_New( \"MeshLayout\", eLayout, nLayout, decomp );\n\tmeshContext = _MeshContext_New( \n\t\tsizeof(MeshContext), \n\t\t\"TestContext\", \n\t\tMyDelete, \n\t\tMyPrint, \n\t\tNULL,\n\t\tNULL, \n\t\tNULL, \n\t\t_AbstractContext_Build, \n\t\t_AbstractContext_Initialise, \n\t\t_AbstractContext_Execute, \n\t\t_AbstractContext_Destroy, \n\t\t\"meshContext\", \n\t\tTrue, \n\t\tMySetDt, \n\t\t0, \n\t\t10, \n\t\tmeshLayout, \n\t\tsizeof(Node), \n\t\tsizeof(Element), \n\t\tCommWorld, \n\t\tdictionary );\n\n\t\n\n\tEntryPoint_ReplaceAll( \n\t\tContext_GetEntryPoint( meshContext, AbstractContext_EP_Solve ),\n\t\t\"test\", \n\t\tMySolve, \n\t\t\"TestMeshContext\" );\n\tEntryPoint_ReplaceAll( Context_GetEntryPoint( meshContext, AbstractContext_EP_Dt ),\n\t\t\"test\", \n\t\tMyDt, \n\t\t\"TestMeshContext\" );\n\tEntryPoint_ReplaceAll( Context_GetEntryPoint( meshContext, AbstractContext_EP_Sync ),\n\t\t\"test\", \n\t\tMySync, \n\t\t\"TestMeshContext\" );\n\tEntryPoint_ReplaceAll( Context_GetEntryPoint( meshContext, AbstractContext_EP_FrequentOutput ),\n\t\t\"test\", \n\t\tMyOutput, \n\t\t\"TestMeshContext\" );\n\n\tif( rank == procToWatch ) {\n\t\tStream* stream = Journal_Register( InfoStream_Type, MeshContext_Type );\n\n\t\tprintf( \"meshContext->entryPointList->count: %u\\n\", meshContext->entryPoint_Register->count );\n\t\tprintf( \"meshContext->entryPointList->_size: %lu\\n\", meshContext->entryPoint_Register->_size );\n\t\tContext_PrintConcise( meshContext, stream );\n\t}\n\t\n\t\n\n\tif( rank == procToWatch ) {\n\t\tStg_Component_Build( meshContext, 0 \n, False );\n\t\tStg_Component_Initialise( meshContext, 0 \n, False );\n\t\tStg_Component_Execute( meshContext, 0 \n, False );\n\t}\n\t\n\t\n\n\tStg_Component_Destroy( meshContext, 0 \n, False );\n\tStg_Class_Delete( meshContext );\n\tStg_Class_Delete( meshLayout );\n\tStg_Class_Delete( decomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( eLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( dictionary );\n\t\n\tFD_Finalise();\n\tStGermain_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tDictionary* dictionary;\n\tTopology*\t\t\tnTopology;\n\tElementLayout*\t\t\teLayout;\n\tNodeLayout*\t\t\tnLayout;\n\tMeshDecomp*\t\t\tdecomp;\n\tMeshLayout*\t\t\tmeshLayout;\n\tMeshContext* meshContext;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) printf( \"Watching rank: %i\\n\", rank );\n\t\n\tStGermain_Init( &argc, &argv );\n\tFD_Init( &argc, &argv );\n\t\n\t\n\n\ttestInfoStream =  Journal_Register( InfoStream_Type, \"testInfoStream\" );\n\n\t\n\n\tdictionary = Dictionary_New();\n\tdictionary->add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tdictionary->add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 3 ) );\n\tdictionary->add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 4 ) );\n\tdictionary->add( dictionary, \"minX\", Dictionary_Entry_Value_FromDouble( 0.0f ) );\n\tdictionary->add( dictionary, \"minY\", Dictionary_Entry_Value_FromDouble( 0.0f ) );\n\tdictionary->add( dictionary, \"minZ\", Dictionary_Entry_Value_FromDouble( 0.0f ) );\n\tdictionary->add( dictionary, \"maxX\", Dictionary_Entry_Value_FromDouble( 300.0f ) );\n\tdictionary->add( dictionary, \"maxY\", Dictionary_Entry_Value_FromDouble( 10.0f ) );\n\tdictionary->add( dictionary, \"maxZ\", Dictionary_Entry_Value_FromDouble( 300.0f ) );\n\t\n\t\n\n\tnTopology = (Topology*)IJK6Topology_New( \"IJKTopology\", dictionary );\n\teLayout = (ElementLayout*)ParallelPipedHexaEL_New( \"PPHexaEL\", 3, dictionary );\n\tnLayout = (NodeLayout*)CornerNL_New( \"CornerNL\", dictionary, eLayout, nTopology );\n\tdecomp = (MeshDecomp*)HexaMD_New( \"HexaMD\", dictionary, MPI_COMM_WORLD, eLayout, nLayout );\n\tmeshLayout = MeshLayout_New( \"MeshLayout\", eLayout, nLayout, decomp );\n\tmeshContext = _MeshContext_New( \n\t\tsizeof(MeshContext), \n\t\t\"TestContext\", \n\t\tMyDelete, \n\t\tMyPrint, \n\t\tNULL,\n\t\tNULL, \n\t\tNULL, \n\t\t_AbstractContext_Build, \n\t\t_AbstractContext_Initialise, \n\t\t_AbstractContext_Execute, \n\t\t_AbstractContext_Destroy, \n\t\t\"meshContext\", \n\t\tTrue, \n\t\tMySetDt, \n\t\t0, \n\t\t10, \n\t\tmeshLayout, \n\t\tsizeof(Node), \n\t\tsizeof(Element), \n\t\tCommWorld, \n\t\tdictionary );\n\n\t\n\n\tEntryPoint_ReplaceAll( \n\t\tContext_GetEntryPoint( meshContext, AbstractContext_EP_Solve ),\n\t\t\"test\", \n\t\tMySolve, \n\t\t\"TestMeshContext\" );\n\tEntryPoint_ReplaceAll( Context_GetEntryPoint( meshContext, AbstractContext_EP_Dt ),\n\t\t\"test\", \n\t\tMyDt, \n\t\t\"TestMeshContext\" );\n\tEntryPoint_ReplaceAll( Context_GetEntryPoint( meshContext, AbstractContext_EP_Sync ),\n\t\t\"test\", \n\t\tMySync, \n\t\t\"TestMeshContext\" );\n\tEntryPoint_ReplaceAll( Context_GetEntryPoint( meshContext, AbstractContext_EP_FrequentOutput ),\n\t\t\"test\", \n\t\tMyOutput, \n\t\t\"TestMeshContext\" );\n\n\tif( rank == procToWatch ) {\n\t\tStream* stream = Journal_Register( InfoStream_Type, MeshContext_Type );\n\n\t\tprintf( \"meshContext->entryPointList->count: %u\\n\", meshContext->entryPoint_Register->count );\n\t\tprintf( \"meshContext->entryPointList->_size: %lu\\n\", meshContext->entryPoint_Register->_size );\n\t\tContext_PrintConcise( meshContext, stream );\n\t}\n\t\n\t\n\n\tif( rank == procToWatch ) {\n\t\tStg_Component_Build( meshContext, 0 \n, False );\n\t\tStg_Component_Initialise( meshContext, 0 \n, False );\n\t\tStg_Component_Execute( meshContext, 0 \n, False );\n\t}\n\t\n\t\n\n\tStg_Component_Destroy( meshContext, 0 \n, False );\n\tStg_Class_Delete( meshContext );\n\tStg_Class_Delete( meshLayout );\n\tStg_Class_Delete( decomp );\n\tStg_Class_Delete( nLayout );\n\tStg_Class_Delete( eLayout );\n\tStg_Class_Delete( nTopology );\n\tStg_Class_Delete( dictionary );\n\t\n\tFD_Finalise();\n\tStGermain_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "raulperula_895", "code": "int main (int argc,char*argv[])\n{\n\tint numtasks, taskid, rc, dest, offset, i, j, tag1, tag2, source, chunksize;\n\tfloat mysum, sum;\n\tfloat update(int myoffset,int chunk,int myid);\n\n\tMPI_Status status;\n\n\t\n\n\tif(numtasks%4 != 0){\n\t\tprintf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n\t\texit(0);\n\t}\n\tprintf(\"MPI task %d has started...\\n\", taskid);\n\tchunksize = (ARRAYSIZE / numtasks);\n\ttag2 = 1;\n\ttag1 = 2;\n\n\t\n\n\tif(taskid == MASTER){\n\t\t\n\n\t\tsum = 0;\n\t\tfor(i=0; i<ARRAYSIZE; i++){\n\t\t\tdata[i] =  i *1.0;\n\t\t\tsum = sum + data[i];\n\t\t}\n\t\tprintf(\"Initialized array sum = %e\\n\",sum);\n\t\t\t\n\t\t\n\n\t\toffset = chunksize;\n\t\tfor(dest=1; dest<numtasks; dest++){\n\t\t\tprintf(\"Sent   %d   elements   to   task   %d   offset= %d\\n\",chunksize,dest,offset);\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\t\n\n\t\toffset = 0;\n\t\tmysum = update(offset, chunksize, taskid);\n\t\t\t\n\t\t\n\n\t\tfor(i=1; i<numtasks; i++){\n\t\t\tsource = i;\n\t\t}\n\n\t\t\n\n\t\t\n\t\tprintf(\"Sample results: \\n\");\n\t\toffset = 0;\n\t\tfor(i=0; i<numtasks; i++){\n\t\t\tfor(j=0; j<5; j++)\n\t\t\t\tprintf(\"  %e\",data[offset+j]);\n\t\t\tprintf(\"\\n\");\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\tprintf(\"*** Final sum= %e ***\\n\",sum);\n\t}\n\n\n\t\n\n\tif(taskid > MASTER){\n\t\t\n\n\t\tsource = MASTER;\n\t\tmysum = update(offset, chunksize, taskid);\n\n\t\t\n\n\t\tdest = MASTER;\n\t}\n\n\t\n\t\n}/* ", "label": "int main (int argc,char*argv[])\n{\n\tint numtasks, taskid, rc, dest, offset, i, j, tag1, tag2, source, chunksize;\n\tfloat mysum, sum;\n\tfloat update(int myoffset,int chunk,int myid);\n\n\tMPI_Init(&argc,&argv);\t\n\tMPI_Status status;\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n\tif(numtasks%4 != 0){\n\t\tprintf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n\t\tMPI_Abort(MPI_COMM_WORLD, rc);\n\t\texit(0);\n\t}\n\tMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n\tprintf(\"MPI task %d has started...\\n\", taskid);\n\tchunksize = (ARRAYSIZE / numtasks);\n\ttag2 = 1;\n\ttag1 = 2;\n\n\t\n\n\tif(taskid == MASTER){\n\t\t\n\n\t\tsum = 0;\n\t\tfor(i=0; i<ARRAYSIZE; i++){\n\t\t\tdata[i] =  i *1.0;\n\t\t\tsum = sum + data[i];\n\t\t}\n\t\tprintf(\"Initialized array sum = %e\\n\",sum);\n\t\t\t\n\t\t\n\n\t\toffset = chunksize;\n\t\tfor(dest=1; dest<numtasks; dest++){\n\t\t\tMPI_Send(&offset,1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&data[offset], chunksize, MPI_FLOAT, dest, tag2, MPI_COMM_WORLD);\n\t\t\tprintf(\"Sent   %d   elements   to   task   %d   offset= %d\\n\",chunksize,dest,offset);\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\t\n\n\t\toffset = 0;\n\t\tmysum = update(offset, chunksize, taskid);\n\t\t\t\n\t\t\n\n\t\tfor(i=1; i<numtasks; i++){\n\t\t\tsource = i;\n\t\t\tMPI_Recv(&offset,1, MPI_INT, source, tag1, MPI_COMM_WORLD,&status);\n\t\t\tMPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n\t\t\tMPI_COMM_WORLD,&status);\n\t\t}\n\n\t\t\n\n\t\tMPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\t\t\n\t\tprintf(\"Sample results: \\n\");\n\t\toffset = 0;\n\t\tfor(i=0; i<numtasks; i++){\n\t\t\tfor(j=0; j<5; j++)\n\t\t\t\tprintf(\"  %e\",data[offset+j]);\n\t\t\tprintf(\"\\n\");\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\tprintf(\"*** Final sum= %e ***\\n\",sum);\n\t}\n\n\n\t\n\n\tif(taskid > MASTER){\n\t\t\n\n\t\tsource = MASTER;\n\t\tMPI_Recv(&offset,1, MPI_INT, source, tag1, MPI_COMM_WORLD,&status);\n\t\tMPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n\t\tMPI_COMM_WORLD,&status);\n\t\tmysum = update(offset, chunksize, taskid);\n\n\t\t\n\n\t\tdest = MASTER;\n\t\tMPI_Send(&offset,1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n\t\tMPI_Send(&data[offset], chunksize, MPI_FLOAT, MASTER, tag2, MPI_COMM_WORLD);\n        MPI_Reduce(&mysum,&sum,1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\t}\n\n\t\n\tMPI_Finalize();\n\t\n}/* "}
{"program": "xyuan_896", "code": "int\nmain (int argc, char **argv)\n{\n  int                 rank;\n  int                 mpiret;\n  MPI_Comm            mpicomm;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n  connectivity = p4est_connectivity_new_star ();\n  p4est = p4est_new_ext (mpicomm, connectivity, 15, 0, 0,\n                         sizeof (user_data_t), init_fn, NULL);\n\n  \n\n  p4est_refine (p4est, 1, refine_fn, init_fn);\n\n  \n\n  p4est_balance (p4est, P4EST_CONNECT_DEFAULT, init_fn);\n\n  \n\n  p4est_partition (p4est, weight_one);\n\n  p4est_check_local_order (p4est, connectivity);\n\n  \n\n  weight_counter = 0;\n  weight_index = (rank == 1) ? 1342 : 0;\n  p4est_partition (p4est, weight_once);\n\n  p4est_check_local_order (p4est, connectivity);\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n\n  \n\n  connectivity = p4est_connectivity_new_periodic ();\n  p4est = p4est_new_ext (mpicomm, connectivity, 15, 0, 0,\n                         sizeof (user_data_t), init_fn, NULL);\n\n  \n\n  p4est_refine (p4est, 1, refine_fn, init_fn);\n\n  \n\n  p4est_balance (p4est, P4EST_CONNECT_DEFAULT, init_fn);\n\n  \n\n  p4est_partition (p4est, weight_one);\n\n  p4est_check_local_order (p4est, connectivity);\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n  sc_finalize ();\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int                 rank;\n  int                 mpiret;\n  MPI_Comm            mpicomm;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n  mpiret = MPI_Comm_rank (mpicomm, &rank);\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n  connectivity = p4est_connectivity_new_star ();\n  p4est = p4est_new_ext (mpicomm, connectivity, 15, 0, 0,\n                         sizeof (user_data_t), init_fn, NULL);\n\n  \n\n  p4est_refine (p4est, 1, refine_fn, init_fn);\n\n  \n\n  p4est_balance (p4est, P4EST_CONNECT_DEFAULT, init_fn);\n\n  \n\n  p4est_partition (p4est, weight_one);\n\n  p4est_check_local_order (p4est, connectivity);\n\n  \n\n  weight_counter = 0;\n  weight_index = (rank == 1) ? 1342 : 0;\n  p4est_partition (p4est, weight_once);\n\n  p4est_check_local_order (p4est, connectivity);\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n\n  \n\n  connectivity = p4est_connectivity_new_periodic ();\n  p4est = p4est_new_ext (mpicomm, connectivity, 15, 0, 0,\n                         sizeof (user_data_t), init_fn, NULL);\n\n  \n\n  p4est_refine (p4est, 1, refine_fn, init_fn);\n\n  \n\n  p4est_balance (p4est, P4EST_CONNECT_DEFAULT, init_fn);\n\n  \n\n  p4est_partition (p4est, weight_one);\n\n  p4est_check_local_order (p4est, connectivity);\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n  sc_finalize ();\n\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "sindrig_899", "code": "int main(int argc, char**argv){\n\t\n\n    int rank, size, n, i, send_count;\n    \n\n    n = 5;\n    if(argc > 1){\n        n = atoi(argv[1]);\n    }\n\n    if(n < 2){\n        printf(\"n needs to be larger than 1\\n\");\n        return 1;\n    }\n\n\n\n    send_count = (n + (size - 1)) / size;\n    \n    int arr[n];\n    if(rank == 0){\n        for(i = 0; i < n; i++){\n            arr[i] = i+1;\n        }\n        printf(\"n=%d, send_count=%d, size=%d\\n\", n, send_count, size); \n    }\n\n    int *recv_buf = malloc(sizeof(int) * send_count);\n\n\n    int squared[send_count];\n    for(i = 0; i < send_count; i++){\n        if((rank+1)*send_count+i-1 > n){\n            \n\n            break;\n        }\n        squared[i] = recv_buf[i] * recv_buf[i];\n    }\n\n    int *squared_all = NULL;\n    if(rank == 0){\n        squared_all = malloc(sizeof(int) * size * send_count);\n    }\n\n\n    int res = 0;\n    if(rank == 0){\n        for(i = 0; i < n; i++){\n            res += squared_all[i];\n        }\n        printf(\"Result: %d\\n\", res);\n    }\n\n\treturn 0;\n}", "label": "int main(int argc, char**argv){\n\t\n\n    int rank, size, n, i, send_count;\n    \n\n    n = 5;\n    if(argc > 1){\n        n = atoi(argv[1]);\n    }\n\n    if(n < 2){\n        printf(\"n needs to be larger than 1\\n\");\n        return 1;\n    }\n\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    send_count = (n + (size - 1)) / size;\n    \n    int arr[n];\n    if(rank == 0){\n        for(i = 0; i < n; i++){\n            arr[i] = i+1;\n        }\n        printf(\"n=%d, send_count=%d, size=%d\\n\", n, send_count, size); \n    }\n\n    int *recv_buf = malloc(sizeof(int) * send_count);\n\n    MPI_Scatter(arr, send_count, MPI_INT, recv_buf, send_count, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int squared[send_count];\n    for(i = 0; i < send_count; i++){\n        if((rank+1)*send_count+i-1 > n){\n            \n\n            break;\n        }\n        squared[i] = recv_buf[i] * recv_buf[i];\n    }\n\n    int *squared_all = NULL;\n    if(rank == 0){\n        squared_all = malloc(sizeof(int) * size * send_count);\n    }\n\n    MPI_Gather(&squared, send_count, MPI_INT, squared_all, send_count, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int res = 0;\n    if(rank == 0){\n        for(i = 0; i < n; i++){\n            res += squared_all[i];\n        }\n        printf(\"Result: %d\\n\", res);\n    }\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "autarchprinceps_900", "code": "int main(int argc, char **argv) {\n\tint err, rank, n, i;\n\tint* vector;\n\tMPI_Status status;\n\n\terr =\n\terr =\n\terr =\n\t\n\tif(rank == 0) {\n\t\tvector = calloc(n, sizeof(int));\n\t} else {\n\t\tvector = malloc(n * sizeof(int));\n\t}\n\tif(rank > 0) {\n\t\t\n\n\t\terr =\n\t}\n\n\t\n\n\tvector[rank] = rank + 1;\n\t\n\n\terr =\n\n\tif(rank == 0) {\n\t\t\n\n\t\terr =\n\t\t\n\n\t\tint result = 1;\n\t\tfor(i = 0; i < n; i++) {\n\t\t\tif(vector[i] != i + 1) {\n\t\t\t\tresult = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tfor(i = 0; i < n; i++) {\n\t\t\tprintf(\"%i \", vector[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\t\tif(result == 0) {\n\t\t\tfprintf(stderr, \"Result does not match expectation\\n\");\n\n\t\t} else {\n\t\t\tprintf(\"Result matches expectation\\n\");\n\t\t}\n\t}\n\t\n\terr =\n\treturn 0;\n}", "label": "int main(int argc, char **argv) {\n\tint err, rank, n, i;\n\tint* vector;\n\tMPI_Status status;\n\n\terr = MPI_Init(&argc, &argv);\n\terr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\terr = MPI_Comm_size(MPI_COMM_WORLD, &n);\n\t\n\tif(rank == 0) {\n\t\tvector = calloc(n, sizeof(int));\n\t} else {\n\t\tvector = malloc(n * sizeof(int));\n\t}\n\tif(rank > 0) {\n\t\t\n\n\t\terr = MPI_Recv(vector, n, MPI_INT, rank - 1, 4711, MPI_COMM_WORLD, &status);\n\t}\n\n\t\n\n\tvector[rank] = rank + 1;\n\t\n\n\terr = MPI_Send(vector, n, MPI_INT, (rank + 1) % n, 4711, MPI_COMM_WORLD);\n\n\tif(rank == 0) {\n\t\t\n\n\t\terr = MPI_Recv(vector, n, MPI_INT, n - 1, 4711, MPI_COMM_WORLD, &status);\n\t\t\n\n\t\tint result = 1;\n\t\tfor(i = 0; i < n; i++) {\n\t\t\tif(vector[i] != i + 1) {\n\t\t\t\tresult = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tfor(i = 0; i < n; i++) {\n\t\t\tprintf(\"%i \", vector[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\t\tif(result == 0) {\n\t\t\tfprintf(stderr, \"Result does not match expectation\\n\");\n\n\t\t} else {\n\t\t\tprintf(\"Result matches expectation\\n\");\n\t\t}\n\t}\n\t\n\terr = MPI_Finalize();\n\treturn 0;\n}"}
{"program": "Cristianohh_901", "code": "main(int argc, char **argv)\n{\n    int rank;\n    char* host;\n\n    host = (char*) malloc(HOST_NAME_MAX * sizeof(char));\n    gethostname(host, HOST_NAME_MAX);\n\n    MASTER(MPI started);\n    printf(\"Process with rank %d running on Node %s Core %d/%d\\n\",rank ,host, sched_getcpu(),get_cpu_id());\n\n    MASTER(Enter OpenMP parallel region);\n#pragma omp parallel\n    {\n#pragma omp master\n        {\n            pid_t pid = getppid();\n            char cmd[1024];\n            sprintf(cmd, \"pstree -p -H %d %d\",pid, pid);\n            system(cmd);\n        }\n#pragma omp critical\n        {\n            printf (\"Rank %d Thread %d running on core %d/%d with pid %d and tid %d\\n\",rank,omp_get_thread_num(), sched_getcpu(),get_cpu_id(), getpid(),gettid());\n        }\n\n    }\n\n    free(host);\n}", "label": "main(int argc, char **argv)\n{\n    int rank;\n    char* host;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    host = (char*) malloc(HOST_NAME_MAX * sizeof(char));\n    gethostname(host, HOST_NAME_MAX);\n\n    MASTER(MPI started);\n    MPI_Barrier(MPI_COMM_WORLD);\n    printf(\"Process with rank %d running on Node %s Core %d/%d\\n\",rank ,host, sched_getcpu(),get_cpu_id());\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MASTER(Enter OpenMP parallel region);\n    MPI_Barrier(MPI_COMM_WORLD);\n#pragma omp parallel\n    {\n#pragma omp master\n        {\n            pid_t pid = getppid();\n            char cmd[1024];\n            sprintf(cmd, \"pstree -p -H %d %d\",pid, pid);\n            system(cmd);\n        }\n#pragma omp critical\n        {\n            printf (\"Rank %d Thread %d running on core %d/%d with pid %d and tid %d\\n\",rank,omp_get_thread_num(), sched_getcpu(),get_cpu_id(), getpid(),gettid());\n        }\n\n    }\n\n    free(host);\n    MPI_Finalize();\n}"}
{"program": "qingu_902", "code": "int main(int argc, char **argv) {\n  int      i, j;\n  char    *in, *out;\n  double   t_start, t_stop;\n  MPI_Comm copy_comm;\n\n\n\n  in  = malloc(MAXSZ);\n  out = malloc(MAXSZ);\n\n  for (i = 0; i < MAXSZ; i++) {\n    in[i]  = 0xAA;\n    out[i] = 0x55;\n  }\n\n  for (i = 1; i <= MAXSZ; i *= 2) {\n    t_start =\n    for (j = 0; j < NITER; j++) {\n      memcpy(out, in, i);\n    }\n    t_stop =\n    printf(\"MEMCPY: %7d\\t%0.9f\\n\", i, t_stop-t_start);\n  }\n\n  for (i = 0; i < MAXSZ; i++) {\n    in[i]  = 0xAA;\n    out[i] = 0x55;\n  }\n\n  for (i = 1; i <= MAXSZ; i *= 2) {\n    t_start =\n    for (j = 0; j < NITER; j++) {\n    }\n    t_stop =\n    printf(\"SNDRCV: %7d\\t%0.9f\\n\", i, t_stop-t_start);\n  }\n\n\n\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  int      i, j;\n  char    *in, *out;\n  double   t_start, t_stop;\n  MPI_Comm copy_comm;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_dup(MPI_COMM_SELF, &copy_comm);\n\n  in  = malloc(MAXSZ);\n  out = malloc(MAXSZ);\n\n  for (i = 0; i < MAXSZ; i++) {\n    in[i]  = 0xAA;\n    out[i] = 0x55;\n  }\n\n  for (i = 1; i <= MAXSZ; i *= 2) {\n    t_start = MPI_Wtime();\n    for (j = 0; j < NITER; j++) {\n      memcpy(out, in, i);\n    }\n    t_stop = MPI_Wtime();\n    printf(\"MEMCPY: %7d\\t%0.9f\\n\", i, t_stop-t_start);\n  }\n\n  for (i = 0; i < MAXSZ; i++) {\n    in[i]  = 0xAA;\n    out[i] = 0x55;\n  }\n\n  for (i = 1; i <= MAXSZ; i *= 2) {\n    t_start = MPI_Wtime();\n    for (j = 0; j < NITER; j++) {\n      MPI_Sendrecv(in, i, MPI_BYTE,\n          0 \n, 0 \n,\n          out, i, MPI_BYTE,\n          0 \n, 0 \n,\n          copy_comm, MPI_STATUS_IGNORE);\n    }\n    t_stop = MPI_Wtime();\n    printf(\"SNDRCV: %7d\\t%0.9f\\n\", i, t_stop-t_start);\n  }\n\n\n  MPI_Comm_free(&copy_comm);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "konstantin-psu_903", "code": "int main ( int argc, char **argv )\n\n{\n  int comm[4];\n  FILE *fp;\n  int i;\n  int j;\n  double M[nodeedge+2][nodeedge+2];\n  int ntasks;\n  int rank;\n  double result[n][n];\n  double w;\n  double wtime;\n\n\n\n\n  wtime =\n\n  if ( rank == 0 ) \n  {\n    printf ( \"\\n\" );\n    printf ( \"LAPLACE_MPI:\\n\" );\n    printf ( \"  C/MPI version\\n\" );\n    printf ( \"  Solve the Laplace equation using MPI.\\n\" );\n  }\n\n  if ( ntasks != nproc )\n  {\n    if ( rank == 0 ) \n    {\n      printf ( \"\\n\" );\n      printf ( \"Fatal error!\\n\" );\n      printf ( \"  MP_PROCS should be set to %i!\\n\", nproc );\n    }\n    exit ( 1 );\n  }\n\n  if ( rank == 0 ) \n  {\n    printf ( \"\\n\" );\n    printf ( \"  MPI has been set up.\\n\" );\n  }\n\n\n  if ( rank == 0 ) \n  {\n    printf ( \"  Initialize the matrix M.\\n\" );\n  }\n  initialize_matrix ( M );\n\n\n  if ( rank == 0 ) \n  {\n    printf ( \"  Set the list of neighbors.\\n\" );\n  }\n  setcomm ( rank, comm );\n\n\n  if ( rank == 0 ) \n  {\n    printf ( \"  Begin the iteration.\\n\" );\n  }\n  w = 1.2;\n  iterate ( w, M, result, rank, comm );\n\n \n  wtime = MPI_Wtime ( ) - wtime;\n\n  printf ( \"  Task %i took %6.3f seconds\\n\", rank, wtime );\n\n\n  if ( rank == 0 )\n  {\n    fp = fopen ( \"laplace_solution.txt\", \"w\" );\n\n    for ( i = 0; i < n; i++ ) \n    {\n      for ( j = 0; j < n; j++ )\n      {\n        fprintf ( fp, \"%f \\n\", result[i][j] );\n      }  \n    }\n    fclose ( fp );\n    printf ( \"  Solution written to \\\"laplace_solution.txt\\\".\\n\" );\n  }\n\n\n\n\n  if ( rank == 0 )\n  {\n    printf ( \"\\n\" );\n    printf ( \"LAPLACE_MPI:\\n\" );\n    printf ( \"  Normal end of execution.\\n\" );\n  }\n  return 0;\n}", "label": "int main ( int argc, char **argv )\n\n{\n  int comm[4];\n  FILE *fp;\n  int i;\n  int j;\n  double M[nodeedge+2][nodeedge+2];\n  int ntasks;\n  int rank;\n  double result[n][n];\n  double w;\n  double wtime;\n\n  MPI_Init ( &argc, &argv );\n\n  MPI_Comm_rank ( MPI_COMM_WORLD, &rank );\n\n  MPI_Comm_size ( MPI_COMM_WORLD, &ntasks );\n\n  wtime = MPI_Wtime ( );\n\n  if ( rank == 0 ) \n  {\n    printf ( \"\\n\" );\n    printf ( \"LAPLACE_MPI:\\n\" );\n    printf ( \"  C/MPI version\\n\" );\n    printf ( \"  Solve the Laplace equation using MPI.\\n\" );\n  }\n\n  if ( ntasks != nproc )\n  {\n    if ( rank == 0 ) \n    {\n      printf ( \"\\n\" );\n      printf ( \"Fatal error!\\n\" );\n      printf ( \"  MP_PROCS should be set to %i!\\n\", nproc );\n    }\n    MPI_Finalize ( );\n    exit ( 1 );\n  }\n\n  if ( rank == 0 ) \n  {\n    printf ( \"\\n\" );\n    printf ( \"  MPI has been set up.\\n\" );\n  }\n\n\n  if ( rank == 0 ) \n  {\n    printf ( \"  Initialize the matrix M.\\n\" );\n  }\n  initialize_matrix ( M );\n\n\n  if ( rank == 0 ) \n  {\n    printf ( \"  Set the list of neighbors.\\n\" );\n  }\n  setcomm ( rank, comm );\n\n\n  if ( rank == 0 ) \n  {\n    printf ( \"  Begin the iteration.\\n\" );\n  }\n  w = 1.2;\n  iterate ( w, M, result, rank, comm );\n\n \n  wtime = MPI_Wtime ( ) - wtime;\n\n  printf ( \"  Task %i took %6.3f seconds\\n\", rank, wtime );\n\n\n  if ( rank == 0 )\n  {\n    fp = fopen ( \"laplace_solution.txt\", \"w\" );\n\n    for ( i = 0; i < n; i++ ) \n    {\n      for ( j = 0; j < n; j++ )\n      {\n        fprintf ( fp, \"%f \\n\", result[i][j] );\n      }  \n    }\n    fclose ( fp );\n    printf ( \"  Solution written to \\\"laplace_solution.txt\\\".\\n\" );\n  }\n\n\n  MPI_Finalize ( );\n\n\n  if ( rank == 0 )\n  {\n    printf ( \"\\n\" );\n    printf ( \"LAPLACE_MPI:\\n\" );\n    printf ( \"  Normal end of execution.\\n\" );\n  }\n  return 0;\n}"}
{"program": "linhbngo_904", "code": "int main(int argc, char * argv[] ) {\n  int rank;     \n\n  int size;     \n\n  int i;        \n\n  int distance; \n\n    \n    \n  \n\n  \n\n  distance = 1;\n  i = 1;\n  while (distance <= size / 2){      \n    if (rank < distance) {\n      printf (\"At time step %d, sender %d sends to %d\\n\", i, rank, rank + distance);\n    }\n    if ((rank >= distance) && (rank < distance * 2)){\n      printf (\"At time step %d, receiver %d receives from %d\\n\", i, rank, rank - distance);\n    }\n    printf (\"Process %d has distance value %d and time step %d\\n\", rank, distance, i);\n    distance = distance * 2;\n    i += 1;\n  }\n    \n  return 0;  \n}", "label": "int main(int argc, char * argv[] ) {\n  int rank;     \n\n  int size;     \n\n  int i;        \n\n  int distance; \n\n    \n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n    \n  \n\n  \n\n  distance = 1;\n  i = 1;\n  while (distance <= size / 2){      \n    if (rank < distance) {\n      printf (\"At time step %d, sender %d sends to %d\\n\", i, rank, rank + distance);\n    }\n    if ((rank >= distance) && (rank < distance * 2)){\n      printf (\"At time step %d, receiver %d receives from %d\\n\", i, rank, rank - distance);\n    }\n    printf (\"Process %d has distance value %d and time step %d\\n\", rank, distance, i);\n    distance = distance * 2;\n    i += 1;\n  }\n    \n  MPI_Finalize();\n  return 0;  \n}"}
{"program": "gnu3ra_905", "code": "int main(int argc, char *argv[]) \n{ \n    MPI_Datatype column, xpose;\n    double t[5], ttmp, tmin, tmax, ttick;\n    static int sizes[5] = { 10, 100, 1000, 10000, 20000 };\n    int i, isMonotone, errs=0, nrows, ncols, isvalid;\n \n\n    ttick =\n\n    for (i=0; i<5; i++) {\n         nrows = ncols = sizes[i];\n         ttmp =\n         \n\n         \n\n         t[i] = MPI_Wtime() - ttmp;\n     }\n\n     \n\n     tmin = 10000;\n     tmax = 0;\n     isvalid = 1;\n     for (i=0; i<5; i++) {\n\t if (t[i] < 10*ttick) {\n\t     \n\n\t     isvalid = 0;\n\t }\n\t else {\n\t     if (t[i] < tmin) tmin = t[i];\n\t     if (t[i] > tmax) tmax = t[i];\n\t }\n     }\n     if (isvalid) {\n\t \n\n\t isMonotone = 1;\n\t for (i=1; i<5; i++) {\n\t     if (t[i] < t[i-1]) isMonotone = 0;\n\t }\n\t if (tmax > 100 * tmin) {\n\t     errs++;\n\t     fprintf( stderr, \"Range of times appears too large\\n\" );\n\t     if (isMonotone) {\n\t\t fprintf( stderr, \"Vector types may use processing proportion to count\\n\" );\n\t     }\n\t     for (i=0; i<5; i++) {\n\t\t fprintf( stderr, \"n = %d, time = %f\\n\", sizes[i], t[i] );\n\t     }\n\t     fflush(stderr);\n\t }\n     }\n     else {\n\t fprintf( stderr, \"Timing failed - recorded times are too small relative to MPI_Wtick\\n\" );\n\t \n\n     }\n\n    if (errs) {\n        printf( \" Found %d errors\\n\", errs );\n    }\n    else {\n        printf( \" No Errors\\n\" );\n    } \n    return 0; \n}", "label": "int main(int argc, char *argv[]) \n{ \n    MPI_Datatype column, xpose;\n    double t[5], ttmp, tmin, tmax, ttick;\n    static int sizes[5] = { 10, 100, 1000, 10000, 20000 };\n    int i, isMonotone, errs=0, nrows, ncols, isvalid;\n \n    MPI_Init(&argc,&argv); \n\n    ttick = MPI_Wtick();\n\n    for (i=0; i<5; i++) {\n         nrows = ncols = sizes[i];\n         ttmp = MPI_Wtime();\n         \n\n         MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column);\n         \n\n         MPI_Type_hvector(ncols, 1, sizeof(int), column, &xpose);\n         MPI_Type_commit(&xpose);\n         t[i] = MPI_Wtime() - ttmp;\n         MPI_Type_free( &xpose );\n         MPI_Type_free( &column );\n     }\n\n     \n\n     tmin = 10000;\n     tmax = 0;\n     isvalid = 1;\n     for (i=0; i<5; i++) {\n\t if (t[i] < 10*ttick) {\n\t     \n\n\t     isvalid = 0;\n\t }\n\t else {\n\t     if (t[i] < tmin) tmin = t[i];\n\t     if (t[i] > tmax) tmax = t[i];\n\t }\n     }\n     if (isvalid) {\n\t \n\n\t isMonotone = 1;\n\t for (i=1; i<5; i++) {\n\t     if (t[i] < t[i-1]) isMonotone = 0;\n\t }\n\t if (tmax > 100 * tmin) {\n\t     errs++;\n\t     fprintf( stderr, \"Range of times appears too large\\n\" );\n\t     if (isMonotone) {\n\t\t fprintf( stderr, \"Vector types may use processing proportion to count\\n\" );\n\t     }\n\t     for (i=0; i<5; i++) {\n\t\t fprintf( stderr, \"n = %d, time = %f\\n\", sizes[i], t[i] );\n\t     }\n\t     fflush(stderr);\n\t }\n     }\n     else {\n\t fprintf( stderr, \"Timing failed - recorded times are too small relative to MPI_Wtick\\n\" );\n\t \n\n     }\n\n    if (errs) {\n        printf( \" Found %d errors\\n\", errs );\n    }\n    else {\n        printf( \" No Errors\\n\" );\n    } \n    MPI_Finalize(); \n    return 0; \n}"}
{"program": "alucas_906", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif ((loop % 2) == rank)\n\t\t{\n\t\t\tstarpu_mpi_send(tab_handle, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif ((loop % 2) == rank)\n\t\t{\n\t\t\tstarpu_mpi_send(tab_handle, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "arturocastro_913", "code": "int main(int argc, char * argv[])\n{\n    const int m = M;\n\n    int ierr =\n\n    if(ierr != MPI_SUCCESS)\n    {\n\tperror(\"MPI init failed. Terminating T800.\\n\");\n\texit(1);\n    }\n\n    int i, j;\n\n    const double begin =\n\n    const double pi = 4.0 * atan(1.0);\n\n    const double h = pi / (m + 1);\n\n\n    for(i = 0; i < m + 2; ++i)\n    {\n       uold[i][M + 1] = sin(i * h);\n    }\n\n    for(i = 0; i < m + 2; ++i)\n    {\n\tfor(j = 0; j < m + 1; ++j)\n        {\n            uold[i][j] = j * h * uold[i][M + 1];\n\t    \n\n        }\n    }\n\n    for(i = 0; i < m + 2; ++i)\n    {\n        for(j = 0; j < m + 2; ++j)\n        {\n            solution[i][j] = sinh(j * h) * sin(i * h) / sinh(pi);\n        }\n    }\n\n    para_range(1, m + 1, numtasks, rank, &ista, &iend);\n\n    printf(\"numtasks = %d\\nrank %d: from %d to %d\\n\", numtasks, rank, ista, iend);\n\n    inext = rank + 1;\n    iprev = rank - 1;\n\n    if(inext == numtasks)\n    {\n\tinext = MPI_PROC_NULL;\n    }\n\n    if(iprev == -1)\n    {\n\tiprev = MPI_PROC_NULL;\n    }\n\n    const double omega = 2.0 / ( 1.0 + sin(pi / (m + 1)) );\n    const double tol = 0.001;\n\n    const int iters = sor(unew, uold, solution, omega, tol, m);\n\n    const double end =\n\n\n    if(rank == 0)\n    {\n\tprintf(\" \\n\");\n\tprintf(\" Omega = %f\\n\", omega);\n\tprintf(\" It took %d iterations.\\n\", iters);\n\t\n\tprintf(\"Total time = %f\\n\\n\\n\", end - begin);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    const int m = M;\n\n    int ierr = MPI_Init(&argc, &argv);\n\n    if(ierr != MPI_SUCCESS)\n    {\n\tperror(\"MPI init failed. Terminating T800.\\n\");\n\texit(1);\n    }\n\n    int i, j;\n\n    const double begin = MPI_Wtime();\n\n    const double pi = 4.0 * atan(1.0);\n\n    const double h = pi / (m + 1);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for(i = 0; i < m + 2; ++i)\n    {\n       uold[i][M + 1] = sin(i * h);\n    }\n\n    for(i = 0; i < m + 2; ++i)\n    {\n\tfor(j = 0; j < m + 1; ++j)\n        {\n            uold[i][j] = j * h * uold[i][M + 1];\n\t    \n\n        }\n    }\n\n    for(i = 0; i < m + 2; ++i)\n    {\n        for(j = 0; j < m + 2; ++j)\n        {\n            solution[i][j] = sinh(j * h) * sin(i * h) / sinh(pi);\n        }\n    }\n\n    para_range(1, m + 1, numtasks, rank, &ista, &iend);\n\n    printf(\"numtasks = %d\\nrank %d: from %d to %d\\n\", numtasks, rank, ista, iend);\n\n    inext = rank + 1;\n    iprev = rank - 1;\n\n    if(inext == numtasks)\n    {\n\tinext = MPI_PROC_NULL;\n    }\n\n    if(iprev == -1)\n    {\n\tiprev = MPI_PROC_NULL;\n    }\n\n    const double omega = 2.0 / ( 1.0 + sin(pi / (m + 1)) );\n    const double tol = 0.001;\n\n    const int iters = sor(unew, uold, solution, omega, tol, m);\n\n    const double end = MPI_Wtime();\n\n    MPI_Finalize();\n\n    if(rank == 0)\n    {\n\tprintf(\" \\n\");\n\tprintf(\" Omega = %f\\n\", omega);\n\tprintf(\" It took %d iterations.\\n\", iters);\n\t\n\tprintf(\"Total time = %f\\n\\n\\n\", end - begin);\n    }\n\n    return 0;\n}"}
{"program": "KAUST-KSL_914", "code": "int\nmain(int argc, char **argv)\n{\n\n\n\n\n  masque=argv[1];\n  nb_work_total=atoi(argv[2]);\n\n\n  \n\n\n\n  \n\n\n  \n  #ifdef DEBUG\n  printf (\"\\n===MAESTRO=== MPI task # %d started OK\",myrank);\n  fflush(stdout);\n  #endif \n\n\n  if (myrank == 0) {\n    master();\n  } else {\n    slave();\n  }\n\n  #ifdef DEBUG\n  printf(\"\\n===MAESTRO=== Task %d ; Bye bye\",myrank);\n  fflush(stdout);\n  #endif\n  \n\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n\n\n\n\n  masque=argv[1];\n  nb_work_total=atoi(argv[2]);\n\n\n  \n\n\n  MPI_Init(&argc, &argv);\n\n  \n\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  \n  #ifdef DEBUG\n  printf (\"\\n===MAESTRO=== MPI task # %d started OK\",myrank);\n  fflush(stdout);\n  #endif \n\n\n  if (myrank == 0) {\n    master();\n  } else {\n    slave();\n  }\n\n  #ifdef DEBUG\n  printf(\"\\n===MAESTRO=== Task %d ; Bye bye\",myrank);\n  fflush(stdout);\n  #endif\n  \n\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "benkirk_915", "code": "int main(int argc, char *argv[])\n{\n    int i = 0, rank;\n    int numprocs;\n    double avg_time = 0.0, max_time = 0.0, min_time = 0.0;\n    double latency = 0.0, t_start = 0.0, t_stop = 0.0;\n    double timer=0.0;\n    int po_ret;\n    options.bench = COLLECTIVE;\n    options.subtype = LAT;\n\n    set_header(HEADER);\n    set_benchmark_name(\"osu_barrier\");\n    po_ret = process_options(argc, argv);\n\n    if (PO_OKAY == po_ret && NONE != options.accel) {\n        if (init_accel()) {\n            fprintf(stderr, \"Error initializing device\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    options.show_size = 0;\n\n\n    switch (po_ret) {\n        case PO_BAD_USAGE:\n            print_bad_usage_message(rank);\n            exit(EXIT_FAILURE);\n        case PO_HELP_MESSAGE:\n            print_help_message(rank);\n            exit(EXIT_SUCCESS);\n        case PO_VERSION_MESSAGE:\n            print_version_message(rank);\n            exit(EXIT_SUCCESS);\n        case PO_OKAY:\n            break;\n    }\n\n    if (numprocs < 2) {\n        if (rank == 0) {\n            fprintf(stderr, \"This test requires at least two processes\\n\");\n        }\n\n\n        return EXIT_FAILURE;\n    }\n\n    print_preamble(rank);\n\n    timer = 0.0;\n\n    for (i = 0; i < options.iterations + options.skip; i++) {\n        t_start =\n        t_stop =\n\n        if (i>=options.skip){\n            timer+=t_stop-t_start;\n        }\n    }\n\n\n    latency = (timer * 1e6) / options.iterations;\n\n    avg_time = avg_time/numprocs;\n\n    print_stats(rank, 0, avg_time, min_time, max_time);\n\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char *argv[])\n{\n    int i = 0, rank;\n    int numprocs;\n    double avg_time = 0.0, max_time = 0.0, min_time = 0.0;\n    double latency = 0.0, t_start = 0.0, t_stop = 0.0;\n    double timer=0.0;\n    int po_ret;\n    options.bench = COLLECTIVE;\n    options.subtype = LAT;\n\n    set_header(HEADER);\n    set_benchmark_name(\"osu_barrier\");\n    po_ret = process_options(argc, argv);\n\n    if (PO_OKAY == po_ret && NONE != options.accel) {\n        if (init_accel()) {\n            fprintf(stderr, \"Error initializing device\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    options.show_size = 0;\n\n    MPI_CHECK(MPI_Init(&argc, &argv));\n    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank));\n    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &numprocs));\n\n    switch (po_ret) {\n        case PO_BAD_USAGE:\n            print_bad_usage_message(rank);\n            MPI_CHECK(MPI_Finalize());\n            exit(EXIT_FAILURE);\n        case PO_HELP_MESSAGE:\n            print_help_message(rank);\n            MPI_CHECK(MPI_Finalize());\n            exit(EXIT_SUCCESS);\n        case PO_VERSION_MESSAGE:\n            print_version_message(rank);\n            MPI_CHECK(MPI_Finalize());\n            exit(EXIT_SUCCESS);\n        case PO_OKAY:\n            break;\n    }\n\n    if (numprocs < 2) {\n        if (rank == 0) {\n            fprintf(stderr, \"This test requires at least two processes\\n\");\n        }\n\n        MPI_CHECK(MPI_Finalize());\n\n        return EXIT_FAILURE;\n    }\n\n    print_preamble(rank);\n\n    timer = 0.0;\n\n    for (i = 0; i < options.iterations + options.skip; i++) {\n        t_start = MPI_Wtime();\n        MPI_CHECK(MPI_Barrier(MPI_COMM_WORLD));\n        t_stop = MPI_Wtime();\n\n        if (i>=options.skip){\n            timer+=t_stop-t_start;\n        }\n    }\n\n    MPI_CHECK(MPI_Barrier(MPI_COMM_WORLD));\n\n    latency = (timer * 1e6) / options.iterations;\n\n    MPI_CHECK(MPI_Reduce(&latency, &min_time, 1, MPI_DOUBLE, MPI_MIN, 0,\n                MPI_COMM_WORLD));\n    MPI_CHECK(MPI_Reduce(&latency, &max_time, 1, MPI_DOUBLE, MPI_MAX, 0,\n                MPI_COMM_WORLD));\n    MPI_CHECK(MPI_Reduce(&latency, &avg_time, 1, MPI_DOUBLE, MPI_SUM, 0,\n                MPI_COMM_WORLD));\n    avg_time = avg_time/numprocs;\n\n    print_stats(rank, 0, avg_time, min_time, max_time);\n    MPI_CHECK(MPI_Finalize());\n\n    return EXIT_SUCCESS;\n}"}
{"program": "CoryMcCartan_916", "code": "int main(int argc, char *argv[])\n{\n\n    int myid, numprocs, i, j;\n    int size, align_size;\n    char *s_buf, *r_buf;\n    double t_start = 0.0, t_end = 0.0, t = 0.0;\n\n\n    align_size = getpagesize();\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    if (myid == 0) {\n        fprintf(stdout,\n        fprintf(stdout, \"# Modified to report: 1 MB == 2^20 bytes\\n\");\n        fprintf(stdout, \"# Size\\t\\tBi-Bandwidth (MB/s) \\n\");\n    }\n\n    for (size = 1; size <= MAX_MSG_SIZE; size *= 2) {\n\n        \n\n        for (i = 0; i < size; i++) {\n            s_buf[i] = 'a';\n            r_buf[i] = 'b';\n        }\n\n        if (size > large_message_size) {\n            loop = loop_large;\n            skip = skip_large;\n            window_size = window_size_large;\n        }\n\n        if (myid == 0) {\n            for (i = 0; i < loop + skip; i++) {\n\n                if (i == skip)\n                    t_start =\n                for (j = 0; j < window_size; j++)\n                for (j = 0; j < window_size; j++)\n\n            }\n\n            t_end =\n            t = t_end - t_start;\n\n        } else if (myid == 1) {\n\n            for (i = 0; i < loop + skip; i++) {\n\n                for (j = 0; j < window_size; j++)\n                for (j = 0; j < window_size; j++)\n\n            }\n\n        }\n\n        if (myid == 0) {\n            double tmp;\n          #if 0\n            tmp = ((size * 1.0) / 1.0e6) * loop * window_size * 2;\n          #else\n            \n\n            tmp = ((size * 1.0) / (1024*1024)) * loop * window_size * 2;\n          #endif\n            fprintf(stdout, \"%d\\t\\t%f\\n\", size, tmp / t);\n        }\n\n    }\n\n    return 0;\n\n}", "label": "int main(int argc, char *argv[])\n{\n\n    int myid, numprocs, i, j;\n    int size, align_size;\n    char *s_buf, *r_buf;\n    double t_start = 0.0, t_end = 0.0, t = 0.0;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\n    align_size = getpagesize();\n    s_buf =\n        (char *) (((unsigned long) s_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n    r_buf =\n        (char *) (((unsigned long) r_buf1 + (align_size - 1)) /\n                  align_size * align_size);\n\n    if (myid == 0) {\n        fprintf(stdout,\n                \"# OSU MPI Bidirectional Bandwidth Test (Version 2.0)\\n\");\n        fprintf(stdout, \"# Modified to report: 1 MB == 2^20 bytes\\n\");\n        fprintf(stdout, \"# Size\\t\\tBi-Bandwidth (MB/s) \\n\");\n    }\n\n    for (size = 1; size <= MAX_MSG_SIZE; size *= 2) {\n\n        \n\n        for (i = 0; i < size; i++) {\n            s_buf[i] = 'a';\n            r_buf[i] = 'b';\n        }\n\n        if (size > large_message_size) {\n            loop = loop_large;\n            skip = skip_large;\n            window_size = window_size_large;\n        }\n\n        if (myid == 0) {\n            for (i = 0; i < loop + skip; i++) {\n\n                if (i == skip)\n                    t_start = MPI_Wtime();\n                for (j = 0; j < window_size; j++)\n                    MPI_Irecv(r_buf, size, MPI_CHAR, 1, 10,\n                              MPI_COMM_WORLD, recv_request + j);\n                for (j = 0; j < window_size; j++)\n                    MPI_Isend(s_buf, size, MPI_CHAR, 1, 100,\n                              MPI_COMM_WORLD, send_request + j);\n                MPI_Waitall(window_size, send_request, reqstat);\n                MPI_Waitall(window_size, recv_request, reqstat);\n\n            }\n\n            t_end = MPI_Wtime();\n            t = t_end - t_start;\n\n        } else if (myid == 1) {\n\n            for (i = 0; i < loop + skip; i++) {\n\n                for (j = 0; j < window_size; j++)\n                    MPI_Irecv(r_buf, size, MPI_CHAR, 0, 100,\n                              MPI_COMM_WORLD, recv_request + j);\n                for (j = 0; j < window_size; j++)\n                    MPI_Isend(s_buf, size, MPI_CHAR, 0, 10,\n                              MPI_COMM_WORLD, send_request + j);\n                MPI_Waitall(window_size, send_request, reqstat);\n                MPI_Waitall(window_size, recv_request, reqstat);\n\n            }\n\n        }\n\n        if (myid == 0) {\n            double tmp;\n          #if 0\n            tmp = ((size * 1.0) / 1.0e6) * loop * window_size * 2;\n          #else\n            \n\n            tmp = ((size * 1.0) / (1024*1024)) * loop * window_size * 2;\n          #endif\n            fprintf(stdout, \"%d\\t\\t%f\\n\", size, tmp / t);\n        }\n\n    }\n\n    MPI_Finalize();\n    return 0;\n\n}"}
{"program": "adammoody_917", "code": "int main(int argc, char **argv)\n{\n   int me,tasks,i, errcount=0;\n   double start,end,diff,avg_diff_usec;\n\n   if(tasks < 2)\n   {\n\t   printf(\"MUST RUN WITH AT LEAST 2 TASKS\\n\");\n\t   errcount++;\n\t   exit(0);\n   }\n\n\n\n   if (!me) {\n     start =\n   }\n\n   for(i=0;i<BARRIER_COUNT;i++)\n\n   if (!me) {\n     end =\n     diff = end - start;\n     avg_diff_usec = diff * (1000000/BARRIER_COUNT);\n     printf(\"AFTER BARRIERS, START TIME = %f, END TIME = %f, DIFF (sec) = %f,\\n\",start,end,diff);\n     printf(\"\\t\\tITERS = %d, AVG (usec) = %f, EXPECTED = %d\\n\",BARRIER_COUNT,avg_diff_usec, EXPECTED_AVG_uSEC);\n     if (avg_diff_usec < EXPECTED_AVG_uSEC) {\n       printf (\"PASSED\\n\");\n     }\n     else if (avg_diff_usec < (2* EXPECTED_AVG_uSEC)) {\n       printf (\"Acceptable\\n\");\n     }\n     else {\n       printf (\"FAILED\\n\");\n     }\n     fflush (stdout);\n   }\n\nreturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n   int me,tasks,i, errcount=0;\n   double start,end,diff,avg_diff_usec;\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&tasks);\n   if(tasks < 2)\n   {\n\t   printf(\"MUST RUN WITH AT LEAST 2 TASKS\\n\");\n\t   errcount++;\n\t   MPI_Finalize();\n\t   exit(0);\n   }\n\n   MPI_Comm_rank(MPI_COMM_WORLD,&me);\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   if (!me) {\n     start = MPI_Wtime();\n   }\n\n   for(i=0;i<BARRIER_COUNT;i++)\n     MPI_Barrier(MPI_COMM_WORLD);\n\n   if (!me) {\n     end = MPI_Wtime();\n     diff = end - start;\n     avg_diff_usec = diff * (1000000/BARRIER_COUNT);\n     printf(\"AFTER BARRIERS, START TIME = %f, END TIME = %f, DIFF (sec) = %f,\\n\",start,end,diff);\n     printf(\"\\t\\tITERS = %d, AVG (usec) = %f, EXPECTED = %d\\n\",BARRIER_COUNT,avg_diff_usec, EXPECTED_AVG_uSEC);\n     if (avg_diff_usec < EXPECTED_AVG_uSEC) {\n       printf (\"PASSED\\n\");\n     }\n     else if (avg_diff_usec < (2* EXPECTED_AVG_uSEC)) {\n       printf (\"Acceptable\\n\");\n     }\n     else {\n       printf (\"FAILED\\n\");\n     }\n     fflush (stdout);\n   }\n\n   MPI_Finalize();\nreturn 0;\n}"}
{"program": "Unidata_918", "code": "int main(int argc, char* argv[])\n{\n   int rank, nprocs, ncid, cmode;\n   MPI_Comm comm = MPI_COMM_SELF;\n   MPI_Info info = MPI_INFO_NULL;\n\n   if (rank > 0)\n      return 2;\n\n   printf(\"\\nWrite using PNETCDF; Read using classic netCDF...\");\n   {\n      \n\n      cmode = NC_CLOBBER;\n      if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR;\n      if (write2(ncid, 1)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      if (nc_open_par(FILENAME, NC_WRITE, comm, info, &ncid)) ERR;\n      if (extend(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      if (nc_open(FILENAME, 0, &ncid)) ERR;\n      if (read2(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n   }\n   SUMMARIZE_ERR;\n\n   printf(\"\\nWrite using CDF-5; Read using PNETCDF...\");\n   {\n      \n\n      cmode = NC_CDF5 | NC_CLOBBER;\n      if (nc_create(FILENAME, cmode, &ncid)) ERR;\n      if (write2(ncid, 0)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      if (nc_open(FILENAME, NC_WRITE, &ncid)) ERR;\n      if (extend(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      cmode = NC_NOCLOBBER;\n      if (nc_open_par(FILENAME, cmode, comm, info, &ncid)) ERR;\n      if (read2(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n   }\n   SUMMARIZE_ERR;\n\n   FINAL_RESULTS;\n   return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n   int rank, nprocs, ncid, cmode;\n   MPI_Comm comm = MPI_COMM_SELF;\n   MPI_Info info = MPI_INFO_NULL;\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (rank > 0)\n      return 2;\n\n   printf(\"\\nWrite using PNETCDF; Read using classic netCDF...\");\n   {\n      \n\n      cmode = NC_CLOBBER;\n      if (nc_create_par(FILENAME, cmode, comm, info, &ncid)) ERR;\n      if (write2(ncid, 1)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      if (nc_open_par(FILENAME, NC_WRITE, comm, info, &ncid)) ERR;\n      if (extend(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      if (nc_open(FILENAME, 0, &ncid)) ERR;\n      if (read2(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n   }\n   SUMMARIZE_ERR;\n\n   printf(\"\\nWrite using CDF-5; Read using PNETCDF...\");\n   {\n      \n\n      cmode = NC_CDF5 | NC_CLOBBER;\n      if (nc_create(FILENAME, cmode, &ncid)) ERR;\n      if (write2(ncid, 0)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      if (nc_open(FILENAME, NC_WRITE, &ncid)) ERR;\n      if (extend(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n      cmode = NC_NOCLOBBER;\n      if (nc_open_par(FILENAME, cmode, comm, info, &ncid)) ERR;\n      if (read2(ncid)) ERR;\n      if (nc_close(ncid)) ERR;\n   }\n   SUMMARIZE_ERR;\n\n   MPI_Finalize();\n   FINAL_RESULTS;\n   return 0;\n}"}
{"program": "jackd_921", "code": "void production_schur_test( int argc, char *argv[] )\n{\n\t\n\n\tint loop;\n\tTMPI_dat This;\n\tTdomain dom;\n\tTmtx_CRS_dist Ad;\n\tTvec_dist x, b;\n\tint i, thisroot, root=0, nloop=7;\n\tTgmres run;\n\tTprecon P;\n\tchar *fname_stub;\n\tdouble precon_time, gmres_time;\n\t\n\n\n\n\n\n\n\n\n\n\n\n\tAd.init = Ad.mtx.init  = 0;\n\tx.init = b.init = 0;\n\tP.init = 0;\n\t\n\t\n\n\t\n\t\n\n\tBMPI_init( argc, argv, &This );\n\t\n\t\n\n\tBMPI_local_comm_create( &This );\n\t\t\n\tif( root==This.this_proc )\n\t\tthisroot = 1;\n\telse\n\t\tthisroot = 0;\n\t\n\t\n\n\n\t\n\n\tif( !This.this_proc )\n\t\tprintf( \"loading matrix...\\n\" );\n\tfname_stub = argv[1];\n\n\tif( !jacobian_load( &Ad, &dom, fname_stub, &This ) )\n\t{\n\t\tif( thisroot )\n\t\t\tprintf( \"Unable to reload the jacobian data\\n\\n\" );\n\t\treturn;\n\t}\n\t\t\n\t\n\n\tif( !This.this_proc )\n\t\tprintf( \"loading preconditioner parameters... %s\\n\", \"./params/precon.txt\" );\n\t\n\tif( !precon_load( &P, &Ad, \"./params/precon.txt\") )\n\t{\n\t\tprintf( \"ERROR : Unable to load preconditioner\\n\" );\n\t\treturn;\t\t\n\t}\n\tif( !This.this_proc )\n\t{\n\t\tprintf( \"calculating preconditioner... \" );\n\t\tprecon_print_name( stdout, P.type );\n\t}\t\n\t\n\t\n\n\tif( !This.this_proc )\n\t\tprintf( \"loading GMRES info... %s\\n\", \"./params/GMRES.txt\" );\n\tif( !GMRES_params_load( &run, \"./params/GMRES.txt\" ) )\n\t{\n\t\tprintf( \"P%d : ERROR loading GMRES info\\n\", This.this_proc );\n\t\texit(1);\n\t}\n\n\t\n\n\t\n\n\tprecon_time = gmres_time = 0.;\n\tfor( loop=0; loop<nloop; loop++ )\n\t{\n\n\t\tprecon_time = MPI_Wtime() - precon_time;\n\n\t\t\n\n\t\t\n\n\t\tprecon_init( &P, &Ad, P.type );\n\t\t\n\t\tif( P.type==PRECON_SCHUR )\n\t\t{\n\t\t\tif( !precon( &P, &Ad, &dom ) )\n\t\t\t{\n\t\t\t\tprintf( \"ERROR : Unable to form preconditioner\\n\" );\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\telse if( !precon( &P, &Ad, NULL ) )\n\t\t{\n\t\t\tprintf( \"ERROR : Unable to form preconditioner\\n\" );\n\t\t\treturn;\n\t\t}\n\t\t\n\t\tprecon_time = MPI_Wtime() - precon_time;\n\n\t\t\n\n\t\tvec_dist_init( &b, &This, Ad.mtx.nrows, Ad.mtx.block, Ad.vtxdist );\n\t\tvec_dist_init_vec( &x, &b );\n\t\t\n\t\t\n\n\t\tfor( i=0; i<x.n; i++ )\n\t\t\tx.dat[i] = 1;\n\t\t\n\t\tmtx_CRS_dist_gemv( &Ad, &x, &b, 1., 0., 'N' );\n\n\t\tvec_dist_clear( &x );\n\t\t\n\t\t\n\n\n\n\n\n\t\tgmresFH( &Ad, &b, &x, &run, &P, 0 );\n\n\t\tgmres_time += run.time_gmres;\n\t}\n\t\n\n\tif( !This.this_proc )\n\t\tfprintf( stdout, \"\\nTimes\\n\\tpreconditioner calculation :\\t%g seconds\\n\\tGMRES iterations :\\t\\t%g seconds\\n\", precon_time/(double)nloop, gmres_time/(double)nloop );\n\t\n\t\n\tif( !This.this_proc )\n\t\tprintf( \"\\nFreeing data...\\n\" );\n\t\n\tprecon_free( &P );\n\tvec_dist_free( &x );\n\tvec_dist_free( &b );\n\tmtx_CRS_dist_free( &Ad );\n\tGMRES_params_free( &run );\n\tBMPI_free( &This );\n\n\n\n\n\n        domain_free( &dom );\n\n\n\t\n\t\n\n}", "label": "void production_schur_test( int argc, char *argv[] )\n{\n\t\n\n\tint loop;\n\tTMPI_dat This;\n\tTdomain dom;\n\tTmtx_CRS_dist Ad;\n\tTvec_dist x, b;\n\tint i, thisroot, root=0, nloop=7;\n\tTgmres run;\n\tTprecon P;\n\tchar *fname_stub;\n\tdouble precon_time, gmres_time;\n\t\n\n\n\n\n\n\n\n\n\n\n\n\tAd.init = Ad.mtx.init  = 0;\n\tx.init = b.init = 0;\n\tP.init = 0;\n\t\n\t\n\n\t\n\t\n\n\tBMPI_init( argc, argv, &This );\n\t\n\t\n\n\tBMPI_local_comm_create( &This );\n\t\t\n\tif( root==This.this_proc )\n\t\tthisroot = 1;\n\telse\n\t\tthisroot = 0;\n\t\n\t\n\n\n\t\n\n\tif( !This.this_proc )\n\t\tprintf( \"loading matrix...\\n\" );\n\tfname_stub = argv[1];\n\n\tif( !jacobian_load( &Ad, &dom, fname_stub, &This ) )\n\t{\n\t\tif( thisroot )\n\t\t\tprintf( \"Unable to reload the jacobian data\\n\\n\" );\n\t\tMPI_Finalize();\n\t\treturn;\n\t}\n\t\t\n\t\n\n\tif( !This.this_proc )\n\t\tprintf( \"loading preconditioner parameters... %s\\n\", \"./params/precon.txt\" );\n\t\n\tif( !precon_load( &P, &Ad, \"./params/precon.txt\") )\n\t{\n\t\tprintf( \"ERROR : Unable to load preconditioner\\n\" );\n\t\tMPI_Finalize();\n\t\treturn;\t\t\n\t}\n\tif( !This.this_proc )\n\t{\n\t\tprintf( \"calculating preconditioner... \" );\n\t\tprecon_print_name( stdout, P.type );\n\t}\t\n\t\n\t\n\n\tif( !This.this_proc )\n\t\tprintf( \"loading GMRES info... %s\\n\", \"./params/GMRES.txt\" );\n\tif( !GMRES_params_load( &run, \"./params/GMRES.txt\" ) )\n\t{\n\t\tprintf( \"P%d : ERROR loading GMRES info\\n\", This.this_proc );\n\t\tMPI_Finalize();\n\t\texit(1);\n\t}\n\n\t\n\n\t\n\n\tprecon_time = gmres_time = 0.;\n\tfor( loop=0; loop<nloop; loop++ )\n\t{\n\n\t\tMPI_Barrier( This.comm );\n\t\tprecon_time = MPI_Wtime() - precon_time;\n\n\t\t\n\n\t\t\n\n\t\tprecon_init( &P, &Ad, P.type );\n\t\t\n\t\tif( P.type==PRECON_SCHUR )\n\t\t{\n\t\t\tif( !precon( &P, &Ad, &dom ) )\n\t\t\t{\n\t\t\t\tprintf( \"ERROR : Unable to form preconditioner\\n\" );\n\t\t\t\tMPI_Finalize();\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\telse if( !precon( &P, &Ad, NULL ) )\n\t\t{\n\t\t\tprintf( \"ERROR : Unable to form preconditioner\\n\" );\n\t\t\tMPI_Finalize();\n\t\t\treturn;\n\t\t}\n\t\t\n\t\tMPI_Barrier( This.comm );\n\t\tprecon_time = MPI_Wtime() - precon_time;\n\n\t\t\n\n\t\tvec_dist_init( &b, &This, Ad.mtx.nrows, Ad.mtx.block, Ad.vtxdist );\n\t\tvec_dist_init_vec( &x, &b );\n\t\t\n\t\t\n\n\t\tfor( i=0; i<x.n; i++ )\n\t\t\tx.dat[i] = 1;\n\t\t\n\t\tmtx_CRS_dist_gemv( &Ad, &x, &b, 1., 0., 'N' );\n\n\t\tvec_dist_clear( &x );\n\t\t\n\t\t\n\n\n\n\n\n\t\tgmresFH( &Ad, &b, &x, &run, &P, 0 );\n\n\t\tgmres_time += run.time_gmres;\n\t}\n\t\n\n\tif( !This.this_proc )\n\t\tfprintf( stdout, \"\\nTimes\\n\\tpreconditioner calculation :\\t%g seconds\\n\\tGMRES iterations :\\t\\t%g seconds\\n\", precon_time/(double)nloop, gmres_time/(double)nloop );\n\t\n\tMPI_Barrier( This.comm );\n\t\n\tif( !This.this_proc )\n\t\tprintf( \"\\nFreeing data...\\n\" );\n\t\n\tprecon_free( &P );\n\tvec_dist_free( &x );\n\tvec_dist_free( &b );\n\tmtx_CRS_dist_free( &Ad );\n\tGMRES_params_free( &run );\n\tBMPI_free( &This );\n\n\n\n\n\n        domain_free( &dom );\n\n\n\t\n\t\n\n\tMPI_Finalize();\n}"}
{"program": "opencb_922", "code": "int main(int argc, char *argv[]) {\n    size_t order, num_variants, stride, num_blocks_per_dim, num_block_coords, max_num_block_coords;\n    int num_mpi_ranks = 1, mpi_rank;\n    \n    if (argc < 3) {\n        printf(\"Usage: mpirun --np N mpi <order> <num_variants> <stride>\\nExample: mpirun --np 4 mpi 2 100000 20\\n\");\n        exit(1);\n    }\n    \n    \n    order = atoi(argv[1]);\n    num_variants = atol(argv[2]);\n    stride = atoi(argv[3]);\n    num_blocks_per_dim = ceil((double) num_variants / stride);\n    num_block_coords = 0;\n    max_num_block_coords = (size_t) pow(num_blocks_per_dim, order);\n    \n    int *block_coords, *my_block_coords;\n    int curr_idx, next_idx;\n    \n    if (mpi_rank == 0) {\n        printf(\"Blocks per dim = %zu\\tMax block coords = %zu\\n\", num_blocks_per_dim, max_num_block_coords);\n        block_coords = calloc(max_num_block_coords * order, sizeof(int));\n        \n        \n\n        do {\n            curr_idx = num_block_coords * order;\n            next_idx = curr_idx + order;\n            memcpy(block_coords + next_idx, block_coords + curr_idx, order * sizeof(int));\n            curr_idx = next_idx;\n            num_block_coords++;\n\n\n        } while (get_next_block(num_blocks_per_dim, order, block_coords + curr_idx));\n    }\n    \n    \n\n    \n    \n\n    int block_counts[num_mpi_ranks];\n    int block_offsets[num_mpi_ranks];\n\n    if (mpi_rank == 0) {\n        printf(\"nbc = %zu\\tmod = %zu\\n\", num_block_coords, num_block_coords % num_mpi_ranks);\n    }\n    \n    for (int p = 0; p < num_mpi_ranks; p++) {\n        if (p < num_block_coords % num_mpi_ranks) {\n            block_counts[p] = order * (num_block_coords / num_mpi_ranks + 1);\n        } else {\n            block_counts[p] = order * (num_block_coords / num_mpi_ranks);\n        }\n    }\n\n    block_offsets[0] = 0;\n    for (int p = 1; p < num_mpi_ranks; p++) {\n        block_offsets[p] = block_offsets[p-1] + block_counts[p-1];\n    }\n    \n    my_block_coords = calloc(block_counts[mpi_rank] * order, sizeof(int));\n    \n    if (mpi_rank == 0) {\n        for (int p = 0; p < num_mpi_ranks; p++) {\n            printf(\"(%d, off %d) \", block_counts[p], block_offsets[p]);\n        }\n        printf(\"\\n\");\n    }\n    \n    \n\n    \n    printf(\"I'm MPI process %d of %d and have to work with %zu blocks\\n\", mpi_rank, num_mpi_ranks, block_counts[mpi_rank] / order);\n    \n    for (int i = 0; i < block_counts[mpi_rank] / order; i++) {\n        printf(\"*%d* (%d %d)\\n\", mpi_rank, my_block_coords[i * order], my_block_coords[i * order + 1]);\n    }\n    \n    \n    if (mpi_rank == 0) {\n        free(block_coords);\n    }\n    free(my_block_coords);\n    \n    \n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    size_t order, num_variants, stride, num_blocks_per_dim, num_block_coords, max_num_block_coords;\n    int num_mpi_ranks = 1, mpi_rank;\n    \n    if (argc < 3) {\n        printf(\"Usage: mpirun --np N mpi <order> <num_variants> <stride>\\nExample: mpirun --np 4 mpi 2 100000 20\\n\");\n        exit(1);\n    }\n    \n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_mpi_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    \n    order = atoi(argv[1]);\n    num_variants = atol(argv[2]);\n    stride = atoi(argv[3]);\n    num_blocks_per_dim = ceil((double) num_variants / stride);\n    num_block_coords = 0;\n    max_num_block_coords = (size_t) pow(num_blocks_per_dim, order);\n    \n    int *block_coords, *my_block_coords;\n    int curr_idx, next_idx;\n    \n    if (mpi_rank == 0) {\n        printf(\"Blocks per dim = %zu\\tMax block coords = %zu\\n\", num_blocks_per_dim, max_num_block_coords);\n        block_coords = calloc(max_num_block_coords * order, sizeof(int));\n        \n        \n\n        do {\n            curr_idx = num_block_coords * order;\n            next_idx = curr_idx + order;\n            memcpy(block_coords + next_idx, block_coords + curr_idx, order * sizeof(int));\n            curr_idx = next_idx;\n            num_block_coords++;\n\n\n        } while (get_next_block(num_blocks_per_dim, order, block_coords + curr_idx));\n    }\n    \n    \n\n    MPI_Bcast(&num_block_coords, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    \n\n    int block_counts[num_mpi_ranks];\n    int block_offsets[num_mpi_ranks];\n\n    if (mpi_rank == 0) {\n        printf(\"nbc = %zu\\tmod = %zu\\n\", num_block_coords, num_block_coords % num_mpi_ranks);\n    }\n    \n    for (int p = 0; p < num_mpi_ranks; p++) {\n        if (p < num_block_coords % num_mpi_ranks) {\n            block_counts[p] = order * (num_block_coords / num_mpi_ranks + 1);\n        } else {\n            block_counts[p] = order * (num_block_coords / num_mpi_ranks);\n        }\n    }\n\n    block_offsets[0] = 0;\n    for (int p = 1; p < num_mpi_ranks; p++) {\n        block_offsets[p] = block_offsets[p-1] + block_counts[p-1];\n    }\n    \n    my_block_coords = calloc(block_counts[mpi_rank] * order, sizeof(int));\n    \n    if (mpi_rank == 0) {\n        for (int p = 0; p < num_mpi_ranks; p++) {\n            printf(\"(%d, off %d) \", block_counts[p], block_offsets[p]);\n        }\n        printf(\"\\n\");\n    }\n    \n    \n\n    MPI_Scatterv(block_coords, block_counts, block_offsets, MPI_INT, \n                 my_block_coords, block_counts[mpi_rank], MPI_INT,\n                 0, MPI_COMM_WORLD);\n    \n    printf(\"I'm MPI process %d of %d and have to work with %zu blocks\\n\", mpi_rank, num_mpi_ranks, block_counts[mpi_rank] / order);\n    \n    for (int i = 0; i < block_counts[mpi_rank] / order; i++) {\n        printf(\"*%d* (%d %d)\\n\", mpi_rank, my_block_coords[i * order], my_block_coords[i * order + 1]);\n    }\n    \n    \n    if (mpi_rank == 0) {\n        free(block_coords);\n    }\n    free(my_block_coords);\n    \n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "tancheng_925", "code": "int main(int argc, char *argv[])\n{   \n\tint rank;\n\tint n_ranks = 6;\n\tint n = 10;\n\n\n#ifndef DUMP\n#endif\n\n\tint END = 50;\n\n#ifdef DUMP\n\tif(rank == 0)\n\t{       m5_dump_stats(0, 0);\n\t\tm5_reset_stats(0, 0);\n\t}\n#endif\n\n\tfor(n = 0; n < END; n++)\n\t{\n\t\tif(rank == 0)\n\t\t{\n\t\t\tu8 cipher_text[16] = {0xec,0xae,0xb0,0x2a,0xf2,0x51,0x45,0x25,0xb4,0x19,0x18,0x70,0x10,0x2,0x5,0x12};\n\t\t\trijndaelDecrypt_Master(decpt_rk, cipher_text, rank);\n\t\t}\n\t\telse if(rank == n_ranks - 1)\n\t\t{\n\t\t\tu8 decipher_text[16] = {0};\n\t\t\trijndaelDecrypt_Final(decpt_rk, decipher_text, rank);\n\t\t}\n\t\telse\n\t\t\trijndaelDecrypt_Middle(decpt_rk, rank);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{   \n\tint rank;\n\tint n_ranks = 6;\n\tint n = 10;\n\n\tMPI_Init(&argc, &argv);\n\n#ifndef DUMP\n\tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n#endif\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tint END = 50;\n\n#ifdef DUMP\n\tif(rank == 0)\n\t{       m5_dump_stats(0, 0);\n\t\tm5_reset_stats(0, 0);\n\t}\n#endif\n\n\tfor(n = 0; n < END; n++)\n\t{\n\t\tif(rank == 0)\n\t\t{\n\t\t\tu8 cipher_text[16] = {0xec,0xae,0xb0,0x2a,0xf2,0x51,0x45,0x25,0xb4,0x19,0x18,0x70,0x10,0x2,0x5,0x12};\n\t\t\trijndaelDecrypt_Master(decpt_rk, cipher_text, rank);\n\t\t}\n\t\telse if(rank == n_ranks - 1)\n\t\t{\n\t\t\tu8 decipher_text[16] = {0};\n\t\t\trijndaelDecrypt_Final(decpt_rk, decipher_text, rank);\n\t\t}\n\t\telse\n\t\t\trijndaelDecrypt_Middle(decpt_rk, rank);\n\t}\n\n\tMPI_Finalize();   \n\treturn 0;\n}"}
{"program": "CoryMcCartan_926", "code": "int main(int argc, char* argv[]) {\n  unsigned long long i, locN_U, locStart, locEnd;\n  rndtype r, recvBuf[2];\n  MPI_Request recvReq;\n  int base, iter, errorCnt = 0, totalErrors;\n  double starttime, finishtime, exectime;\n\n\n  parseArgs(argc, argv);\n\n  if (rank == 0) {\n    printf(\"Number of nodes = %d\\n\", nprocs); fflush(stdout);\n    printf(\"Problem size = %lld (2**%d)\\n\", m, n); fflush(stdout);\n    printf(\"Number of updates = %lld (2**%d)\\n\", N_U, N_U_log2); fflush(stdout);\n  }\n  randInit();\n\n  A = malloc(sizeof(rndtype)*m/nprocs);\n  if (A == NULL) {\n    printf(\"malloc failed for main array.\\n\"); fflush(stdout);\n  }\n  \n  base = rank * (m / nprocs);\n  for (i=0; i < m / nprocs; i++) {\n    A[i] = i + base;\n  }\n\n  \n\n  for (iter=0; iter < 2; iter++) {\n    doneCount = 0;\n    doneWithRecvs = 0;\n\n    if (iter == 0) {\n      starttime = now_time();\n    }\n\n\n    locN_U = N_U / nprocs;\n    locStart = locN_U * rank;\n    locEnd = locStart+locN_U;\n    r = getNthRandom(locStart);\n    for (i=locStart; i < locEnd; i++) {\n      int loc = idxToLocale(r & idxMask);\n      if (loc < 0 || loc > nprocs-1) {\n        printf(\"error: r=%llu, r&mask=%llu, loc=%d\\n\", r, r&idxMask, loc);\n      }\n      if (loc == rank) {\n        A[ind2localIdx(r & idxMask)] ^= r;\n      } else {\n        dosend(loc, r, 0, &recvReq, recvBuf);\n      }\n      getNextRandom(&r);\n    }\n    for (i = 0; i < nprocs; i++) {\n      dosend(i, 0, 1, &recvReq, recvBuf);\n    }\n    while (!doneWithRecvs) {\n      tryRecv(&recvReq, recvBuf);\n    }\n\n    if (iter == 0) {\n      finishtime = now_time();\n      exectime = (finishtime - starttime) / 1.0e+6;\n    }\n  }\n\n  for (i=0; i < m/nprocs; i++) {\n    if (A[i] != i + base) {\n      errorCnt++;\n    }\n  }\n\n\n  if (rank == 0) {\n    printf(\"Number of errors: %d\\n\", totalErrors); fflush(stdout);\n    if (totalErrors <= errorTolerance * N_U) {\n      printf(\"Validation: SUCCESS\\n\"); fflush(stdout);\n    } else {\n      printf(\"Validation: FAILURE\\n\"); fflush(stdout);\n    }\n    printf(\"Execution time = %lf\\n\", exectime); fflush(stdout);\n    printf(\"Performance (GUPS) = %lf\\n\", (N_U / exectime) * 1.0e-9); fflush(stdout);\n  }\n\n\n  return 0;\n}", "label": "int main(int argc, char* argv[]) {\n  unsigned long long i, locN_U, locStart, locEnd;\n  rndtype r, recvBuf[2];\n  MPI_Request recvReq;\n  int base, iter, errorCnt = 0, totalErrors;\n  double starttime, finishtime, exectime;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  parseArgs(argc, argv);\n\n  if (rank == 0) {\n    printf(\"Number of nodes = %d\\n\", nprocs); fflush(stdout);\n    printf(\"Problem size = %lld (2**%d)\\n\", m, n); fflush(stdout);\n    printf(\"Number of updates = %lld (2**%d)\\n\", N_U, N_U_log2); fflush(stdout);\n  }\n  randInit();\n\n  A = malloc(sizeof(rndtype)*m/nprocs);\n  if (A == NULL) {\n    printf(\"malloc failed for main array.\\n\"); fflush(stdout);\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n  \n  base = rank * (m / nprocs);\n  for (i=0; i < m / nprocs; i++) {\n    A[i] = i + base;\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  \n\n  for (iter=0; iter < 2; iter++) {\n    doneCount = 0;\n    doneWithRecvs = 0;\n\n    if (iter == 0) {\n      starttime = now_time();\n    }\n\n    MPI_Irecv(recvBuf, 2*sizeof(rndtype), MPI_BYTE, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD, &recvReq);\n\n    locN_U = N_U / nprocs;\n    locStart = locN_U * rank;\n    locEnd = locStart+locN_U;\n    r = getNthRandom(locStart);\n    for (i=locStart; i < locEnd; i++) {\n      int loc = idxToLocale(r & idxMask);\n      if (loc < 0 || loc > nprocs-1) {\n        printf(\"error: r=%llu, r&mask=%llu, loc=%d\\n\", r, r&idxMask, loc);\n      }\n      if (loc == rank) {\n        A[ind2localIdx(r & idxMask)] ^= r;\n      } else {\n        dosend(loc, r, 0, &recvReq, recvBuf);\n      }\n      getNextRandom(&r);\n    }\n    for (i = 0; i < nprocs; i++) {\n      dosend(i, 0, 1, &recvReq, recvBuf);\n    }\n    while (!doneWithRecvs) {\n      tryRecv(&recvReq, recvBuf);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (iter == 0) {\n      finishtime = now_time();\n      exectime = (finishtime - starttime) / 1.0e+6;\n    }\n  }\n\n  for (i=0; i < m/nprocs; i++) {\n    if (A[i] != i + base) {\n      errorCnt++;\n    }\n  }\n\n  MPI_Reduce(&errorCnt, &totalErrors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    printf(\"Number of errors: %d\\n\", totalErrors); fflush(stdout);\n    if (totalErrors <= errorTolerance * N_U) {\n      printf(\"Validation: SUCCESS\\n\"); fflush(stdout);\n    } else {\n      printf(\"Validation: FAILURE\\n\"); fflush(stdout);\n    }\n    printf(\"Execution time = %lf\\n\", exectime); fflush(stdout);\n    printf(\"Performance (GUPS) = %lf\\n\", (N_U / exectime) * 1.0e-9); fflush(stdout);\n  }\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "arjona00_927", "code": "int main(int argc, char** argv) {\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: avg num_elements_per_proc\\n\");\n    exit(1);\n  }\n\n  int num_elements_per_proc = atoi(argv[1]);\n\n\n  int world_rank;\n  int world_size;\n\n  \n\n  srand(time(NULL)*world_rank); \n\n  float *rand_nums = NULL;\n  rand_nums = create_rand_nums(num_elements_per_proc);\n\n  \n\n  float local_sum = 0;\n  int i;\n  for (i = 0; i < num_elements_per_proc; i++) {\n    local_sum += rand_nums[i];\n  }\n\n  \n\n  \n\n  float global_sum;\n  float mean = global_sum / (num_elements_per_proc * world_size);\n\n  \n\n  float local_sq_diff = 0;\n  for (i = 0; i < num_elements_per_proc; i++) {\n    local_sq_diff += (rand_nums[i] - mean) * (rand_nums[i] - mean);\n  }\n\n  \n\n  \n\n  float global_sq_diff;\n\n  \n\n  \n\n  if (world_rank == 0) {\n    float stddev = sqrt(global_sq_diff /\n                        (num_elements_per_proc * world_size));\n    printf(\"Mean - %f, Standard deviation = %f\\n\", mean, stddev);\n  }\n\n  \n\n  free(rand_nums);\n \n}", "label": "int main(int argc, char** argv) {\n  if (argc != 2) {\n    fprintf(stderr, \"Usage: avg num_elements_per_proc\\n\");\n    exit(1);\n  }\n\n  int num_elements_per_proc = atoi(argv[1]);\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  srand(time(NULL)*world_rank); \n\n  float *rand_nums = NULL;\n  rand_nums = create_rand_nums(num_elements_per_proc);\n\n  \n\n  float local_sum = 0;\n  int i;\n  for (i = 0; i < num_elements_per_proc; i++) {\n    local_sum += rand_nums[i];\n  }\n\n  \n\n  \n\n  float global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_FLOAT, MPI_SUM,\n                MPI_COMM_WORLD);\n  float mean = global_sum / (num_elements_per_proc * world_size);\n\n  \n\n  float local_sq_diff = 0;\n  for (i = 0; i < num_elements_per_proc; i++) {\n    local_sq_diff += (rand_nums[i] - mean) * (rand_nums[i] - mean);\n  }\n\n  \n\n  \n\n  float global_sq_diff;\n  MPI_Reduce(&local_sq_diff, &global_sq_diff, 1, MPI_FLOAT, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n\n  \n\n  \n\n  if (world_rank == 0) {\n    float stddev = sqrt(global_sq_diff /\n                        (num_elements_per_proc * world_size));\n    printf(\"Mean - %f, Standard deviation = %f\\n\", mean, stddev);\n  }\n\n  \n\n  free(rand_nums);\n \n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n}"}
{"program": "bmi-forum_929", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\tCommWorld;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\tint\t\tprocToWatch;\n\t\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tSet*\t\tsetA;\n\t\tSet*\t\tsetB;\n\t\tSet*\t\tsetC;\n\t\tunsigned\tint_I;\n\n\t\tsetA = Set_New( NULL, int, compareFunc, copyFunc, deleteFunc );\n\t\tsetB = Set_New( NULL, int, compareFunc, copyFunc, deleteFunc );\n\n\t\t\n\n\t\tfor( int_I = 0; int_I < 100000; int_I++ ) {\n\t\t\tSet_Insert( setA, &int_I );\n\t\t}\n\n\t\tfor( int_I = 50000; int_I < 150000; int_I++ ) {\n\t\t\tSet_Insert( setB, &int_I );\n\t\t}\n\n\t\tsetC = Set_Subtraction( setA, setB );\n\t\tSet_Traverse( setC, printSet, NULL );\n\n\t\tStg_Class_Delete( setA );\n\t\tStg_Class_Delete( setB );\n\t\tStg_Class_Delete( setC );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\tCommWorld;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\tint\t\tprocToWatch;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\tif( rank == procToWatch ) {\n\t\tSet*\t\tsetA;\n\t\tSet*\t\tsetB;\n\t\tSet*\t\tsetC;\n\t\tunsigned\tint_I;\n\n\t\tsetA = Set_New( NULL, int, compareFunc, copyFunc, deleteFunc );\n\t\tsetB = Set_New( NULL, int, compareFunc, copyFunc, deleteFunc );\n\n\t\t\n\n\t\tfor( int_I = 0; int_I < 100000; int_I++ ) {\n\t\t\tSet_Insert( setA, &int_I );\n\t\t}\n\n\t\tfor( int_I = 50000; int_I < 150000; int_I++ ) {\n\t\t\tSet_Insert( setB, &int_I );\n\t\t}\n\n\t\tsetC = Set_Subtraction( setA, setB );\n\t\tSet_Traverse( setC, printSet, NULL );\n\n\t\tStg_Class_Delete( setA );\n\t\tStg_Class_Delete( setB );\n\t\tStg_Class_Delete( setC );\n\t}\n\t\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "PSUmodeling_931", "code": "int main(int argc, char *argv[])\n{\n  UserData data;\n  SUNLinearSolver LS;\n  void *cvode_mem;\n  realtype abstol, reltol, t, tout;\n  N_Vector u;\n  int iout, my_pe, npes, retval, jpre;\n  sunindextype neq, local_N, mudq, mldq, mukeep, mlkeep;\n  MPI_Comm comm;\n\n  data = NULL;\n  LS = NULL;\n  cvode_mem = NULL;\n  u = NULL;\n\n  \n\n  neq = NVARS*MX*MY;\n\n  \n\n  comm = MPI_COMM_WORLD;\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      fprintf(stderr, \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\\n\",\n              npes, NPEX*NPEY);\n    return(1);\n  }\n\n  \n\n  local_N = NVARS*MXSUB*MYSUB;\n\n  \n\n  data = (UserData) malloc(sizeof *data);\n  InitUserData(my_pe, local_N, comm, data);\n\n  \n \n  u = N_VNew_Parallel(comm, local_N, neq);\n  SetInitialProfiles(u, data);\n  abstol = ATOL;\n  reltol = RTOL;\n\n  \n\n  cvode_mem = CVodeCreate(CV_BDF);\n\n  \n\n  retval = CVodeSetUserData(cvode_mem, data);\n\n  \n\n  retval = CVodeInit(cvode_mem, f, T0, u);\n  if(check_retval(&retval, \"CVodeInit\", 1, my_pe)) return(1);\n\n  \n\n  retval = CVodeSStolerances(cvode_mem, reltol, abstol);\n  if (check_retval(&retval, \"CVodeSStolerances\", 1, my_pe)) return(1);\n\n  \n\n  LS = SUNLinSol_SPGMR(u, PREC_LEFT, 0);\n\n  \n\n  retval = CVodeSetLinearSolver(cvode_mem, LS, NULL);\n\n  \n\n  mudq = mldq = NVARS*MXSUB;\n  mukeep = mlkeep = NVARS;\n  retval = CVBBDPrecInit(cvode_mem, local_N, mudq, mldq, \n                       mukeep, mlkeep, ZERO, flocal, NULL);\n\n  \n\n  if (my_pe == 0) PrintIntro(npes, mudq, mldq, mukeep, mlkeep);\n\n  \n\n  for (jpre = PREC_LEFT; jpre <= PREC_RIGHT; jpre++) {\n\n  \n\n\n  if (jpre == PREC_RIGHT) {\n\n    SetInitialProfiles(u, data);\n\n    retval = CVodeReInit(cvode_mem, T0, u);\n\n    retval = CVBBDPrecReInit(cvode_mem, mudq, mldq, ZERO);\n\n    retval = SUNLinSol_SPGMRSetPrecType(LS, PREC_RIGHT);\n\n    if (my_pe == 0) {\n      printf(\"\\n\\n-------------------------------------------------------\");\n      printf(\"------------\\n\");\n    }\n\n  }\n\n\n  if (my_pe == 0) {\n    printf(\"\\n\\nPreconditioner type is:  jpre = %s\\n\\n\",\n\t   (jpre == PREC_LEFT) ? \"PREC_LEFT\" : \"PREC_RIGHT\");\n  }\n\n  \n\n\n  for (iout = 1, tout = TWOHR; iout <= NOUT; iout++, tout += TWOHR) {\n    retval = CVode(cvode_mem, tout, u, &t, CV_NORMAL);\n    if(check_retval(&retval, \"CVode\", 1, my_pe)) break;\n    PrintOutput(cvode_mem, my_pe, comm, u, t);\n  }\n\n  \n\n\n  if (my_pe == 0) PrintFinalStats(cvode_mem);\n\n  } \n\n\n  \n\n  N_VDestroy_Parallel(u);\n  free(data);\n  CVodeFree(&cvode_mem);\n  SUNLinSolFree(LS);\n\n\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  UserData data;\n  SUNLinearSolver LS;\n  void *cvode_mem;\n  realtype abstol, reltol, t, tout;\n  N_Vector u;\n  int iout, my_pe, npes, retval, jpre;\n  sunindextype neq, local_N, mudq, mldq, mukeep, mlkeep;\n  MPI_Comm comm;\n\n  data = NULL;\n  LS = NULL;\n  cvode_mem = NULL;\n  u = NULL;\n\n  \n\n  neq = NVARS*MX*MY;\n\n  \n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &my_pe);\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      fprintf(stderr, \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\\n\",\n              npes, NPEX*NPEY);\n    MPI_Finalize();\n    return(1);\n  }\n\n  \n\n  local_N = NVARS*MXSUB*MYSUB;\n\n  \n\n  data = (UserData) malloc(sizeof *data);\n  if(check_retval((void *)data, \"malloc\", 2, my_pe)) MPI_Abort(comm, 1);\n  InitUserData(my_pe, local_N, comm, data);\n\n  \n \n  u = N_VNew_Parallel(comm, local_N, neq);\n  if(check_retval((void *)u, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  SetInitialProfiles(u, data);\n  abstol = ATOL;\n  reltol = RTOL;\n\n  \n\n  cvode_mem = CVodeCreate(CV_BDF);\n  if(check_retval((void *)cvode_mem, \"CVodeCreate\", 0, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  retval = CVodeSetUserData(cvode_mem, data);\n  if(check_retval(&retval, \"CVodeSetUserData\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  retval = CVodeInit(cvode_mem, f, T0, u);\n  if(check_retval(&retval, \"CVodeInit\", 1, my_pe)) return(1);\n\n  \n\n  retval = CVodeSStolerances(cvode_mem, reltol, abstol);\n  if (check_retval(&retval, \"CVodeSStolerances\", 1, my_pe)) return(1);\n\n  \n\n  LS = SUNLinSol_SPGMR(u, PREC_LEFT, 0);\n  if (check_retval((void *)LS, \"SUNLinSol_SPGMR\", 0, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  retval = CVodeSetLinearSolver(cvode_mem, LS, NULL);\n  if (check_retval(&retval, \"CVodeSetLinearSolver\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  mudq = mldq = NVARS*MXSUB;\n  mukeep = mlkeep = NVARS;\n  retval = CVBBDPrecInit(cvode_mem, local_N, mudq, mldq, \n                       mukeep, mlkeep, ZERO, flocal, NULL);\n  if(check_retval(&retval, \"CVBBDPrecInit\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  if (my_pe == 0) PrintIntro(npes, mudq, mldq, mukeep, mlkeep);\n\n  \n\n  for (jpre = PREC_LEFT; jpre <= PREC_RIGHT; jpre++) {\n\n  \n\n\n  if (jpre == PREC_RIGHT) {\n\n    SetInitialProfiles(u, data);\n\n    retval = CVodeReInit(cvode_mem, T0, u);\n    if(check_retval(&retval, \"CVodeReInit\", 1, my_pe)) MPI_Abort(comm, 1);\n\n    retval = CVBBDPrecReInit(cvode_mem, mudq, mldq, ZERO);\n    if(check_retval(&retval, \"CVBBDPrecReInit\", 1, my_pe)) MPI_Abort(comm, 1);\n\n    retval = SUNLinSol_SPGMRSetPrecType(LS, PREC_RIGHT);\n    if(check_retval(&retval, \"SUNLinSol_SPGMRSetPrecType\", 1, my_pe)) MPI_Abort(comm, 1);\n\n    if (my_pe == 0) {\n      printf(\"\\n\\n-------------------------------------------------------\");\n      printf(\"------------\\n\");\n    }\n\n  }\n\n\n  if (my_pe == 0) {\n    printf(\"\\n\\nPreconditioner type is:  jpre = %s\\n\\n\",\n\t   (jpre == PREC_LEFT) ? \"PREC_LEFT\" : \"PREC_RIGHT\");\n  }\n\n  \n\n\n  for (iout = 1, tout = TWOHR; iout <= NOUT; iout++, tout += TWOHR) {\n    retval = CVode(cvode_mem, tout, u, &t, CV_NORMAL);\n    if(check_retval(&retval, \"CVode\", 1, my_pe)) break;\n    PrintOutput(cvode_mem, my_pe, comm, u, t);\n  }\n\n  \n\n\n  if (my_pe == 0) PrintFinalStats(cvode_mem);\n\n  } \n\n\n  \n\n  N_VDestroy_Parallel(u);\n  free(data);\n  CVodeFree(&cvode_mem);\n  SUNLinSolFree(LS);\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "lapesd_932", "code": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing Stats Exporter test...\\n\", stdout);\n  ctx_init ();\n  trec_init ();\n\n  int num_exp_repl = 1;\n  int num_run = 1;\n  int num_repts = 1;\n  fputs (\"Recording performance metrics...\\n\", stdout);\n  record (num_exp_repl, num_run, num_repts);\n  fputs (\"Performance metrics recorded.\\n\", stdout);\n\n  fputs (\"Exporting data...\\n\", stdout);\n  iore_stex_t stex =\n    { };\n  stex.report_type.task = true;\n  stex.report_type.test = true;\n  stex.data_format = IORE_STEX_FORMAT_CSV;\n  stex.export_dir = \"/tmp/\";\n  if (export (stex))\n    fputs (\"Failed exporting data.\\n\", stderr);\n  else\n    fputs (\"Data exported.\\n\", stdout);\n\n  fputs (\"Displaying to string...\\n\", stdout);\n  tostr (stex);\n  fputs (\"String displayed.\\n\", stdout);\n\n  trec_destroy ();\n  fputs (\"Finalizing Stats Exporter test.\\n\", stdout);\n}", "label": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing Stats Exporter test...\\n\", stdout);\n  MPI_Init (&argc, &argv);\n  ctx_init ();\n  trec_init ();\n\n  int num_exp_repl = 1;\n  int num_run = 1;\n  int num_repts = 1;\n  fputs (\"Recording performance metrics...\\n\", stdout);\n  record (num_exp_repl, num_run, num_repts);\n  fputs (\"Performance metrics recorded.\\n\", stdout);\n\n  fputs (\"Exporting data...\\n\", stdout);\n  iore_stex_t stex =\n    { };\n  stex.report_type.task = true;\n  stex.report_type.test = true;\n  stex.data_format = IORE_STEX_FORMAT_CSV;\n  stex.export_dir = \"/tmp/\";\n  if (export (stex))\n    fputs (\"Failed exporting data.\\n\", stderr);\n  else\n    fputs (\"Data exported.\\n\", stdout);\n\n  fputs (\"Displaying to string...\\n\", stdout);\n  tostr (stex);\n  fputs (\"String displayed.\\n\", stdout);\n\n  trec_destroy ();\n  MPI_Finalize ();\n  fputs (\"Finalizing Stats Exporter test.\\n\", stdout);\n}"}
{"program": "OphidiaBigData_934", "code": "int main(int argc, char *argv[])\n{\n\n\t\n\n\tint size, myrank, res = -1;\n\n\tif (!myrank) {\n\t\tfprintf(stdout, OPH_VERSION, PACKAGE_VERSION);\n\t\tfprintf(stdout, \"%s\", OPH_DISCLAIMER);\n\t}\n\n\tif (argc != 2) {\n\t\tif (!myrank)\n\t\t\tfprintf(stdout, \"USAGE: ./oph_analytics_framework \\\"operator=value;param=value;...\\\"\\n\");\n\t\tres = 0;\n\t}\n\n\tif (!strcmp(argv[1], \"-v\")) {\n\t\tres = 0;\n\t}\n\n\tif (!strcmp(argv[1], \"-x\")) {\n\t\tif (!myrank)\n\t\t\tfprintf(stdout, \"%s\", OPH_WARRANTY);\n\t\tres = 0;\n\t}\n\n\tif (!strcmp(argv[1], \"-z\")) {\n\t\tif (!myrank)\n\t\t\tfprintf(stdout, \"%s\", OPH_CONDITIONS);\n\t\tres = 0;\n\t}\n\n\toph_handle_signals();\n\n\tif (res) {\n\t\tstruct timeval start_time, end_time, total_time;\n\n#ifdef OPH_PARALLEL_LOCATION\n\t\tchar log_prefix[OPH_COMMON_BUFFER_LEN];\n\t\tsnprintf(log_prefix, OPH_COMMON_BUFFER_LEN, OPH_FRAMEWORK_LOG_PATH_PREFIX, OPH_PARALLEL_LOCATION);\n\t\tset_log_prefix(log_prefix);\n\t\tpmesg(LOG_DEBUG, __FILE__, __LINE__, \"Set logging directory to '%s'\\n\", log_prefix);\n#endif\n#if defined(OPH_TIME_DEBUG_1) || defined(OPH_TIME_DEBUG_2)\n\t\tmsglevel = LOG_DEBUG_T;\n#endif\n\n\t\tif (!myrank)\n\t\t\tgettimeofday(&start_time, NULL);\n\n\t\tif (!myrank)\n\t\t\tpmesg(LOG_INFO, __FILE__, __LINE__, \"Task string:\\n%s\\n\", argv[1]);\n\n\t\tif ((res = oph_af_execute_framework(argv[1], size, myrank)))\n\t\t\tpmesg(LOG_ERROR, __FILE__, __LINE__, \"Framework execution failed! ERROR: %d\\n\", res);\n\n\t\tif (!myrank) {\n\t\t\tgettimeofday(&end_time, NULL);\n\t\t\ttimeval_subtract(&total_time, &end_time, &start_time);\n\t\t\tprintf(\"Proc %d: Total execution:\\t Time %d,%06d sec\\n\", myrank, (int) total_time.tv_sec, (int) total_time.tv_usec);\n\t\t}\n\t}\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\n\t\n\n\tint size, myrank, res = -1;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\tif (!myrank) {\n\t\tfprintf(stdout, OPH_VERSION, PACKAGE_VERSION);\n\t\tfprintf(stdout, \"%s\", OPH_DISCLAIMER);\n\t}\n\n\tif (argc != 2) {\n\t\tif (!myrank)\n\t\t\tfprintf(stdout, \"USAGE: ./oph_analytics_framework \\\"operator=value;param=value;...\\\"\\n\");\n\t\tres = 0;\n\t}\n\n\tif (!strcmp(argv[1], \"-v\")) {\n\t\tres = 0;\n\t}\n\n\tif (!strcmp(argv[1], \"-x\")) {\n\t\tif (!myrank)\n\t\t\tfprintf(stdout, \"%s\", OPH_WARRANTY);\n\t\tres = 0;\n\t}\n\n\tif (!strcmp(argv[1], \"-z\")) {\n\t\tif (!myrank)\n\t\t\tfprintf(stdout, \"%s\", OPH_CONDITIONS);\n\t\tres = 0;\n\t}\n\n\toph_handle_signals();\n\n\tif (res) {\n\t\tstruct timeval start_time, end_time, total_time;\n\n#ifdef OPH_PARALLEL_LOCATION\n\t\tchar log_prefix[OPH_COMMON_BUFFER_LEN];\n\t\tsnprintf(log_prefix, OPH_COMMON_BUFFER_LEN, OPH_FRAMEWORK_LOG_PATH_PREFIX, OPH_PARALLEL_LOCATION);\n\t\tset_log_prefix(log_prefix);\n\t\tpmesg(LOG_DEBUG, __FILE__, __LINE__, \"Set logging directory to '%s'\\n\", log_prefix);\n#endif\n#if defined(OPH_TIME_DEBUG_1) || defined(OPH_TIME_DEBUG_2)\n\t\tmsglevel = LOG_DEBUG_T;\n#endif\n\n\t\tif (!myrank)\n\t\t\tgettimeofday(&start_time, NULL);\n\n\t\tif (!myrank)\n\t\t\tpmesg(LOG_INFO, __FILE__, __LINE__, \"Task string:\\n%s\\n\", argv[1]);\n\n\t\tif ((res = oph_af_execute_framework(argv[1], size, myrank)))\n\t\t\tpmesg(LOG_ERROR, __FILE__, __LINE__, \"Framework execution failed! ERROR: %d\\n\", res);\n\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t\tif (!myrank) {\n\t\t\tgettimeofday(&end_time, NULL);\n\t\t\ttimeval_subtract(&total_time, &end_time, &start_time);\n\t\t\tprintf(\"Proc %d: Total execution:\\t Time %d,%06d sec\\n\", myrank, (int) total_time.tv_sec, (int) total_time.tv_usec);\n\t\t}\n\t}\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "luk51000_935", "code": "int\nmain(int argc, char* argv[]) {\n    int       error;\n    NODE_T    root;\n\n    Cache_io_rank(MPI_COMM_WORLD, io_comm);\n\n    Cscanf(io_comm,\n        \"Enter max depth, cutoff depth, max work, and max children\",\n        \"%d %d %d %d\", &max_depth, &cutoff_depth, &max_work,\n        &max_children);\n\n    Setup_term_detect();\n\n    error = Initialize_soln(max_depth);\n    Cerror_test(io_comm, \"Initialize_soln\", error);\n\n    error = Allocate_lists(max_depth, max_children);\n    Cerror_test(io_comm, \"Allocate_lists\", error);\n\n    error = Allocate_type_arrays(max_depth, max_children);\n    Cerror_test(io_comm, \"Allocate_type_arrays\", error);\n\n    if (my_rank == 0) {\n        Get_root(&root);\n    } else {\n        root = NODE_NULL;\n    }\n   \n    Par_tree_search(root, MPI_COMM_WORLD);\n\n#ifdef STATS\n    Print_stats(io_comm);\n#endif\n\n    Clean_up_queues(MPI_COMM_WORLD);\n\n    Free_lists();\n    Free_type_arrays();\n    Free_soln();\n\n\n    return 0;\n}", "label": "int\nmain(int argc, char* argv[]) {\n    int       error;\n    NODE_T    root;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_dup(MPI_COMM_WORLD, &io_comm);\n    Cache_io_rank(MPI_COMM_WORLD, io_comm);\n\n    Cscanf(io_comm,\n        \"Enter max depth, cutoff depth, max work, and max children\",\n        \"%d %d %d %d\", &max_depth, &cutoff_depth, &max_work,\n        &max_children);\n\n    Setup_term_detect();\n\n    error = Initialize_soln(max_depth);\n    Cerror_test(io_comm, \"Initialize_soln\", error);\n\n    error = Allocate_lists(max_depth, max_children);\n    Cerror_test(io_comm, \"Allocate_lists\", error);\n\n    error = Allocate_type_arrays(max_depth, max_children);\n    Cerror_test(io_comm, \"Allocate_type_arrays\", error);\n\n    if (my_rank == 0) {\n        Get_root(&root);\n    } else {\n        root = NODE_NULL;\n    }\n   \n    Par_tree_search(root, MPI_COMM_WORLD);\n\n#ifdef STATS\n    Print_stats(io_comm);\n#endif\n\n    Clean_up_queues(MPI_COMM_WORLD);\n\n    Free_lists();\n    Free_type_arrays();\n    Free_soln();\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ClaudioNahmad_941", "code": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nprocs, len, sum;\n    int global_sum;\n    int errs=0, toterrs, errcode;\n    char *filename;\n    MPI_File fh;\n    MPI_Status status;\n\n\n    double wr_stime, wr_etime, wr_time, wr_sumtime;\n    double rd_stime, rd_etime, rd_time, rd_sumtime;\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: shared_fp -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+10);\n    }\n\n    buf = (int *) malloc(COUNT * sizeof(int));\n\n\n    for (i=0; i<COUNT; i++) buf[i] = COUNT*rank + i;\n\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n\n    wr_stime =\n\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_write_shared\");\n    }\n    wr_etime =\n\n    for (i=0; i<COUNT; i++) buf[i] = 0;\n\n\n    rd_stime =\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_seek_shared\");\n    }\n\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_read_shared\");\n    }\n\n    rd_etime =\n\n    sum = 0;\n    for (i=0; i<COUNT; i++) sum += buf[i];\n\n\n    wr_time = wr_etime - wr_stime;\n    rd_time = rd_etime - rd_stime;\n\n\n    if (global_sum != (((COUNT*nprocs - 1)*(COUNT*nprocs))/2)) {\n\terrs++;\n\tfprintf(stderr, \"Error: sum %d, global_sum %d, %d\\n\",\n\t\tsum, global_sum,(((COUNT*nprocs - 1)*(COUNT*nprocs))/2));\n    }\n\n    free(buf);\n    free(filename);\n\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n#ifdef TIMING\n            fprintf( stderr, \"nprocs: %d bytes: %d write: %f read %f\\n\",\n                 nprocs, COUNT*sizeof(int), wr_sumtime, rd_sumtime);\n#endif\n\t}\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nprocs, len, sum;\n    int global_sum;\n    int errs=0, toterrs, errcode;\n    char *filename;\n    MPI_File fh;\n    MPI_Status status;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double wr_stime, wr_etime, wr_time, wr_sumtime;\n    double rd_stime, rd_etime, rd_time, rd_sumtime;\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: shared_fp -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+10);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n    buf = (int *) malloc(COUNT * sizeof(int));\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    for (i=0; i<COUNT; i++) buf[i] = COUNT*rank + i;\n\n    errcode = MPI_File_open(MPI_COMM_WORLD, filename,\n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n\n    wr_stime = MPI_Wtime();\n\n    errcode = MPI_File_write_ordered(fh, buf, COUNT, MPI_INT, &status);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_write_shared\");\n    }\n    wr_etime = MPI_Wtime();\n\n    for (i=0; i<COUNT; i++) buf[i] = 0;\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    rd_stime = MPI_Wtime();\n    errcode = MPI_File_seek_shared(fh, 0, MPI_SEEK_SET);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_seek_shared\");\n    }\n\n    errcode = MPI_File_read_ordered(fh, buf, COUNT, MPI_INT, &status);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_read_shared\");\n    }\n\n    rd_etime = MPI_Wtime();\n    MPI_File_close(&fh);\n\n    sum = 0;\n    for (i=0; i<COUNT; i++) sum += buf[i];\n\n    MPI_Allreduce(&sum, &global_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    wr_time = wr_etime - wr_stime;\n    rd_time = rd_etime - rd_stime;\n\n    MPI_Allreduce(&wr_time, &wr_sumtime, 1,\n        MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Allreduce(&rd_time, &rd_sumtime, 1,\n        MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    if (global_sum != (((COUNT*nprocs - 1)*(COUNT*nprocs))/2)) {\n\terrs++;\n\tfprintf(stderr, \"Error: sum %d, global_sum %d, %d\\n\",\n\t\tsum, global_sum,(((COUNT*nprocs - 1)*(COUNT*nprocs))/2));\n    }\n\n    free(buf);\n    free(filename);\n\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n#ifdef TIMING\n            fprintf( stderr, \"nprocs: %d bytes: %d write: %f read %f\\n\",\n                 nprocs, COUNT*sizeof(int), wr_sumtime, rd_sumtime);\n#endif\n\t}\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "mpip_943", "code": "int main(int argc, char **argv)\n{\n  int parallel;\n  MPI_Comm comm_cart_1d, comm_cart_2d, comm_cart_3d;\n  \n  \n\n  ptrdiff_t n[3]        = {128,128,128};\n  int       np[3]       = {1,1,1};\n  int       loops       = 1;\n  int       verbose     = 0;\n  int       inplace     = 0;\n  int       cmp_fftw    = 0;\n  int       cmp_decomp  = 0;\n  int       cmp_flags   = 0;\n  int       transposed  = 0;\n  int       print_timer = 0;\n  unsigned  pfft_flags  = 0;\n\n  \n\n  pfft_init();\n \n  \n\n  init_parameters(argc, argv, n, np, &pfft_flags, &loops, &transposed, &verbose, &inplace, &cmp_fftw, &cmp_decomp, &cmp_flags, &print_timer);\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: Procmesh of size %d x %d x %d does not fit to number of allocated processes.\\n\", np[0], np[1], np[2]);\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"       Please allocate %d processes (mpiexec -np %d ...) or change the procmesh (with -pfft_np * * *).\\n\", np[0]*np[1]*np[2], np[0]*np[1]*np[2]);\n    return 1;\n  }\n\n  int num_serial_dims = (np[0]==1) + (np[1]==1) + (np[2]==1); \n\n  if( cmp_decomp || num_serial_dims==0){\n    pfft_printf(MPI_COMM_WORLD, \"* PFFT runtimes (3d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_3d, loops, pfft_flags, transposed, inplace, verbose, cmp_flags, print_timer);\n    pfft_printf(MPI_COMM_WORLD, \"\\n\");\n  }\n\n  \n\n  if( num_serial_dims >= 1 ){\n    if( cmp_decomp || num_serial_dims==1){\n      \n \n      if(np[1]==1){ np[1] = np[2]; np[2] = 1; }\n      if(np[0]==1){ np[0] = np[1]; np[1] = 1; }\n      if(np[1]==1){ np[1] = np[2]; np[2] = 1; }\n  \n      if( pfft_create_procmesh(2, MPI_COMM_WORLD, np, &comm_cart_2d) )\n        pfft_printf(MPI_COMM_WORLD, \"Error in creation of 2d procmesh of size %d x %d\\n\", np[0], np[1]);\n      pfft_printf(MPI_COMM_WORLD, \"* PFFT runtimes (2d data decomposition):\\n\");\n      loop_pfft_tests(n, comm_cart_2d, loops, pfft_flags, transposed, inplace, verbose, cmp_flags, print_timer);\n      pfft_printf(MPI_COMM_WORLD, \"\\n\");\n    }\n  }\n\n  \n\n  if( num_serial_dims >= 2 ){\n    \n \n    if(np[1]==1){ np[1] = np[2]; np[2] = 1; }\n    if(np[0]==1){ np[0] = np[1]; np[1] = 1; }\n\n    if( pfft_create_procmesh(1, MPI_COMM_WORLD, np, &comm_cart_1d) )\n      pfft_printf(MPI_COMM_WORLD, \"Error in creation of 2d procmesh of size %d\\n\", np[0]);\n    pfft_printf(MPI_COMM_WORLD, \"* PFFT runtimes (1d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_1d, loops, pfft_flags, transposed, inplace, verbose, cmp_flags, print_timer);\n    pfft_printf(MPI_COMM_WORLD, \"\\n\");\n\n    if(cmp_fftw){\n      pfft_printf(MPI_COMM_WORLD, \"* FFTW_MPI runtimes (1d data decomposition):\\n\");\n      loop_fftw_tests(n, parallel=1, loops, transposed, inplace, verbose);\n    }\n  }\n\n  \n\n  if( np[0]*np[1]*np[2] == 1 ){\n    if(cmp_fftw){\n      pfft_printf(MPI_COMM_WORLD, \"* serial FFTW runtimes (no data decomposition at all):\\n\");\n      loop_fftw_tests(n, parallel=0, loops, transposed, inplace, verbose);\n      pfft_printf(MPI_COMM_WORLD, \"\\n\");\n    }\n  }\n\n  \n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int parallel;\n  MPI_Comm comm_cart_1d, comm_cart_2d, comm_cart_3d;\n  \n  \n\n  ptrdiff_t n[3]        = {128,128,128};\n  int       np[3]       = {1,1,1};\n  int       loops       = 1;\n  int       verbose     = 0;\n  int       inplace     = 0;\n  int       cmp_fftw    = 0;\n  int       cmp_decomp  = 0;\n  int       cmp_flags   = 0;\n  int       transposed  = 0;\n  int       print_timer = 0;\n  unsigned  pfft_flags  = 0;\n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n \n  \n\n  init_parameters(argc, argv, n, np, &pfft_flags, &loops, &transposed, &verbose, &inplace, &cmp_fftw, &cmp_decomp, &cmp_flags, &print_timer);\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: Procmesh of size %d x %d x %d does not fit to number of allocated processes.\\n\", np[0], np[1], np[2]);\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"       Please allocate %d processes (mpiexec -np %d ...) or change the procmesh (with -pfft_np * * *).\\n\", np[0]*np[1]*np[2], np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  int num_serial_dims = (np[0]==1) + (np[1]==1) + (np[2]==1); \n\n  if( cmp_decomp || num_serial_dims==0){\n    pfft_printf(MPI_COMM_WORLD, \"* PFFT runtimes (3d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_3d, loops, pfft_flags, transposed, inplace, verbose, cmp_flags, print_timer);\n    pfft_printf(MPI_COMM_WORLD, \"\\n\");\n    MPI_Comm_free(&comm_cart_3d);\n  }\n\n  \n\n  if( num_serial_dims >= 1 ){\n    if( cmp_decomp || num_serial_dims==1){\n      \n \n      if(np[1]==1){ np[1] = np[2]; np[2] = 1; }\n      if(np[0]==1){ np[0] = np[1]; np[1] = 1; }\n      if(np[1]==1){ np[1] = np[2]; np[2] = 1; }\n  \n      if( pfft_create_procmesh(2, MPI_COMM_WORLD, np, &comm_cart_2d) )\n        pfft_printf(MPI_COMM_WORLD, \"Error in creation of 2d procmesh of size %d x %d\\n\", np[0], np[1]);\n      pfft_printf(MPI_COMM_WORLD, \"* PFFT runtimes (2d data decomposition):\\n\");\n      loop_pfft_tests(n, comm_cart_2d, loops, pfft_flags, transposed, inplace, verbose, cmp_flags, print_timer);\n      pfft_printf(MPI_COMM_WORLD, \"\\n\");\n      MPI_Comm_free(&comm_cart_2d);\n    }\n  }\n\n  \n\n  if( num_serial_dims >= 2 ){\n    \n \n    if(np[1]==1){ np[1] = np[2]; np[2] = 1; }\n    if(np[0]==1){ np[0] = np[1]; np[1] = 1; }\n\n    if( pfft_create_procmesh(1, MPI_COMM_WORLD, np, &comm_cart_1d) )\n      pfft_printf(MPI_COMM_WORLD, \"Error in creation of 2d procmesh of size %d\\n\", np[0]);\n    pfft_printf(MPI_COMM_WORLD, \"* PFFT runtimes (1d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_1d, loops, pfft_flags, transposed, inplace, verbose, cmp_flags, print_timer);\n    pfft_printf(MPI_COMM_WORLD, \"\\n\");\n    MPI_Comm_free(&comm_cart_1d);\n\n    if(cmp_fftw){\n      pfft_printf(MPI_COMM_WORLD, \"* FFTW_MPI runtimes (1d data decomposition):\\n\");\n      loop_fftw_tests(n, parallel=1, loops, transposed, inplace, verbose);\n    }\n  }\n\n  \n\n  if( np[0]*np[1]*np[2] == 1 ){\n    if(cmp_fftw){\n      pfft_printf(MPI_COMM_WORLD, \"* serial FFTW runtimes (no data decomposition at all):\\n\");\n      loop_fftw_tests(n, parallel=0, loops, transposed, inplace, verbose);\n      pfft_printf(MPI_COMM_WORLD, \"\\n\");\n    }\n  }\n\n  \n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "dnoliver_944", "code": "int main( int argc, char* argv[] )\n{\n\tcheck_parameters( argc, argv );\n\n\tint encryption_method = -1;\n\tlong success_key = -1;\n    int proc_id, num_procs;\n\tint chunk, first, last;\n\tlong work_size;\n\tunsigned char encrypted_text[8];\n\tunsigned char iv[ IV_LENGTH ] = {1,2,3,4,5,6,7,8};\n\tunsigned char key[ KEY_LENGTH ];\n\tint keygen_characters[10] = {'0','1','2','3','4','5','6','7','8','9'};\n\ttime_t start_time, end_time;\n\n\t\n\n\n\t\n\n\n\t\n\n\n\t\n\n\tstart_time = time(NULL);\n\n\tread_parameters( argv, encrypted_text, &work_size );\n\n\tEncryptor decryptor[2];\n\tinit_decryptor( &decryptor[0], DECRYPT, BLOWFISH, iv, encrypted_text );\n\tinit_decryptor( &decryptor[1], DECRYPT, CAST5, iv, encrypted_text );\n\n\tchunk = work_size / num_procs;\n\tfirst = proc_id * chunk;\n\n\t\n\n\t\n\n\tif( proc_id == num_procs - 1 ) {\n\t\tlast = work_size;\n\t}\n\telse {\n\t\tlast = first + chunk;\n\t}\n\t\n\tmemset(key,ASCII_SPACE,KEY_LENGTH);\n\n\tfor( long i = first; i < last && success_key == - 1; i++ ) {\n\n\t\tkey[KEY_LENGTH-1] = keygen_characters[i % 10];\n\t\tkey[KEY_LENGTH-2] = i/10? keygen_characters[(i/10) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-3] = i/100? keygen_characters[(i/100) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-4] = i/1000? keygen_characters[(i/1000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-5] = i/10000? keygen_characters[(i/10000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-6] = i/100000? keygen_characters[(i/100000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-7] = i/1000000? keygen_characters[(i/1000000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-8] = i/10000000? keygen_characters[(i/10000000) % 10] : ASCII_SPACE;\n\t\t\n\t\tencryptor_execute( &decryptor[0], key );\n\t\t\n\n        \n\n        \n\n        \n\n\n        if( memcmp( (char *)decryptor[0].output, \"Frase\", 5 ) == 0 ) {\n\t\t\tsuccess_key = i;\n            encryption_method = BLOWFISH;\n            break;\n        }\n\t\t\n\t\tencryptor_execute( &decryptor[1], key );\n\t\t\n\n        \n\n        \n\n        \n\n\n        if( memcmp( (char *)decryptor[1].output, \"Frase\", 5 ) == 0 ) {\n\t\t\tsuccess_key = i;\n            encryption_method = CAST5;\n            break;\n        }\n\t}\n\n\t\n\n\tend_time = time( NULL );\n\n\tif( success_key != -1 ) {\n\t\tprint_result( success_key, encryption_method, difftime( end_time, start_time ) );\n\t}\n\n\t\n\n\n    return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\n\tcheck_parameters( argc, argv );\n\n\tint encryption_method = -1;\n\tlong success_key = -1;\n    int proc_id, num_procs;\n\tint chunk, first, last;\n\tlong work_size;\n\tunsigned char encrypted_text[8];\n\tunsigned char iv[ IV_LENGTH ] = {1,2,3,4,5,6,7,8};\n\tunsigned char key[ KEY_LENGTH ];\n\tint keygen_characters[10] = {'0','1','2','3','4','5','6','7','8','9'};\n\ttime_t start_time, end_time;\n\n\t\n\n    MPI_Init( &argc, &argv );\n\n\t\n\n    MPI_Comm_rank( MPI_COMM_WORLD, &proc_id );\n\n\t\n\n    MPI_Comm_size( MPI_COMM_WORLD, &num_procs );\n\n\t\n\n\tstart_time = time(NULL);\n\n\tread_parameters( argv, encrypted_text, &work_size );\n\n\tEncryptor decryptor[2];\n\tinit_decryptor( &decryptor[0], DECRYPT, BLOWFISH, iv, encrypted_text );\n\tinit_decryptor( &decryptor[1], DECRYPT, CAST5, iv, encrypted_text );\n\n\tchunk = work_size / num_procs;\n\tfirst = proc_id * chunk;\n\n\t\n\n\t\n\n\tif( proc_id == num_procs - 1 ) {\n\t\tlast = work_size;\n\t}\n\telse {\n\t\tlast = first + chunk;\n\t}\n\t\n\tmemset(key,ASCII_SPACE,KEY_LENGTH);\n\n\tfor( long i = first; i < last && success_key == - 1; i++ ) {\n\n\t\tkey[KEY_LENGTH-1] = keygen_characters[i % 10];\n\t\tkey[KEY_LENGTH-2] = i/10? keygen_characters[(i/10) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-3] = i/100? keygen_characters[(i/100) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-4] = i/1000? keygen_characters[(i/1000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-5] = i/10000? keygen_characters[(i/10000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-6] = i/100000? keygen_characters[(i/100000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-7] = i/1000000? keygen_characters[(i/1000000) % 10] : ASCII_SPACE;\n\t\tkey[KEY_LENGTH-8] = i/10000000? keygen_characters[(i/10000000) % 10] : ASCII_SPACE;\n\t\t\n\t\tencryptor_execute( &decryptor[0], key );\n\t\t\n\n        \n\n        \n\n        \n\n\n        if( memcmp( (char *)decryptor[0].output, \"Frase\", 5 ) == 0 ) {\n\t\t\tsuccess_key = i;\n            encryption_method = BLOWFISH;\n            break;\n        }\n\t\t\n\t\tencryptor_execute( &decryptor[1], key );\n\t\t\n\n        \n\n        \n\n        \n\n\n        if( memcmp( (char *)decryptor[1].output, \"Frase\", 5 ) == 0 ) {\n\t\t\tsuccess_key = i;\n            encryption_method = CAST5;\n            break;\n        }\n\t}\n\n\t\n\n\tend_time = time( NULL );\n\n\tif( success_key != -1 ) {\n\t\tprint_result( success_key, encryption_method, difftime( end_time, start_time ) );\n\t}\n\n\t\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "indiependente_945", "code": "int main(int argc, char** argv)\n{\n\tint my_rank, p;\n\tint matrix[N][N], *buffer;\n\n\tint count, blocklength, stride;\n\tint i,j;\n\n\tMPI_Datatype TMP_COLUMN, COLUMN, ROW;\n\tMPI_Aint lb, extent;\n\n\n\tif (N % p != 0)\n\n\n\tcount = N / p;\n\tblocklength = N;\n\n\n\tif (my_rank == MASTER)\n\t{\n\t\tprintf(\"ROW lb = %ld extent = %ld\\n\", lb, extent);\n\t\tprintf(\"COLUMN lb = %ld extent = %ld\\n\", lb, extent);\n\t\tprintf(\"TMP_COLUMN lb = %ld extent = %ld\\n\", lb, extent);\n\t\tordered_fill_matrix(matrix);\n\t\tprint_matrix(matrix, my_rank);\n\t}\n\n\n\tbuffer = malloc(count * blocklength * sizeof(int));\n\n\n\n\tif (my_rank == MASTER)\n\t\tprint_matrix(matrix, my_rank);\n\n\n\tfree(buffer);\n}", "label": "int main(int argc, char** argv)\n{\n\tint my_rank, p;\n\tint matrix[N][N], *buffer;\n\n\tint count, blocklength, stride;\n\tint i,j;\n\n\tMPI_Datatype TMP_COLUMN, COLUMN, ROW;\n\tMPI_Aint lb, extent;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\n\tif (N % p != 0)\n\t\t{MPI_Finalize(); return 1;}\n\n\n\tcount = N / p;\n\tblocklength = N;\n\n\tMPI_Type_contiguous(blocklength, MPI_INT, &ROW);\n\tMPI_Type_commit(&ROW);\n\tMPI_Type_vector(blocklength, 1, N, MPI_INT, &TMP_COLUMN);\n\tMPI_Type_create_resized(TMP_COLUMN, 0, sizeof(int), &COLUMN);\n\tMPI_Type_commit(&COLUMN);\n\n\tif (my_rank == MASTER)\n\t{\n\t\tMPI_Type_get_extent(ROW, &lb, &extent);\n\t\tprintf(\"ROW lb = %ld extent = %ld\\n\", lb, extent);\n\t\tMPI_Type_get_extent(COLUMN, &lb, &extent);\n\t\tprintf(\"COLUMN lb = %ld extent = %ld\\n\", lb, extent);\n\t\tMPI_Type_get_extent(TMP_COLUMN, &lb, &extent);\n\t\tprintf(\"TMP_COLUMN lb = %ld extent = %ld\\n\", lb, extent);\n\t\tordered_fill_matrix(matrix);\n\t\tprint_matrix(matrix, my_rank);\n\t}\n\n\n\tbuffer = malloc(count * blocklength * sizeof(int));\n\n\tMPI_Scatter(matrix, count, ROW, buffer, count * blocklength, MPI_INT, MASTER, MPI_COMM_WORLD);\n\n\tMPI_Gather(buffer, count * blocklength, MPI_INT, matrix, count, COLUMN, MASTER, MPI_COMM_WORLD);\n\n\tif (my_rank == MASTER)\n\t\tprint_matrix(matrix, my_rank);\n\n\n\tfree(buffer);\n\tMPI_Type_free(&ROW);\n\tMPI_Type_free(&COLUMN);\n\tMPI_Finalize();\n}"}
{"program": "BackupTheBerlios_947", "code": "int main(int argc, char *argv[])\n{\n  int i,n=5;\n\n\n  for (i=0; i<n; i++) {\n    Py_Initialize();\n    PyRun_SimpleString(helloworld);\n    Py_Finalize();\n  }\n\n  Py_Initialize();\n  PyRun_SimpleString(helloworld);\n  Py_Finalize();\n\n  Py_Initialize();\n  PyRun_SimpleString(\"from mpi4py import MPI\\n\");\n  Py_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n  int i,n=5;\n\n  MPI_Init(&argc, &argv);\n\n  for (i=0; i<n; i++) {\n    Py_Initialize();\n    PyRun_SimpleString(helloworld);\n    Py_Finalize();\n  }\n\n  Py_Initialize();\n  PyRun_SimpleString(helloworld);\n  MPI_Finalize();\n  Py_Finalize();\n\n  Py_Initialize();\n  PyRun_SimpleString(\"from mpi4py import MPI\\n\");\n  Py_Finalize();\n\n  return 0;\n}"}
{"program": "Unidata_948", "code": "int main(int argc, char *argv[])\n{\n    char *fname=\"tst_default_format_pnetcdf.nc\";\n    int err, exp_err=NC_NOERR, nerrs=0, ncid, cmode;\n\n\n    if (argc == 2) fname = argv[1];\n\n    default_format = NC_FORMAT_CLASSIC;\n\n    \n\n    cmode = NC_64BIT_OFFSET | NC_64BIT_DATA;\n    err = nc_create_par(fname, cmode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid);\n    if (err != NC_EINVAL) {\n        printf(\"Error at %s line %d: expect NC_EINVAL but got %d\\n\",\n               __FILE__, __LINE__, err);\n        nerrs++;\n    }\n#ifdef USE_HDF5\n    \n\n    cmode = NC_NETCDF4 | NC_64BIT_OFFSET;\n    err = nc_create_par(fname, cmode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid);\n    if (err != NC_EINVAL) {\n        printf(\"Error at %s line %d: expect NC_EINVAL but got %d\\n\",\n               __FILE__, __LINE__, err);\n        nerrs++;\n    }\n#endif\n\n    \n\n    cmode = 0;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_CLASSIC);\n\n    \n\n    cmode = NC_64BIT_OFFSET;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_OFFSET);\n\n#ifdef ENABLE_CDF5\n    \n\n    cmode = NC_64BIT_DATA;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_DATA);\n#endif\n\n    \n\n    default_format = NC_FORMAT_64BIT_OFFSET;\n    err = nc_set_default_format(default_format, NULL); ERR\n\n    \n\n    cmode = 0;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_OFFSET);\n\n#ifdef ENABLE_CDF5\n    \n\n    cmode = NC_64BIT_DATA;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_DATA);\n\n    \n\n    default_format = NC_FORMAT_64BIT_DATA;\n    err = nc_set_default_format(default_format, NULL); ERR\n\n    \n\n    cmode = 0;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_DATA);\n#endif\n\n    \n\n    cmode = NC_64BIT_OFFSET;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_OFFSET);\n\n    return (nerrs > 0);\n}", "label": "int main(int argc, char *argv[])\n{\n    char *fname=\"tst_default_format_pnetcdf.nc\";\n    int err, exp_err=NC_NOERR, nerrs=0, ncid, cmode;\n\n    MPI_Init(&argc, &argv);\n\n    if (argc == 2) fname = argv[1];\n\n    default_format = NC_FORMAT_CLASSIC;\n\n    \n\n    cmode = NC_64BIT_OFFSET | NC_64BIT_DATA;\n    err = nc_create_par(fname, cmode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid);\n    if (err != NC_EINVAL) {\n        printf(\"Error at %s line %d: expect NC_EINVAL but got %d\\n\",\n               __FILE__, __LINE__, err);\n        nerrs++;\n    }\n#ifdef USE_HDF5\n    \n\n    cmode = NC_NETCDF4 | NC_64BIT_OFFSET;\n    err = nc_create_par(fname, cmode, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid);\n    if (err != NC_EINVAL) {\n        printf(\"Error at %s line %d: expect NC_EINVAL but got %d\\n\",\n               __FILE__, __LINE__, err);\n        nerrs++;\n    }\n#endif\n\n    \n\n    cmode = 0;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_CLASSIC);\n\n    \n\n    cmode = NC_64BIT_OFFSET;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_OFFSET);\n\n#ifdef ENABLE_CDF5\n    \n\n    cmode = NC_64BIT_DATA;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_DATA);\n#endif\n\n    \n\n    default_format = NC_FORMAT_64BIT_OFFSET;\n    err = nc_set_default_format(default_format, NULL); ERR\n\n    \n\n    cmode = 0;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_OFFSET);\n\n#ifdef ENABLE_CDF5\n    \n\n    cmode = NC_64BIT_DATA;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_DATA);\n\n    \n\n    default_format = NC_FORMAT_64BIT_DATA;\n    err = nc_set_default_format(default_format, NULL); ERR\n\n    \n\n    cmode = 0;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_DATA);\n#endif\n\n    \n\n    cmode = NC_64BIT_OFFSET;\n    nerrs += create_check_pnetcdf(fname, cmode, NC_FORMAT_64BIT_OFFSET);\n\n    MPI_Finalize();\n    return (nerrs > 0);\n}"}
{"program": "qingu_949", "code": "int main(int argc, char **argv)\n{\n    int errs = 0;\n    int i;\n    int rank, size, lrank, lsize, rsize;\n    int buf[2];\n    MPI_Comm newcomm, ic, localcomm, stagger_comm;\n    MPI_Request rreq;\n\n\n\n    if (size < 2) {\n        printf(\"this test requires at least 2 processes\\n\");\n    }\n\n#ifdef TEST_IDUP\n\n    \n\n\n    if (rank == 0) {\n        for (i = 1; i < size; ++i) {\n            buf[0] = 0x01234567;\n            buf[1] = 0x89abcdef;\n        }\n    }\n    else {\n        buf[0] = rank;\n        buf[1] = size + rank;\n    }\n\n    \n\n    buf[0] = rank;\n    buf[1] = 0xfeedface;\n    check(buf[1] == (size * (size-1) / 2));\n\n\n    \n\n\n    \n\n    stagger_comm = MPI_COMM_NULL;\n    if (rank % 2) {\n    }\n\n\n    \n\n    if (lrank == 0) {\n        for (i = 1; i < rsize; ++i) {\n            buf[0] = 0x01234567;\n            buf[1] = 0x89abcdef;\n        }\n    }\n    else {\n        buf[0] = lrank;\n        buf[1] = lsize + lrank;\n    }\n\n    \n\n    buf[0] = lrank;\n    buf[1] = 0xfeedface;\n    check(buf[1] == (rsize * (rsize-1) / 2));\n\n    \n\n\n    if (stagger_comm != MPI_COMM_NULL) {\n    }\n\n#endif \n\n\n    if (rank == 0) {\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int errs = 0;\n    int i;\n    int rank, size, lrank, lsize, rsize;\n    int buf[2];\n    MPI_Comm newcomm, ic, localcomm, stagger_comm;\n    MPI_Request rreq;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size < 2) {\n        printf(\"this test requires at least 2 processes\\n\");\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n#ifdef TEST_IDUP\n\n    \n\n\n    if (rank == 0) {\n        for (i = 1; i < size; ++i) {\n            buf[0] = 0x01234567;\n            buf[1] = 0x89abcdef;\n            MPI_Recv(buf, 2, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        MPI_Comm_idup(MPI_COMM_WORLD, &newcomm, &rreq);\n        MPI_Wait(&rreq, MPI_STATUS_IGNORE);\n    }\n    else {\n        MPI_Comm_idup(MPI_COMM_WORLD, &newcomm, &rreq);\n        buf[0] = rank;\n        buf[1] = size + rank;\n        MPI_Ssend(buf, 2, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Wait(&rreq, MPI_STATUS_IGNORE);\n    }\n\n    \n\n    buf[0] = rank;\n    buf[1] = 0xfeedface;\n    MPI_Allreduce(&buf[0], &buf[1], 1, MPI_INT, MPI_SUM, newcomm);\n    check(buf[1] == (size * (size-1) / 2));\n\n    MPI_Comm_free(&newcomm);\n\n    \n\n    MPI_Comm_split(MPI_COMM_WORLD, rank % 2, rank, &localcomm);\n    MPI_Intercomm_create(localcomm, 0, MPI_COMM_WORLD, (rank == 0 ? 1 : 0), 1234, &ic);\n\n    \n\n    stagger_comm = MPI_COMM_NULL;\n    if (rank % 2) {\n        MPI_Comm_dup(localcomm, &stagger_comm);\n    }\n\n    MPI_Comm_rank(ic, &lrank);\n    MPI_Comm_size(ic, &lsize);\n    MPI_Comm_remote_size(ic, &rsize);\n\n    \n\n    if (lrank == 0) {\n        for (i = 1; i < rsize; ++i) {\n            buf[0] = 0x01234567;\n            buf[1] = 0x89abcdef;\n            MPI_Recv(buf, 2, MPI_INT, i, 0, ic, MPI_STATUS_IGNORE);\n        }\n        MPI_Comm_idup(ic, &newcomm, &rreq);\n        MPI_Wait(&rreq, MPI_STATUS_IGNORE);\n    }\n    else {\n        MPI_Comm_idup(ic, &newcomm, &rreq);\n        buf[0] = lrank;\n        buf[1] = lsize + lrank;\n        MPI_Ssend(buf, 2, MPI_INT, 0, 0, ic);\n        MPI_Wait(&rreq, MPI_STATUS_IGNORE);\n    }\n\n    \n\n    buf[0] = lrank;\n    buf[1] = 0xfeedface;\n    MPI_Allreduce(&buf[0], &buf[1], 1, MPI_INT, MPI_SUM, newcomm);\n    check(buf[1] == (rsize * (rsize-1) / 2));\n\n    \n\n    MPI_Comm_free(&localcomm);\n\n    if (stagger_comm != MPI_COMM_NULL) {\n        MPI_Comm_free(&stagger_comm);\n    }\n    MPI_Comm_free(&newcomm);\n    MPI_Comm_free(&ic);\n\n#endif \n\n\n    MPI_Reduce((rank == 0 ? MPI_IN_PLACE : &errs), &errs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "radarsat1_950", "code": "int main(int argc, char *argv[])\n{\n\n#ifdef SICONOS_HAS_MPI\n#endif\n\n  int i=0;\n  printf(\"start test #%i\\n\",i);\n  int info = test_0();\n  if(!info)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n  i++;\n  printf(\"start test #%i\\n\",i);\n  int info_test = test_1();\n  info += info_test;\n  if(!info_test)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n  i++;\n  printf(\"start test #%i\\n\",i);\n  info_test = test_2();\n\n\n#ifndef WITH_MUMPS\n  info += info_test;\n#endif\n  if(!info_test)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n#ifndef WITH_MUMPS\n    printf(\"end test #%i  not  successful\\n\",i);\n#else\n    printf(\"end test #%i  not  successful (as predicted with mumps)\\n\",i);\n#endif\n  }\n\n\n\n  i++;\n  printf(\"start test #%i ConvexQP_ADDM\\n\",i);\n  info_test = test_3();\n  info += info_test;\n  if(!info_test)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n\n  i++;\n  printf(\"start test #%i ConvexQP_ADDM_ACCELERATION\\n\",i);\n  info_test = test_4();\n  info += info_test;\n  if(!info)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n  i++;\n  printf(\"start test #%i ConvexQP_ADDM_ACCELERATION_AND_RESTART\\n\",i);\n  info_test = test_5();\n  info += info_test;\n  if(!info)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n#ifdef SICONOS_HAS_MPI\n#endif\n\n  return info;\n}", "label": "int main(int argc, char *argv[])\n{\n\n#ifdef SICONOS_HAS_MPI\n  MPI_Init(&argc, &argv);\n#endif\n\n  int i=0;\n  printf(\"start test #%i\\n\",i);\n  int info = test_0();\n  if(!info)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n  i++;\n  printf(\"start test #%i\\n\",i);\n  int info_test = test_1();\n  info += info_test;\n  if(!info_test)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n  i++;\n  printf(\"start test #%i\\n\",i);\n  info_test = test_2();\n\n\n#ifndef WITH_MUMPS\n  info += info_test;\n#endif\n  if(!info_test)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n#ifndef WITH_MUMPS\n    printf(\"end test #%i  not  successful\\n\",i);\n#else\n    printf(\"end test #%i  not  successful (as predicted with mumps)\\n\",i);\n#endif\n  }\n\n\n\n  i++;\n  printf(\"start test #%i ConvexQP_ADDM\\n\",i);\n  info_test = test_3();\n  info += info_test;\n  if(!info_test)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n\n  i++;\n  printf(\"start test #%i ConvexQP_ADDM_ACCELERATION\\n\",i);\n  info_test = test_4();\n  info += info_test;\n  if(!info)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n  i++;\n  printf(\"start test #%i ConvexQP_ADDM_ACCELERATION_AND_RESTART\\n\",i);\n  info_test = test_5();\n  info += info_test;\n  if(!info)\n  {\n    printf(\"end test #%i successful\\n\",i);\n  }\n  else\n  {\n    printf(\"end test #%i  not  successful\\n\",i);\n  }\n\n#ifdef SICONOS_HAS_MPI\n  MPI_Finalize();\n#endif\n\n  return info;\n}"}
{"program": "byu-vv-lab_952", "code": "int main (int argc, char *argv[])\n{\n  int numtasks, rank, sendcount, recvcount, source;\n  float sendbuf[SIZE * SIZE] = {\n    1.0, 2.0, 3.0, 4.0,\n    5.0, 6.0, 7.0, 8.0,\n    9.0, 10.0, 11.0, 12.0,\n    13.0, 14.0, 15.0, 16.0};\n  float recvbuf[SIZE];\n  \n\n  if (numtasks == SIZE) {\n    source = 1;\n    sendcount = SIZE;\n    recvcount = SIZE;\n    if(source == rank){\n      \n\n      \n\n      for(int i=0; i<SIZE; i++)\n\trecvbuf[i] = *(sendbuf + rank * SIZE + i);\n    }\n    else\n    \n    printf(\"rank= %d  Results: %f %f %f %f\\n\",rank,recvbuf[0],\n\t   recvbuf[1],recvbuf[2],recvbuf[3]);\n    \n\n    for(int i=0; i<SIZE; i++)\n      assert(recvbuf[i] == *(sendbuf + SIZE * rank + i));\n  }\n  else\n    printf(\"Must specify %d processors. Terminating.\\n\",SIZE);\n  \n}", "label": "int main (int argc, char *argv[])\n{\n  int numtasks, rank, sendcount, recvcount, source;\n  float sendbuf[SIZE * SIZE] = {\n    1.0, 2.0, 3.0, 4.0,\n    5.0, 6.0, 7.0, 8.0,\n    9.0, 10.0, 11.0, 12.0,\n    13.0, 14.0, 15.0, 16.0};\n  float recvbuf[SIZE];\n  \n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n  if (numtasks == SIZE) {\n    source = 1;\n    sendcount = SIZE;\n    recvcount = SIZE;\n    if(source == rank){\n      \n\n      \n\n      for(int i=0; i<SIZE; i++)\n\trecvbuf[i] = *(sendbuf + rank * SIZE + i);\n      MPI_Scatter(sendbuf,sendcount,MPI_FLOAT,MPI_IN_PLACE,recvcount,\n\t\t  MPI_FLOAT,source,MPI_COMM_WORLD);\n    }\n    else\n      MPI_Scatter(sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,\n\t\t  MPI_FLOAT,source,MPI_COMM_WORLD);\n    \n    printf(\"rank= %d  Results: %f %f %f %f\\n\",rank,recvbuf[0],\n\t   recvbuf[1],recvbuf[2],recvbuf[3]);\n    \n\n    for(int i=0; i<SIZE; i++)\n      assert(recvbuf[i] == *(sendbuf + SIZE * rank + i));\n  }\n  else\n    printf(\"Must specify %d processors. Terminating.\\n\",SIZE);\n  \n  MPI_Finalize();\n}"}
{"program": "keremsahin1_953", "code": "int main(int iArgCnt, char* sArrArgs[])\n{\n   int iIterationNo = 0;\n   int iArrayIndex = 0;\n   double dSubResult = 0, dTotalResult = 0;\n   double dTime0 = 0, dTime1 = 0, dTimeDiff = 0, dMinTimeDiff = 1000, dMaxTimeDiff = 0;\n   MPI_Status statusX, statusY;\n\n   parseInputs(iArgCnt, sArrArgs);\n\n\n   initArrays();\n\n   for(iIterationNo = 0; iIterationNo < GiIterationCnt; iIterationNo++)\n   {\n      dSubResult = 0;\n\n      dTime0 =\n      if(GiProcessCnt > 1)\n      {\n         if(GiProcessRank == 0)\n         {\n            for(iArrayIndex = 1; iArrayIndex < GiProcessCnt; iArrayIndex++)\n            {\n\n               \n\n            }\n\n            for(iArrayIndex = 0; iArrayIndex < GiSubVectorLength; iArrayIndex++)\n            {\n               dSubResult += (GdArrX[iArrayIndex] * GdArrY[iArrayIndex]);\n            }\n            \n\n         }\n         else\n         {\n\n            for(iArrayIndex = 0; iArrayIndex < GiSubVectorLength; iArrayIndex++)\n            {\n               dSubResult += (GdArrSubX[iArrayIndex] * GdArrSubY[iArrayIndex]);\n            }\n\n            \n\n         }\n      }\n      else\n      {\n         for(iArrayIndex = 0; iArrayIndex < GiVectorLength; iArrayIndex++)\n         {\n            dSubResult += (GdArrX[iArrayIndex] * GdArrY[iArrayIndex]);\n         }\n      }\n\n\n      dTime1 =\n      dTimeDiff = (dTime1 - dTime0);\n\n      if(dTimeDiff > dMaxTimeDiff)\n         dMaxTimeDiff = dTimeDiff;\n      if(dTimeDiff < dMinTimeDiff)\n         dMinTimeDiff = dTimeDiff;\n   }\n\n   if(GiProcessRank == 0)\n      printf(\"Result=%f\\nMin Time=%f uSec\\nMax Time=%f uSec\\n\", dTotalResult, (1.e6 * dMinTimeDiff), (1.e6 * dMaxTimeDiff));\n   \n\n   return 0;\n}", "label": "int main(int iArgCnt, char* sArrArgs[])\n{\n   int iIterationNo = 0;\n   int iArrayIndex = 0;\n   double dSubResult = 0, dTotalResult = 0;\n   double dTime0 = 0, dTime1 = 0, dTimeDiff = 0, dMinTimeDiff = 1000, dMaxTimeDiff = 0;\n   MPI_Status statusX, statusY;\n\n   parseInputs(iArgCnt, sArrArgs);\n\n   MPI_Init(&iArgCnt, &sArrArgs);\n   MPI_Comm_size(MPI_COMM_WORLD, &GiProcessCnt);\n   MPI_Comm_rank(MPI_COMM_WORLD, &GiProcessRank);\n\n   initArrays();\n\n   for(iIterationNo = 0; iIterationNo < GiIterationCnt; iIterationNo++)\n   {\n      dSubResult = 0;\n\n      MPI_Barrier(MPI_COMM_WORLD);\n      dTime0 = MPI_Wtime();\n      if(GiProcessCnt > 1)\n      {\n         if(GiProcessRank == 0)\n         {\n            for(iArrayIndex = 1; iArrayIndex < GiProcessCnt; iArrayIndex++)\n            {\n               MPI_Send((GdArrX + (GiSubVectorLength * iArrayIndex)), GiSubVectorLength, MPI_DOUBLE, iArrayIndex, 0, MPI_COMM_WORLD);\n               MPI_Send((GdArrY + (GiSubVectorLength * iArrayIndex)), GiSubVectorLength, MPI_DOUBLE, iArrayIndex, 0, MPI_COMM_WORLD);\n\n               \n\n            }\n\n            for(iArrayIndex = 0; iArrayIndex < GiSubVectorLength; iArrayIndex++)\n            {\n               dSubResult += (GdArrX[iArrayIndex] * GdArrY[iArrayIndex]);\n            }\n            \n\n         }\n         else\n         {\n            MPI_Recv(GdArrSubX, GiSubVectorLength, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &statusX);\n            MPI_Recv(GdArrSubY, GiSubVectorLength, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &statusY);\n\n            for(iArrayIndex = 0; iArrayIndex < GiSubVectorLength; iArrayIndex++)\n            {\n               dSubResult += (GdArrSubX[iArrayIndex] * GdArrSubY[iArrayIndex]);\n            }\n\n            \n\n         }\n      }\n      else\n      {\n         for(iArrayIndex = 0; iArrayIndex < GiVectorLength; iArrayIndex++)\n         {\n            dSubResult += (GdArrX[iArrayIndex] * GdArrY[iArrayIndex]);\n         }\n      }\n\n      MPI_Barrier(MPI_COMM_WORLD);\n      MPI_Reduce(&dSubResult, &dTotalResult, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n      dTime1 = MPI_Wtime();\n      dTimeDiff = (dTime1 - dTime0);\n\n      if(dTimeDiff > dMaxTimeDiff)\n         dMaxTimeDiff = dTimeDiff;\n      if(dTimeDiff < dMinTimeDiff)\n         dMinTimeDiff = dTimeDiff;\n   }\n\n   if(GiProcessRank == 0)\n      printf(\"Result=%f\\nMin Time=%f uSec\\nMax Time=%f uSec\\n\", dTotalResult, (1.e6 * dMinTimeDiff), (1.e6 * dMaxTimeDiff));\n   \n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "mpip_954", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  long double err;\n  pfftl_complex *in, *out;\n  pfftl_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfftl_init();\n\n  \n\n  if( pfftl_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfftl_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfftl_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfftl_alloc_complex(alloc_local);\n  out = pfftl_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfftl_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfftl_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfftl_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfftl_execute(plan_forw);\n  \n  \n\n  pfftl_execute(plan_back);\n  \n  \n\n  ptrdiff_t l;\n  for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfftl_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfftl_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfftl_printf(comm_cart_2d, \"maxerror = %6.2Le;\\n\", err);\n  \n  \n\n  pfftl_destroy_plan(plan_forw);\n  pfftl_destroy_plan(plan_back);\n  pfftl_free(in); pfftl_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  long double err;\n  pfftl_complex *in, *out;\n  pfftl_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfftl_init();\n\n  \n\n  if( pfftl_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfftl_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfftl_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfftl_alloc_complex(alloc_local);\n  out = pfftl_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfftl_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfftl_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfftl_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfftl_execute(plan_forw);\n  \n  \n\n  pfftl_execute(plan_back);\n  \n  \n\n  ptrdiff_t l;\n  for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfftl_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfftl_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfftl_printf(comm_cart_2d, \"maxerror = %6.2Le;\\n\", err);\n  \n  \n\n  pfftl_destroy_plan(plan_forw);\n  pfftl_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfftl_free(in); pfftl_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_955", "code": "int main(int argc, char **argv)\n{\n    int i, rank, len, err;\n    int errs = 0;\n    char *filename, *tmp;\n    MPI_File fh;\n    char string[MPI_MAX_ERROR_STRING];\n\n\n#if VERBOSE\n    if (!rank) {\n\tfprintf(stderr, \"Tests if errors are reported correctly...\\n\");\n\tfprintf(stderr, \"Should say \\\"Invalid displacement argument\\\"\\n\\n\");\n    }\n#endif\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+10);\n    }\n    \n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    err =\n    err =\n    \n\n\n    \n\n    if (err != MPI_SUCCESS) {\n\tif (!rank) {\n#if VERBOSE\n\t    fprintf(stderr, \"%s\\n\", string);\n#else\n\t    \n\n\t    if (strstr( string, \"displacement\" ) == 0) {\n\t\tfprintf( stderr, \"Unexpected error message %s\\n\", string );\n\t\terrs++;\n\t    }\n#endif\n\t}\n    }\n    else {\n\terrs++;\n\tfprintf( stderr, \"File set view did not return an error\\n\" );\n    }\n\n\n    free(filename);\n    free(tmp);\n\n    if (!rank) {\n\tif (errs == 0) {\n\t    printf( \" No Errors\\n\" );\n\t}\n\telse {\n\t    printf( \" Found %d errors\\n\", errs );\n\t}\n    }\n\n    return 0; \n}", "label": "int main(int argc, char **argv)\n{\n    int i, rank, len, err;\n    int errs = 0;\n    char *filename, *tmp;\n    MPI_File fh;\n    char string[MPI_MAX_ERROR_STRING];\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n#if VERBOSE\n    if (!rank) {\n\tfprintf(stderr, \"Tests if errors are reported correctly...\\n\");\n\tfprintf(stderr, \"Should say \\\"Invalid displacement argument\\\"\\n\\n\");\n    }\n#endif\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: simple -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+10);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    \n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE+MPI_MODE_RDWR,\n\t\t        MPI_INFO_NULL, &fh);\n    err = MPI_File_set_view(fh, -1, MPI_BYTE, MPI_BYTE, \"native\", \n                            MPI_INFO_NULL);\n    \n\n\n    \n\n    if (err != MPI_SUCCESS) {\n\tMPI_Error_string(err, string, &len);\n\tif (!rank) {\n#if VERBOSE\n\t    fprintf(stderr, \"%s\\n\", string);\n#else\n\t    \n\n\t    if (strstr( string, \"displacement\" ) == 0) {\n\t\tfprintf( stderr, \"Unexpected error message %s\\n\", string );\n\t\terrs++;\n\t    }\n#endif\n\t}\n    }\n    else {\n\terrs++;\n\tfprintf( stderr, \"File set view did not return an error\\n\" );\n    }\n\n    MPI_File_close(&fh);\n\n    free(filename);\n    free(tmp);\n\n    if (!rank) {\n\tif (errs == 0) {\n\t    printf( \" No Errors\\n\" );\n\t}\n\telse {\n\t    printf( \" Found %d errors\\n\", errs );\n\t}\n    }\n\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "mercury-hpc_957", "code": "static na_return_t\nna_test_mpi_init(struct na_test_info *na_test_info)\n{\n    int mpi_initialized = 0;\n    na_return_t ret;\n    int rc;\n\n    rc =\n    NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n    NA_TEST_CHECK_ERROR(mpi_initialized, error, ret, NA_PROTOCOL_ERROR,\n        \"MPI was already initialized\");\n\n#    ifdef NA_MPI_HAS_GNI_SETUP\n    \n\n    ret = NA_MPI_Gni_job_setup();\n    NA_TEST_CHECK_NA_ERROR(error, ret, \"Could not setup GNI job\");\n#    endif\n\n    if (na_test_info->listen || na_test_info->mpi_static) {\n        int provided;\n\n        rc =\n        NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n\n        NA_TEST_CHECK_ERROR(provided != MPI_THREAD_MULTIPLE, error, ret,\n            NA_PROTOCOL_ERROR, \"MPI_THREAD_MULTIPLE cannot be set\");\n\n        \n\n        if (na_test_info->mpi_static) {\n            int color, global_rank;\n\n            rc =\n            NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret,\n\n            \n\n            color = (na_test_info->listen) ? 1 : 2;\n\n            \n\n            rc =\n            NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret,\n\n#    ifdef NA_HAS_MPI\n            \n\n            NA_MPI_Set_init_intra_comm(na_test_info->mpi_comm);\n#    endif\n        } else\n            na_test_info->mpi_comm = MPI_COMM_WORLD; \n\n    } else {\n        rc =\n        NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n\n        na_test_info->mpi_comm = MPI_COMM_WORLD; \n\n    }\n\n    rc =\n    NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n\n    rc =\n    NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n\n    return NA_SUCCESS;\n\nerror:\n    na_test_mpi_finalize(na_test_info);\n\n    return ret;\n}", "label": "static na_return_t\nna_test_mpi_init(struct na_test_info *na_test_info)\n{\n    int mpi_initialized = 0;\n    na_return_t ret;\n    int rc;\n\n    rc = MPI_Initialized(&mpi_initialized);\n    NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n        \"MPI_Initialized() failed\");\n    NA_TEST_CHECK_ERROR(mpi_initialized, error, ret, NA_PROTOCOL_ERROR,\n        \"MPI was already initialized\");\n\n#    ifdef NA_MPI_HAS_GNI_SETUP\n    \n\n    ret = NA_MPI_Gni_job_setup();\n    NA_TEST_CHECK_NA_ERROR(error, ret, \"Could not setup GNI job\");\n#    endif\n\n    if (na_test_info->listen || na_test_info->mpi_static) {\n        int provided;\n\n        rc = MPI_Init_thread(NULL, NULL, MPI_THREAD_MULTIPLE, &provided);\n        NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n            \"MPI_Init_thread() failed\");\n\n        NA_TEST_CHECK_ERROR(provided != MPI_THREAD_MULTIPLE, error, ret,\n            NA_PROTOCOL_ERROR, \"MPI_THREAD_MULTIPLE cannot be set\");\n\n        \n\n        if (na_test_info->mpi_static) {\n            int color, global_rank;\n\n            rc = MPI_Comm_rank(MPI_COMM_WORLD, &global_rank);\n            NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret,\n                NA_PROTOCOL_ERROR, \"MPI_Comm_rank() failed\");\n\n            \n\n            color = (na_test_info->listen) ? 1 : 2;\n\n            \n\n            rc = MPI_Comm_split(\n                MPI_COMM_WORLD, color, global_rank, &na_test_info->mpi_comm);\n            NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret,\n                NA_PROTOCOL_ERROR, \"MPI_Comm_split() failed\");\n\n#    ifdef NA_HAS_MPI\n            \n\n            NA_MPI_Set_init_intra_comm(na_test_info->mpi_comm);\n#    endif\n        } else\n            na_test_info->mpi_comm = MPI_COMM_WORLD; \n\n    } else {\n        rc = MPI_Init(NULL, NULL);\n        NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n            \"MPI_Init() failed\");\n\n        na_test_info->mpi_comm = MPI_COMM_WORLD; \n\n    }\n\n    rc = MPI_Comm_rank(na_test_info->mpi_comm, &na_test_info->mpi_comm_rank);\n    NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n        \"MPI_Comm_rank() failed\");\n\n    rc = MPI_Comm_size(na_test_info->mpi_comm, &na_test_info->mpi_comm_size);\n    NA_TEST_CHECK_ERROR(rc != MPI_SUCCESS, error, ret, NA_PROTOCOL_ERROR,\n        \"MPI_Comm_size() failed\");\n\n    return NA_SUCCESS;\n\nerror:\n    na_test_mpi_finalize(na_test_info);\n\n    return ret;\n}"}
{"program": "arthurazs_960", "code": "int main(int argc, char *argv[]) {\n\n    int numprocs, rank, master = 0;\n\n\n    \n\n    int a = 1, b = 6;\n    double h;\n    int trapezoids = 1237;\n    h = (double)(b-a)/trapezoids;\n\n    \n\n    int local_n = trapezoids / numprocs;  \n\n    int extra = trapezoids % numprocs;    \n\n    if (extra > rank) {                   \n\n        local_n++;                        \n\n    }                                     \n\n\n    double local_a = a + rank * local_n * h;\n    if (extra != 0 && extra <= rank) {\n        \n\n        \n\n        \n\n        local_a = local_a + (extra * h);\n    }\n    double local_b = local_a + local_n * h;\n\n    \n\n    double integral_local = calculate(local_a, local_b, local_n, h);\n    printf(\n        \"P(%d): Estimated %f for a(%.2f) to b(%.2f)\\n\",\n        rank, integral_local, local_a, local_b);\n    double integral;\n\n\n    if (rank == master) {\n        double expected = 1.79175946923;\n\n        printf(\"\\nP(%d): MPI_Reduce\\n\", rank);\n        printf(\"Integral (%d to %d) of 1/x\\n\\n\", a, b);\n\n        printf(\"Expected  result  = %.11f\\n\", expected);\n        printf(\"Estimated result  = %.11f\\n\", integral);\n        printf(\"                    -------------\\n\");\n\n        double difference = expected - integral;\n        difference = fabsf(difference); \n\n        printf(\"Difference        = %.11f\\n\", difference);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n\n    int numprocs, rank, master = 0;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    \n\n    int a = 1, b = 6;\n    double h;\n    int trapezoids = 1237;\n    h = (double)(b-a)/trapezoids;\n\n    \n\n    int local_n = trapezoids / numprocs;  \n\n    int extra = trapezoids % numprocs;    \n\n    if (extra > rank) {                   \n\n        local_n++;                        \n\n    }                                     \n\n\n    double local_a = a + rank * local_n * h;\n    if (extra != 0 && extra <= rank) {\n        \n\n        \n\n        \n\n        local_a = local_a + (extra * h);\n    }\n    double local_b = local_a + local_n * h;\n\n    \n\n    double integral_local = calculate(local_a, local_b, local_n, h);\n    printf(\n        \"P(%d): Estimated %f for a(%.2f) to b(%.2f)\\n\",\n        rank, integral_local, local_a, local_b);\n    double integral;\n\n    MPI_Reduce(\n        &integral_local, &integral,\n        1, MPI_DOUBLE, MPI_SUM,\n        master, MPI_COMM_WORLD);\n\n    if (rank == master) {\n        double expected = 1.79175946923;\n\n        printf(\"\\nP(%d): MPI_Reduce\\n\", rank);\n        printf(\"Integral (%d to %d) of 1/x\\n\\n\", a, b);\n\n        printf(\"Expected  result  = %.11f\\n\", expected);\n        printf(\"Estimated result  = %.11f\\n\", integral);\n        printf(\"                    -------------\\n\");\n\n        double difference = expected - integral;\n        difference = fabsf(difference); \n\n        printf(\"Difference        = %.11f\\n\", difference);\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "gnu3ra_962", "code": "int main( int argc, char **argv )\n{\n\n    MPI_Comm      comm;\n    MPI_Datatype  elemtype;\n    double        time_init, time_final;\n    double       *sbuff, *rbuff;\n    int           rank, size;\n    int          *sendcounts, *recvcounts, *rdispls, *sdispls;\n    int           num_itr, block_size, ii, jj, idx;\n\n\n      comm = MPI_COMM_WORLD;\n\n\n      if ( argc > 1 && argv[1] != NULL )\n          block_size = atoi( argv[1] );\n      else\n          block_size = 1;\n\n      if ( argc > 2 && argv[2] != NULL )\n          num_itr = atoi( argv[2] );\n      else\n          num_itr = 1;\n\n      \n\n      sbuff = (double *)malloc( size * size * block_size * sizeof(double) );\n      rbuff = (double *)malloc( size * size * block_size * sizeof(double) );\n      if (!sbuff || !rbuff) {\n        fprintf( stderr, \"Could not allocated buffers!\\n\" );\n      }\n\n      \n\n      for ( ii = 0; ii < size*size; ii++ ) {\n        for ( jj = 0; jj < block_size; jj++ ) {\n          idx        = ii * block_size + jj;\n          sbuff[idx] = ii + 100*rank;\n          rbuff[idx] = -ii;\n        }\n      }\n\n      \n\n      sendcounts = (int *)malloc( size * sizeof(int) );\n      recvcounts = (int *)malloc( size * sizeof(int) );\n      rdispls    = (int *)malloc( size * sizeof(int) );\n      sdispls    = (int *)malloc( size * sizeof(int) );\n      if (!sendcounts || !recvcounts || !rdispls || !sdispls) {\n        fprintf( stderr, \"Could not allocate arg items!\\n\" );\n      }\n      for ( ii = 0; ii < size; ii++ ) {\n        sendcounts[ii] = ii;\n        sdispls[ii]    = (ii * (ii+1))/2;\n        recvcounts[ii] = rank;\n        rdispls[ii]    = ii * rank;\n      }\n\n      time_init   =\n\n      for ( idx = 0; idx < num_itr; idx++ ) {\n      }\n     \n      \n\n      time_final  =\n      \n\n      \n      free( sdispls );\n      free( rdispls );\n      free( recvcounts );\n      free( sendcounts );\n      free( rbuff );\n      free( sbuff );\n\n    return 0;\n}", "label": "int main( int argc, char **argv )\n{\n\n    MPI_Comm      comm;\n    MPI_Datatype  elemtype;\n    double        time_init, time_final;\n    double       *sbuff, *rbuff;\n    int           rank, size;\n    int          *sendcounts, *recvcounts, *rdispls, *sdispls;\n    int           num_itr, block_size, ii, jj, idx;\n\n    MPI_Init( &argc, &argv );\n\n      comm = MPI_COMM_WORLD;\n\n      MPI_Comm_size( comm, &size );\n      MPI_Comm_rank( comm, &rank );\n\n      if ( argc > 1 && argv[1] != NULL )\n          block_size = atoi( argv[1] );\n      else\n          block_size = 1;\n\n      if ( argc > 2 && argv[2] != NULL )\n          num_itr = atoi( argv[2] );\n      else\n          num_itr = 1;\n\n      \n\n      MPI_Type_contiguous( block_size, MPI_DOUBLE, &elemtype );\n      MPI_Type_commit( &elemtype );\n      sbuff = (double *)malloc( size * size * block_size * sizeof(double) );\n      rbuff = (double *)malloc( size * size * block_size * sizeof(double) );\n      if (!sbuff || !rbuff) {\n        fprintf( stderr, \"Could not allocated buffers!\\n\" );\n        MPI_Abort( comm, 1 );\n      }\n\n      \n\n      for ( ii = 0; ii < size*size; ii++ ) {\n        for ( jj = 0; jj < block_size; jj++ ) {\n          idx        = ii * block_size + jj;\n          sbuff[idx] = ii + 100*rank;\n          rbuff[idx] = -ii;\n        }\n      }\n\n      \n\n      sendcounts = (int *)malloc( size * sizeof(int) );\n      recvcounts = (int *)malloc( size * sizeof(int) );\n      rdispls    = (int *)malloc( size * sizeof(int) );\n      sdispls    = (int *)malloc( size * sizeof(int) );\n      if (!sendcounts || !recvcounts || !rdispls || !sdispls) {\n        fprintf( stderr, \"Could not allocate arg items!\\n\" );\n        MPI_Abort( comm, 1 );\n      }\n      for ( ii = 0; ii < size; ii++ ) {\n        sendcounts[ii] = ii;\n        sdispls[ii]    = (ii * (ii+1))/2;\n        recvcounts[ii] = rank;\n        rdispls[ii]    = ii * rank;\n      }\n\n      MPI_Barrier( comm );\n      MPI_Barrier( comm );\n      time_init   = MPI_Wtime();\n\n      for ( idx = 0; idx < num_itr; idx++ ) {\n          MPI_Alltoallv( sbuff, sendcounts, sdispls, elemtype,\n                         rbuff, recvcounts, rdispls, elemtype, comm );\n      }\n     \n      \n\n      time_final  = MPI_Wtime();\n      \n      fprintf( stdout, \"time taken by %dx%d MPI_AlltoAllv() at rank %d = %f\\n\",\n                       block_size, num_itr, rank, time_final - time_init );\n\n      MPI_Type_free( &elemtype );\n      \n      free( sdispls );\n      free( rdispls );\n      free( recvcounts );\n      free( sendcounts );\n      free( rbuff );\n      free( sbuff );\n\n    MPI_Finalize( );\n    return 0;\n}"}
{"program": "pf-aics-riken_963", "code": "int\nmain(int argc, char *argv[])\n{\n    char line[LINELEN];\n    int rank;\n    FILE *ifp, *ofp;\n\n\n    if (argc != 2) {\n        if (rank == 0) {\n            fprintf(stderr, \"specify an input file\\n\");\n        }\n    }\n\n    if (rank == 0) {\n        int sum_count = 0;\n        int sum_point = 0;\n        ifp = fopen(argv[1], \"r\");\n        while (fgets(line, sizeof(line), ifp) != NULL) {\n            char *count_s, *point_s;\n            char *cp = line;\n            int len = (int)strlen(line);\n\n            \n\n            if (cp[len-1] == '\\n') {\n                cp[len-1] = '\\0';\n            }\n\n            \n\n            cp = strchr(line, ' ');\n            count_s = cp + 1;\n\n            \n\n            cp = strchr(line, '/');\n            point_s = cp + 1;\n            cp[0] = '\\0';\n\n            sum_count += atoi(count_s);\n            sum_point += atoi(point_s);\n        }\n        fclose(ifp);\n\n        double pi = 4.0 * sum_count / sum_point;\n\n        ofp = fopen(\"mpi_pi.out\", \"w\");\n        fprintf(ofp, \"%f\\n\", pi);\n        fclose(ofp);\n    }\n\n    return 0;\n}", "label": "int\nmain(int argc, char *argv[])\n{\n    char line[LINELEN];\n    int rank;\n    FILE *ifp, *ofp;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (argc != 2) {\n        if (rank == 0) {\n            fprintf(stderr, \"specify an input file\\n\");\n        }\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    if (rank == 0) {\n        int sum_count = 0;\n        int sum_point = 0;\n        ifp = fopen(argv[1], \"r\");\n        while (fgets(line, sizeof(line), ifp) != NULL) {\n            char *count_s, *point_s;\n            char *cp = line;\n            int len = (int)strlen(line);\n\n            \n\n            if (cp[len-1] == '\\n') {\n                cp[len-1] = '\\0';\n            }\n\n            \n\n            cp = strchr(line, ' ');\n            count_s = cp + 1;\n\n            \n\n            cp = strchr(line, '/');\n            point_s = cp + 1;\n            cp[0] = '\\0';\n\n            sum_count += atoi(count_s);\n            sum_point += atoi(point_s);\n        }\n        fclose(ifp);\n\n        double pi = 4.0 * sum_count / sum_point;\n\n        ofp = fopen(\"mpi_pi.out\", \"w\");\n        fprintf(ofp, \"%f\\n\", pi);\n        fclose(ofp);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "nschloe_965", "code": "int main(int argc, char *argv[])\n{\nint rank, depth;\nint i, go;\nint *branching_degree=NULL, *num_cpus=NULL;\nuint64_t *local_memory=NULL, *total_memory=NULL;\nchar **type_name=NULL;\n\n\n  depth = Zoltan_Get_Topology(&branching_degree, &num_cpus, &type_name, &total_memory, &local_memory);\n\n\n  if (rank == 0){\n\n\n    printf(\"A %s with %d CPUs has:\\n\",type_name[0],num_cpus[0]);\n\n    for (i=1; i < depth; i++){\n      if (i < depth-1){\n        printf(\"  %d %s (%d total CPUs) %swith\\n\",\n              branching_degree[i-1], type_name[i],num_cpus[i],\n              ((num_cpus[i] > 1) ? \"each \" : \"\"));\n      } \n      else{\n        printf(\"  %d %s\\n\", branching_degree[i-1], type_name[i]);\n      }\n    }\n\n    printf(\"\\n\");\n\n    go = 0;\n    for (i=0; i <depth; i++){\n      if ((total_memory[i] > 0) || (local_memory[i] > 0 )){\n        go = 1;\n        break;\n      }\n    }\n\n    if (go){\n      printf(\"Total memory (for all children), Local memory (for object at level)\\n\");\n      for (i=0; i < depth; i++){\n        printf(\"Memory at level %s: (%f10.0KB, %f10.0KB)\\n\",\n         type_name[i],(float)total_memory[i]/1024.0,(float)local_memory[i]/1024.0);\n      } \n    }\n    else{\n        printf(\"Memory available at each level is not available\\n\");\n    }\n  }\n\n  \n\n\n\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\nint rank, depth;\nint i, go;\nint *branching_degree=NULL, *num_cpus=NULL;\nuint64_t *local_memory=NULL, *total_memory=NULL;\nchar **type_name=NULL;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  depth = Zoltan_Get_Topology(&branching_degree, &num_cpus, &type_name, &total_memory, &local_memory);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0){\n\n\n    printf(\"A %s with %d CPUs has:\\n\",type_name[0],num_cpus[0]);\n\n    for (i=1; i < depth; i++){\n      if (i < depth-1){\n        printf(\"  %d %s (%d total CPUs) %swith\\n\",\n              branching_degree[i-1], type_name[i],num_cpus[i],\n              ((num_cpus[i] > 1) ? \"each \" : \"\"));\n      } \n      else{\n        printf(\"  %d %s\\n\", branching_degree[i-1], type_name[i]);\n      }\n    }\n\n    printf(\"\\n\");\n\n    go = 0;\n    for (i=0; i <depth; i++){\n      if ((total_memory[i] > 0) || (local_memory[i] > 0 )){\n        go = 1;\n        break;\n      }\n    }\n\n    if (go){\n      printf(\"Total memory (for all children), Local memory (for object at level)\\n\");\n      for (i=0; i < depth; i++){\n        printf(\"Memory at level %s: (%f10.0KB, %f10.0KB)\\n\",\n         type_name[i],(float)total_memory[i]/1024.0,(float)local_memory[i]/1024.0);\n      } \n    }\n    else{\n        printf(\"Memory available at each level is not available\\n\");\n    }\n  }\n\n  \n\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "srcc-msu_966", "code": "int main(int argc, char* argv[])\n{\n   int i, j, k, rank, size, rows, cols, myrows, mycols, last_rows, last_cols, myi, myj, l, m, jmax;\n   double t1, t2, sum, s;\n\n\n\n\n\n\n   myrows=mycols=rows=cols=(N-1)/size+1;\n   last_rows=last_cols=N-rows*(size-1);\n   if(rank==size-1) myrows=mycols=last_rows;\n\n\n\n\n   double **a=(double**)malloc(sizeof(double*)*myrows);\n   for(i=0; i<myrows; i++)\n      a[i]=(double*)malloc(sizeof(double)*N);\n\n   double **b=(double**)malloc(sizeof(double*)*N);\n   for(i=0; i<N; i++)\n      b[i]=(double*)malloc(sizeof(double)*mycols);\n\n   double **c=(double**)malloc(sizeof(double*)*myrows);\n   for(i=0; i<myrows; i++)\n      c[i]=(double*)malloc(sizeof(double)*N);\n\n   double *b0=(double*)malloc(sizeof(double)*N*cols);\n\n   double **buf=(double**)malloc(sizeof(double*)*N);\n\n   for(i=0; i<N; i++)\n      buf[i]=b0+i*cols;\n\n\n\n   for(i=0; i<myrows; i++){\n      myi=i+rank*rows;\n      for(j=0; j<N; j++){\n         a[i][j]=(myi+j)*0.0001e0;\n         c[i][j]=0.0e0;\n      }\n   }\n\n   for(i=0; i<N; i++)\n      for(j=0; j<mycols; j++){\n         myj=j+rank*cols;\n         b[i][j]=(i-myj)*0.0001e0;\n      }\n\n\n\n\n   for(m=0; m<size; m++){\n      if(rank==m)\n         for(j=0; j<mycols; j++)\n            for(k=0; k<N; k++)\n               buf[k][j]=b[k][j];\n\n\n\n      l=cols*N;\n      if(m==size-1) jmax=last_cols;\n      else jmax=cols;\n\n\n\n      for(i=0; i<myrows; i++)\n         for(j=0; j<jmax; j++)\n            for(k=0; k<N; k++)\n               c[i][m*cols+j]+=a[i][k]*buf[k][j];\n   }\n\n\n\n   for(i=0, sum=0.0e0; i<myrows; i++)\n      for(j=0; j<N; j++)\n         sum+=c[i][j];\n\n   if(rank==0) printf(\"N= %d, Nproc=%d, Sum=%lf, Time=%lf\\n\", N, size, s, t2-t1);\n\n}", "label": "int main(int argc, char* argv[])\n{\n   int i, j, k, rank, size, rows, cols, myrows, mycols, last_rows, last_cols, myi, myj, l, m, jmax;\n   double t1, t2, sum, s;\n\n\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\n\n   myrows=mycols=rows=cols=(N-1)/size+1;\n   last_rows=last_cols=N-rows*(size-1);\n   if(rank==size-1) myrows=mycols=last_rows;\n\n\n\n\n   double **a=(double**)malloc(sizeof(double*)*myrows);\n   for(i=0; i<myrows; i++)\n      a[i]=(double*)malloc(sizeof(double)*N);\n\n   double **b=(double**)malloc(sizeof(double*)*N);\n   for(i=0; i<N; i++)\n      b[i]=(double*)malloc(sizeof(double)*mycols);\n\n   double **c=(double**)malloc(sizeof(double*)*myrows);\n   for(i=0; i<myrows; i++)\n      c[i]=(double*)malloc(sizeof(double)*N);\n\n   double *b0=(double*)malloc(sizeof(double)*N*cols);\n\n   double **buf=(double**)malloc(sizeof(double*)*N);\n\n   for(i=0; i<N; i++)\n      buf[i]=b0+i*cols;\n\n\n\n   for(i=0; i<myrows; i++){\n      myi=i+rank*rows;\n      for(j=0; j<N; j++){\n         a[i][j]=(myi+j)*0.0001e0;\n         c[i][j]=0.0e0;\n      }\n   }\n\n   for(i=0; i<N; i++)\n      for(j=0; j<mycols; j++){\n         myj=j+rank*cols;\n         b[i][j]=(i-myj)*0.0001e0;\n      }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n   t1=MPI_Wtime();\n\n\n\n   for(m=0; m<size; m++){\n      if(rank==m)\n         for(j=0; j<mycols; j++)\n            for(k=0; k<N; k++)\n               buf[k][j]=b[k][j];\n\n\n\n      l=cols*N;\n      MPI_Bcast(b0, l, MPI_DOUBLE, m, MPI_COMM_WORLD);\n      if(m==size-1) jmax=last_cols;\n      else jmax=cols;\n\n\n\n      for(i=0; i<myrows; i++)\n         for(j=0; j<jmax; j++)\n            for(k=0; k<N; k++)\n               c[i][m*cols+j]+=a[i][k]*buf[k][j];\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n   t2=MPI_Wtime();\n\n\n\n   for(i=0, sum=0.0e0; i<myrows; i++)\n      for(j=0; j<N; j++)\n         sum+=c[i][j];\n   MPI_Reduce(&sum, &s, 1, MPI_DOUBLE, MPI_SUM, 0,\n              MPI_COMM_WORLD);\n\n   if(rank==0) printf(\"N= %d, Nproc=%d, Sum=%lf, Time=%lf\\n\", N, size, s, t2-t1);\n\n   MPI_Finalize();\n}"}
{"program": "certik_967", "code": "int main(int argc, char **argv) { \n  \n\n  int  feastparam[64]; \n  double epsout;\n  int loop;\n  char UPLO='F'; \n\n  \n\n  FILE *fp;\n  char name[]=\"../../system2\";\n  int  N,nnz;\n  double *sa;\n  int *isa,*jsa;\n  \n\n  struct timeval t1, t2;\n  int  i,k,err;\n  int  M0,M,info;\n  double Emin,Emax,trace;\n  double *X; \n\n  double *E,*res; \n\n\n\n\nint lrank,lnumprocs,color,key;\nint rank,numprocs;\nMPI_Comm NEW_COMM_WORLD;\nMPI_Init(&argc,&argv); \nMPI_Comm_size(MPI_COMM_WORLD,&numprocs); \nMPI_Comm_rank(MPI_COMM_WORLD,&rank); \n\n\n\n  \n\n\n  \n\n  fp = fopen (name, \"r\");\n  err=fscanf (fp, \"%d%d\\n\",&N,&nnz);\n  sa=calloc(2*nnz,sizeof(double)); \n\n  isa=calloc(N+1,sizeof(int));\n  jsa=calloc(nnz,sizeof(int));\n\n  for (i=0;i<=N;i++){\n    *(isa+i)=0;\n  };\n  *(isa)=1;\n  for (k=0;k<=nnz-1;k++){\n    err=fscanf(fp,\"%d%d%lf%lf\\n\",&i,jsa+k,sa+2*k,sa+2*k+1);\n    *(isa+i)=*(isa+i)+1;\n  };\n  fclose(fp);\n  for (i=1;i<=N;i++){\n    *(isa+i)=*(isa+i)+*(isa+i-1);\n  };\n\n  \n\n  if (rank==0){  \n  printf(\"sparse matrix -system2- size %.d\\n\",N);\n  printf(\"nnz %d \\n\",nnz);\n  }\n\n  \n\n\n\n\n\n\n  if (rank<=numprocs/2-1) {\n    color=1;} \n\n  else {\n    color=2; \n\n  }\n\n  \n\n  key=0;\n  \n\n\n  \n\n if (color==1) { \n\n  Emin=(double) -0.35;\n  Emax=(double) 0.0;\n  M0=40; \n\n  }\n else if(color==2){ \n\n  Emin=(double) 0.0;\n  Emax=(double) 0.23;\n  M0=40; \n\n  }\n\n\n\n gettimeofday(&t1,NULL);\n\n  \n\n  E=calloc(M0,sizeof(double));  \n\n  res=calloc(M0,sizeof(double));\n\n  X=calloc(2*N*M0,sizeof(double));\n\n\n\n  \n\n  feastinit(feastparam);\n  feastparam[8]=NEW_COMM_WORLD;  \n\n  zfeast_hcsrev(&UPLO,&N,sa,isa,jsa,feastparam,&epsout,&loop,&Emin,&Emax,&M0,E,X,&M,res,&info);\n\n  gettimeofday(&t2,NULL);\n  \n\nif (lrank==0) {\n  printf(\"interval # %d\\n\",color);\n  printf(\"FEAST OUTPUT INFO %d\\n\",info);\n  if (info==0) {\n    printf(\"*************************************************\\n\");\n    printf(\"************** REPORT ***************************\\n\");\n    printf(\"*************************************************\\n\");\n    printf(\"# of processors %d \\n\",lnumprocs);\n    printf(\"SIMULATION TIME %f\\n\",(t2.tv_sec-t1.tv_sec)*1.0+(t2.tv_usec-t1.tv_usec)*0.000001);\n    printf(\"# Search interval [Emin,Emax] %.15e %.15e\\n\",Emin,Emax);\n    printf(\"# mode found/subspace %d %d \\n\",M,M0);\n    printf(\"# iterations %d \\n\",loop);\n    trace=(double) 0.0;\n    for (i=0;i<=M-1;i=i+1){\n      trace=trace+*(E+i);\n    }\t  \n    printf(\"TRACE %.15e\\n\", trace);\n    printf(\"Relative error on the Trace %.15e\\n\",epsout );\n    printf(\"Eigenvalues/Residuals\\n\");\n    for (i=0;i<=M-1;i=i+1){\n      printf(\"   %d %.15e %.15e\\n\",i,*(E+i),*(res+i));\n    }\n  }\n}\n\nMPI_Finalize(); \n\n  return 0;\n}", "label": "int main(int argc, char **argv) { \n  \n\n  int  feastparam[64]; \n  double epsout;\n  int loop;\n  char UPLO='F'; \n\n  \n\n  FILE *fp;\n  char name[]=\"../../system2\";\n  int  N,nnz;\n  double *sa;\n  int *isa,*jsa;\n  \n\n  struct timeval t1, t2;\n  int  i,k,err;\n  int  M0,M,info;\n  double Emin,Emax,trace;\n  double *X; \n\n  double *E,*res; \n\n\n\n\nint lrank,lnumprocs,color,key;\nint rank,numprocs;\nMPI_Comm NEW_COMM_WORLD;\nMPI_Init(&argc,&argv); \nMPI_Comm_size(MPI_COMM_WORLD,&numprocs); \nMPI_Comm_rank(MPI_COMM_WORLD,&rank); \n\n\n\n  \n\n\n  \n\n  fp = fopen (name, \"r\");\n  err=fscanf (fp, \"%d%d\\n\",&N,&nnz);\n  sa=calloc(2*nnz,sizeof(double)); \n\n  isa=calloc(N+1,sizeof(int));\n  jsa=calloc(nnz,sizeof(int));\n\n  for (i=0;i<=N;i++){\n    *(isa+i)=0;\n  };\n  *(isa)=1;\n  for (k=0;k<=nnz-1;k++){\n    err=fscanf(fp,\"%d%d%lf%lf\\n\",&i,jsa+k,sa+2*k,sa+2*k+1);\n    *(isa+i)=*(isa+i)+1;\n  };\n  fclose(fp);\n  for (i=1;i<=N;i++){\n    *(isa+i)=*(isa+i)+*(isa+i-1);\n  };\n\n  \n\n  if (rank==0){  \n  printf(\"sparse matrix -system2- size %.d\\n\",N);\n  printf(\"nnz %d \\n\",nnz);\n  }\n\n  \n\n\n\n\n\n\n  if (rank<=numprocs/2-1) {\n    color=1;} \n\n  else {\n    color=2; \n\n  }\n\n  \n\n  key=0;\n MPI_Comm_split(MPI_COMM_WORLD,color,key,&NEW_COMM_WORLD);\n MPI_Comm_rank(NEW_COMM_WORLD,&lrank);\n  \n\n\n  \n\n if (color==1) { \n\n  Emin=(double) -0.35;\n  Emax=(double) 0.0;\n  M0=40; \n\n  }\n else if(color==2){ \n\n  Emin=(double) 0.0;\n  Emax=(double) 0.23;\n  M0=40; \n\n  }\n\n\n\n gettimeofday(&t1,NULL);\n\n  \n\n  E=calloc(M0,sizeof(double));  \n\n  res=calloc(M0,sizeof(double));\n\n  X=calloc(2*N*M0,sizeof(double));\n\n\n\n  \n\n  feastinit(feastparam);\n  feastparam[8]=NEW_COMM_WORLD;  \n\n  zfeast_hcsrev(&UPLO,&N,sa,isa,jsa,feastparam,&epsout,&loop,&Emin,&Emax,&M0,E,X,&M,res,&info);\n\n  gettimeofday(&t2,NULL);\n  \n\nif (lrank==0) {\n  printf(\"interval # %d\\n\",color);\n  printf(\"FEAST OUTPUT INFO %d\\n\",info);\n  if (info==0) {\n    printf(\"*************************************************\\n\");\n    printf(\"************** REPORT ***************************\\n\");\n    printf(\"*************************************************\\n\");\n    MPI_Comm_size(NEW_COMM_WORLD,&lnumprocs);\n    printf(\"# of processors %d \\n\",lnumprocs);\n    printf(\"SIMULATION TIME %f\\n\",(t2.tv_sec-t1.tv_sec)*1.0+(t2.tv_usec-t1.tv_usec)*0.000001);\n    printf(\"# Search interval [Emin,Emax] %.15e %.15e\\n\",Emin,Emax);\n    printf(\"# mode found/subspace %d %d \\n\",M,M0);\n    printf(\"# iterations %d \\n\",loop);\n    trace=(double) 0.0;\n    for (i=0;i<=M-1;i=i+1){\n      trace=trace+*(E+i);\n    }\t  \n    printf(\"TRACE %.15e\\n\", trace);\n    printf(\"Relative error on the Trace %.15e\\n\",epsout );\n    printf(\"Eigenvalues/Residuals\\n\");\n    for (i=0;i<=M-1;i=i+1){\n      printf(\"   %d %.15e %.15e\\n\",i,*(E+i),*(res+i));\n    }\n  }\n}\n\nMPI_Finalize(); \n\n  return 0;\n}"}
{"program": "ghisvail_969", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 2; n[1] = 2; n[2] = 4;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT, g_hat\", MPI_COMM_WORLD);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_apr_complex_3d(\n      out, local_no, local_o_start,\n      \"PFFT, g\", MPI_COMM_WORLD);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT^H, g_hat\", MPI_COMM_WORLD);\n \n  \n\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 2; n[1] = 2; n[2] = 4;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT, g_hat\", MPI_COMM_WORLD);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_apr_complex_3d(\n      out, local_no, local_o_start,\n      \"PFFT, g\", MPI_COMM_WORLD);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  pfft_apr_complex_3d(\n      in, local_ni, local_i_start,\n      \"PFFT^H, g_hat\", MPI_COMM_WORLD);\n \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2c_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "germasch_970", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  \n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  if (strcmp(mrc_crds_type(crds), \"rectilinear\") == 0) {\n    mrctest_set_crds_rectilinear_1(domain);\n  }\n\n  struct mrc_fld *m3 = mrc_domain_m3_create(domain);\n  mrc_fld_set_name(m3, \"test_m3\");\n  mrc_fld_set_param_int(m3, \"nr_comps\", 2);\n  mrc_fld_set_from_options(m3);\n  mrc_fld_setup(m3);\n  mrc_fld_set_comp_name(m3, 0, \"fld0\");\n  mrc_fld_set_comp_name(m3, 1, \"fld1\");\n  mrc_fld_view(m3);\n  \n  set_m3(m3);\n  check_m3(m3);\n\n  switch (testcase) {\n  case 1:\n    test_write_m3(m3);\n    break;\n  case 2:\n    test_write_read_m3(m3);\n    break;\n  }\n  mrc_fld_destroy(m3);\n  mrc_domain_destroy(domain);\n\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  \n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  if (strcmp(mrc_crds_type(crds), \"rectilinear\") == 0) {\n    mrctest_set_crds_rectilinear_1(domain);\n  }\n\n  struct mrc_fld *m3 = mrc_domain_m3_create(domain);\n  mrc_fld_set_name(m3, \"test_m3\");\n  mrc_fld_set_param_int(m3, \"nr_comps\", 2);\n  mrc_fld_set_from_options(m3);\n  mrc_fld_setup(m3);\n  mrc_fld_set_comp_name(m3, 0, \"fld0\");\n  mrc_fld_set_comp_name(m3, 1, \"fld1\");\n  mrc_fld_view(m3);\n  \n  set_m3(m3);\n  check_m3(m3);\n\n  switch (testcase) {\n  case 1:\n    test_write_m3(m3);\n    break;\n  case 2:\n    test_write_read_m3(m3);\n    break;\n  }\n  mrc_fld_destroy(m3);\n  mrc_domain_destroy(domain);\n\n  MPI_Finalize();\n}"}
{"program": "lorenzgerber_973", "code": "int main(void) {\n   char       greeting[MAX_STRING];  \n\n   int        comm_sz;               \n\n   int        my_rank;               \n\n\n   \n\n\n   \n\n\n   \n\n\n   if (my_rank != 0) { \n      \n\n      sprintf(greeting, \"Greetings from process %d of %d!\", \n            my_rank, comm_sz);\n      \n\n   } else {  \n      \n\n      printf(\"Greetings from process %d of %d!\\n\", my_rank, comm_sz);\n      for (int q = 1; q < comm_sz; q++) {\n         \n\n         \n\n         printf(\"%s\\n\", greeting);\n      } \n   }\n\n   \n\n\n   return 0;\n}", "label": "int main(void) {\n   char       greeting[MAX_STRING];  \n\n   int        comm_sz;               \n\n   int        my_rank;               \n\n\n   \n\n   MPI_Init(NULL, NULL); \n\n   \n\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_sz); \n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank); \n\n   if (my_rank != 0) { \n      \n\n      sprintf(greeting, \"Greetings from process %d of %d!\", \n            my_rank, comm_sz);\n      \n\n      MPI_Send(greeting, strlen(greeting)+1, MPI_CHAR, 0, 0,\n            MPI_COMM_WORLD); \n   } else {  \n      \n\n      printf(\"Greetings from process %d of %d!\\n\", my_rank, comm_sz);\n      for (int q = 1; q < comm_sz; q++) {\n         \n\n         MPI_Recv(greeting, MAX_STRING, MPI_CHAR, q,\n            0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         \n\n         printf(\"%s\\n\", greeting);\n      } \n   }\n\n   \n\n   MPI_Finalize(); \n\n   return 0;\n}"}
{"program": "cenezaraujo_974", "code": "int main (int argc, char *argv[]) {\n   int rank, p, size = 8, tag = 0;\n   int left, right;\n   char send_buffer1[8], recv_buffer1[8];\n   char send_buffer2[8], recv_buffer2[8];\n   MPI_Status status;\n\n   left = (rank-1 + p) % p;\n   right = (rank + 1) %p;\n\n   sprintf(send_buffer1, \"N:%d\\n\", rank);   \n   sprintf(send_buffer2, \"N:%d\\n\", rank);   \n\n   if (rank % 2 == 0) {  \n      if (rank == 0 && p % 2 != 0){\n      } else if (rank == p - 1 && p % 2 != 0) {\n      } else {\n   \n      }\n   } else {\n      \n   }\n\n\n   printf(\"-------------------------\\n\");\n   printf(\"Processo %s\", send_buffer1);\n   printf(\"Vizinho Direita %s\", recv_buffer1);\n   printf(\"Vizinho Esquerda %s\", recv_buffer2);\n   printf(\"-------------------------\\n\");\n\n   return 0;\n}", "label": "int main (int argc, char *argv[]) {\n   int rank, p, size = 8, tag = 0;\n   int left, right;\n   char send_buffer1[8], recv_buffer1[8];\n   char send_buffer2[8], recv_buffer2[8];\n   MPI_Status status;\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n   left = (rank-1 + p) % p;\n   right = (rank + 1) %p;\n\n   sprintf(send_buffer1, \"N:%d\\n\", rank);   \n   sprintf(send_buffer2, \"N:%d\\n\", rank);   \n\n   if (rank % 2 == 0) {  \n      if (rank == 0 && p % 2 != 0){\n         MPI_Send(send_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD);\n         MPI_Recv(recv_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD, &status);\n         MPI_Send(send_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD);\n         MPI_Recv(recv_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD, &status);\n      } else if (rank == p - 1 && p % 2 != 0) {\n         MPI_Send(send_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD);\n         MPI_Send(send_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD);\n         MPI_Recv(recv_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD, &status);\n         MPI_Recv(recv_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD, &status);\n      } else {\n         MPI_Send(send_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD);\n         MPI_Send(send_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD);\n   \n         MPI_Recv(recv_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD, &status);\n         MPI_Recv(recv_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD, &status);\n      }\n   } else {\n      MPI_Recv(recv_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD, &status);\n      MPI_Recv(recv_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD, &status);\n      \n      MPI_Send(send_buffer1, size, MPI_CHAR, left, tag, MPI_COMM_WORLD);\n      MPI_Send(send_buffer2, size, MPI_CHAR, right, tag, MPI_COMM_WORLD);\n   }\n\n\n   printf(\"-------------------------\\n\");\n   printf(\"Processo %s\", send_buffer1);\n   printf(\"Vizinho Direita %s\", recv_buffer1);\n   printf(\"Vizinho Esquerda %s\", recv_buffer2);\n   printf(\"-------------------------\\n\");\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "strosek_976", "code": "int main(int argc, char** argv)\n{\n    int rank, size, interval, remainder, i, j;\n    double time1, time2, time3;  \n\n\n    MPI_Request ireq[128]; \n\n    MPI_Status stat;       \n\n\n    \n\n    \n\n    interval = N/(size);\n    remainder = N % (size);\n\n    if (rank==0)  \n\n    {\n        if (argc>1) srandom(atoi(*(argv+1)));  \n\n        \n\n        for(i=0;i<N;i++) for(j=0;j<N;j++) \n        { A[i][j] = random() % 4;\n            B[i][j] = random() % 4;\n        } \n\n\n        time1 =     \n\n        \n\n        \n\n        \n\n\n        \n\n\n        \n\n        for(i=1;i<size;i++)\n        for(i=1;i<size;i++)\n\n\n        compinterval(0,interval);                \n\n        compinterval(size*interval, remainder);  \n\n\n        \n\n        for(i=1;i<size;i++)\n        for(i=1;i<size;i++)\n        {\n            \n\n        }\n\n        time2 =  \n\n\n        \n\n        cmul();\n        time3 =\n\n        printf(\"approx %d-process time Tp: %f sec.\\n\",size,time2-time1);\n        printf(\"approx 1-process (conventional) time T1: %f sec.\\n\",time3-time2);\n        printf(\"efficiency : %f\\n\",(time3-time2)/((time2-time1)*size));\n        \n\n    }\n    else          \n\n    {\n\n        \n\n        compinterval(rank*interval, interval);\n        \n\n    }\n}", "label": "int main(int argc, char** argv)\n{\n    int rank, size, interval, remainder, i, j;\n    double time1, time2, time3;  \n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    MPI_Comm_size(MPI_COMM_WORLD,&size);\n\n    MPI_Request ireq[128]; \n\n    MPI_Status stat;       \n\n\n    \n\n    \n\n    interval = N/(size);\n    remainder = N % (size);\n\n    if (rank==0)  \n\n    {\n        if (argc>1) srandom(atoi(*(argv+1)));  \n\n        \n\n        for(i=0;i<N;i++) for(j=0;j<N;j++) \n        { A[i][j] = random() % 4;\n            B[i][j] = random() % 4;\n        } \n\n\n        time1 = MPI_Wtime();     \n\n        \n\n        \n\n        \n\n        MPI_Bcast(B,N*N,MPI_DOUBLE,0,MPI_COMM_WORLD); \n\n        \n\n\n        \n\n        for(i=1;i<size;i++)\n            MPI_Isend(A+(i*interval),interval*N,MPI_DOUBLE,i,i,\n                    MPI_COMM_WORLD,ireq+i);\n        for(i=1;i<size;i++)\n            MPI_Waitany(size,ireq,&j,&stat); \n\n\n        compinterval(0,interval);                \n\n        compinterval(size*interval, remainder);  \n\n\n        \n\n        for(i=1;i<size;i++)\n            MPI_Irecv(AB+(i*interval),interval*N,MPI_DOUBLE,i,i,\n                    MPI_COMM_WORLD,ireq+i);\n        for(i=1;i<size;i++)\n        {\n            MPI_Waitany(size,ireq,&j,&stat);\n            \n\n        }\n\n        time2 = MPI_Wtime();  \n\n\n        \n\n        cmul();\n        time3 = MPI_Wtime();\n\n        printf(\"approx %d-process time Tp: %f sec.\\n\",size,time2-time1);\n        printf(\"approx 1-process (conventional) time T1: %f sec.\\n\",time3-time2);\n        printf(\"efficiency : %f\\n\",(time3-time2)/((time2-time1)*size));\n        \n\n    }\n    else          \n\n    {\n        MPI_Bcast(B,N*N,MPI_DOUBLE,0,MPI_COMM_WORLD); \n\n        \n\n        MPI_Recv(A+(rank*interval),interval*N,MPI_DOUBLE,0,rank,\n                MPI_COMM_WORLD,&stat);\n        compinterval(rank*interval, interval);\n        \n\n        MPI_Send(AB+(rank*interval),interval*N,MPI_DOUBLE,0,rank,\n                MPI_COMM_WORLD);\n    }\n    MPI_Finalize();\n}"}
{"program": "lapesd_978", "code": "int \nmain (int argc, char **argv)\n{\n\tint nprocs, rank;\n\tint sqrnprocs;\n\tMPI_File file;\n\tMPI_Status status;\n\tint mode = MPI_MODE_CREATE | MPI_MODE_RDWR;\n\tint s;\n\tdouble **bw;\n\tdouble **br;\n\t\n\n\tint ret;\n\tssize_t total;\n\tMPI_Datatype view;\n\tint ndims = 2;\n\tint sizes[] = { SIZE, SIZE };\n\tint subsizes[ndims];\n\tint starts[ndims];\n\n\tassert (argc == 2);\n\n\tsqrnprocs = sqrt (nprocs);\n\n\ts = SIZE / sqrnprocs;\n\tsubsizes[0] = s;\n\tsubsizes[1] = s;\n\t\n\tbw = array2d (s, s);\n\tfillarray2d (bw, s, s, nprocs, rank);\n\n\tstarts[0] = rank / sqrnprocs * s;\n\tstarts[1] = rank % sqrnprocs * s;\t\n\n\n\tret =\n        assert (ret == MPI_SUCCESS);\n\n\n\tret = \n\tassert (ret == MPI_SUCCESS);\n        total = s * s * sizeof (double);\n\t\n\tfprintf (stdout, \"Rank %d: %ld bytes written.\\n\", rank, total);\n\n\tbr = array2d (s, s);\n\n\tret =\n\tassert (ret == MPI_SUCCESS);\n        total = s * s * sizeof (double);\n\t\n\tfprintf (stdout, \"Rank %d: %ld bytes read.\\n\", rank, total);\t\n\n\tprintarray2d (br, s, s, rank);\n\n\n\n\texit (EXIT_FAILURE);\n}", "label": "int \nmain (int argc, char **argv)\n{\n\tint nprocs, rank;\n\tint sqrnprocs;\n\tMPI_File file;\n\tMPI_Status status;\n\tint mode = MPI_MODE_CREATE | MPI_MODE_RDWR;\n\tint s;\n\tdouble **bw;\n\tdouble **br;\n\t\n\n\tint ret;\n\tssize_t total;\n\tMPI_Datatype view;\n\tint ndims = 2;\n\tint sizes[] = { SIZE, SIZE };\n\tint subsizes[ndims];\n\tint starts[ndims];\n\n\tassert (argc == 2);\n\n\tMPI_Init (&argc, &argv);\n\tMPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n\tMPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\tsqrnprocs = sqrt (nprocs);\n\n\ts = SIZE / sqrnprocs;\n\tsubsizes[0] = s;\n\tsubsizes[1] = s;\n\t\n\tbw = array2d (s, s);\n\tfillarray2d (bw, s, s, nprocs, rank);\n\n\tstarts[0] = rank / sqrnprocs * s;\n\tstarts[1] = rank % sqrnprocs * s;\t\n\n\tMPI_Type_create_subarray (ndims, sizes, subsizes, starts, MPI_ORDER_C, MPI_DOUBLE, &view);\n\tMPI_Type_commit (&view);\n\n\tret = MPI_File_open (MPI_COMM_WORLD, argv[1], mode, MPI_INFO_NULL, &file);\n        assert (ret == MPI_SUCCESS);\n\n\tMPI_File_set_view (file, 0, MPI_DOUBLE, view, \"native\", MPI_INFO_NULL);\n\n\tret = MPI_File_write_all (file, &(bw[0][0]), s * s, MPI_DOUBLE, &status); \n\tassert (ret == MPI_SUCCESS);\n        total = s * s * sizeof (double);\n\t\n\tfprintf (stdout, \"Rank %d: %ld bytes written.\\n\", rank, total);\n\tMPI_Barrier (MPI_COMM_WORLD);\n\n\tbr = array2d (s, s);\n\n\tMPI_File_seek (file, 0, MPI_SEEK_SET);\n\tret = MPI_File_read_all (file, &(br[0][0]), s * s, MPI_DOUBLE, &status);\n\tassert (ret == MPI_SUCCESS);\n        total = s * s * sizeof (double);\n\t\n\tfprintf (stdout, \"Rank %d: %ld bytes read.\\n\", rank, total);\t\n\tMPI_Barrier (MPI_COMM_WORLD);\n\n\tprintarray2d (br, s, s, rank);\n\tMPI_Barrier (MPI_COMM_WORLD);\n\n\tMPI_File_close (&file);\n\tMPI_Type_free (&view);\n\n\tMPI_Finalize ();\n\n\texit (EXIT_FAILURE);\n}"}
{"program": "germasch_979", "code": "int\nmain(int argc, char **argv)\n{\n  mrc_class_mrc_fld.watch = true;\n  libmrc_params_init(argc, argv);\n\n  struct kdv *kdv = kdv_create(MPI_COMM_WORLD);\n  kdv_set_from_options(kdv);\n  kdv_setup(kdv);\n  kdv_view(kdv);\n\n  \n\n  struct mrc_crds *crds = mrc_domain_get_crds(kdv->domain);\n  struct mrc_fld *x = kdv_get_fld(kdv, NR_FLDS, \"x\");\n  mrc_fld_set_comp_name(x, U, \"u\");\n\n  \n\n  mrc_f1_foreach(x, ix, 0, 0) {\n    \n\n    MRC_F1(x, U, ix) = -12. * 1./sqr(cosh(CRDX(ix))); \n\n  } mrc_f1_foreach_end;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, NULL, NULL);\n  mrc_ts_set_context(ts, kdv_to_mrc_obj(kdv));\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  kdv_destroy(kdv);\n\n  libmrc_finalize();\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  mrc_class_mrc_fld.watch = true;\n  libmrc_params_init(argc, argv);\n\n  struct kdv *kdv = kdv_create(MPI_COMM_WORLD);\n  kdv_set_from_options(kdv);\n  kdv_setup(kdv);\n  kdv_view(kdv);\n\n  \n\n  struct mrc_crds *crds = mrc_domain_get_crds(kdv->domain);\n  struct mrc_fld *x = kdv_get_fld(kdv, NR_FLDS, \"x\");\n  mrc_fld_set_comp_name(x, U, \"u\");\n\n  \n\n  mrc_f1_foreach(x, ix, 0, 0) {\n    \n\n    MRC_F1(x, U, ix) = -12. * 1./sqr(cosh(CRDX(ix))); \n\n  } mrc_f1_foreach_end;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, NULL, NULL);\n  mrc_ts_set_context(ts, kdv_to_mrc_obj(kdv));\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  mrc_ts_view(ts);\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  kdv_destroy(kdv);\n\n  libmrc_finalize();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "mpip_980", "code": "int main(int argc, char **argv)\n{\n  int nthreads=1; \n\n  int runs=1; \n\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n\n  \n\n  pfft_get_args(argc,argv,\"-pfft_omp_threads\",1,PFFT_INT,&nthreads);\n  pfft_get_args(argc,argv,\"-pfft_runs\",1,PFFT_INT,&runs);\n  pfft_printf(MPI_COMM_WORLD, \"# %4d threads will be used for openmp (default is 1)\\n\", nthreads);\n\n  \n\n  n[0] = NNN;n[1] =NNN; n[2] =NNN;\n  np[0] = 1; np[1] = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  pfft_plan_with_nthreads(nthreads);\n\n \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_TUNE| PFFT_SHIFTED_IN);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_TUNE| PFFT_SHIFTED_OUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  for(int i=0; i<runs; i++)\n  {\n    \n\n    pfft_execute(plan_forw);\n\n    \n\n    \n\n    \n\n    pfft_execute(plan_back);\n\n    \n\n    ptrdiff_t l;\n    for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n      in[l] /= (n[0]*n[1]*n[2]);\n  }\n\n  pfft_print_average_timer_adv(plan_forw, MPI_COMM_WORLD);\n  pfft_print_average_timer_adv(plan_back, MPI_COMM_WORLD);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after %d forward and backward trafos of size n=(%td, %td, %td):\\n\", runs, n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int nthreads=1; \n\n  int runs=1; \n\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n\n  \n\n  pfft_get_args(argc,argv,\"-pfft_omp_threads\",1,PFFT_INT,&nthreads);\n  pfft_get_args(argc,argv,\"-pfft_runs\",1,PFFT_INT,&runs);\n  pfft_printf(MPI_COMM_WORLD, \"# %4d threads will be used for openmp (default is 1)\\n\", nthreads);\n\n  \n\n  n[0] = NNN;n[1] =NNN; n[2] =NNN;\n  np[0] = 1; np[1] = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  pfft_plan_with_nthreads(nthreads);\n\n \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_TUNE| PFFT_SHIFTED_IN);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT| PFFT_TUNE| PFFT_SHIFTED_OUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  for(int i=0; i<runs; i++)\n  {\n    \n\n    pfft_execute(plan_forw);\n\n    \n\n    \n\n    \n\n    pfft_execute(plan_back);\n\n    \n\n    ptrdiff_t l;\n    for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n      in[l] /= (n[0]*n[1]*n[2]);\n  }\n\n  pfft_print_average_timer_adv(plan_forw, MPI_COMM_WORLD);\n  pfft_print_average_timer_adv(plan_back, MPI_COMM_WORLD);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after %d forward and backward trafos of size n=(%td, %td, %td):\\n\", runs, n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "sunwell1994_981", "code": "int main(int argc, char **argv)\n{\n\tchar input_file_name[1024];\n\tchar model_file_name[1024];\n\tchar model_file[1024];\n\tconst char *error_msg;\n\tint right;\n\tparse_command_line(argc, argv, input_file_name, model_file_name);\n\n\tint pid, total_processes;\n\tfor ( right = 15; right < 78; right++) {\n\t\tnew_train(model_file, input_file_name, model_file_name, pid, right);\n\t}\n\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tchar input_file_name[1024];\n\tchar model_file_name[1024];\n\tchar model_file[1024];\n\tconst char *error_msg;\n\tint right;\n\tparse_command_line(argc, argv, input_file_name, model_file_name);\n\n\tint pid, total_processes;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &pid);\n\tMPI_Comm_size(MPI_COMM_WORLD, &total_processes);\n\tfor ( right = 15; right < 78; right++) {\n\t\tnew_train(model_file, input_file_name, model_file_name, pid, right);\n\t}\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "timoteus_983", "code": "int main(int argc, char **argv)\n{\n    srand(time(NULL));\n\n\n    rock_indx_t *indx = NULL;\n    rock_elem_t *elem = NULL;\n    rock_desc_t *desc = NULL;\n    rock_mpart_t *mpart = NULL;\n\n    \n\n    rock_uint_t proc_order = 3; \n\n    rock_uint_t proc_dims[] = {2, 4, 2}; \n\n    rock_mesh_t *mesh = rock_mesh_init(MPI_COMM_WORLD, proc_order, proc_dims);\n\n    if (mesh->rank == ROCK_MASTER) {\n\n        \n\n        rock_uint_t order = 3;\n        rock_uint_t nnz = 1e6; \n\n        rock_uint_t dim_size[] = {300, 2, 25000};\n        desc = rock_desc_init(order, dim_size);\n\n        \n\n        mpart = rock_mpart_init(mesh->order, mesh->dim_size);\n\n        \n\n        rock_uint_t num_part_dims = 1;\n        rock_uint_t part_dims[] = {1}; \n\n        rock_mpart_desc_based(desc, mpart, num_part_dims, part_dims);\n\n        \n\n        elem = rock_elem_init(nnz);\n        indx = rock_indx_init(nnz);\n\n        \n\n        rock_elem_sample(desc, elem);\n        rock_indx_sample(desc, indx);\n    }\n\n    \n\n\n    \n\n    rock_dist_t *dist = rock_dist_init(indx, elem, mpart, mesh);\n\n    \n\n    rock_indx_scatter(&indx, dist);\n\n    \n\n    rock_elem_scatter(&elem, dist);\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n    rock_indx_gather(&indx, dist);\n\n    \n\n    rock_elem_gather(&elem, dist);\n\n    \n\n\n\n    if (mesh->rank == ROCK_MASTER) {\n        rock_desc_free(desc);\n        rock_mpart_free(mpart);\n        rock_indx_free(indx);\n        rock_elem_free(elem);\n    }\n\n    rock_mesh_free(mesh);\n    rock_dist_free(dist);\n\n}", "label": "int main(int argc, char **argv)\n{\n    srand(time(NULL));\n\n    MPI_Init(&argc, &argv);\n\n    rock_indx_t *indx = NULL;\n    rock_elem_t *elem = NULL;\n    rock_desc_t *desc = NULL;\n    rock_mpart_t *mpart = NULL;\n\n    \n\n    rock_uint_t proc_order = 3; \n\n    rock_uint_t proc_dims[] = {2, 4, 2}; \n\n    rock_mesh_t *mesh = rock_mesh_init(MPI_COMM_WORLD, proc_order, proc_dims);\n\n    if (mesh->rank == ROCK_MASTER) {\n\n        \n\n        rock_uint_t order = 3;\n        rock_uint_t nnz = 1e6; \n\n        rock_uint_t dim_size[] = {300, 2, 25000};\n        desc = rock_desc_init(order, dim_size);\n\n        \n\n        mpart = rock_mpart_init(mesh->order, mesh->dim_size);\n\n        \n\n        rock_uint_t num_part_dims = 1;\n        rock_uint_t part_dims[] = {1}; \n\n        rock_mpart_desc_based(desc, mpart, num_part_dims, part_dims);\n\n        \n\n        elem = rock_elem_init(nnz);\n        indx = rock_indx_init(nnz);\n\n        \n\n        rock_elem_sample(desc, elem);\n        rock_indx_sample(desc, indx);\n    }\n\n    \n\n\n    \n\n    rock_dist_t *dist = rock_dist_init(indx, elem, mpart, mesh);\n\n    \n\n    rock_indx_scatter(&indx, dist);\n\n    \n\n    rock_elem_scatter(&elem, dist);\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n    rock_indx_gather(&indx, dist);\n\n    \n\n    rock_elem_gather(&elem, dist);\n\n    \n\n\n    MPI_Barrier(mesh->comm);\n\n    if (mesh->rank == ROCK_MASTER) {\n        rock_desc_free(desc);\n        rock_mpart_free(mpart);\n        rock_indx_free(indx);\n        rock_elem_free(elem);\n    }\n\n    rock_mesh_free(mesh);\n    rock_dist_free(dist);\n\n    MPI_Finalize();\n}"}
{"program": "nci-australia_987", "code": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int *scnt, *rcnt;\n  int *sdpl, *rdpl;\n  int dsize;\n  int ssize, rsize;\n\n\n\n  scnt = malloc( sizeof(int)*nprocs );\n  sdpl = malloc( sizeof(int)*nprocs );\n  rcnt = malloc( sizeof(int)*nprocs );\n  rdpl = malloc( sizeof(int)*nprocs );\n  \n  for( i=0; i<nprocs; i++ )\n    {\n      scnt[i]=COUNT*(i+1)*(myrank+1);\n      rcnt[i]=COUNT*(i+1)*(myrank+1);\n      sdpl[i]=COUNT*((i*(i+1))/2)*(myrank+1);\n      rdpl[i]=COUNT*((i*(i+1))/2)*(myrank+1);\n    }\n  \n  ssize=0; for(i=0; i<nprocs; i++) ssize+=scnt[i];\n  rsize=0; for(i=0; i<nprocs; i++) rsize+=rcnt[i];\n  \n  sbuf = (char*) malloc( COUNT*dsize*ssize );\n  rbuf = (char*) malloc( COUNT*dsize*rsize );\n\n  for( i=0; i<REPEAT; i++ )\n    {\n    }\n\n  fprintf(stdout, \"DONE (rank %d)!\\n\", myrank);\n  \n}", "label": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int *scnt, *rcnt;\n  int *sdpl, *rdpl;\n  int dsize;\n  int ssize, rsize;\n\n  MPI_Init( &argc, &argv );\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  MPI_Type_size(DATATYPE, &dsize);\n\n  scnt = malloc( sizeof(int)*nprocs );\n  sdpl = malloc( sizeof(int)*nprocs );\n  rcnt = malloc( sizeof(int)*nprocs );\n  rdpl = malloc( sizeof(int)*nprocs );\n  \n  for( i=0; i<nprocs; i++ )\n    {\n      scnt[i]=COUNT*(i+1)*(myrank+1);\n      rcnt[i]=COUNT*(i+1)*(myrank+1);\n      sdpl[i]=COUNT*((i*(i+1))/2)*(myrank+1);\n      rdpl[i]=COUNT*((i*(i+1))/2)*(myrank+1);\n    }\n  \n  ssize=0; for(i=0; i<nprocs; i++) ssize+=scnt[i];\n  rsize=0; for(i=0; i<nprocs; i++) rsize+=rcnt[i];\n  \n  sbuf = (char*) malloc( COUNT*dsize*ssize );\n  rbuf = (char*) malloc( COUNT*dsize*rsize );\n\n  for( i=0; i<REPEAT; i++ )\n    {\n      MPI_Alltoallv( sbuf, scnt, sdpl, DATATYPE,\n\t\t     rbuf, rcnt, rdpl, DATATYPE,\n\t\t     MPI_COMM_WORLD );\n    }\n\n  fprintf(stdout, \"DONE (rank %d)!\\n\", myrank);\n  \n  MPI_Finalize();\n}"}
{"program": "andresf01_990", "code": "int main(int argc,char *argv[]){\n\tint size, rank, dest, source, count, tag=1;\n\tint inmsg, outmsg=5;\n\tMPI_Status Stat;\n\n\n\tif (rank == 0) {\n\t  dest = 1;\n\t  source = 5;\n\t }\n\tif (rank == 1) {\n\t  dest = 2;\n\t  source = 0;\n\t }\n\tif (rank == 2) {\n          dest = 3;\n          source = 1;\n\n         }\n\tif (rank == 3) {\n          dest = 4;\n          source = 2;\n         }\n\tif (rank == 4) {\n          dest = 5;\n          source = 3;\n         }\n\tif (rank == 5) {\n          dest = 0;\n          source = 4;\n         }\n\n\tprintf(\"Task %d: Received %d (number) from task %d with tag %d \\n\",\n\t\t   rank, inmsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\n}", "label": "int main(int argc,char *argv[]){\n\tint size, rank, dest, source, count, tag=1;\n\tint inmsg, outmsg=5;\n\tMPI_Status Stat;\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t  dest = 1;\n\t  source = 5;\n\t  MPI_Send(&rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n\t  MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n\t }\n\tif (rank == 1) {\n\t  dest = 2;\n\t  source = 0;\n\t  MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n\t  MPI_Send(&rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n\t }\n\tif (rank == 2) {\n          dest = 3;\n          source = 1;\n          MPI_Send(&rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n          MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n\n         }\n\tif (rank == 3) {\n          dest = 4;\n          source = 2;\n          MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n          MPI_Send(&rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n         }\n\tif (rank == 4) {\n          dest = 5;\n          source = 3;\n          MPI_Send(&rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n          MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n         }\n\tif (rank == 5) {\n          dest = 0;\n          source = 4;\n          MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n          MPI_Send(&rank, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n         }\n\n\tMPI_Get_count(&Stat, MPI_CHAR, &count);\n\tprintf(\"Task %d: Received %d (number) from task %d with tag %d \\n\",\n\t\t   rank, inmsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\n\tMPI_Finalize();\n}"}
{"program": "itsjareds_993", "code": "int main(int argc, char *argv[])\n{\n\tint done = 0, n, myid, numprocs, i;\n\tdouble startwtime, endwtime;\n\tint  namelen;\n\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n\n#ifdef __GMP_H\n\tmpf_t mpf_mypi, mpf_pi, mpf_h, mpf_sum, mpf_x, mpf_fret, mpf_tmp;\n\tvoid * packed_mpf_mypi, * packed_mpf_pi;\n\tint mpf_size, pos;\n#endif\n\n\n\tfprintf(stdout,\"Process %d of %d on %s\\n\",\n\t\tmyid, numprocs, processor_name);\n\n#ifdef __GMP_H\n\n\n#define MPF_PREC 256\n\n\t\n\n\tmpf_set_default_prec(MPF_PREC);\n\tcommit_mpf(&(MPI_MPF), MPF_PREC, MPI_COMM_WORLD);\n\tcreate_mpf_op(&(MPI_MPF_SUM), _mpi_mpf_add, MPI_COMM_WORLD);\n\n\tif(myid == 0)\n\t{\n\t\tprintf(\"----- Start (MPF or MPFR) -----\\n\");\n\t\tn = 16384;\n\t\tstartwtime =\n\t}\n\n\t\n\n\n\tmpf_init(mpf_h);\n\tmpf_init(mpf_x);\n\tmpf_init(mpf_pi);\n\tmpf_init(mpf_sum);\n\tmpf_init(mpf_mypi);\n\tmpf_init(mpf_tmp);\n\tmpf_init(mpf_fret);\n\n\tmpf_set_ui(mpf_h, n);\n\tmpf_ui_div(mpf_h, 1UL, mpf_h);\n\n\tmpf_set_ui(mpf_sum, 0UL);\n\n\t\n\n\tmpf_set_ui(mpf_tmp, 1UL);\n\tmpf_div_ui(mpf_tmp, mpf_tmp, 2UL);\n\n\tfor (i = myid + 1; i <= n; i += numprocs)\n\t{\n\t\tmpf_set_ui(mpf_x, (unsigned long)i);\n\t\tmpf_sub(mpf_x, mpf_x, mpf_tmp);\n\t\tmpf_mul(mpf_x, mpf_x, mpf_h);\n\n\t\tmpf_f(mpf_fret, mpf_x);\n\t\tmpf_add(mpf_sum, mpf_sum, mpf_fret);\n\t}\n\n\tmpf_mul(mpf_mypi, mpf_sum, mpf_h);\n\n\tpacked_mpf_mypi = allocbuf_mpf(mpf_get_prec(mpf_mypi), 1);\n\tpacked_mpf_pi = allocbuf_mpf(mpf_get_prec(mpf_pi), 1);\n\tpack_mpf(mpf_mypi, 1, packed_mpf_mypi);\n\tpack_mpf(mpf_pi, 1, packed_mpf_pi);\n\tunpack_mpf(packed_mpf_pi, mpf_pi, 1);\n\n\tif (myid == 0)\n\t{\n\t\tendwtime =\n\t\tprintf(\"mpf_pi :\"); mpf_out_str(stdout, 10, 0, mpf_pi); printf(\"\\n\");\n\t\tprintf(\"mpf_h :\"); mpf_out_str(stdout, 10, 0, mpf_h); printf(\"\\n\");\n\t\tprintf(\"wall clock time = %f\\n\", endwtime - startwtime);\t\t\n\t\tprintf(\"----- End (MPF or MPFR) -----\\n\");\n\t}\n\n\t\n\n\tmpf_clear(mpf_mypi);\n\tmpf_clear(mpf_sum);\n\tmpf_clear(mpf_x);\n\tmpf_clear(mpf_h);\n\tmpf_clear(mpf_pi);\n\tmpf_clear(mpf_fret);\n\tmpf_clear(mpf_tmp);\n\n\t\n\n\tfree_mpf(&(MPI_MPF));\n\tfree_mpf_op(&(MPI_MPF_SUM));\n#endif\n\n}", "label": "int main(int argc, char *argv[])\n{\n\tint done = 0, n, myid, numprocs, i;\n\tdouble startwtime, endwtime;\n\tint  namelen;\n\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n\n#ifdef __GMP_H\n\tmpf_t mpf_mypi, mpf_pi, mpf_h, mpf_sum, mpf_x, mpf_fret, mpf_tmp;\n\tvoid * packed_mpf_mypi, * packed_mpf_pi;\n\tint mpf_size, pos;\n#endif\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\tMPI_Get_processor_name(processor_name, &namelen);\n\n\tfprintf(stdout,\"Process %d of %d on %s\\n\",\n\t\tmyid, numprocs, processor_name);\n\n#ifdef __GMP_H\n\n\n#define MPF_PREC 256\n\n\t\n\n\tmpf_set_default_prec(MPF_PREC);\n\tcommit_mpf(&(MPI_MPF), MPF_PREC, MPI_COMM_WORLD);\n\tcreate_mpf_op(&(MPI_MPF_SUM), _mpi_mpf_add, MPI_COMM_WORLD);\n\n\tif(myid == 0)\n\t{\n\t\tprintf(\"----- Start (MPF or MPFR) -----\\n\");\n\t\tn = 16384;\n\t\tstartwtime = MPI_Wtime();\n\t}\n\n\t\n\n\tMPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tmpf_init(mpf_h);\n\tmpf_init(mpf_x);\n\tmpf_init(mpf_pi);\n\tmpf_init(mpf_sum);\n\tmpf_init(mpf_mypi);\n\tmpf_init(mpf_tmp);\n\tmpf_init(mpf_fret);\n\n\tmpf_set_ui(mpf_h, n);\n\tmpf_ui_div(mpf_h, 1UL, mpf_h);\n\n\tmpf_set_ui(mpf_sum, 0UL);\n\n\t\n\n\tmpf_set_ui(mpf_tmp, 1UL);\n\tmpf_div_ui(mpf_tmp, mpf_tmp, 2UL);\n\n\tfor (i = myid + 1; i <= n; i += numprocs)\n\t{\n\t\tmpf_set_ui(mpf_x, (unsigned long)i);\n\t\tmpf_sub(mpf_x, mpf_x, mpf_tmp);\n\t\tmpf_mul(mpf_x, mpf_x, mpf_h);\n\n\t\tmpf_f(mpf_fret, mpf_x);\n\t\tmpf_add(mpf_sum, mpf_sum, mpf_fret);\n\t}\n\n\tmpf_mul(mpf_mypi, mpf_sum, mpf_h);\n\n\tpacked_mpf_mypi = allocbuf_mpf(mpf_get_prec(mpf_mypi), 1);\n\tpacked_mpf_pi = allocbuf_mpf(mpf_get_prec(mpf_pi), 1);\n\tpack_mpf(mpf_mypi, 1, packed_mpf_mypi);\n\tpack_mpf(mpf_pi, 1, packed_mpf_pi);\n\tMPI_Reduce(packed_mpf_mypi, packed_mpf_pi, 1, MPI_MPF, MPI_MPF_SUM, 0, MPI_COMM_WORLD);\n\tunpack_mpf(packed_mpf_pi, mpf_pi, 1);\n\n\tif (myid == 0)\n\t{\n\t\tendwtime = MPI_Wtime();\n\t\tprintf(\"mpf_pi :\"); mpf_out_str(stdout, 10, 0, mpf_pi); printf(\"\\n\");\n\t\tprintf(\"mpf_h :\"); mpf_out_str(stdout, 10, 0, mpf_h); printf(\"\\n\");\n\t\tprintf(\"wall clock time = %f\\n\", endwtime - startwtime);\t\t\n\t\tprintf(\"----- End (MPF or MPFR) -----\\n\");\n\t}\n\n\t\n\n\tmpf_clear(mpf_mypi);\n\tmpf_clear(mpf_sum);\n\tmpf_clear(mpf_x);\n\tmpf_clear(mpf_h);\n\tmpf_clear(mpf_pi);\n\tmpf_clear(mpf_fret);\n\tmpf_clear(mpf_tmp);\n\n\t\n\n\tfree_mpf(&(MPI_MPF));\n\tfree_mpf_op(&(MPI_MPF_SUM));\n#endif\n\tMPI_Finalize();\n\n}"}
{"program": "mpip_995", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n\n  \n\n  n[0] = 128; n[1] = 128; n[2] = 128;\n  np[0] = 1; np[1] = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n\n  pfft_print_average_timer(plan_forw, MPI_COMM_WORLD);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  pfft_print_average_timer(plan_back, MPI_COMM_WORLD);\n\n  \n\n  ptrdiff_t l;\n  for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n\n  \n\n  n[0] = 128; n[1] = 128; n[2] = 128;\n  np[0] = 1; np[1] = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n\n  pfft_print_average_timer(plan_forw, MPI_COMM_WORLD);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  pfft_print_average_timer(plan_back, MPI_COMM_WORLD);\n\n  \n\n  ptrdiff_t l;\n  for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu-user_996", "code": "int main(int argc, char *argv[]) {\n    int    proc_id;       \n\n    int    n_proc;        \n\n    int    n_rows = 1000; \n\n    int    n_cols = 1000; \n\n\n    \n\n\n    if (n_proc == 1) {\n        puts(\"Error: requires two or more processes (no work will be done).\");\n        exit(1);\n    }\n\n    if (proc_id == MASTER) {\n        master(n_proc, n_rows, n_cols);\n    }\n    else {\n        slave(proc_id, n_rows, n_cols);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    int    proc_id;       \n\n    int    n_proc;        \n\n    int    n_rows = 1000; \n\n    int    n_cols = 1000; \n\n\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n\n    if (n_proc == 1) {\n        puts(\"Error: requires two or more processes (no work will be done).\");\n        MPI_Finalize();\n        exit(1);\n    }\n\n    if (proc_id == MASTER) {\n        master(n_proc, n_rows, n_cols);\n    }\n    else {\n        slave(proc_id, n_rows, n_cols);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "artpol84_998", "code": "int main(int argc, char* argv[])\n{\n  int rank;\n  int size;\n  int i = 1;\n\n\n  if (rank == 0)\n    printf(\"*** Will print ten rows of dots.\\n\");  \n\n  printf(\"Hello, world, I am %d of %d\\n\", rank, size);\n\n  double iters = 5e5;\n  for (i = 1; i < (int)iters; i++)\n  { int buf;\n    MPI_Status status;\n\n    buf = i;\n    if (rank == 0) {\n      \n\n    }\n    \n\n    if (i != buf) {\n      fprintf(stderr, \"****** INCORRECT RESULT:  %d\\n\", i);\n      exit(1);\n    }\n    if (rank != 0) {\n      \n\n    }\n\n    if (rank == 0) {\n      if (i % (int)(iters/100) == 0) {printf(\".\"); fflush(stdout);}\n      if (i % (int)(iters/10) == 0) printf(\"\\n\");\n    }\n  }\n  if( rank == 0 ){\n    printf(\"Finish\\n\");\n  }\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  int rank;\n  int size;\n  int i = 1;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0)\n    printf(\"*** Will print ten rows of dots.\\n\");  \n\n  printf(\"Hello, world, I am %d of %d\\n\", rank, size);\n\n  double iters = 5e5;\n  for (i = 1; i < (int)iters; i++)\n  { int buf;\n    MPI_Status status;\n\n    buf = i;\n    if (rank == 0) {\n      \n\n      MPI_Send(&buf, 1, MPI_INT, (rank+1)%size, 0, MPI_COMM_WORLD);\n    }\n    \n\n    MPI_Recv(&buf, 1, MPI_INT, (rank-1+size)%size, 0, MPI_COMM_WORLD, &status);\n    if (i != buf) {\n      fprintf(stderr, \"****** INCORRECT RESULT:  %d\\n\", i);\n      exit(1);\n    }\n    if (rank != 0) {\n      \n\n      MPI_Send(&buf, 1, MPI_INT, (rank+1)%size, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n      if (i % (int)(iters/100) == 0) {printf(\".\"); fflush(stdout);}\n      if (i % (int)(iters/10) == 0) printf(\"\\n\");\n    }\n  }\n  if( rank == 0 ){\n    printf(\"Finish\\n\");\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "ajylee_999", "code": "int main(int argc, char* argv[])\n{\n  int rank;\n  int size;\n\n  assert(argc == 4);\n  \n\n  int Px = atoi(argv[1]);\n  int Py = atoi(argv[2]);\n  int Pz = atoi(argv[3]);\n  assert(Px * Py * Pz == size);\n\n  \n\n  int N = 96;\n  int M = 100;\n  int R = 10;\n  \n\n\n  \n\n  int Nx = N / Px;\n  int Ny = N / Py;\n  int Nz = N / Pz;\n  assert(Nx * Px == N);\n  assert(Ny * Py == N);\n  assert(Nz * Pz == N);\n\n  \n\n  int nx = rank / (Py * Pz);\n  int ny = (rank - nx * Py * Pz) / Pz;\n  int nz = rank % Pz;\n\n  \n\n  int rxm = nz + Pz * (ny + Py * ((nx - 1 + Px) % Px));\n  int rxp = nz + Pz * (ny + Py * ((nx + 1) % Px));\n  int rym = nz + Pz * ((ny - 1 + Py) % Py + Py * nx);\n  int ryp = nz + Pz * ((ny + 1) % Py + Py * nx);\n  int rzm = (nz - 1 + Pz) % Pz + Pz * (ny + Py * nx);\n  int rzp = (nz + 1) % Pz + Pz * (ny + Py * nx);\n  \n  int NNN = Nx * Ny * Nz;\n  \n\n  double* a = (double*)malloc(M * NNN * sizeof(double));\n  double* b = (double*)malloc(M * NNN * sizeof(double));\n  \n  \n\n  int n = 0;\n  for (int m = 0; m < M; m++)\n    for (int x = nx * Nx; x < (nx + 1) * Nx; x++)\n      for (int y = ny * Ny; y < (ny + 1) * Ny; y++)\n\tfor (int z = nz * Nz; z < (nz + 1) * Nz; z++, n++)\n\t  a[n] = x + y * y + z * z * z;\n\n  \n\n  int K = (Nx + 4) * (Ny + 4) * (Nz + 4);\n  double* w = (double*)malloc(K * sizeof(double));\n\n  double h = 0.1;  \n\n\n  double t0 = clock();\n  for (int r = 0; r < R; r++)\n    for (int m = 0; m < M; m++)\n      fd(a + m * NNN, b + m * NNN, w,\n\t h, \n\t Nx, Ny, Nz,\n\t rank, rxm, rxp, rym, ryp, rzm, rzp);\n  double t =  clock() - t0;\n\n  printf(\"Rank: %4d, Time: %f, GFLOPS: %f\\n\", rank, t / CLOCKS_PER_SEC,\n\t 19.0e-9 * R * M * NNN * CLOCKS_PER_SEC / t);\n\n  double s = 0.0;\n  for (int n = 0; n < NNN; n++)\n    s += b[n];\n  double sum;\n\n  if (rank == 0)\n    printf(\"Sum:    %f\\n\", sum);\n\n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  MPI_Init(&argc, &argv);\n  int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  assert(argc == 4);\n  \n\n  int Px = atoi(argv[1]);\n  int Py = atoi(argv[2]);\n  int Pz = atoi(argv[3]);\n  assert(Px * Py * Pz == size);\n\n  \n\n  int N = 96;\n  int M = 100;\n  int R = 10;\n  \n\n\n  \n\n  int Nx = N / Px;\n  int Ny = N / Py;\n  int Nz = N / Pz;\n  assert(Nx * Px == N);\n  assert(Ny * Py == N);\n  assert(Nz * Pz == N);\n\n  \n\n  int nx = rank / (Py * Pz);\n  int ny = (rank - nx * Py * Pz) / Pz;\n  int nz = rank % Pz;\n\n  \n\n  int rxm = nz + Pz * (ny + Py * ((nx - 1 + Px) % Px));\n  int rxp = nz + Pz * (ny + Py * ((nx + 1) % Px));\n  int rym = nz + Pz * ((ny - 1 + Py) % Py + Py * nx);\n  int ryp = nz + Pz * ((ny + 1) % Py + Py * nx);\n  int rzm = (nz - 1 + Pz) % Pz + Pz * (ny + Py * nx);\n  int rzp = (nz + 1) % Pz + Pz * (ny + Py * nx);\n  \n  int NNN = Nx * Ny * Nz;\n  \n\n  double* a = (double*)malloc(M * NNN * sizeof(double));\n  double* b = (double*)malloc(M * NNN * sizeof(double));\n  \n  \n\n  int n = 0;\n  for (int m = 0; m < M; m++)\n    for (int x = nx * Nx; x < (nx + 1) * Nx; x++)\n      for (int y = ny * Ny; y < (ny + 1) * Ny; y++)\n\tfor (int z = nz * Nz; z < (nz + 1) * Nz; z++, n++)\n\t  a[n] = x + y * y + z * z * z;\n\n  \n\n  int K = (Nx + 4) * (Ny + 4) * (Nz + 4);\n  double* w = (double*)malloc(K * sizeof(double));\n\n  double h = 0.1;  \n\n\n  double t0 = clock();\n  for (int r = 0; r < R; r++)\n    for (int m = 0; m < M; m++)\n      fd(a + m * NNN, b + m * NNN, w,\n\t h, \n\t Nx, Ny, Nz,\n\t rank, rxm, rxp, rym, ryp, rzm, rzp);\n  double t =  clock() - t0;\n\n  printf(\"Rank: %4d, Time: %f, GFLOPS: %f\\n\", rank, t / CLOCKS_PER_SEC,\n\t 19.0e-9 * R * M * NNN * CLOCKS_PER_SEC / t);\n\n  double s = 0.0;\n  for (int n = 0; n < NNN; n++)\n    s += b[n];\n  double sum;\n  MPI_Reduce(&s, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0)\n    printf(\"Sum:    %f\\n\", sum);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "NLeSC_1000", "code": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Request *request;\n    MPI_Status status;\n\n\n\n    request = malloc(size * sizeof(MPI_Request));\n\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n       for (i=0;i<10;i++) {\n          buffer[i] = 42+i;\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"ISend to %d\\n\", i);\n\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"ISend failed! %d\\n\", error);\n          }\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"Waiting for ISend to %d\\n\", i);\n\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Wait for ISend failed! %d\\n\", error);\n          }\n       }\n    } else {\n       fprintf(stderr, \"IRecv from to %d\\n\", 0);\n\n       error =\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Irecv failed! %d\\n\", error);\n       } else {\n\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Wait failed! %d\\n\", error);\n          } else {\n             fprintf(stderr, \"Received from 0: {\");\n\n             for (i=0;i<10;i++) {\n                fprintf(stderr, \"%d \", buffer[i]);\n             }\n\n             fprintf(stderr, \"}\\n\");\n          }\n       }\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Request *request;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    request = malloc(size * sizeof(MPI_Request));\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n       for (i=0;i<10;i++) {\n          buffer[i] = 42+i;\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"ISend to %d\\n\", i);\n\n          error = MPI_Isend(buffer, 10, MPI_INTEGER, i, 0, MPI_COMM_WORLD, &(request[i]));\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"ISend failed! %d\\n\", error);\n          }\n       }\n\n       for (i=1;i<size;i++) {\n          fprintf(stderr, \"Waiting for ISend to %d\\n\", i);\n\n          error = MPI_Wait(&(request[i]), &status);\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Wait for ISend failed! %d\\n\", error);\n          }\n       }\n    } else {\n       fprintf(stderr, \"IRecv from to %d\\n\", 0);\n\n       error = MPI_Irecv(buffer, 10, MPI_INTEGER, 0, 0, MPI_COMM_WORLD, &(request[0]));\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Irecv failed! %d\\n\", error);\n       } else {\n\n          error = MPI_Wait(&(request[0]), &status);\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Wait failed! %d\\n\", error);\n          } else {\n             fprintf(stderr, \"Received from 0: {\");\n\n             for (i=0;i<10;i++) {\n                fprintf(stderr, \"%d \", buffer[i]);\n             }\n\n             fprintf(stderr, \"}\\n\");\n          }\n       }\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "tenstream_1002", "code": "int main(int argc, char *argv[]) {\n  int        numprocs, myid, fcomm;\n\n  PetscInitialize(&argc,&argv,(char*)0,help);\n  PetscInitializeFortran();\n\n  fcomm =\n  fprintf(stderr,\"I'm number %d of %d.\\n\", myid, numprocs);\n\n  if(myid==0) {\n    master(fcomm);\n  }\n  else {\n    slave(fcomm);\n  }\n\n  PetscFinalize();\n  return(0);\n\n}", "label": "int main(int argc, char *argv[]) {\n  int        numprocs, myid, fcomm;\n\n  MPI_Init(&argc,&argv);\n  PetscInitialize(&argc,&argv,(char*)0,help);\n  PetscInitializeFortran();\n\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n  fcomm = MPI_Comm_c2f(MPI_COMM_WORLD);\n  fprintf(stderr,\"I'm number %d of %d.\\n\", myid, numprocs);\n\n  if(myid==0) {\n    master(fcomm);\n  }\n  else {\n    slave(fcomm);\n  }\n\n  PetscFinalize();\n  MPI_Finalize();\n  return(0);\n\n}"}
{"program": "danielpeter_1003", "code": "int main(int argc, char **argv)\n{\n\n    \n\n\n    \n\n    FTI_Init(\"config.fti\");\n\n    int rank, nbProc, gridSize, steps, offset, i, ex = 6000;\n    float *grid, res = 1;\n    char filename[25];\n    double t1, t2;\n\n    \n\n\n    \n\n    gridSize = atoi(argv[1])/nbProc;\n    steps = atoi(argv[2]);\n    offset = gridSize * rank;\n    grid = malloc(gridSize * sizeof(float));\n    \n\n\n    \n\n    for (i = 0; i < gridSize; i++) {\n        grid[i] = i * 0.1;\n    }\n\n    \n\n    t1 =\n    t2 =\n    for (i = 0; i < steps; i++) {\n\n        \n\n        if (FTI_Fail) {\n            myRestore(gridSize, &i, &res, grid, FTI_File);\n            FTI_Restarted();\n        }\n\n        \n\n        res = compute(grid, gridSize, res+1) / (float)nbProc;\n\n        \n\n        if (rank == 0) {\n            printf(\"Step : %d and res : %f \\n\", i, res);\n        }\n\n        \n\n        if ((i+1)%30 == 0) {\n            myCheckpoint(gridSize, i+1, res, grid, FTI_File);\n            FTI_Checkpointed();\n        }\n    }\n\n    \n\n    if (rank == 0) {\n        printf(\"Final result = %f computed in %f seconds\\n\", res,\n    }\n\n    \n\n    free(grid);\n\n    FTI_Finalize();\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    FTI_Init(\"config.fti\");\n\n    int rank, nbProc, gridSize, steps, offset, i, ex = 6000;\n    float *grid, res = 1;\n    char filename[25];\n    double t1, t2;\n\n    \n\n    MPI_Comm_rank(FTI_COMM_WORLD, &rank);\n    MPI_Comm_size(FTI_COMM_WORLD, &nbProc);\n\n    \n\n    gridSize = atoi(argv[1])/nbProc;\n    steps = atoi(argv[2]);\n    offset = gridSize * rank;\n    grid = malloc(gridSize * sizeof(float));\n    \n\n\n    \n\n    for (i = 0; i < gridSize; i++) {\n        grid[i] = i * 0.1;\n    }\n\n    \n\n    t1 = MPI_Wtime();\n    t2 = MPI_Wtime();\n    for (i = 0; i < steps; i++) {\n\n        \n\n        if (FTI_Fail) {\n            myRestore(gridSize, &i, &res, grid, FTI_File);\n            FTI_Restarted();\n        }\n\n        \n\n        res = compute(grid, gridSize, res+1) / (float)nbProc;\n\n        \n\n        if (rank == 0) {\n            printf(\"Step : %d and res : %f \\n\", i, res);\n        }\n\n        \n\n        if ((i+1)%30 == 0) {\n            myCheckpoint(gridSize, i+1, res, grid, FTI_File);\n            FTI_Checkpointed();\n        }\n    }\n\n    \n\n    if (rank == 0) {\n        printf(\"Final result = %f computed in %f seconds\\n\", res, MPI_Wtime() - t2);\n    }\n\n    \n\n    free(grid);\n\n    FTI_Finalize();\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "syftalent_1004", "code": "int main(int argc, char** argv)\n{\n\n    MPI_File fh;\n    MPI_Datatype file_type, mem_type;\n    int *data = NULL;\n    int *verify = NULL;\n    int data_size = DATA_SIZE;\n    int i, j,k, nr_errors=0;\n    MPI_Aint disp[BLK_COUNT];\n    int block_lens[BLK_COUNT];\n    char* filename = \"unnamed.dat\";\n\n    disp[0] = (MPI_Aint)(PAD);\n    disp[1] = (MPI_Aint)(data_size*1 + PAD);\n    disp[2] = (MPI_Aint)(data_size*2 + PAD);\n\n    block_lens[0] = data_size;\n    block_lens[1] = data_size;\n    block_lens[2] = data_size;\n\n    data = malloc(data_size);\n    verify = malloc(data_size*BLK_COUNT + HEADER + PAD);\n    for(i=0 ; i<data_size/sizeof(int) ; i++)\n        data[i] = i;\n\n\n\n    if( 1 < argc ) filename = argv[1];\n\n    CHECK(MPI_File_open (MPI_COMM_WORLD, filename,\n\t\tMPI_MODE_RDWR | MPI_MODE_CREATE | MPI_MODE_DELETE_ON_CLOSE,\n                      MPI_INFO_NULL, &fh) != 0);\n\n    CHECK(MPI_File_set_view(fh, HEADER, MPI_BYTE,\n                                         file_type, \"native\", MPI_INFO_NULL));\n\n    \n\n    CHECK(MPI_File_write_at_all (fh, 0,\n                                 data, 1, mem_type,\n                                 MPI_STATUS_IGNORE));\n    \n\n    CHECK(MPI_File_set_view(fh, 0, MPI_BYTE, MPI_BYTE,\n\t\t\"native\", MPI_INFO_NULL));\n    CHECK(MPI_File_read_at_all(fh, 0,\n\t\tverify, (HEADER+PAD+BLK_COUNT*DATA_SIZE)/sizeof(int), MPI_INT,\n\t\tMPI_STATUS_IGNORE));\n\n    \n\n    for (i=0; i<(HEADER+PAD)/sizeof(int); i++) {\n\tif (verify[i] != 0) {\n\t    nr_errors++;\n\t    fprintf(stderr, \"expected 0, read %d\\n\", verify[i]);\n\t}\n    }\n    \n\n    for (j=0; j<BLK_COUNT; j++ ) {\n\tfor (k=0; k<(DATA_SIZE/sizeof(int)); k++) {\n\t    if (verify[(HEADER+PAD)/sizeof(int) + k + j*(DATA_SIZE/sizeof(int))] !=\n\t\t    data[k]) {\n\t\tnr_errors++;\n\t\tfprintf(stderr, \"expcted %d, read %d\\n\", data[k],\n\t\t\tverify[(HEADER+PAD)/sizeof(int) + k + j*(DATA_SIZE/sizeof(int))]);\n\t    }\n\t    i++;\n\t}\n    }\n\n\n\n    if (nr_errors == 0) printf(\" No Errors\\n\");\n\n\n    free(data);\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n\n    MPI_File fh;\n    MPI_Datatype file_type, mem_type;\n    int *data = NULL;\n    int *verify = NULL;\n    int data_size = DATA_SIZE;\n    int i, j,k, nr_errors=0;\n    MPI_Aint disp[BLK_COUNT];\n    int block_lens[BLK_COUNT];\n    char* filename = \"unnamed.dat\";\n\n    MPI_Init (&argc, &argv);\n    disp[0] = (MPI_Aint)(PAD);\n    disp[1] = (MPI_Aint)(data_size*1 + PAD);\n    disp[2] = (MPI_Aint)(data_size*2 + PAD);\n\n    block_lens[0] = data_size;\n    block_lens[1] = data_size;\n    block_lens[2] = data_size;\n\n    data = malloc(data_size);\n    verify = malloc(data_size*BLK_COUNT + HEADER + PAD);\n    for(i=0 ; i<data_size/sizeof(int) ; i++)\n        data[i] = i;\n\n    MPI_Type_create_hindexed_block(BLK_COUNT, data_size, disp, MPI_BYTE, &file_type);\n    MPI_Type_commit(&file_type);\n\n    MPI_Type_create_hvector(BLK_COUNT, data_size, 0, MPI_BYTE, &mem_type);\n    MPI_Type_commit(&mem_type);\n\n    if( 1 < argc ) filename = argv[1];\n\n    CHECK(MPI_File_open (MPI_COMM_WORLD, filename,\n\t\tMPI_MODE_RDWR | MPI_MODE_CREATE | MPI_MODE_DELETE_ON_CLOSE,\n                      MPI_INFO_NULL, &fh) != 0);\n\n    CHECK(MPI_File_set_view(fh, HEADER, MPI_BYTE,\n                                         file_type, \"native\", MPI_INFO_NULL));\n\n    \n\n    CHECK(MPI_File_write_at_all (fh, 0,\n                                 data, 1, mem_type,\n                                 MPI_STATUS_IGNORE));\n    \n\n    CHECK(MPI_File_set_view(fh, 0, MPI_BYTE, MPI_BYTE,\n\t\t\"native\", MPI_INFO_NULL));\n    CHECK(MPI_File_read_at_all(fh, 0,\n\t\tverify, (HEADER+PAD+BLK_COUNT*DATA_SIZE)/sizeof(int), MPI_INT,\n\t\tMPI_STATUS_IGNORE));\n\n    \n\n    for (i=0; i<(HEADER+PAD)/sizeof(int); i++) {\n\tif (verify[i] != 0) {\n\t    nr_errors++;\n\t    fprintf(stderr, \"expected 0, read %d\\n\", verify[i]);\n\t}\n    }\n    \n\n    for (j=0; j<BLK_COUNT; j++ ) {\n\tfor (k=0; k<(DATA_SIZE/sizeof(int)); k++) {\n\t    if (verify[(HEADER+PAD)/sizeof(int) + k + j*(DATA_SIZE/sizeof(int))] !=\n\t\t    data[k]) {\n\t\tnr_errors++;\n\t\tfprintf(stderr, \"expcted %d, read %d\\n\", data[k],\n\t\t\tverify[(HEADER+PAD)/sizeof(int) + k + j*(DATA_SIZE/sizeof(int))]);\n\t    }\n\t    i++;\n\t}\n    }\n\n    MPI_File_close(&fh);\n\n    MPI_Type_free (&mem_type);\n    MPI_Type_free(&file_type);\n\n    if (nr_errors == 0) printf(\" No Errors\\n\");\n\n    MPI_Finalize ();\n\n    free(data);\n    return 0;\n}"}
{"program": "tcsiwula_1005", "code": "int main(int argc, char* argv[]) {\n    int             my_rank;\n    int             p;\n    float*          local_A;\n    float*          global_x;\n    float*          local_x;\n    float*          local_y;\n    int             m, n;\n    int             local_m, local_n;\n    MPI_Comm        comm;\n\n    comm = MPI_COMM_WORLD;\n\n    if (my_rank == 0) {\n        printf(\"Enter the order of the matrix (m x n)\\n\");\n        scanf(\"%d %d\", &m, &n);\n    }\n\n    local_m = m/p;\n    local_n = n/p;\n\n    local_A = malloc(local_m*n*sizeof(float));\n    Gen_array(local_A, local_m*n, my_rank);\n\n    local_x = malloc(local_n*sizeof(float));\n    Gen_array(local_x, local_n, 10*my_rank);\n    local_y = malloc(local_m*sizeof(float));\n    global_x = malloc(n*sizeof(float));\n\n    if(my_rank == 0){\n   start =\n }\n    Parallel_matrix_vector_prod(local_A, m, n, local_x, global_x,\n        local_y, local_m, local_n, comm);\n        if(my_rank == 0){\n   finish =\n   elapsed = finish - start;\n   printf(\"Elapsed time = %.14e seconds\\n\", elapsed);\n   printf(\"Resolution of MPI_Wtime = %.14e seconds\\n\",\n}\n    Print_vector(\"The product is\", local_y, local_m, my_rank, p, comm);\n\n    free(local_A);\n    free(local_x);\n    free(local_y);\n    free(global_x);\n\n\n    return 0;\n}", "label": "int main(int argc, char* argv[]) {\n    int             my_rank;\n    int             p;\n    float*          local_A;\n    float*          global_x;\n    float*          local_x;\n    float*          local_y;\n    int             m, n;\n    int             local_m, local_n;\n    MPI_Comm        comm;\n\n    MPI_Init(&argc, &argv);\n    comm = MPI_COMM_WORLD;\n    MPI_Comm_size(comm, &p);\n    MPI_Comm_rank(comm, &my_rank);\n\n    if (my_rank == 0) {\n        printf(\"Enter the order of the matrix (m x n)\\n\");\n        scanf(\"%d %d\", &m, &n);\n    }\n    MPI_Bcast(&m, 1, MPI_INT, 0, comm);\n    MPI_Bcast(&n, 1, MPI_INT, 0, comm);\n\n    local_m = m/p;\n    local_n = n/p;\n\n    local_A = malloc(local_m*n*sizeof(float));\n    Gen_array(local_A, local_m*n, my_rank);\n\n    local_x = malloc(local_n*sizeof(float));\n    Gen_array(local_x, local_n, 10*my_rank);\n    local_y = malloc(local_m*sizeof(float));\n    global_x = malloc(n*sizeof(float));\n\n    if(my_rank == 0){\n   start = MPI_Wtime();\n }\n    Parallel_matrix_vector_prod(local_A, m, n, local_x, global_x,\n        local_y, local_m, local_n, comm);\n        if(my_rank == 0){\n   finish = MPI_Wtime();\n   elapsed = finish - start;\n   printf(\"Elapsed time = %.14e seconds\\n\", elapsed);\n   printf(\"Resolution of MPI_Wtime = %.14e seconds\\n\", MPI_Wtick());\n}\n    Print_vector(\"The product is\", local_y, local_m, my_rank, p, comm);\n\n    free(local_A);\n    free(local_x);\n    free(local_y);\n    free(global_x);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "gyaikhom_1006", "code": "int main(int argc, char *argv[])\n{\n    bc_init(BC_ERR); \n\n    \n\n    cmplx_t = bc_dtype_create(sizeof(struct complex_s));\n    \n    switch (bc_rank) {\n    case 0:\n        master();\n        break;\n    case 1:\n    case 2:\n    case 3:\n    case 4:\n    case 5:\n    case 6:\n    case 7:\n        slave();\n        break;\n    default:\n        printf (\"[%d] I don't have to do anything\\n\", bc_rank);\n    }\n    \n    \n\n    bc_dtype_destroy(cmplx_t);\n    \n    bc_final();\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Init(&argc, &argv);\n    bc_init(BC_ERR); \n\n    \n\n    cmplx_t = bc_dtype_create(sizeof(struct complex_s));\n    \n    switch (bc_rank) {\n    case 0:\n        master();\n        break;\n    case 1:\n    case 2:\n    case 3:\n    case 4:\n    case 5:\n    case 6:\n    case 7:\n        slave();\n        break;\n    default:\n        printf (\"[%d] I don't have to do anything\\n\", bc_rank);\n    }\n    \n    \n\n    bc_dtype_destroy(cmplx_t);\n    \n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "eliask_1007", "code": "main(int argc, char *argv[])\n{\n  double rn;\n  int i, myid, len, nprocs;\n  MPI_Status  status;\n  char *packed;\n\n\n  \n\n            \n\n\n\n\n  if(nprocs < 2)\n  {\n    fprintf(stderr,\"ERROR: At least 2 processes required\\n\");\n    exit(1);\n  }\n  \n  if (myid==0)\t\n\n  {\n    init_sprng(SEED,SPRNG_DEFAULT);\t\n\n    printf(\"Process %d: Print information about stream:\\n\",myid);\n    print_sprng();\n\n    printf(\"Process %d: Print 2 random numbers in [0,1):\\n\", myid);\n    for (i=0;i<2;i++)\n    {\n      rn = sprng();\t\t\n\n      printf(\"Process %d: %f\\n\", myid, rn);\n    }\n\n    len = pack_sprng(&packed);\t\n\n    \n\n\n    free(packed);\t\t\n\n    printf(\" Process 0 sends stream to process 1\\n\");\n  }\n  else if(myid == 1)  \n\n  {\n    init_sprng(SEED,SPRNG_DEFAULT);   \n\n\n    \n    if ((packed = (char *) malloc(len)) == NULL) \n\n    {\n      fprintf(stderr,\"ERROR: process %d: Cannot allocate memory\\n\", myid);\n      exit(1);\n    }\n\n\n    unpack_sprng(packed);\t\n\n    printf(\" Process 1 has received the packed stream\\n\");\n    printf(\"Process %d: Print information about stream:\\n\",myid);\n    print_sprng();\n    free(packed);\t\t\n\n\n    printf(\" Process 1 prints 2 numbers from received stream:\\n\");\n   for (i=0;i<2;i++)\t\t\n    {\n      rn = sprng();\t\t\n\n      printf(\"Process %d: %f\\n\", myid, rn);\n    }\n\n  }\n\n\n}", "label": "main(int argc, char *argv[])\n{\n  double rn;\n  int i, myid, len, nprocs;\n  MPI_Status  status;\n  char *packed;\n\n\n  \n\n            \n  MPI_Init(&argc, &argv);\t\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\t\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs); \n\n\n  if(nprocs < 2)\n  {\n    fprintf(stderr,\"ERROR: At least 2 processes required\\n\");\n    MPI_Finalize();\n    exit(1);\n  }\n  \n  if (myid==0)\t\n\n  {\n    init_sprng(SEED,SPRNG_DEFAULT);\t\n\n    printf(\"Process %d: Print information about stream:\\n\",myid);\n    print_sprng();\n\n    printf(\"Process %d: Print 2 random numbers in [0,1):\\n\", myid);\n    for (i=0;i<2;i++)\n    {\n      rn = sprng();\t\t\n\n      printf(\"Process %d: %f\\n\", myid, rn);\n    }\n\n    len = pack_sprng(&packed);\t\n\n    \n\n    MPI_Send(&len, 1, MPI_INT, 1, 0, MPI_COMM_WORLD); \n    MPI_Send(packed, len, MPI_BYTE, 1, 0, MPI_COMM_WORLD); \n\n    free(packed);\t\t\n\n    printf(\" Process 0 sends stream to process 1\\n\");\n  }\n  else if(myid == 1)  \n\n  {\n    init_sprng(SEED,SPRNG_DEFAULT);   \n\n    MPI_Recv(&len, 1, MPI_INT, 0, MPI_ANY_TAG,\n             MPI_COMM_WORLD, &status); \n\n    \n    if ((packed = (char *) malloc(len)) == NULL) \n\n    {\n      fprintf(stderr,\"ERROR: process %d: Cannot allocate memory\\n\", myid);\n      MPI_Finalize();\n      exit(1);\n    }\n\n    MPI_Recv(packed, len, MPI_BYTE, 0, MPI_ANY_TAG,\n             MPI_COMM_WORLD, &status); \n\n    unpack_sprng(packed);\t\n\n    printf(\" Process 1 has received the packed stream\\n\");\n    printf(\"Process %d: Print information about stream:\\n\",myid);\n    print_sprng();\n    free(packed);\t\t\n\n\n    printf(\" Process 1 prints 2 numbers from received stream:\\n\");\n   for (i=0;i<2;i++)\t\t\n    {\n      rn = sprng();\t\t\n\n      printf(\"Process %d: %f\\n\", myid, rn);\n    }\n\n  }\n\n  MPI_Finalize();\t\t\n\n}"}
{"program": "predicador37_1009", "code": "int main(int argc, char **argv) {\n\n\n    int rank, size, namelen, version, subversion, psize, parent_rank;\n\n    MPI_Comm parent;\n\n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n\n\n\n\n\n\n\n\n\n\n    printf(\"I\u2019m\tworker %d of %d on %s running MPI %d.%d\\n\", rank, size, processor_name, version, subversion);\n\n\n\n\n    if (parent == MPI_COMM_NULL) {\n        printf(\"Error: no parent process found!\\n\");\n        exit(1);\n    }\n\n\n\n\n    if (psize != 1) {\n\n        printf(\"Error: number of parents (%d) should be 1.\\n\", psize);\n        exit(2);\n    }\n\n\n\n\n    int sendrank = rank;\n\n\n    printf(\"Worker %d: Success!\\n\", rank);\n\n\n\n    printf(\"Rank received from parent is %d\\n\", parent_rank);\n\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n\n\n    int rank, size, namelen, version, subversion, psize, parent_rank;\n\n    MPI_Comm parent;\n\n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n    MPI_Init(&argc, &argv);\n\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n\n    MPI_Get_version(&version, &subversion);\n\n\n    printf(\"I\u2019m\tworker %d of %d on %s running MPI %d.%d\\n\", rank, size, processor_name, version, subversion);\n\n\n    MPI_Comm_get_parent(&parent);\n\n\n    if (parent == MPI_COMM_NULL) {\n        printf(\"Error: no parent process found!\\n\");\n        exit(1);\n    }\n\n\n    MPI_Comm_remote_size(parent, &psize);\n\n\n    if (psize != 1) {\n\n        printf(\"Error: number of parents (%d) should be 1.\\n\", psize);\n        exit(2);\n    }\n\n\n\n\n    int sendrank = rank;\n\n\n    printf(\"Worker %d: Success!\\n\", rank);\n\n\n    MPI_Send(&rank, 1, MPI_INT, 0, 0, parent);\n    MPI_Bcast(&parent_rank, 1, MPI_INT, 0, parent);\n\n    printf(\"Rank received from parent is %d\\n\", parent_rank);\n\n    MPI_Comm_disconnect(&parent);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_1010", "code": "int main( int argc, char* argv[] )\n{\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\n\tdouble*\t\tone4d;\n\n\tIndex i, j, k, l;\n\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\t}\n\n\tJournal_Printf( stream, \"4D as 1D\\n\" );\n\tone4d = Memory_Alloc_4DArrayAs1D_Unnamed( double, 4, 3, 2, 3 );\n\n\t\n\t\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 3; ++j )\n\t\t{\n\t\t\tfor ( k = 0; k < 2; ++k )\n\t\t\t{\n\t\t\t\tfor ( l = 0; l < 3; ++l )\n\t\t\t\t{\n\t\t\t\t\tMemory_Access4D( one4d, i, j, k, l, 3, 2, 3 ) = i + (j / 10.0) + (k / 100.0) + (l / 1000.0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t\n\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 3; ++j )\n\t\t{\n\t\t\tfor ( k = 0; k < 2; ++k )\n\t\t\t{\n\t\t\t\tfor ( l = 0; l < 3; ++l )\n\t\t\t\t{\n\t\t\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access4D( one4d, i, j, k, l, 3, 2, 3 ) );\n\t\t\t\t}\n\t\t\t\tJournal_Printf( stream, \"\\n\");\n\t\t\t}\n\t\t}\n\t}\n\n\tMemory_Free( one4d );\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] )\n{\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\tStream* stream;\n\n\tdouble*\t\tone4d;\n\n\tIndex i, j, k, l;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\t\n\tstream = Journal_Register ( \"info\", \"MyInfo\" );\n\t\n\tif( argc >= 2 )\n\t{\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse\n\t{\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch )\n\t{\n\t\tJournal_Printf( stream,  \"Watching rank: %i\\n\", rank );\n\t}\n\n\tJournal_Printf( stream, \"4D as 1D\\n\" );\n\tone4d = Memory_Alloc_4DArrayAs1D_Unnamed( double, 4, 3, 2, 3 );\n\n\t\n\t\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 3; ++j )\n\t\t{\n\t\t\tfor ( k = 0; k < 2; ++k )\n\t\t\t{\n\t\t\t\tfor ( l = 0; l < 3; ++l )\n\t\t\t\t{\n\t\t\t\t\tMemory_Access4D( one4d, i, j, k, l, 3, 2, 3 ) = i + (j / 10.0) + (k / 100.0) + (l / 1000.0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t\n\n\tfor ( i = 0; i < 4; ++i )\n\t{\n\t\tfor ( j = 0; j < 3; ++j )\n\t\t{\n\t\t\tfor ( k = 0; k < 2; ++k )\n\t\t\t{\n\t\t\t\tfor ( l = 0; l < 3; ++l )\n\t\t\t\t{\n\t\t\t\t\tJournal_Printf( stream, \"%lf \", Memory_Access4D( one4d, i, j, k, l, 3, 2, 3 ) );\n\t\t\t\t}\n\t\t\t\tJournal_Printf( stream, \"\\n\");\n\t\t\t}\n\t\t}\n\t}\n\n\tMemory_Free( one4d );\n\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "pf-aics-riken_1012", "code": "int\nmain(int argc, char **argv)\n{\n    int nprocs, rank;\n    _Bool noiolb = 0;\n    char *dirname = NULL;\n\n\n    if (!(argc == 2 || argc == 3)) {\n\tshow_help(rank);\n\treturn 1;\n    } else if (argc == 2) {\n\tdirname = argv[1];\n\tif (!check_directory(dirname)) {\n\t    show_help(rank);\n\t    return 1;\n\t}\n    } else if (argc == 3) {\n\tint ret = strcmp(argv[1], \"-s\");\n\tif (ret == 0) {\n\t    noiolb = 1;\n\t}\n\tdirname = argv[2];\n\tif (!check_directory(dirname)) {\n\t    show_help(rank);\n\t    return 1;\n\t}\n    }\n\n    kmr_init();\n    KMR *mr = kmr_create_context(MPI_COMM_WORLD, MPI_INFO_NULL, 0);\n    mr->trace_iolb = 1;\n    mr->verbosity = 5;\n\n    KMR_KVS *kvs_infiles = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    kmr_map_once(kvs_infiles, (void *)dirname, kmr_noopt, 1, read_files);\n\n    KMR_KVS *kvs_targets = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    if (noiolb) {\n\t\n\n\tkmr_shuffle(kvs_infiles, kvs_targets, kmr_noopt);\n    } else {\n\t\n\n\tkmr_assign_file(kvs_infiles, kvs_targets, kmr_noopt);\n    }\n\n    KMR_KVS *kvs_times = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_FLOAT8);\n    kmr_map(kvs_targets, kvs_times, NULL, kmr_noopt, benchmark);\n\n    KMR_KVS *kvs_all_times = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_FLOAT8);\n    kmr_shuffle(kvs_times, kvs_all_times, kmr_noopt);\n\n    kmr_reduce(kvs_all_times, NULL, NULL, kmr_noopt, summarize);\n\n    kmr_free_context(mr);\n    kmr_fin();\n    return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n    int nprocs, rank;\n    _Bool noiolb = 0;\n    char *dirname = NULL;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (!(argc == 2 || argc == 3)) {\n\tshow_help(rank);\n\tMPI_Finalize();\n\treturn 1;\n    } else if (argc == 2) {\n\tdirname = argv[1];\n\tif (!check_directory(dirname)) {\n\t    show_help(rank);\n\t    MPI_Finalize();\n\t    return 1;\n\t}\n    } else if (argc == 3) {\n\tint ret = strcmp(argv[1], \"-s\");\n\tif (ret == 0) {\n\t    noiolb = 1;\n\t}\n\tdirname = argv[2];\n\tif (!check_directory(dirname)) {\n\t    show_help(rank);\n\t    MPI_Finalize();\n\t    return 1;\n\t}\n    }\n\n    kmr_init();\n    KMR *mr = kmr_create_context(MPI_COMM_WORLD, MPI_INFO_NULL, 0);\n    mr->trace_iolb = 1;\n    mr->verbosity = 5;\n\n    KMR_KVS *kvs_infiles = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    kmr_map_once(kvs_infiles, (void *)dirname, kmr_noopt, 1, read_files);\n\n    KMR_KVS *kvs_targets = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    if (noiolb) {\n\t\n\n\tkmr_shuffle(kvs_infiles, kvs_targets, kmr_noopt);\n    } else {\n\t\n\n\tkmr_assign_file(kvs_infiles, kvs_targets, kmr_noopt);\n    }\n\n    KMR_KVS *kvs_times = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_FLOAT8);\n    kmr_map(kvs_targets, kvs_times, NULL, kmr_noopt, benchmark);\n\n    KMR_KVS *kvs_all_times = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_FLOAT8);\n    kmr_shuffle(kvs_times, kvs_all_times, kmr_noopt);\n\n    kmr_reduce(kvs_all_times, NULL, NULL, kmr_noopt, summarize);\n\n    kmr_free_context(mr);\n    kmr_fin();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "lipari_1013", "code": "int\nmain(int argc, char *argv[])\n{\n        int id, ntasks;\n\tstruct timespec t;\n\n\tclock_gettime (CLOCK_MONOTONIC, &t);\n        if (id == 0) {\n                printf(\"0: completed MPI_Init in %0.3fs.  There are %d tasks\\n\",\n\t\t   time_since (t)/1000, ntasks);\n                fflush(stdout);\n        }\n\n\tif (id == 0)\n\t\tclock_gettime (CLOCK_MONOTONIC, &t);\n        if (id == 0) {\n                printf(\"0: completed first barrier in %0.3fs\\n\",\n\t\t\t\ttime_since (t) / 1000);\n                fflush(stdout);\n        }\n\n\tif (id == 0)\n\t\tclock_gettime (CLOCK_MONOTONIC, &t);\n        if (id == 0) {\n                printf(\"0: completed MPI_Finalize in %0.3fs\\n\",\n\t\t\t\ttime_since (t) / 1000);\n                fflush(stdout);\n        }\n        return 0;\n}", "label": "int\nmain(int argc, char *argv[])\n{\n        int id, ntasks;\n\tstruct timespec t;\n\n\tclock_gettime (CLOCK_MONOTONIC, &t);\n        MPI_Init(&argc, &argv);\n        MPI_Comm_rank(MPI_COMM_WORLD, &id);\n        MPI_Comm_size(MPI_COMM_WORLD, &ntasks);\n        if (id == 0) {\n                printf(\"0: completed MPI_Init in %0.3fs.  There are %d tasks\\n\",\n\t\t   time_since (t)/1000, ntasks);\n                fflush(stdout);\n        }\n\n\tif (id == 0)\n\t\tclock_gettime (CLOCK_MONOTONIC, &t);\n        MPI_Barrier(MPI_COMM_WORLD);\n        if (id == 0) {\n                printf(\"0: completed first barrier in %0.3fs\\n\",\n\t\t\t\ttime_since (t) / 1000);\n                fflush(stdout);\n        }\n\n\tif (id == 0)\n\t\tclock_gettime (CLOCK_MONOTONIC, &t);\n        MPI_Finalize();\n        if (id == 0) {\n                printf(\"0: completed MPI_Finalize in %0.3fs\\n\",\n\t\t\t\ttime_since (t) / 1000);\n                fflush(stdout);\n        }\n        return 0;\n}"}
{"program": "thiagovsk_1014", "code": "t main(int argc, char **argv) {\n\n\n  int process_rank;\n  int process_size;\n  int target_process_rank = 1;\n  int process_tag = 1;\n  MPI_Request request;\n\n  \n\n\n  printf(\"\\n ReadingCards::Init Iniciando processo de leitura de cartoes \\n\");\n\n  \n\n  printf(\"\\n ReadingCards::Rank :%d \\n\",process_rank);\n\n  \n\n\n  \n\n  char card_content[80];\n\n  \n\n  char message_content[80];\n\n  \n\n  FILE *file_card;\n\n  \n\n  file_card = fopen(\"card_1.txt\",\"r\");\n\n  \n\n\n  \n\n  while( fscanf(file_card,\"%s\",card_content) != EOF ){\n    \n\n    sprintf(message_content, \"%s\", card_content);\n\n    \n\n  }\n\n  \n\n  fclose(file_card);\n\n  \n\n\n  printf(\"\\n ReadingCards::Bsend Enviando Mensagem ao Processo de Rank = %d... \\n\",target_process_rank);\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  printf(\"\\n ReadingCards::Bsend Mensagem enviada: \\n\\n %s\\n\\n\",message_content);\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  printf(\"\\n ReadingCards::Barrier Barreira Ativa, Sincronizando Processos... \\n\");\n\n  \n\n}\n", "label": "t main(int argc, char **argv) {\n\n\n  int process_rank;\n  int process_size;\n  int target_process_rank = 1;\n  int process_tag = 1;\n  MPI_Request request;\n\n  \n\n  MPI_Init(&argc, &argv); \n\n  printf(\"\\n ReadingCards::Init Iniciando processo de leitura de cartoes \\n\");\n\n  \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &process_rank);\n  printf(\"\\n ReadingCards::Rank :%d \\n\",process_rank);\n\n  \n\n  MPI_Comm_size(MPI_COMM_WORLD, &process_size);\n\n  \n\n  char card_content[80];\n\n  \n\n  char message_content[80];\n\n  \n\n  FILE *file_card;\n\n  \n\n  file_card = fopen(\"card_1.txt\",\"r\");\n\n  \n\n\n  \n\n  while( fscanf(file_card,\"%s\",card_content) != EOF ){\n    \n\n    sprintf(message_content, \"%s\", card_content);\n\n    \n\n  }\n\n  \n\n  fclose(file_card);\n\n  \n\n\n  printf(\"\\n ReadingCards::Bsend Enviando Mensagem ao Processo de Rank = %d... \\n\",target_process_rank);\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  MPI_Bsend(message_content, strlen(message_content), MPI_BYTE,\n             target_process_rank,process_tag, MPI_COMM_WORLD);\n\n  printf(\"\\n ReadingCards::Bsend Mensagem enviada: \\n\\n %s\\n\\n\",message_content);\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  printf(\"\\n ReadingCards::Barrier Barreira Ativa, Sincronizando Processos... \\n\");\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  MPI_Finalize();\n}\n"}
{"program": "kishan-bhugul_1017", "code": "int main (int argc, char *argv[]){\n\t\n\tint rank;\n\tint np;\n\tint tag = 0;\n\t\n\t\n\tint sizeOfArray = 16;\n\tint arrayOfNum[16]={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16};\n\tint blockSize = sizeOfArray/np;\n\t\n\t\n\tif (rank == 0){\n\t\t\n\t\tint i;\n\t\tfor(i=0;i<sizeOfArray;i++){\n\t\t\tarrayOfNum[i] = i+1;\n\t\t}\n\t\t\n\n\t\t\n\t\tint sum=0;\n\t\t\n\t\t\n\n\t\tfor(i=0;i<blockSize;i++){\n\t\t\tsum = sum + arrayOfNum[i];\n\t\t}\n\t\tprintf(\"Value: %d from Process %d\\n\", sum, rank);\n\t\tfor(i=1;i<np;i++){\n\t\t\tint dest = i;\n\t\t}\n\t\t\n\t\tint receivedNum;\n\t\tMPI_Status status;\n\t\tfor(i=1;i<np;i++){\n\t\t\tint src = i;\n\t\t\tsum = sum + receivedNum;\n\t\t}\n\t\t\n\t\tprintf (\"The Sum is: %d\\n\", sum);\n\t\t\n\t\t\n\t}else{\n\t\t\n\t\tint received[blockSize];\n\t\t\n\t\tint src = 0;\n\t\tMPI_Status status;\n\t\t\n\t\tint i;\n\t\tint toSend = 0;\n\t\t\t\t\n\t\tfor(i=0;i<blockSize;i++){\n\t\t\ttoSend = toSend + received[i];\n\t\t}\n\t\tprintf(\"Value: %d from Process %d\\n\", toSend, rank);\n\t\tint dest = 0;\n\t}\n\t\n\treturn 0;\n}", "label": "int main (int argc, char *argv[]){\n\t\n\tint rank;\n\tint np;\n\tint tag = 0;\n\t\n\tMPI_Init (&argc, &argv);\n\tMPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size (MPI_COMM_WORLD, &np);\n\t\n\tint sizeOfArray = 16;\n\tint arrayOfNum[16]={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16};\n\tint blockSize = sizeOfArray/np;\n\t\n\t\n\tif (rank == 0){\n\t\t\n\t\tint i;\n\t\tfor(i=0;i<sizeOfArray;i++){\n\t\t\tarrayOfNum[i] = i+1;\n\t\t}\n\t\t\n\n\t\t\n\t\tint sum=0;\n\t\t\n\t\t\n\n\t\tfor(i=0;i<blockSize;i++){\n\t\t\tsum = sum + arrayOfNum[i];\n\t\t}\n\t\tprintf(\"Value: %d from Process %d\\n\", sum, rank);\n\t\tfor(i=1;i<np;i++){\n\t\t\tint dest = i;\n\t\t\tMPI_Send (&arrayOfNum[i*blockSize], blockSize, MPI_INT, dest, tag, MPI_COMM_WORLD);\t\t\n\t\t}\n\t\t\n\t\tint receivedNum;\n\t\tMPI_Status status;\n\t\tfor(i=1;i<np;i++){\n\t\t\tint src = i;\n\t\t\tMPI_Recv(&receivedNum, 1, MPI_INT, src, tag, MPI_COMM_WORLD, &status);\n\t\t\tsum = sum + receivedNum;\n\t\t}\n\t\t\n\t\tprintf (\"The Sum is: %d\\n\", sum);\n\t\t\n\t\t\n\t}else{\n\t\t\n\t\tint received[blockSize];\n\t\t\n\t\tint src = 0;\n\t\tMPI_Status status;\n\t\tMPI_Recv(&received, blockSize, MPI_INT, src, tag, MPI_COMM_WORLD, &status);\n\t\t\n\t\tint i;\n\t\tint toSend = 0;\n\t\t\t\t\n\t\tfor(i=0;i<blockSize;i++){\n\t\t\ttoSend = toSend + received[i];\n\t\t}\n\t\tprintf(\"Value: %d from Process %d\\n\", toSend, rank);\n\t\tint dest = 0;\n\t\tMPI_Send (&toSend, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n\t}\n\t\n\tMPI_Finalize ();\n\treturn 0;\n}"}
{"program": "JasonRuonanWang_1018", "code": "int main(int argc, char *argv[])\n{\n    int rank, size;\n\n#if ADIOS2_USE_MPI\n#else\n    rank = 0;\n    size = 1;\n#endif\n\n    adios2_error errio;\n    \n\n    const size_t Nx = 10;\n    float *myFloats;\n    myFloats = malloc(sizeof(float) * Nx);\n\n    unsigned int i;\n    for (i = 0; i < Nx; ++i)\n    {\n        myFloats[i] = (float)i;\n    }\n\n#if ADIOS2_USE_MPI\n    adios2_adios *adios = adios2_init(MPI_COMM_WORLD, adios2_debug_mode_on);\n#else\n    adios2_adios *adios = adios2_init(adios2_debug_mode_on);\n#endif\n\n    check_handler(adios, \"adios\");\n\n    adios2_io *io = adios2_declare_io(adios, \"BPFile_Write\");\n    check_handler(io, \"io\");\n\n    \n\n    size_t shape[1];\n    shape[0] = (size_t)size * Nx;\n\n    size_t start[1];\n    start[0] = (size_t)rank * Nx;\n\n    size_t count[1];\n    count[0] = Nx;\n\n    adios2_variable *variable =\n        adios2_define_variable(io, \"bpFloats\", adios2_type_float, 1, shape,\n                               start, count, adios2_constant_dims_true);\n    check_handler(variable, \"variable\");\n\n    adios2_engine *engine = adios2_open(io, \"myVector_c.bp\", adios2_mode_write);\n    check_handler(engine, \"engine\");\n\n    errio = adios2_put(engine, variable, myFloats, adios2_mode_deferred);\n    check_error(errio);\n\n    errio = adios2_close(engine);\n    check_error(errio);\n\n    \n\n    errio = adios2_finalize(adios);\n    check_error(errio);\n\n    free(myFloats);\n\n#if ADIOS2_USE_MPI\n#endif\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, size;\n\n#if ADIOS2_USE_MPI\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n#else\n    rank = 0;\n    size = 1;\n#endif\n\n    adios2_error errio;\n    \n\n    const size_t Nx = 10;\n    float *myFloats;\n    myFloats = malloc(sizeof(float) * Nx);\n\n    unsigned int i;\n    for (i = 0; i < Nx; ++i)\n    {\n        myFloats[i] = (float)i;\n    }\n\n#if ADIOS2_USE_MPI\n    adios2_adios *adios = adios2_init(MPI_COMM_WORLD, adios2_debug_mode_on);\n#else\n    adios2_adios *adios = adios2_init(adios2_debug_mode_on);\n#endif\n\n    check_handler(adios, \"adios\");\n\n    adios2_io *io = adios2_declare_io(adios, \"BPFile_Write\");\n    check_handler(io, \"io\");\n\n    \n\n    size_t shape[1];\n    shape[0] = (size_t)size * Nx;\n\n    size_t start[1];\n    start[0] = (size_t)rank * Nx;\n\n    size_t count[1];\n    count[0] = Nx;\n\n    adios2_variable *variable =\n        adios2_define_variable(io, \"bpFloats\", adios2_type_float, 1, shape,\n                               start, count, adios2_constant_dims_true);\n    check_handler(variable, \"variable\");\n\n    adios2_engine *engine = adios2_open(io, \"myVector_c.bp\", adios2_mode_write);\n    check_handler(engine, \"engine\");\n\n    errio = adios2_put(engine, variable, myFloats, adios2_mode_deferred);\n    check_error(errio);\n\n    errio = adios2_close(engine);\n    check_error(errio);\n\n    \n\n    errio = adios2_finalize(adios);\n    check_error(errio);\n\n    free(myFloats);\n\n#if ADIOS2_USE_MPI\n    MPI_Finalize();\n#endif\n\n    return 0;\n}"}
{"program": "callmetaste_1019", "code": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,nbsols,mysolnum;\n   MPI_Status status; \n\n   adainit();\n\n   dimension_broadcast(myid,&n);\n   monomials_broadcast(myid,n);\n   copy_broadcast(myid); \n   start_system_broadcast(myid,n,&nbsols);\n\n   solutions_distribute(myid,nbsols,n,numprocs,&mysolnum);\n\n  \n\n\n   adafinal();\n\n   return 0;\n}", "label": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,nbsols,mysolnum;\n   MPI_Status status; \n\n   adainit();\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n   dimension_broadcast(myid,&n);\n   monomials_broadcast(myid,n);\n   copy_broadcast(myid); \n   start_system_broadcast(myid,n,&nbsols);\n\n   MPI_Bcast(&nbsols,1,MPI_INT,0,MPI_COMM_WORLD);\n   solutions_distribute(myid,nbsols,n,numprocs,&mysolnum);\n\n  \n\n\n   MPI_Finalize();\n   adafinal();\n\n   return 0;\n}"}
{"program": "Wrent_1021", "code": "int main(int argc, char **argv)\n{\n\n  start_timer();\n  stop_timer();\n  Setup_Proc_Grid();\n\n  Setup_Grid();\n\n  Solve();\n\n  Write_Grid();\n\n  Clean_Up();\n\n  print_timer();\n\n  Debug(\"MPI_Finalize\", 0);\n  printf(\"(%i) %i\\n\", proc_rank, sum_count);\n  printf(\"(%i) %i\\n\", proc_rank, data_sent);\n\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n\n  start_timer();\n  stop_timer();\n  Setup_Proc_Grid();\n\n  Setup_Grid();\n\n  Solve();\n\n  Write_Grid();\n\n  Clean_Up();\n\n  print_timer();\n\n  Debug(\"MPI_Finalize\", 0);\n  printf(\"(%i) %i\\n\", proc_rank, sum_count);\n  printf(\"(%i) %i\\n\", proc_rank, data_sent);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "qingu_1024", "code": "int main(int argc, char *argv[])\n{\n    int rank, size, verbose=0;\n    int wrank;\n    MPI_Comm comm;\n\n\n    if (getenv(\"MPITEST_VERBOSE\"))\n        verbose = 1;\n\n\n    \n\n    if (comm == MPI_COMM_NULL)\n        printf(\"Expected a non-null communicator, but got MPI_COMM_NULL\\n\");\n    else {\n        if (rank == 0 && verbose)\n            printf(\"Created subcommunicator of size %d\\n\", size);\n    }\n\n    \n\n    if ((wrank % 2) && (comm != MPI_COMM_NULL))\n        printf(\"Expected MPI_COMM_NULL, but did not get one\\n\");\n    if (wrank % 2 == 0) {\n        if (comm == MPI_COMM_NULL)\n            printf(\"Expected a non-null communicator, but got MPI_COMM_NULL\\n\");\n        else {\n            if (rank == 0 && verbose)\n                printf(\"Created subcommunicator of size %d\\n\", size);\n        }\n    }\n\n    \n\n    if (wrank == 0)\n        printf(\" No errors\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, size, verbose=0;\n    int wrank;\n    MPI_Comm comm;\n\n    MPI_Init(&argc, &argv);\n\n    if (getenv(\"MPITEST_VERBOSE\"))\n        verbose = 1;\n\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n\n    \n\n    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm);\n    if (comm == MPI_COMM_NULL)\n        printf(\"Expected a non-null communicator, but got MPI_COMM_NULL\\n\");\n    else {\n        MPI_Comm_rank(comm, &rank);\n        MPI_Comm_size(comm, &size);\n        if (rank == 0 && verbose)\n            printf(\"Created subcommunicator of size %d\\n\", size);\n        MPI_Comm_free(&comm);\n    }\n\n    \n\n    MPI_Comm_split_type(MPI_COMM_WORLD, (wrank % 2 == 0) ? MPI_COMM_TYPE_SHARED : MPI_UNDEFINED,\n                        0, MPI_INFO_NULL, &comm);\n    if ((wrank % 2) && (comm != MPI_COMM_NULL))\n        printf(\"Expected MPI_COMM_NULL, but did not get one\\n\");\n    if (wrank % 2 == 0) {\n        if (comm == MPI_COMM_NULL)\n            printf(\"Expected a non-null communicator, but got MPI_COMM_NULL\\n\");\n        else {\n            MPI_Comm_rank(comm, &rank);\n            MPI_Comm_size(comm, &size);\n            if (rank == 0 && verbose)\n                printf(\"Created subcommunicator of size %d\\n\", size);\n            MPI_Comm_free(&comm);\n        }\n    }\n\n    \n\n    if (wrank == 0)\n        printf(\" No errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mnv104_1026", "code": "int main(int argc, char *argv[])\n{\n\tchar *path = NULL;\n\n\t\n\n\tif (node_specific_alloc(argc, argv) < 0) {\n\t\tnode_specific_dealloc();\n\t\treturn -1;\n\t}\n\t\n\n\tif ((path = check_validity()) == NULL) {\n\t\tif (RANK == 0) {\n\t\t\tpanic(\"Skipping fsck because of errors in parameters\\n\");\n\t\t}\n\t\tnode_specific_dealloc();\n\t\treturn -1;\n\t}\n\t\n\n\tif (do_fsck(path) < 0) {\n\t\tif (RANK == 0) {\n\t\t\tpanic(\"fsck did not finish successfully!\\n\");\n\t\t}\n\t\tnode_specific_dealloc();\n\t\treturn -1;\n\t}\n\tif (RANK == 0) {\n\t\t\n\n\t\tpanic(\"fsck finished successfully!\\n\");\n\t}\n\tnode_specific_dealloc();\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tchar *path = NULL;\n\n\tMPI_Init(&argc, &argv);\n\t\n\n\tif (node_specific_alloc(argc, argv) < 0) {\n\t\tnode_specific_dealloc();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\t\n\n\tif ((path = check_validity()) == NULL) {\n\t\tif (RANK == 0) {\n\t\t\tpanic(\"Skipping fsck because of errors in parameters\\n\");\n\t\t}\n\t\tnode_specific_dealloc();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\t\n\n\tif (do_fsck(path) < 0) {\n\t\tif (RANK == 0) {\n\t\t\tpanic(\"fsck did not finish successfully!\\n\");\n\t\t}\n\t\tnode_specific_dealloc();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\tif (RANK == 0) {\n\t\t\n\n\t\tpanic(\"fsck finished successfully!\\n\");\n\t}\n\tnode_specific_dealloc();\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "cmcantalupo_1027", "code": "int main(int argc, char **argv)\n{\n    int err = 0;\n    int index = 0;\n    int rank = 0;\n    int num_iter = 100000000;\n    double sum = 0.0;\n    uint64_t region_id = 0;\n\n    err =\n    if (!err) {\n        err = geopm_prof_region(\"loop_0\", GEOPM_REGION_HINT_UNKNOWN, &region_id);\n    }\n    if (!err) {\n        err = geopm_prof_enter(region_id);\n    }\n    if (!err) {\n#pragma omp parallel default(shared) private(index)\n{\n        (void)geopm_tprof_init(num_iter);\n#pragma omp for reduction(+:sum)\n        for (index = 0; index < num_iter; ++index) {\n            sum += (double)index;\n            (void)geopm_tprof_post();\n        }\n}\n        err = geopm_prof_exit(region_id);\n    }\n    if (!err) {\n        err =\n    }\n    if (!err && !rank) {\n        printf(\"sum = %e\\n\\n\", sum);\n    }\n\n    int tmp_err =\n\n    return err ? err : tmp_err;\n}", "label": "int main(int argc, char **argv)\n{\n    int err = 0;\n    int index = 0;\n    int rank = 0;\n    int num_iter = 100000000;\n    double sum = 0.0;\n    uint64_t region_id = 0;\n\n    err = MPI_Init(&argc, &argv);\n    if (!err) {\n        err = geopm_prof_region(\"loop_0\", GEOPM_REGION_HINT_UNKNOWN, &region_id);\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (!err) {\n        err = geopm_prof_enter(region_id);\n    }\n    if (!err) {\n#pragma omp parallel default(shared) private(index)\n{\n        (void)geopm_tprof_init(num_iter);\n#pragma omp for reduction(+:sum)\n        for (index = 0; index < num_iter; ++index) {\n            sum += (double)index;\n            (void)geopm_tprof_post();\n        }\n}\n        err = geopm_prof_exit(region_id);\n    }\n    if (!err) {\n        err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    }\n    if (!err && !rank) {\n        printf(\"sum = %e\\n\\n\", sum);\n    }\n\n    int tmp_err = MPI_Finalize();\n\n    return err ? err : tmp_err;\n}"}
{"program": "airqinc_1028", "code": "int main(int argc, char** argv){\n\n\tsrand(100);\n\n    \n\n\n    \n\n    int size;\n\n    \n\n    int rank;\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\t\n    int name_len;\n\n    if(argc!=3 && rank==0){\n            printf(\"\\n >> Wrong params...\\n\");\n\t\t\tprintf(\" >> A( m x n ) * B( n x p ) = C( m x p )\\n\");\n\t\t\tprintf(\" >> Usage: mpiexec -n [numProcesses = dimM] ./ejercicio4 [dimN] [dimP]\\n\\n\");\n            exit(0);\n    }\n\telse if(argc!=3){ exit(0);}\n\n    \n\n    int dim_m = size;\n    int dim_n = atoi(argv[1]);\n\tint dim_p = atoi(argv[2]);\n\n\t\n\n\tfloat random;\n\tfloat valor;\n    int i,j;\n\n\tfloat *matrixA = (float*) malloc( dim_m * dim_n * sizeof( float ));\n\tfloat *matrixB = (float*) malloc( dim_n * dim_p * sizeof( float ));\n\tfloat *matrixC = (float*) malloc( dim_m * dim_p * sizeof( float ));\n\n\t\n\n\t\n\tif(rank==0){\n\t\tprintf(\" \\n >> Procesador %s, proceso %d de %d procesos existentes.\\n >> Tengo que multiplicar dos matrices de dimensiones: A ( %d x %d ) y  B ( %d x %d )\\n\",processor_name, rank, size, dim_m, dim_n, dim_n, dim_p);\n\t\t\n\n\t\tfor (i=0;i<dim_m;i++){\n\t\t\tfor(j=0;j<dim_n;j++){\n\t\t\t\trandom = (float)rand()/(float)RAND_MAX;\n\t\t\t\trandom = random*10;\n\t\t\t\tmatrixA[i * dim_n + j]= random;\n\t\t\t}\n\t\t}\n\n\t\t\n\n\t\tfor (i=0;i<dim_n;i++){\n\t\t\tfor(j=0;j<dim_p;j++){\n\t\t\t\trandom = (float)rand()/(float)RAND_MAX;\n\t\t\t\trandom = random*10;\n\t\t\t\tmatrixB[i * dim_p+ j]= random;\n            }\n\t\t}\n\n\t\t\n\n\t\tprintf(\"\\n >> A matrix is...\\n\\n\");\n\t\tfor (i=0;i<dim_m;i++){\n\t\t\tprintf(\"| \");\n\t\t\tfor(j=0;j<dim_n;j++){\n\t\t\t\tvalor=matrixA[i * dim_n + j];\n\t\t\t\tprintf(\"%f\\t\",valor);\n            }\n\t\t\tprintf(\"|\\n\");\n\t\t}\n\n\t\t\n\n\t\tprintf(\"\\n >> B matrix is...\\n\\n\");\n\t\tfor (i=0;i<dim_n;i++){\n\t\t\tprintf(\"| \");\n\t\t\tfor(j=0;j<dim_p;j++){\n\t\t\t\tvalor=matrixB[i * dim_p+ j];\n\t\t\t\tprintf(\"%f\\t\",valor);\n\t            }\n\t\t\tprintf(\"|\\n\");\n\t\t}\n\t}\n\n\t\n\n\n\t\n\n\n\t\n\n\n\tif(rank==0){\n\t\tdelay(3000);\n\t\t\n\n\t\tprintf(\"\\n >> C matrix is...\\n\\n\");\n\t\tfor (i=0;i<dim_m;i++){\n\t\t\tprintf(\"| \");\n\t\t\tfor(j=0;j<dim_p;j++){\n\t\t\t\tvalor=matrixC[i * dim_p + j];\n\t\t\t\tprintf(\"%f\\t\",valor);\n\t\t\t}\n\t\t\tprintf(\"|\\n\");\n\t\t}\n\t}\n\tprintf(\"\\n\");\n\n    \n\n}\nv", "label": "int main(int argc, char** argv){\n\n\tsrand(100);\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\t\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    if(argc!=3 && rank==0){\n            printf(\"\\n >> Wrong params...\\n\");\n\t\t\tprintf(\" >> A( m x n ) * B( n x p ) = C( m x p )\\n\");\n\t\t\tprintf(\" >> Usage: mpiexec -n [numProcesses = dimM] ./ejercicio4 [dimN] [dimP]\\n\\n\");\n            MPI_Finalize();\n            exit(0);\n    }\n\telse if(argc!=3){ MPI_Finalize(); exit(0);}\n\n    \n\n    int dim_m = size;\n    int dim_n = atoi(argv[1]);\n\tint dim_p = atoi(argv[2]);\n\n\t\n\n\tfloat random;\n\tfloat valor;\n    int i,j;\n\n\tfloat *matrixA = (float*) malloc( dim_m * dim_n * sizeof( float ));\n\tfloat *matrixB = (float*) malloc( dim_n * dim_p * sizeof( float ));\n\tfloat *matrixC = (float*) malloc( dim_m * dim_p * sizeof( float ));\n\n\t\n\n\t\n\tif(rank==0){\n\t\tprintf(\" \\n >> Procesador %s, proceso %d de %d procesos existentes.\\n >> Tengo que multiplicar dos matrices de dimensiones: A ( %d x %d ) y  B ( %d x %d )\\n\",processor_name, rank, size, dim_m, dim_n, dim_n, dim_p);\n\t\t\n\n\t\tfor (i=0;i<dim_m;i++){\n\t\t\tfor(j=0;j<dim_n;j++){\n\t\t\t\trandom = (float)rand()/(float)RAND_MAX;\n\t\t\t\trandom = random*10;\n\t\t\t\tmatrixA[i * dim_n + j]= random;\n\t\t\t}\n\t\t}\n\n\t\t\n\n\t\tfor (i=0;i<dim_n;i++){\n\t\t\tfor(j=0;j<dim_p;j++){\n\t\t\t\trandom = (float)rand()/(float)RAND_MAX;\n\t\t\t\trandom = random*10;\n\t\t\t\tmatrixB[i * dim_p+ j]= random;\n            }\n\t\t}\n\n\t\t\n\n\t\tprintf(\"\\n >> A matrix is...\\n\\n\");\n\t\tfor (i=0;i<dim_m;i++){\n\t\t\tprintf(\"| \");\n\t\t\tfor(j=0;j<dim_n;j++){\n\t\t\t\tvalor=matrixA[i * dim_n + j];\n\t\t\t\tprintf(\"%f\\t\",valor);\n            }\n\t\t\tprintf(\"|\\n\");\n\t\t}\n\n\t\t\n\n\t\tprintf(\"\\n >> B matrix is...\\n\\n\");\n\t\tfor (i=0;i<dim_n;i++){\n\t\t\tprintf(\"| \");\n\t\t\tfor(j=0;j<dim_p;j++){\n\t\t\t\tvalor=matrixB[i * dim_p+ j];\n\t\t\t\tprintf(\"%f\\t\",valor);\n\t            }\n\t\t\tprintf(\"|\\n\");\n\t\t}\n\t}\n\n\t\n\n\n\t\n\n\n\t\n\n\n\tif(rank==0){\n\t\tdelay(3000);\n\t\t\n\n\t\tprintf(\"\\n >> C matrix is...\\n\\n\");\n\t\tfor (i=0;i<dim_m;i++){\n\t\t\tprintf(\"| \");\n\t\t\tfor(j=0;j<dim_p;j++){\n\t\t\t\tvalor=matrixC[i * dim_p + j];\n\t\t\t\tprintf(\"%f\\t\",valor);\n\t\t\t}\n\t\t\tprintf(\"|\\n\");\n\t\t}\n\t}\n\tprintf(\"\\n\");\n\n    \n\n    MPI_Finalize(); return 0;\n}\nv"}
{"program": "germasch_1030", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  switch (testcase) {\n  case 1:\n    mrc_crds_set_type(crds, \"uniform\");\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    test_read_write(domain);\n    break;\n  case 2: ;\n    mrc_crds_set_type(crds, \"rectilinear\");\n    mrc_crds_set_param_int(crds, \"sw\", 2);\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    mrctest_set_crds_rectilinear_1(domain);\n    test_read_write(domain);\n    break;\n  }\n  mrc_domain_destroy(domain);\n\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  switch (testcase) {\n  case 1:\n    mrc_crds_set_type(crds, \"uniform\");\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    test_read_write(domain);\n    break;\n  case 2: ;\n    mrc_crds_set_type(crds, \"rectilinear\");\n    mrc_crds_set_param_int(crds, \"sw\", 2);\n    mrc_domain_set_from_options(domain);\n    mrc_domain_setup(domain);\n    mrctest_set_crds_rectilinear_1(domain);\n    test_read_write(domain);\n    break;\n  }\n  mrc_domain_destroy(domain);\n\n  MPI_Finalize();\n}"}
{"program": "alucas_1031", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\tstarpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(unsigned));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tunsigned last_loop = nloops - 1;\n\tunsigned last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (!((loop == 0) && (rank == 0)))\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(token_handle, &req, (rank+size-1)%size, tag, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t\telse {\n\t\t\ttoken = 0;\n\t\t\tfprintf(stdout, \"Start with token value %d\\n\", token);\n\t\t}\n\n\t\tincrement_token();\n\t\t\n\t\tif (!((loop == last_loop) && (rank == last_rank)))\n\t\t{\n\t\t\tstarpu_mpi_req req;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_isend(token_handle, &req, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t\telse {\n\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tfprintf(stdout, \"Finished : token value %d\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\tstarpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(unsigned));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tunsigned last_loop = nloops - 1;\n\tunsigned last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (!((loop == 0) && (rank == 0)))\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(token_handle, &req, (rank+size-1)%size, tag, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t\telse {\n\t\t\ttoken = 0;\n\t\t\tfprintf(stdout, \"Start with token value %d\\n\", token);\n\t\t}\n\n\t\tincrement_token();\n\t\t\n\t\tif (!((loop == last_loop) && (rank == last_rank)))\n\t\t{\n\t\t\tstarpu_mpi_req req;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_isend(token_handle, &req, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t\telse {\n\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tfprintf(stdout, \"Finished : token value %d\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t}\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n"}
{"program": "oscartt89_1032", "code": "int main(int argc, char **argv) {\n\tint         myrank;\n\tchar *workload_file;\n\t\n\tif (argc < 2){\n\t    fprintf(stderr, \"ERROR. Use: ./mpi_master_slave workload_file\\n\");\n\t    exit(-1);\n\t}\n\t\n\tworkload_file = argv[1];\n\t\n\n\n\t\n\tif (myrank == 0) {\n\t\tmaster(workload_file);\n\t} else {\n\t\tslave();\n\t}\n\t\n\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv) {\n\tint         myrank;\n\tchar *workload_file;\n\t\n\tif (argc < 2){\n\t    fprintf(stderr, \"ERROR. Use: ./mpi_master_slave workload_file\\n\");\n\t    exit(-1);\n\t}\n\t\n\tworkload_file = argv[1];\n\t\n\tMPI_Init(&argc, &argv);   \n\n\tMPI_Comm_rank(\n\tMPI_COMM_WORLD,   \n\n\t&myrank);      \n\n\t\n\tif (myrank == 0) {\n\t\tmaster(workload_file);\n\t} else {\n\t\tslave();\n\t}\n\t\n\tMPI_Finalize();       \n\n\n\treturn 0;\n}"}
{"program": "gnu3ra_1034", "code": "int main(int argc, char *argv[])\n{\n    \n\n    MPI_Datatype oneslice, twoslice, threeslice;\n    int errs = 0;\n    MPI_Aint sizeofint, bufsize, position;\n    void *buffer;\n\t\n    int i, j, k;\n\t\n    \n\n    for (i = 0; i < 100; i++) {\n\tfor (j = 0; j < 100; j++) {\n\t    for (k = 0; k < 100; k++) {\n\t\ta[i][j][k] = i*1000000+j*1000+k;\n\t    }\n\t}\n    }\n\t\n    \n\n  \n    parse_args(argc, argv);\n\n    \n\n    \n\n    \n\n    \n\n\t\n\t\n    \n\n    position = 0;\n\n\n    if (bufsize != 2916)\n    {\n        fprintf(stderr,\" Error on pack size! Got %d; expecting %d\\n\", (int) bufsize, 2916);\n    }\n    buffer = (void *) malloc((unsigned) bufsize);\n\n    \n\n\n    \n\n    position = 0;\n\t\n    \n\n    for (i = 0; i < 9; i++) {\n\tfor (j = 0; j < 9; j++) {\n\t    for (k = 0; k < 9; k++) {\n\t       \n\n\t\tif (e[i][j][k] != a[i][j+2][k*2+1]) {\n\t\t    errs++;\n\t\t    if (verbose) {\n\t\t\tprintf(\"Error in location %d x %d x %d: %d, should be %d.\\n\",\n\t\t\t       i, j, k, e[i][j][k], a[i][j+2][k*2+1]);\n\t\t    }\n\t\t}\n\t    }\n\t}\n    } \n  \n    \n\n    free(buffer);\n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    \n\n    MPI_Datatype oneslice, twoslice, threeslice;\n    int errs = 0;\n    MPI_Aint sizeofint, bufsize, position;\n    void *buffer;\n\t\n    int i, j, k;\n\t\n    \n\n    for (i = 0; i < 100; i++) {\n\tfor (j = 0; j < 100; j++) {\n\t    for (k = 0; k < 100; k++) {\n\t\ta[i][j][k] = i*1000000+j*1000+k;\n\t    }\n\t}\n    }\n\t\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Type_extent(MPI_INT, &sizeofint);\n  \n    parse_args(argc, argv);\n\n    \n\n    \n\n    \n\n    \n\n    MPI_Type_vector(9, 1, 2, MPI_INT, &oneslice);\n    MPI_Type_hvector(9, 1, 100*sizeofint, oneslice, &twoslice);\n    MPI_Type_hvector(9, 1, 100*100*sizeofint, twoslice, &threeslice);\n\t\n    MPI_Type_commit(&threeslice);\n\t\n    \n\n    position = 0;\n\n\n    MPI_Pack_external_size((char*)\"external32\", 1, threeslice, &bufsize);\n    if (bufsize != 2916)\n    {\n        fprintf(stderr,\" Error on pack size! Got %d; expecting %d\\n\", (int) bufsize, 2916);\n    }\n    buffer = (void *) malloc((unsigned) bufsize);\n\n    \n\n    MPI_Pack_external((char*)\"external32\",\n\t\t      &(a[0][2][1]),\n\t\t      1, threeslice,\n\t\t      buffer,\n\t\t      bufsize,\n\t\t      &position);\n\n    \n\n    position = 0;\n    MPI_Unpack_external((char*)\"external32\",\n\t\t\tbuffer,\n\t\t\tbufsize,\n\t\t\t&position,\n\t\t\te, 9*9*9,\n\t\t\tMPI_INT);\n\t\n    \n\n    for (i = 0; i < 9; i++) {\n\tfor (j = 0; j < 9; j++) {\n\t    for (k = 0; k < 9; k++) {\n\t       \n\n\t\tif (e[i][j][k] != a[i][j+2][k*2+1]) {\n\t\t    errs++;\n\t\t    if (verbose) {\n\t\t\tprintf(\"Error in location %d x %d x %d: %d, should be %d.\\n\",\n\t\t\t       i, j, k, e[i][j][k], a[i][j+2][k*2+1]);\n\t\t    }\n\t\t}\n\t    }\n\t}\n    } \n  \n    \n\n    free(buffer);\n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n\n    MPI_Type_free(&oneslice);\n    MPI_Type_free(&twoslice);\n    MPI_Type_free(&threeslice);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "bmi-forum_1041", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\tStream_SetFileBranch( Journal_GetTypedStream( ErrorStream_Type ), stJournal->stdOut );\n\tstJournal->firewallProducesAssert = False;\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tIndexSet*\t\t\tis;\n\t\t\n\t\tprintf( \"Watching rank: %i\\n\", rank );\n\t\t\n\t\tprintf( \"* Test Construction *\\n\" );\n\t\tis = IndexSet_New( 24 );\n\t\t\n\t\tprintf( \"* Test Insertion(Add) over limit *\\n\" );\n\t\tIndexSet_Add( is, 24 );\n\t\tprintf( \"* Shouldn't get here\\n\" );\n\n\t\tStg_Class_Delete( is );\n\t}\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tint\t\t\t\tprocToWatch;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\tStream_SetFileBranch( Journal_GetTypedStream( ErrorStream_Type ), stJournal->stdOut );\n\tstJournal->firewallProducesAssert = False;\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tIndexSet*\t\t\tis;\n\t\t\n\t\tprintf( \"Watching rank: %i\\n\", rank );\n\t\t\n\t\tprintf( \"* Test Construction *\\n\" );\n\t\tis = IndexSet_New( 24 );\n\t\t\n\t\tprintf( \"* Test Insertion(Add) over limit *\\n\" );\n\t\tIndexSet_Add( is, 24 );\n\t\tprintf( \"* Shouldn't get here\\n\" );\n\n\t\tStg_Class_Delete( is );\n\t}\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "wayne927_1042", "code": "int main(int argc, char* argv[])\n{\n\n        \n    decompose_nmax = 10;\n    decompose_lmax = 0;\n    int n_elements = decompose_nmax + 1;\n    float* mat_cos = (float*)malloc(n_elements*sizeof(float));\n    float* mat_cos_reduced = (float*)malloc(n_elements*sizeof(float));\n    \n    float center[] = {  2.223352e+01,  -2.108813e+02,  1.285330e+02 };\n    float scale_radius = 3.424658e+00;\n    \n    float given[] = {center[0]+HOST_CENTER_X, center[1]+HOST_CENTER_Y, center[2]+HOST_CENTER_Z, scale_radius};\n    \n    char* in_filename = \"/data/ngan/VLII/data/subhalo_np150_dir/subhalo_full_23.std\";\n    \n    \n\n    read_snapshot(in_filename, given);\n    \n    \n\n    scf_integral_spherical(mat_cos);\n    \n\n    \n\n\n    char* out_filename = \"./subhalo23.scf\";\n\n    FILE* fout;\n    \n    if(ThisTask == 0)\n    {\n        \n        fout = fopen(out_filename, \"w\");\n\n        if(fout == NULL)\n        {\n            perror(\"Can't write to matrix file!\\n\");\n            exit(1);\n        }\n    \n        fwrite(&decompose_nmax, sizeof(decompose_nmax), 1, fout);\n        fwrite(&decompose_lmax, sizeof(decompose_lmax), 1, fout);\n        fwrite(mat_cos_reduced, sizeof(float), n_elements, fout);\n        \n        \n\n    \n        fclose(fout);\n        \n        printf(\"Wrote SCF matrix to %s\\n\", out_filename);\n    }\n    \n    free(xx);\n    free(yy);\n    free(zz);\n    free(mm);\n    free(mat_cos);\n    free(mat_cos_reduced);\n\n   \n    return 0;\n    \n}", "label": "int main(int argc, char* argv[])\n{\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &ThisTask);\n    MPI_Comm_size(MPI_COMM_WORLD, &NTask);\n\n        \n    decompose_nmax = 10;\n    decompose_lmax = 0;\n    int n_elements = decompose_nmax + 1;\n    float* mat_cos = (float*)malloc(n_elements*sizeof(float));\n    float* mat_cos_reduced = (float*)malloc(n_elements*sizeof(float));\n    \n    float center[] = {  2.223352e+01,  -2.108813e+02,  1.285330e+02 };\n    float scale_radius = 3.424658e+00;\n    \n    float given[] = {center[0]+HOST_CENTER_X, center[1]+HOST_CENTER_Y, center[2]+HOST_CENTER_Z, scale_radius};\n    \n    char* in_filename = \"/data/ngan/VLII/data/subhalo_np150_dir/subhalo_full_23.std\";\n    \n    \n\n    read_snapshot(in_filename, given);\n    \n    \n\n    scf_integral_spherical(mat_cos);\n    \n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    MPI_Reduce(mat_cos, mat_cos_reduced, n_elements, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    char* out_filename = \"./subhalo23.scf\";\n\n    FILE* fout;\n    \n    if(ThisTask == 0)\n    {\n        \n        fout = fopen(out_filename, \"w\");\n\n        if(fout == NULL)\n        {\n            perror(\"Can't write to matrix file!\\n\");\n            MPI_Finalize();\n            exit(1);\n        }\n    \n        fwrite(&decompose_nmax, sizeof(decompose_nmax), 1, fout);\n        fwrite(&decompose_lmax, sizeof(decompose_lmax), 1, fout);\n        fwrite(mat_cos_reduced, sizeof(float), n_elements, fout);\n        \n        \n\n    \n        fclose(fout);\n        \n        printf(\"Wrote SCF matrix to %s\\n\", out_filename);\n    }\n    \n    free(xx);\n    free(yy);\n    free(zz);\n    free(mm);\n    free(mat_cos);\n    free(mat_cos_reduced);\n\n    MPI_Finalize(); \n   \n    return 0;\n    \n}"}
{"program": "scafacos_1043", "code": "int\nmain(int argc, char **argv, char **envp)\n{\n         int i,ret,npes;\n         int num_interfaces;\n         ptl_handle_ni_t nih;\n         ptl_handle_eq_t eqh;\n         ptl_ni_limits_t ptl_limits;\n         pid_t child;\n         ptl_process_id_t rnk;\n\n         child = fork();\n         if ((ret=PtlInit(&num_interfaces)) != PTL_OK) {\n                 printf(\"%s: PtlInit failed: %d\\n\", FUNCTION_NAME, ret);\n                 exit(1);\n         }\n         printf(\"%s: PtlInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n\n         if ((ret=PtlNIInit(\n                 IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n                 PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n                 printf(\"%s: PtlNIInit 1 failed: %d\\n\", FUNCTION_NAME, ret);\n         } \n\n         if((ret = PtlNIFini(nih)) !=PTL_OK){\n                 printf(\"%s: PtlNIFini failed: %d\\n\", FUNCTION_NAME, ret);\n         }\n         PtlFini();\n\n         if ((ret=PtlInit(&num_interfaces)) != PTL_OK) {\n           printf(\"%s: PtlInit failed: %d\\n\", FUNCTION_NAME, ret);\n           exit(1);\n         }\n         if ((ret=PtlNIInit(\n             IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n             PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n             printf(\"%s: PtlNIInit 2 failed: %d\\n\", FUNCTION_NAME, ret);\n             exit(1);\n         } \n#if 0\n         if ((ret=PtlNIInit(\n                 IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n                 PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n                 printf(\"%s: PtlNIInit failed: %d\\n\", FUNCTION_NAME, ret);\n                 exit(1);\n         }\n#endif\n         printf(\"%s: PtlNIInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n         if ((ret=PtlEQAlloc(nih, 4096, NULL, &eqh)) != PTL_OK) {\n                 printf(\"%s: PtlEQAlloc failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n                 exit(1);\n         }\n         printf(\"%s: PtlEQAlloc succeeds (%d:%d)\\n\", FUNCTION_NAME, child, ret);\n\n\n         if ((ret=PtlGetId(nih,&rnk)) !=PTL_OK) {\n                 printf(\"%s: PtlGetId failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n                 exit(1);\n         }\n         printf(\"%s: nid=%d pid=%d(%d)\\n\",FUNCTION_NAME,rnk.nid,rnk.pid,child);\n\n         if(child){\n           printf(\"%s: mpi_init and finalize succeed(%d)\\n\",FUNCTION_NAME,child);\n         }\n}", "label": "int\nmain(int argc, char **argv, char **envp)\n{\n         int i,ret,npes;\n         int num_interfaces;\n         ptl_handle_ni_t nih;\n         ptl_handle_eq_t eqh;\n         ptl_ni_limits_t ptl_limits;\n         pid_t child;\n         ptl_process_id_t rnk;\n\n         child = fork();\n         if ((ret=PtlInit(&num_interfaces)) != PTL_OK) {\n                 printf(\"%s: PtlInit failed: %d\\n\", FUNCTION_NAME, ret);\n                 exit(1);\n         }\n         printf(\"%s: PtlInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n\n         if ((ret=PtlNIInit(\n                 IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n                 PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n                 printf(\"%s: PtlNIInit 1 failed: %d\\n\", FUNCTION_NAME, ret);\n         } \n\n         if((ret = PtlNIFini(nih)) !=PTL_OK){\n                 printf(\"%s: PtlNIFini failed: %d\\n\", FUNCTION_NAME, ret);\n         }\n         PtlFini();\n\n         if ((ret=PtlInit(&num_interfaces)) != PTL_OK) {\n           printf(\"%s: PtlInit failed: %d\\n\", FUNCTION_NAME, ret);\n           exit(1);\n         }\n         if ((ret=PtlNIInit(\n             IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n             PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n             printf(\"%s: PtlNIInit 2 failed: %d\\n\", FUNCTION_NAME, ret);\n             exit(1);\n         } \n#if 0\n         if ((ret=PtlNIInit(\n                 IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n                 PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n                 printf(\"%s: PtlNIInit failed: %d\\n\", FUNCTION_NAME, ret);\n                 exit(1);\n         }\n#endif\n         printf(\"%s: PtlNIInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n         if ((ret=PtlEQAlloc(nih, 4096, NULL, &eqh)) != PTL_OK) {\n                 printf(\"%s: PtlEQAlloc failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n                 exit(1);\n         }\n         printf(\"%s: PtlEQAlloc succeeds (%d:%d)\\n\", FUNCTION_NAME, child, ret);\n\n\n         if ((ret=PtlGetId(nih,&rnk)) !=PTL_OK) {\n                 printf(\"%s: PtlGetId failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n                 exit(1);\n         }\n         printf(\"%s: nid=%d pid=%d(%d)\\n\",FUNCTION_NAME,rnk.nid,rnk.pid,child);\n\n         if(child){\n           MPI_Init(&argc,&argv);\n           MPI_Finalize();\n           printf(\"%s: mpi_init and finalize succeed(%d)\\n\",FUNCTION_NAME,child);\n         }\n}"}
{"program": "gyaikhom_1045", "code": "int main(int argc, char **argv) {\n    char option;\n    extern char *optarg;\n    extern int optopt;\n\n    bc_init(BC_NULL);\n\n    while ((option = getopt(argc, argv, \"prld:PRLD:\")) != -1) {\n        switch (option) {\n            case 'p': \n\n                putget_flag = 1;\n                break;\n            case 'r': \n\n                bc_rtt_flag = 1;\n                break;\n            case 'l': \n\n                bc_lse_flag = 1;\n                break;\n            case 'P': \n\n                sendrecv_flag = 1;\n                break;\n            case 'R': \n\n                mpi_rtt_flag = 1;\n                break;\n            case 'L': \n\n                mpi_lse_flag = 1;\n                break;\n            case '?': \n\n                fprintf(stderr, \"Unrecognised option -%c.\\n\", optopt);\n                bc_final();\n                return 1;\n            case ':': \n\n                fprintf(stderr, \"Option -%c requires an operand.\\n\", optopt);\n            default:\n                bc_final();\n                return 1;\n        }\n    }\n\n    if (bc_rank == 0) {\n        master = 1;\n        gnuplot = fopen(\"master.gnuplot\", \"w\");\n    } else {\n        gnuplot = fopen(\"slave.gnuplot\", \"w\");\n    }\n\n    print_settings(gnuplot);\n    test();\n    fclose(gnuplot);\n\n    bc_final();\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    char option;\n    extern char *optarg;\n    extern int optopt;\n\n    MPI_Init(&argc, &argv);\n    bc_init(BC_NULL);\n\n    while ((option = getopt(argc, argv, \"prld:PRLD:\")) != -1) {\n        switch (option) {\n            case 'p': \n\n                putget_flag = 1;\n                break;\n            case 'r': \n\n                bc_rtt_flag = 1;\n                break;\n            case 'l': \n\n                bc_lse_flag = 1;\n                break;\n            case 'P': \n\n                sendrecv_flag = 1;\n                break;\n            case 'R': \n\n                mpi_rtt_flag = 1;\n                break;\n            case 'L': \n\n                mpi_lse_flag = 1;\n                break;\n            case '?': \n\n                fprintf(stderr, \"Unrecognised option -%c.\\n\", optopt);\n                bc_final();\n                MPI_Finalize();\n                return 1;\n            case ':': \n\n                fprintf(stderr, \"Option -%c requires an operand.\\n\", optopt);\n            default:\n                bc_final();\n                MPI_Finalize();\n                return 1;\n        }\n    }\n\n    if (bc_rank == 0) {\n        master = 1;\n        gnuplot = fopen(\"master.gnuplot\", \"w\");\n    } else {\n        gnuplot = fopen(\"slave.gnuplot\", \"w\");\n    }\n\n    print_settings(gnuplot);\n    test();\n    fclose(gnuplot);\n\n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "htapiagroup_1046", "code": "int main( int argc, char** argv )\n{\n \n    int direct;\n    int rank, size;\n    int *restrict buff = NULL;\n    size_t bytes;\n    int i;\n \n    \n\n    direct = getenv(\"MPICH_RDMA_ENABLED_CUDA\")==NULL?0:atoi(getenv (\"MPICH_RDMA_ENABLED_CUDA\"));\n    if(direct != 1){\n        printf (\"MPICH_RDMA_ENABLED_CUDA not enabled!\\n\");\n        exit (EXIT_FAILURE);\n    }\n \n    \n\n \n    \n\n    bytes = size*sizeof(int);\n    buff = (int*)malloc(bytes);\n \n    \n\n    #pragma acc data copy(rank, buff[0:size])\n    {\n        \n\n        #pragma acc host_data use_device(rank, buff)\n        {\n        }\n    }\n \n    \n\n    for(i=0; i<size; i++){\n        if(buff[i] != i) {\n            printf (\"Alltoall Failed!\\n\");\n            exit (EXIT_FAILURE);\n        }\n    }\n    if(rank==0)\n        printf(\"Success!\\n\");\n \n    \n\n    free(buff);\n \n \n    return 0;\n}", "label": "int main( int argc, char** argv )\n{\n    MPI_Init (&argc, &argv);\n \n    int direct;\n    int rank, size;\n    int *restrict buff = NULL;\n    size_t bytes;\n    int i;\n \n    \n\n    direct = getenv(\"MPICH_RDMA_ENABLED_CUDA\")==NULL?0:atoi(getenv (\"MPICH_RDMA_ENABLED_CUDA\"));\n    if(direct != 1){\n        printf (\"MPICH_RDMA_ENABLED_CUDA not enabled!\\n\");\n        exit (EXIT_FAILURE);\n    }\n \n    \n\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n    MPI_Comm_size (MPI_COMM_WORLD, &size);\n \n    \n\n    bytes = size*sizeof(int);\n    buff = (int*)malloc(bytes);\n \n    \n\n    #pragma acc data copy(rank, buff[0:size])\n    {\n        \n\n        #pragma acc host_data use_device(rank, buff)\n        {\n            MPI_Allgather(&rank, 1, MPI_INT, buff, 1, MPI_INT, MPI_COMM_WORLD);\n        }\n    }\n \n    \n\n    for(i=0; i<size; i++){\n        if(buff[i] != i) {\n            printf (\"Alltoall Failed!\\n\");\n            exit (EXIT_FAILURE);\n        }\n    }\n    if(rank==0)\n        printf(\"Success!\\n\");\n \n    \n\n    free(buff);\n \n    MPI_Finalize();\n \n    return 0;\n}"}
{"program": "ghisvail_1047", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3], N[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  pfft_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r_3d(\n      n, in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n \n  \n\n  plan_back = pfft_plan_r2r_3d(\n      n, out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2r_3d(n, local_ni, local_i_start,\n      in);\n\n  int myrank, size;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_forw);\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_r2r_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3], N[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  pfft_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r_3d(\n      n, in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n \n  \n\n  plan_back = pfft_plan_r2r_3d(\n      n, out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2r_3d(n, local_ni, local_i_start,\n      in);\n\n  int myrank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_forw);\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_r2r_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "sg0_1048", "code": "int main(int argc, char **argv)\n{\n  int rank, nprocs;\n  int g_A;\n  int *local_A=NULL, *local_B=NULL, *output_A=NULL;\n  int dims[DIM]={SIZE,SIZE}, dims2[DIM], lo[DIM]={SIZE-SIZE,SIZE-SIZE}, hi[DIM]={SIZE-1,SIZE-1}, ld=SIZE;\n  int value=SIZE;\n  \n\n\n#if defined(USE_ELEMENTAL)\n  \n\n  ElInitialize( &argc, &argv );\n  ElMPICommRank( MPI_COMM_WORLD, &rank );\n  ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n  \n\n  ElGlobalArraysConstruct_i( &eliga );\n  \n\n  ElGlobalArraysInitialize_i( eliga );\n#else\n\n\n  MA_init(C_INT, 1000, 1000);\n\n  GA_Initialize();\n#endif\n\n  local_A=(int*)malloc(SIZE*SIZE*sizeof(int));\n  output_A=(int*)malloc(SIZE*SIZE*sizeof(int));\n  memset (output_A, 0, SIZE*SIZE*sizeof(int));\n  for(int j=0; j<SIZE; j++)\n      for(int i=0; i<SIZE; i++) local_A[i+j*ld]=(i + j);\n\n  local_B=(int*)malloc(SIZE*SIZE*sizeof(int));\n  memset (local_B, 0, SIZE*SIZE*sizeof(int));\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysCreate_i( eliga, DIM, dims, \"array_A\", NULL, NULL, &g_A );\n  ElGlobalArraysFill_i( eliga, g_A, &value );\n  ElGlobalArraysPrint_i( eliga, g_A );\n  \n\n  ElGlobalArraysPut_i( eliga, g_A, lo, hi, local_A, &ld );\n  ElGlobalArraysSync_i( eliga );\n  \n\n  ElGlobalArraysGet_i( eliga, g_A, lo, hi, local_B, &ld );\n  ElGlobalArraysSync_i( eliga );\n  ElGlobalArraysPrint_i( eliga, g_A );\n#else\n  g_A = NGA_Create(C_INT, DIM, dims, \"array_A\", NULL);\n  GA_Fill(g_A, &value);\n  GA_Print(g_A);\n\n  NGA_Put(g_A, lo, hi, local_A, &ld);\n  \n  GA_Sync();\n  \n  NGA_Get(g_A, lo, hi, local_B, &ld);\n\n  GA_Sync();\n  \n  GA_Print(g_A);\n#endif\n\n  \n\n\n  if(rank==0)\n    {\n      printf(\" Original local buffer to be accumulated: \\n\");\n\n      for(int i=0; i<SIZE; i++)\n\t{\n\t  for(int j=0; j<SIZE; j++)\n\t    printf(\"%d \", local_A[i*ld+j]);\n\t  printf(\"\\n\");\n\t}\n      printf(\"\\n\");\n      printf(\" Get returns: \\n\");\n      for(int i=0; i<SIZE; i++)\n\t{\n\t  for(int j=0; j<SIZE; j++)\n\t    printf(\"%d \", local_B[i*ld + j]);\n\t  printf(\"\\n\");\n\t}\n\n      printf(\"\\n\");\n      for(int i=0; i<SIZE; i++)\n\t{\n\t  for(int j=0; j<SIZE; j++)\n\t    {\n\t      if(local_B[i*ld+j]!=output_A[i*ld+j])\n\t\t  GA_Error(\"ERROR\", -99);\n\t    }\n\t}\n    }\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDestroy_i( eliga, g_A );\n#else\n  GA_Destroy(g_A);\n#endif\n  if(rank == 0)\n    printf (\"OK. Test passed\\n\");\n\n    free (local_A);\n    free (local_B);\n    free (output_A);\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysTerminate_i( eliga );\n    \n\n    ElGlobalArraysDestruct_i( eliga );\n    ElFinalize();\n#else\n    GA_Terminate();\n#endif\n}", "label": "int main(int argc, char **argv)\n{\n  int rank, nprocs;\n  int g_A;\n  int *local_A=NULL, *local_B=NULL, *output_A=NULL;\n  int dims[DIM]={SIZE,SIZE}, dims2[DIM], lo[DIM]={SIZE-SIZE,SIZE-SIZE}, hi[DIM]={SIZE-1,SIZE-1}, ld=SIZE;\n  int value=SIZE;\n  \n\n\n#if defined(USE_ELEMENTAL)\n  \n\n  ElInitialize( &argc, &argv );\n  ElMPICommRank( MPI_COMM_WORLD, &rank );\n  ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n  \n\n  ElGlobalArraysConstruct_i( &eliga );\n  \n\n  ElGlobalArraysInitialize_i( eliga );\n#else\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  MA_init(C_INT, 1000, 1000);\n\n  GA_Initialize();\n#endif\n\n  local_A=(int*)malloc(SIZE*SIZE*sizeof(int));\n  output_A=(int*)malloc(SIZE*SIZE*sizeof(int));\n  memset (output_A, 0, SIZE*SIZE*sizeof(int));\n  for(int j=0; j<SIZE; j++)\n      for(int i=0; i<SIZE; i++) local_A[i+j*ld]=(i + j);\n\n  local_B=(int*)malloc(SIZE*SIZE*sizeof(int));\n  memset (local_B, 0, SIZE*SIZE*sizeof(int));\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysCreate_i( eliga, DIM, dims, \"array_A\", NULL, NULL, &g_A );\n  ElGlobalArraysFill_i( eliga, g_A, &value );\n  ElGlobalArraysPrint_i( eliga, g_A );\n  \n\n  ElGlobalArraysPut_i( eliga, g_A, lo, hi, local_A, &ld );\n  ElGlobalArraysSync_i( eliga );\n  \n\n  ElGlobalArraysGet_i( eliga, g_A, lo, hi, local_B, &ld );\n  ElGlobalArraysSync_i( eliga );\n  ElGlobalArraysPrint_i( eliga, g_A );\n#else\n  g_A = NGA_Create(C_INT, DIM, dims, \"array_A\", NULL);\n  GA_Fill(g_A, &value);\n  GA_Print(g_A);\n\n  NGA_Put(g_A, lo, hi, local_A, &ld);\n  \n  GA_Sync();\n  \n  NGA_Get(g_A, lo, hi, local_B, &ld);\n\n  GA_Sync();\n  \n  GA_Print(g_A);\n#endif\n\n  \n\n  MPI_Reduce (local_A, output_A, SIZE*SIZE, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if(rank==0)\n    {\n      printf(\" Original local buffer to be accumulated: \\n\");\n\n      for(int i=0; i<SIZE; i++)\n\t{\n\t  for(int j=0; j<SIZE; j++)\n\t    printf(\"%d \", local_A[i*ld+j]);\n\t  printf(\"\\n\");\n\t}\n      printf(\"\\n\");\n      printf(\" Get returns: \\n\");\n      for(int i=0; i<SIZE; i++)\n\t{\n\t  for(int j=0; j<SIZE; j++)\n\t    printf(\"%d \", local_B[i*ld + j]);\n\t  printf(\"\\n\");\n\t}\n\n      printf(\"\\n\");\n      for(int i=0; i<SIZE; i++)\n\t{\n\t  for(int j=0; j<SIZE; j++)\n\t    {\n\t      if(local_B[i*ld+j]!=output_A[i*ld+j])\n\t\t  GA_Error(\"ERROR\", -99);\n\t    }\n\t}\n    }\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDestroy_i( eliga, g_A );\n#else\n  GA_Destroy(g_A);\n#endif\n  if(rank == 0)\n    printf (\"OK. Test passed\\n\");\n\n    free (local_A);\n    free (local_B);\n    free (output_A);\n\n#if defined(USE_ELEMENTAL)\n    ElGlobalArraysTerminate_i( eliga );\n    \n\n    ElGlobalArraysDestruct_i( eliga );\n    ElFinalize();\n#else\n    GA_Terminate();\n    MPI_Finalize();\n#endif\n}"}
{"program": "syftalent_1049", "code": "int\nmain(\n    int\t\t\t\t\targc,\n    char *\t\t\t\targv[])\n{\n    int\t\t\t\t\tnp;\n    int\t\t\t\t\trank;\n    MPI_Win\t\t\t\twin;\n    int\t\t\t\t\tdata[2];\n    \n\n    neighbor_rank = (my_rank + 1) % np;\n    \n    if (np < 2)\n    {\n\tif (rank == 0)\n\t{\n\t    printf(\"\\nERROR: fence_get_simple must be at least (2) \"\n\t\t   \"processes\\n\\n\");\n\t}\n\t\n\treturn 1;\n    }\n\n    data[0] = 0;\n    data[1] = 0;\n\n    if (rank == 0)\n    {\n\tint\t\t\t\tcount;\n\t\n\n\tdo\n\t{\n\t    {\n\t\tcount = data[0];\n\t    }\n\t}\n\twhile (count < np - 1);\n\n\tprintf(\"\\nPassively accumulated a value of %d from %d processes\\n\\n\",\n\t       data[1], data[0]);\n\t\n    }\n    else\n    {\n\n\tdata[0] = 1;\n\tdata[1] = rank;\n\t\n\t{\n\t}\n    \n    }\n\n    \n    return 0;\n}", "label": "int\nmain(\n    int\t\t\t\t\targc,\n    char *\t\t\t\targv[])\n{\n    int\t\t\t\t\tnp;\n    int\t\t\t\t\trank;\n    MPI_Win\t\t\t\twin;\n    int\t\t\t\t\tdata[2];\n    \n    MPI_Init(NULL, NULL);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &np);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    neighbor_rank = (my_rank + 1) % np;\n    \n    if (np < 2)\n    {\n\tif (rank == 0)\n\t{\n\t    printf(\"\\nERROR: fence_get_simple must be at least (2) \"\n\t\t   \"processes\\n\\n\");\n\t}\n\t\n\tMPI_Finalize();\n\treturn 1;\n    }\n\n    data[0] = 0;\n    data[1] = 0;\n\n    if (rank == 0)\n    {\n\tint\t\t\t\tcount;\n\t\n\tMPI_Win_create(&data, sizeof(int), sizeof(int), MPI_INFO_NULL,\n\t\t       MPI_COMM_WORLD, &win);\n\n\tdo\n\t{\n\t    MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);\n\t    {\n\t\tcount = data[0];\n\t    }\n\t    MPI_Win_unlock(0, win);\n\t}\n\twhile (count < np - 1);\n\n\tprintf(\"\\nPassively accumulated a value of %d from %d processes\\n\\n\",\n\t       data[1], data[0]);\n\t\n\tMPI_Win_free(&win);\n    }\n    else\n    {\n\tMPI_Win_create(NULL, 0, 0, MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n\tdata[0] = 1;\n\tdata[1] = rank;\n\t\n\tMPI_Win_lock(MPI_LOCK_SHARED, 0, 0, win);\n\t{\n\t    MPI_Accumulate(data, 2, MPI_INT,\n\t\t0, 0, 2, MPI_INT, MPI_SUM, win);\n\t}\n\tMPI_Win_unlock(0, win);\n    \n\tMPI_Win_free(&win);\n    }\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "xyuan_1051", "code": "int\nmain (int argc, char **argv)\n{\n  int                 mpiret;\n  unsigned            crc;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n  mpi_context_t       mpi_context, *mpi = &mpi_context;\n  const p4est_locidx_t given[5] = { 3, 7, 36, 10, 10 };\n\n  \n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpi->mpicomm = MPI_COMM_WORLD;\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpi->mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  if (mpi->mpisize != 5)\n    sc_abort_collective (\"This example requires MPI with np=5.\");\n\n  \n\n  connectivity = p4est_connectivity_new_star ();\n  p4est = p4est_new_ext (mpi->mpicomm, connectivity, 15, 0, 0, 0, NULL, NULL);\n\n  \n\n  (void) p4est_partition_given (p4est, given);\n  p4est_refine (p4est, 1, refine_fn, NULL);\n  p4est_vtk_write_file (p4est, NULL, \"second_refined\");\n\n  \n\n  p4est_balance (p4est, P4EST_CONNECT_FULL, NULL);\n  p4est_vtk_write_file (p4est, NULL, \"second_balanced\");\n  crc = p4est_checksum (p4est);\n\n  \n\n  P4EST_GLOBAL_STATISTICSF (\"Tree checksum 0x%08x\\n\", crc);\n  if (mpi->mpirank == 0)\n    SC_CHECK_ABORT (crc == 0x324eb631U, \"Checksum mismatch\");\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n\n  \n\n  sc_finalize ();\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int                 mpiret;\n  unsigned            crc;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n  mpi_context_t       mpi_context, *mpi = &mpi_context;\n  const p4est_locidx_t given[5] = { 3, 7, 36, 10, 10 };\n\n  \n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpi->mpicomm = MPI_COMM_WORLD;\n  mpiret = MPI_Comm_size (mpi->mpicomm, &mpi->mpisize);\n  SC_CHECK_MPI (mpiret);\n  mpiret = MPI_Comm_rank (mpi->mpicomm, &mpi->mpirank);\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpi->mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  if (mpi->mpisize != 5)\n    sc_abort_collective (\"This example requires MPI with np=5.\");\n\n  \n\n  connectivity = p4est_connectivity_new_star ();\n  p4est = p4est_new_ext (mpi->mpicomm, connectivity, 15, 0, 0, 0, NULL, NULL);\n\n  \n\n  (void) p4est_partition_given (p4est, given);\n  p4est_refine (p4est, 1, refine_fn, NULL);\n  p4est_vtk_write_file (p4est, NULL, \"second_refined\");\n\n  \n\n  p4est_balance (p4est, P4EST_CONNECT_FULL, NULL);\n  p4est_vtk_write_file (p4est, NULL, \"second_balanced\");\n  crc = p4est_checksum (p4est);\n\n  \n\n  P4EST_GLOBAL_STATISTICSF (\"Tree checksum 0x%08x\\n\", crc);\n  if (mpi->mpirank == 0)\n    SC_CHECK_ABORT (crc == 0x324eb631U, \"Checksum mismatch\");\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n\n  \n\n  sc_finalize ();\n\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "byu-vv-lab_1057", "code": "int main(int argc, char *argv[]) {\n    int rank, size, i, n;\n\n\n    int sendbuf[size];\n    int recvbuf;\n\n    if(rank == 0){\n      for (int i=0; i<size; i++)\n\tsendbuf[i] = 1 + rank + size*i;\n    }\n    \n    if(rank != 6)\n\n    printf(\"Proc %d: \", rank);\n    for (int i=0; i<size; i++) printf(\"%d \", sendbuf[i]);\n    printf(\"\\n\");\n\n    int recvcounts[size];\n    for (int i=0; i<size; i++)\n        recvcounts[i] = 1;\n\n\n    printf(\"Proc %d: %d\\n\", rank, recvbuf);\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    int rank, size, i, n;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int sendbuf[size];\n    int recvbuf;\n\n    if(rank == 0){\n      for (int i=0; i<size; i++)\n\tsendbuf[i] = 1 + rank + size*i;\n    }\n    \n    if(rank != 6)\n      MPI_Bcast(sendbuf, size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    printf(\"Proc %d: \", rank);\n    for (int i=0; i<size; i++) printf(\"%d \", sendbuf[i]);\n    printf(\"\\n\");\n\n    int recvcounts[size];\n    for (int i=0; i<size; i++)\n        recvcounts[i] = 1;\n\n    MPI_Reduce_scatter(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\n    printf(\"Proc %d: %d\\n\", rank, recvbuf);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "predicador37_1059", "code": "int main(int argc, char **argv) {\n    int rank, size, namelen, version, subversion, psize, n_esclavo;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Comm parent;\n    MPI_Status stat;\n\n    printf(\"[proceso %d] Proceso con identificador %d  de un total de  %d  en  %s  ejecutando  MPI  %d.%d\\n\", rank,\n           rank, size,\n           processor_name, version, subversion);\n\n\n\n    if (parent == MPI_COMM_NULL) {\n        printf(\"[proceso %d] Error: no se ha encontrado proceso padre.\\n\", rank);\n        exit(1);\n    }\n\n\n    if (psize != 1) {\n\n        printf(\"[proceso %d] Error:  el n\u00famero de padres deber\u00eda ser 1 y no %d.\\n\", rank, psize);\n        exit(2);\n    }\n\n    printf(\"PROCESO %d recibe n\u00famero de puntos por esclavo: %d\\n\", rank, n_esclavo);\n    double y_esclavo[n_esclavo], y[N+1];\n    int j=0;\n\n\n    int i=0;\n    double suma_A=0;\n    double suma_B=0;\n    printf(\"Recibiendo...\\n\");\n    for (i=0;i<n_esclavo/2;i++){\n        suma_A += y_esclavo[2*i];\n        suma_B += y_esclavo[2*i+1];\n        printf(\"PROCESO %d, F(i) es %f\\n\", rank, y_esclavo[i]);\n        printf(\"PROCESO %d suma A %f\\n\", rank, suma_A);\n        printf(\"PROCESO %d suma B %f\\n\", rank, suma_B);\n    }\n    double suma = 4 * suma_A + 2 * suma_B;\n\n    printf(\"PROCESO %d suma A %f\\n\", rank, suma_A);\n    printf(\"PROCESO %d suma B %f\\n\", rank, suma_B);\n    printf(\"PROCESO %d suma %f\\n\", rank, suma);\n\n\n    return 0;\n}\n", "label": "int main(int argc, char **argv) {\n    int rank, size, namelen, version, subversion, psize, n_esclavo;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Comm parent;\n    MPI_Status stat;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Get_processor_name(processor_name, &namelen);\n    MPI_Get_version(&version, &subversion);\n\n    printf(\"[proceso %d] Proceso con identificador %d  de un total de  %d  en  %s  ejecutando  MPI  %d.%d\\n\", rank,\n           rank, size,\n           processor_name, version, subversion);\n\n    MPI_Comm_get_parent(&parent);\n\n\n    if (parent == MPI_COMM_NULL) {\n        printf(\"[proceso %d] Error: no se ha encontrado proceso padre.\\n\", rank);\n        exit(1);\n    }\n\n    MPI_Comm_remote_size(parent, &psize);\n\n    if (psize != 1) {\n\n        printf(\"[proceso %d] Error:  el n\u00famero de padres deber\u00eda ser 1 y no %d.\\n\", rank, psize);\n        exit(2);\n    }\n\n    MPI_Bcast(&n_esclavo, 1, MPI_INT,0, parent);\n    printf(\"PROCESO %d recibe n\u00famero de puntos por esclavo: %d\\n\", rank, n_esclavo);\n    double y_esclavo[n_esclavo], y[N+1];\n    int j=0;\n\n    MPI_Scatter(NULL, n_esclavo, MPI_DOUBLE, y_esclavo, n_esclavo, MPI_DOUBLE,0, parent);\n\n    int i=0;\n    double suma_A=0;\n    double suma_B=0;\n    printf(\"Recibiendo...\\n\");\n    for (i=0;i<n_esclavo/2;i++){\n        suma_A += y_esclavo[2*i];\n        suma_B += y_esclavo[2*i+1];\n        printf(\"PROCESO %d, F(i) es %f\\n\", rank, y_esclavo[i]);\n        printf(\"PROCESO %d suma A %f\\n\", rank, suma_A);\n        printf(\"PROCESO %d suma B %f\\n\", rank, suma_B);\n    }\n    double suma = 4 * suma_A + 2 * suma_B;\n\n    printf(\"PROCESO %d suma A %f\\n\", rank, suma_A);\n    printf(\"PROCESO %d suma B %f\\n\", rank, suma_B);\n    printf(\"PROCESO %d suma %f\\n\", rank, suma);\n\n    MPI_Reduce(&suma, NULL, 1, MPI_DOUBLE, MPI_SUM, 0, parent);\n\n    MPI_Comm_disconnect(&parent);\n    MPI_Finalize();\n    return 0;\n}\n"}
{"program": "rpereira-dev_1060", "code": "int main(int argc, char **argv)\n{\n    int rank, nproc, tag, nvals_per_proc;\n\n\n\n    if (nproc == 1)\n    {\n        printf(\"Il faut au moins 2 processus MPI\\n\"); fflush(stdout);\n    }\n\n    tag = 1000;\n    nvals_per_proc = 100000;\n    if (rank == 0)\n    {\n        int isnd;\n        int dst_odd = 1;\n        int n_odd   = nproc / 2;\n        int         tab_snd[nvals_per_proc];\n        MPI_Request tab_req[n_odd];\n        int tab_vals[n_odd*nvals_per_proc];\n\n        for(isnd = 0 ; isnd < n_odd ; isnd++)\n        {\n            \n\n            int *isnd_arr = tab_vals+isnd*nvals_per_proc;\n            fill_val_array(dst_odd, isnd_arr, nvals_per_proc);\n\n            \n\n            printf(\"P0 initiates send values [%d, %d] to process P%d\\n\", isnd_arr[0], isnd_arr[nvals_per_proc-1], dst_odd);\n            dst_odd += 2;\n        }\n        \n\n    }\n    else if (rank % 2 == 1)\n    {\n        MPI_Request req;\n        int my_val_arr[nvals_per_proc];\n\n\n        check_val_array(rank, my_val_arr, nvals_per_proc);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, nproc, tag, nvals_per_proc;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    if (nproc == 1)\n    {\n        printf(\"Il faut au moins 2 processus MPI\\n\"); fflush(stdout);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    tag = 1000;\n    nvals_per_proc = 100000;\n    if (rank == 0)\n    {\n        int isnd;\n        int dst_odd = 1;\n        int n_odd   = nproc / 2;\n        int         tab_snd[nvals_per_proc];\n        MPI_Request tab_req[n_odd];\n        int tab_vals[n_odd*nvals_per_proc];\n\n        for(isnd = 0 ; isnd < n_odd ; isnd++)\n        {\n            \n\n            int *isnd_arr = tab_vals+isnd*nvals_per_proc;\n            fill_val_array(dst_odd, isnd_arr, nvals_per_proc);\n\n            \n\n            MPI_Isend(isnd_arr, nvals_per_proc, MPI_INT, dst_odd, tag, MPI_COMM_WORLD, tab_req+isnd);\n            printf(\"P0 initiates send values [%d, %d] to process P%d\\n\", isnd_arr[0], isnd_arr[nvals_per_proc-1], dst_odd);\n            dst_odd += 2;\n        }\n        \n\n        MPI_Waitall(n_odd, tab_req, MPI_STATUSES_IGNORE);\n    }\n    else if (rank % 2 == 1)\n    {\n        MPI_Request req;\n        int my_val_arr[nvals_per_proc];\n\n        MPI_Irecv(my_val_arr, nvals_per_proc, MPI_INT, 0, tag, MPI_COMM_WORLD, &req);\n        MPI_Wait(&req, MPI_STATUS_IGNORE);\n\n        check_val_array(rank, my_val_arr, nvals_per_proc);\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ebaty_1062", "code": "int main(int argc, char **argv) {\n\n\tint rank;\n\n\tint node_size;\n\n\t\n\n\tFILE *fp;\n\tfp = fopen(kFileName, \"w\");\n\tif ( rank == 0 ) {\n\t\tif ( fp == NULL ) {\n\t\t\tprintf(\"can't open %s.\\n\", kFileName);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tlong long i;\n\tfor(i = 4; i <= (1LL << 27); i <<= 1) {\n\t\tlong long size;\n\t\tdouble ave_time = 0;\n\n\t\tprintf(\"%s, %d\\n\", __FUNCTION__, i);\n\t\tint j;\n\t\tfor(j = 0; j < 10; ++j) {\n\t\t\tmpi_result result = sendData(i, node_size);\n\t\t\tave_time += result.time;\n\t\t\tsize = result.size;\n\t\t}\n\t\tave_time /= 10.0f;\n\n\t\tif ( rank == 0 ) {\n\t\t\tfprintf(fp, \"%lld\\t%lf\\n\", size, ave_time);\n\t\t}\n\t}\n\n\tif ( rank == 0 ) fclose(fp);\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv) {\n\tMPI_Init(&argc, &argv);\n\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint node_size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &node_size);\n\n\t\n\n\tFILE *fp;\n\tfp = fopen(kFileName, \"w\");\n\tif ( rank == 0 ) {\n\t\tif ( fp == NULL ) {\n\t\t\tprintf(\"can't open %s.\\n\", kFileName);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tlong long i;\n\tfor(i = 4; i <= (1LL << 27); i <<= 1) {\n\t\tlong long size;\n\t\tdouble ave_time = 0;\n\n\t\tprintf(\"%s, %d\\n\", __FUNCTION__, i);\n\t\tint j;\n\t\tfor(j = 0; j < 10; ++j) {\n\t\t\tmpi_result result = sendData(i, node_size);\n\t\t\tave_time += result.time;\n\t\t\tsize = result.size;\n\t\t}\n\t\tave_time /= 10.0f;\n\n\t\tif ( rank == 0 ) {\n\t\t\tfprintf(fp, \"%lld\\t%lf\\n\", size, ave_time);\n\t\t}\n\t}\n\n\tif ( rank == 0 ) fclose(fp);\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "MihawkHu_1063", "code": "int main(int argc, char* argv[]) {\r\n   int *loc_mat;\r\n   int n, loc_n, p, my_rank;\r\n   MPI_Comm comm;\r\n   MPI_Datatype blk_col_mpi_t;\r\n#  ifdef DEBUG\r\n   int i, j;\r\n#  endif\r\n\r\n   comm = MPI_COMM_WORLD;\r\n   \r\n   n = Read_n(my_rank, comm);\r\n   loc_n = n/p;\r\n   loc_mat = malloc(n*loc_n*sizeof(int));\r\n\r\n#  ifdef DEBUG\r\n   printf(\"Proc %d > p = %d, n = %d, loc_n = %d\\n\",\r\n         my_rank, p, n, loc_n);\r\n\r\n   \n\r\n   \n\r\n   for (i = 0; i < n; i++)\r\n      for (j = 0; j < loc_n; j++)\r\n         loc_mat[i*loc_n + j] = -1;\r\n#  endif   \r\n   \r\n   \n\r\n   blk_col_mpi_t = Build_blk_col_type(n, loc_n);\r\n\r\n   Read_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, comm);\r\n   Print_local_matrix(loc_mat, n, loc_n, my_rank);\r\n   Print_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, comm);\r\n\r\n   free(loc_mat);\r\n\r\n   \n\r\n\r\n   return 0;\r\n}", "label": "int main(int argc, char* argv[]) {\r\n   int *loc_mat;\r\n   int n, loc_n, p, my_rank;\r\n   MPI_Comm comm;\r\n   MPI_Datatype blk_col_mpi_t;\r\n#  ifdef DEBUG\r\n   int i, j;\r\n#  endif\r\n\r\n   MPI_Init(&argc, &argv);\r\n   comm = MPI_COMM_WORLD;\r\n   MPI_Comm_size(comm, &p);\r\n   MPI_Comm_rank(comm, &my_rank);\r\n   \r\n   n = Read_n(my_rank, comm);\r\n   loc_n = n/p;\r\n   loc_mat = malloc(n*loc_n*sizeof(int));\r\n\r\n#  ifdef DEBUG\r\n   printf(\"Proc %d > p = %d, n = %d, loc_n = %d\\n\",\r\n         my_rank, p, n, loc_n);\r\n\r\n   \n\r\n   \n\r\n   for (i = 0; i < n; i++)\r\n      for (j = 0; j < loc_n; j++)\r\n         loc_mat[i*loc_n + j] = -1;\r\n#  endif   \r\n   \r\n   \n\r\n   blk_col_mpi_t = Build_blk_col_type(n, loc_n);\r\n\r\n   Read_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, comm);\r\n   Print_local_matrix(loc_mat, n, loc_n, my_rank);\r\n   Print_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, comm);\r\n\r\n   free(loc_mat);\r\n\r\n   \n\r\n   MPI_Type_free(&blk_col_mpi_t);\r\n\r\n   MPI_Finalize();\r\n   return 0;\r\n}"}
{"program": "nerscadmin_1065", "code": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int dsize;\n  MPI_Comm newcomm;\n  MPI_Request request;\n  int newrank, newnprocs;\n\n\n\n  sbuf=(char*)malloc(SIZE*dsize);\n  rbuf=(char*)malloc(SIZE*dsize);\n\n  \n\n\n\n\n\n\n  if( (myrank%2) ) {\n  }\n\n \n  for( i=0; i<REPEAT; i++ ) \n    {\n\n    } \n\n  return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\n  int i, j;\n  int myrank, nprocs;\n  char *sbuf,  *rbuf;\n  int dsize;\n  MPI_Comm newcomm;\n  MPI_Request request;\n  int newrank, newnprocs;\n\n  MPI_Init( &argc, &argv );\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  MPI_Type_size(DATATYPE, &dsize);\n\n  sbuf=(char*)malloc(SIZE*dsize);\n  rbuf=(char*)malloc(SIZE*dsize);\n\n  \n\n  MPI_Comm_split( MPI_COMM_WORLD, myrank%2, myrank, &newcomm );\n  MPI_Comm_rank( newcomm, &newrank );\n  MPI_Comm_size( newcomm, &newnprocs );\n\n\n\n\n\n  if( (myrank%2) ) {\n      MPI_Isend(sbuf, SIZE, DATATYPE, (newrank+1)%newnprocs, 33, newcomm, &request);\n      MPI_Recv(rbuf, SIZE, DATATYPE, (newrank-1+newnprocs)%newnprocs, 33, newcomm, MPI_STATUS_IGNORE);\n      MPI_Wait(&request, MPI_STATUS_IGNORE);\n  }\n\n \n  for( i=0; i<REPEAT; i++ ) \n    {\n\n    } \n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "reimashi_1070", "code": "void main(int argc, char* argv[]) {\n\tif(argc >= 2){\n\t\t\n\n\t\tint numProcs;\n\t\tint hijos= atoi(argv[1]);\n\t\tint matriz[hijos][hijos];\n\t\tint matriz2[hijos][hijos];\n\t\tint vector[hijos];\n\n\t\tMPI_Comm intercom,intracom;\n\n\t  \n\t   MPI_Win res_ventana;\n\n\t\tif(numProcs!=1){\n\t\t\tprintf(\"numero de procesos incorrecto\\n\");\n\t\t\tMPI_Finalize;\n\t\t\texit(0);\n\t\t}\n\n\t\tprintf(\"numero de hijos: %d\\n\", hijos);\n\n\t\tfor (int i=0; i<hijos; i++)\n\t\t\tfor (int j=0; j<hijos; j++) {\n\t\t\t\tmatriz[i][j]=i+j;\n\t\t\t\tmatriz2[i][j]=+i+j;\n\t\t}\n\n\t\tprintf(\"Matriz:\\n\");\n\t\t for (i=0; i<hijos; i++) {\n\t\t\tprintf(\" \");\n\t\t\tfor (j=0; j<hijos; j++){\n\t\t\t\tprintf(\"%3d\",matriz[i][j]);\n\t\t\t\tprintf(\"\\n\");\n\t\t\t}\n\t\t}\n\n\n\t\t\n\t\tfor (i=0; i<hijos; i++) {\n\t   }\n\t   \n\t   printf(\"Resultado:\\n\");\n\t   for (i=0; i<hijos; i++){\n\t\t  printf(\"  %3d\\n\",vector[i]);\n\t\t  }\n\t   \n\t   \n   }\n   else{\n\t\t\n\n\t\tint result;\n\t\tint i,j,hijos;\n\t\t MPI_Win res_win;\n\t\tMPI_Comm intercom;\n\n\n\n\n\t\tint fila[hijos], vector[hijos];\n\n  \n\n\t\t\n\n\t   \n\n\t   result=0;\n\t   for (i=0;i<6;i++){\n\t\t  result += vector[i]*fila[i];\n\t\t}\n\t   \n\t   \n\t   \n\n\n\t\n\t\t\n\n   \n\t\t\n\t\t\n   }\n\n}", "label": "void main(int argc, char* argv[]) {\n\tif(argc >= 2){\n\t\t\n\n\t\tint numProcs;\n\t\tint hijos= atoi(argv[1]);\n\t\tint matriz[hijos][hijos];\n\t\tint matriz2[hijos][hijos];\n\t\tint vector[hijos];\n\n\t\tMPI_Comm intercom,intracom;\n\n\t\tMPI_Init(&argc, &argv);\n\t\tMPI_Comm_size(MPI_COMM_WORLD,&numProcs);\n\t  \n\t   MPI_Win res_ventana;\n\n\t\tif(numProcs!=1){\n\t\t\tprintf(\"numero de procesos incorrecto\\n\");\n\t\t\tMPI_Finalize;\n\t\t\texit(0);\n\t\t}\n\n\t\tprintf(\"numero de hijos: %d\\n\", hijos);\n\n\t\tfor (int i=0; i<hijos; i++)\n\t\t\tfor (int j=0; j<hijos; j++) {\n\t\t\t\tmatriz[i][j]=i+j;\n\t\t\t\tmatriz2[i][j]=+i+j;\n\t\t}\n\n\t\tprintf(\"Matriz:\\n\");\n\t\t for (i=0; i<hijos; i++) {\n\t\t\tprintf(\" \");\n\t\t\tfor (j=0; j<hijos; j++){\n\t\t\t\tprintf(\"%3d\",matriz[i][j]);\n\t\t\t\tprintf(\"\\n\");\n\t\t\t}\n\t\t}\n\n\t\tMPI_Comm_spawn(\"p4\",MPI_ARGV_NULL,hijos,MPI_INFO_NULL,0,MPI_COMM_WORLD,&intercom,MPI_ERRCODES_IGNORE);\n\t\tMPI_Intercomm_merge(intercom, 0, &intracom);\n\n\t\tMPI_Win_create(&vector[0], hijos*sizeof(int), 1, MPI_INFO_NULL, intracom, &res_ventana);\n\t\t\n\t\tfor (i=0; i<hijos; i++) {\n\t\t  MPI_Send(&matriz[i][0],hijos,MPI_INT,i,TAG_MATRIZ,intercom);\n\t   }\n\t   \n\t   MPI_Win_fence(0, res_ventana);\n\t   printf(\"Resultado:\\n\");\n\t   MPI_Win_fence(0, res_ventana);\n\t   for (i=0; i<hijos; i++){\n\t\t  printf(\"  %3d\\n\",vector[i]);\n\t\t  }\n\t   \n\t   MPI_Win_free(&res_ventana);\n\t   \n\t   MPI_Finalize();\n   }\n   else{\n\t\t\n\n\t\tint result;\n\t\tint i,j,hijos;\n\t\t MPI_Win res_win;\n\t\tMPI_Comm intercom;\n\n\t   MPI_Init(&argc, &argv);\n\n\t   MPI_Comm_get_parent(&intercom);\n\n\t    MPI_Intercomm_merge(intercom, 0, &intracom);\n\n\t\tMPI_Comm_size(intercom, &hijos);\n\t\tint fila[hijos], vector[hijos];\n\n  \n\t   MPI_Win_create(MPI_BOTTOM, 0, 1, MPI_INFO_NULL, intracom, &res_win);\n\n\t   MPI_Recv(fila,hijos,MPI_INT,MPI_ANY_SOURCE,TAG_MATRIZ,intercom,MPI_STATUS_IGNORE);\n\t   MPI_Recv(vector,hijos,MPI_INT,MPI_ANY_SOURCE,TAG_VECTOR,intercom,MPI_STATUS_IGNORE);\n\t\t\n\n\t   \n\n\t   result=0;\n\t   for (i=0;i<6;i++){\n\t\t  result += vector[i]*fila[i];\n\t\t}\n\t   \n\t   \n\t   \n\n\n\t   MPI_Win_fence(0, res_win);\n\t\n\t\t\n\n\t   MPI_Win_fence(0, res_win);\n   \n   MPI_Win_free(&res_win);\n\t\t\n\t\t\n\t\tMPI_Finalize();\n   }\n\n}"}
{"program": "lapesd_1072", "code": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing AFIO POSIX test...\\n\", stdout);\n  ctx_init ();\n\n  if (ctx.num_procs > MAX_PROCESSES)\n    {\n      MPI_Comm new_comm;\n      MPI_Group grp, new_grp;\n      int range[3] =\n\t{ 0, MAX_PROCESSES - 1, 1 };\n      ctx.comm = new_comm;\n    }\n\n  if (ctx.comm != MPI_COMM_NULL)\n    {\n      const iore_afio_vtable_t *backend = afio_pool[IORE_AFIO_POSIX];\n      iore_file_t file =\n\t{ };\n      file.name = TEST_FILE_NAME;\n\n      const int num_tests = 9;\n      int i;\n      int rerr;\n      for (i = 1; i <= num_tests; i++)\n\t{\n\t  switch (i)\n\t    {\n\t    case 1:\n\t      rerr = test_wr_oset01 (backend, file);\n\t      break;\n\t    case 2:\n\t      rerr = test_wr_oset02 (backend, file);\n\t      break;\n\t    case 3:\n\t      rerr = test_wr_oset03 (backend, file);\n\t      break;\n\t    case 4:\n\t      rerr = test_wr_oset04 (backend, file);\n\t      break;\n\t    case 5:\n\t      rerr = test_wr_dset01 (backend, file);\n\t      break;\n\t    case 6:\n\t      rerr = test_wr_dset02 (backend, file);\n\t      break;\n\t    case 7:\n\t      rerr = test_wr_dset03 (backend, file);\n\t      break;\n\t    case 8:\n\t      rerr = test_wr_dset04 (backend, file);\n\t      break;\n\t    case 9:\n\t      rerr = test_to_str ();\n\t      break;\n\t    }\n\t  if (rerr)\n\t    fprintf (stdout, \"[Task %d] Test %d: ...FAIL!\\n\", ctx.task_id, i);\n\t  else\n\t    fprintf (stdout, \"[Task %d] Test %d: ...SUCCESS!\\n\", ctx.task_id,\n\t\t     i);\n\t}\n    }\n\n  fputs (\"Finalizing AFIO POSIX test.\\n\", stdout);\n}", "label": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing AFIO POSIX test...\\n\", stdout);\n  MPI_Init (&argc, &argv);\n  ctx_init ();\n\n  if (ctx.num_procs > MAX_PROCESSES)\n    {\n      MPI_Comm new_comm;\n      MPI_Group grp, new_grp;\n      int range[3] =\n\t{ 0, MAX_PROCESSES - 1, 1 };\n      MPI_Comm_group (MPI_COMM_WORLD, &grp);\n      MPI_Group_range_incl (grp, 1, &range, &new_grp);\n      MPI_Comm_create (MPI_COMM_WORLD, new_grp, &new_comm);\n      ctx.comm = new_comm;\n    }\n\n  if (ctx.comm != MPI_COMM_NULL)\n    {\n      const iore_afio_vtable_t *backend = afio_pool[IORE_AFIO_POSIX];\n      iore_file_t file =\n\t{ };\n      file.name = TEST_FILE_NAME;\n\n      const int num_tests = 9;\n      int i;\n      int rerr;\n      for (i = 1; i <= num_tests; i++)\n\t{\n\t  switch (i)\n\t    {\n\t    case 1:\n\t      rerr = test_wr_oset01 (backend, file);\n\t      break;\n\t    case 2:\n\t      rerr = test_wr_oset02 (backend, file);\n\t      break;\n\t    case 3:\n\t      rerr = test_wr_oset03 (backend, file);\n\t      break;\n\t    case 4:\n\t      rerr = test_wr_oset04 (backend, file);\n\t      break;\n\t    case 5:\n\t      rerr = test_wr_dset01 (backend, file);\n\t      break;\n\t    case 6:\n\t      rerr = test_wr_dset02 (backend, file);\n\t      break;\n\t    case 7:\n\t      rerr = test_wr_dset03 (backend, file);\n\t      break;\n\t    case 8:\n\t      rerr = test_wr_dset04 (backend, file);\n\t      break;\n\t    case 9:\n\t      rerr = test_to_str ();\n\t      break;\n\t    }\n\t  if (rerr)\n\t    fprintf (stdout, \"[Task %d] Test %d: ...FAIL!\\n\", ctx.task_id, i);\n\t  else\n\t    fprintf (stdout, \"[Task %d] Test %d: ...SUCCESS!\\n\", ctx.task_id,\n\t\t     i);\n\t}\n    }\n\n  MPI_Finalize ();\n  fputs (\"Finalizing AFIO POSIX test.\\n\", stdout);\n}"}
{"program": "cbries_1074", "code": "t main (int argc, char **argv)\n{\n\tint i, m;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\tdouble messungTime = 0.0;\n\tdouble messungTimeSumme = 0.0;\n\tint datalen = 0;\n\tchar *data = NULL;\n\tchar *localData = NULL;\n\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tif(argc<2) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Usage: hw10b bytes\\n\");\t\n\t\t}\n\t\treturn 1;\n\t}\n\n\tsrand(time(NULL));\n\n\t\n\n\tdatalen = atoi(argv[1]);\n\t\n\tif(datalen%nprocs > 0) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Count of processes should divide datalen.\\n\");\n\t\t}\n\t\treturn 1;\n\t}\n\n\tm = datalen/nprocs;\n\n\tdata = (char*) malloc(datalen*sizeof(char));\n\tlocalData = (char*) malloc(m*sizeof(char));\n\tfor(i=0; i<m; i++) {\n\t\tlocalData[i] = rand()%('Z'-'A')+'A';\n\t}\n\n\n\tif(myrank==0) {\n\t\tprintf(\"data:\");\n\t\tfor(i=0; i<datalen; i++) {\n\t\t\tprintf(\"%c\", data[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\t}\n\n\t\n\n\n\treturn 0;\n}\n/*", "label": "t main (int argc, char **argv)\n{\n\tint i, m;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\tdouble messungTime = 0.0;\n\tdouble messungTimeSumme = 0.0;\n\tint datalen = 0;\n\tchar *data = NULL;\n\tchar *localData = NULL;\n\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tif(argc<2) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Usage: hw10b bytes\\n\");\t\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\tsrand(time(NULL));\n\n\t\n\n\tdatalen = atoi(argv[1]);\n\t\n\tif(datalen%nprocs > 0) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Count of processes should divide datalen.\\n\");\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\tm = datalen/nprocs;\n\n\tdata = (char*) malloc(datalen*sizeof(char));\n\tlocalData = (char*) malloc(m*sizeof(char));\n\tfor(i=0; i<m; i++) {\n\t\tlocalData[i] = rand()%('Z'-'A')+'A';\n\t}\n\n\tMPI_Allgather(localData, m, MPI_BYTE, data, m, MPI_BYTE, MPI_COMM_WORLD);\n\n\tif(myrank==0) {\n\t\tprintf(\"data:\");\n\t\tfor(i=0; i<datalen; i++) {\n\t\t\tprintf(\"%c\", data[i]);\n\t\t}\n\t\tprintf(\"\\n\");\n\t}\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n/*"}
{"program": "etmc_1076", "code": "int main(int argc, char **argv)\n{\n  MPI_File fp;\n\n  LemonWriter *w;\n  LemonReader *r;\n  LemonRecordHeader *h;\n\n  char *data;\n  char *data_read;\n  int mpisize;\n  int rank;\n  char const *type;\n\n  int ME_flag=1, MB_flag=1, status=0;\n\n  int latDist[] = {0, 0, 0, 0};\n  int periods[] = {1, 1, 1, 1};\n  \n\n  int latSizes[] = {4, 4, 4, 4};\n  int latVol = 256;\n\n  MPI_Comm cartesian;\n  int i;\n\n\n  for (i = 0; i < 4; ++i)\n    latSizes[i] *= latDist[i];\n  latVol *= mpisize;\n\n   \n\n  w = lemonCreateWriter(&fp, cartesian);\n\n  data = (char*)malloc(257);\n  for (i = 0; i < latVol/mpisize; ++i)\n    data[i] = (char)(rank + 48);\n  data[256] = '\\0';\n\n  h = lemonCreateHeader(MB_flag, ME_flag, \"parallel-test\", latVol);\n  status = lemonWriteRecordHeader(h, w);\n\n  lemonDestroyHeader(h);\n\n  lemonWriteLatticeParallelNonBlocking(w, data, sizeof(char), latSizes);\n  lemonFinishWriting(w);\n\n  lemonWriterCloseRecord(w);\n  lemonDestroyWriter(w);\n\n  \n\n  data_read = (char*)malloc(257);\n\n  r = lemonCreateReader(&fp, cartesian);\n\n  if (lemonReaderNextRecord(r))\n    fprintf(stderr, \"Node %d reports: next record failed.\\n\", rank);\n\n  type = lemonReaderType(r);\n  if (strncmp(type, \"parallel-test\", 13))\n    fprintf(stderr, \"Node %d reports: wrong type read.\\n\", rank);\n\n  lemonReadLatticeParallelNonBlocking(r, data_read, sizeof(char), latSizes);\n  lemonFinishReading(r);\n  data_read[256] = '\\0';\n  if (strncmp(data_read, data, 256))\n  {\n    fprintf(stderr, \"Node %d reports: wrong data read.\\n\", rank);\n    fprintf(stderr, \"Node %d wanted: %s\\n\", rank, data);\n    fprintf(stderr, \"Node %d got   : %s\\n\", rank, data_read);\n  }\n  else\n    fprintf(stderr, \"Node %d reports data okay.\\n\", rank);\n\n  lemonDestroyReader(r);\n\n\n  free(data);\n  free(data_read);\n\n  return(0);\n}", "label": "int main(int argc, char **argv)\n{\n  MPI_File fp;\n\n  LemonWriter *w;\n  LemonReader *r;\n  LemonRecordHeader *h;\n\n  char *data;\n  char *data_read;\n  int mpisize;\n  int rank;\n  char const *type;\n\n  int ME_flag=1, MB_flag=1, status=0;\n\n  int latDist[] = {0, 0, 0, 0};\n  int periods[] = {1, 1, 1, 1};\n  \n\n  int latSizes[] = {4, 4, 4, 4};\n  int latVol = 256;\n\n  MPI_Comm cartesian;\n  int i;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpisize);\n\n  MPI_Dims_create(mpisize, 4, latDist);\n  MPI_Cart_create(MPI_COMM_WORLD, 4, latDist, periods, 1, &cartesian);\n  for (i = 0; i < 4; ++i)\n    latSizes[i] *= latDist[i];\n  latVol *= mpisize;\n  MPI_Comm_rank(cartesian, &rank);\n\n   \n\n  MPI_File_open(cartesian, \"parallel_non_blocking.test\", MPI_MODE_WRONLY | MPI_MODE_CREATE, MPI_INFO_NULL, &fp);\n  w = lemonCreateWriter(&fp, cartesian);\n\n  data = (char*)malloc(257);\n  for (i = 0; i < latVol/mpisize; ++i)\n    data[i] = (char)(rank + 48);\n  data[256] = '\\0';\n\n  h = lemonCreateHeader(MB_flag, ME_flag, \"parallel-test\", latVol);\n  status = lemonWriteRecordHeader(h, w);\n\n  lemonDestroyHeader(h);\n\n  lemonWriteLatticeParallelNonBlocking(w, data, sizeof(char), latSizes);\n  lemonFinishWriting(w);\n\n  lemonWriterCloseRecord(w);\n  lemonDestroyWriter(w);\n  MPI_File_close(&fp);\n\n  \n\n  data_read = (char*)malloc(257);\n\n  MPI_File_open(cartesian, \"parallel_non_blocking.test\", MPI_MODE_RDONLY, MPI_INFO_NULL, &fp);\n  r = lemonCreateReader(&fp, cartesian);\n\n  if (lemonReaderNextRecord(r))\n    fprintf(stderr, \"Node %d reports: next record failed.\\n\", rank);\n\n  type = lemonReaderType(r);\n  if (strncmp(type, \"parallel-test\", 13))\n    fprintf(stderr, \"Node %d reports: wrong type read.\\n\", rank);\n\n  lemonReadLatticeParallelNonBlocking(r, data_read, sizeof(char), latSizes);\n  lemonFinishReading(r);\n  data_read[256] = '\\0';\n  if (strncmp(data_read, data, 256))\n  {\n    fprintf(stderr, \"Node %d reports: wrong data read.\\n\", rank);\n    fprintf(stderr, \"Node %d wanted: %s\\n\", rank, data);\n    fprintf(stderr, \"Node %d got   : %s\\n\", rank, data_read);\n  }\n  else\n    fprintf(stderr, \"Node %d reports data okay.\\n\", rank);\n\n  lemonDestroyReader(r);\n\n  MPI_File_close(&fp);\n  MPI_Finalize();\n\n  free(data);\n  free(data_read);\n\n  return(0);\n}"}
{"program": "danielpeter_1077", "code": "int main(int argc, char **argv)\n{\n\n    \n\n\n    \n\n    FTI_Init(argv[3], MPI_COMM_WORLD);\n\n    int rank, nbProc, steps, i;\n    unsigned long gridSize, j;\n    float *grid, ranDif, res = 1;\n    double t;\n\n    \n\n    srand(time(NULL)*rank);\n\n    \n\n    gridSize = atoi(argv[1])*262150; \n\n    steps = atoi(argv[2]);\n    grid = malloc(gridSize * sizeof(float));\n    float memsize = (float) gridSize*sizeof(float);\n    memsize = memsize /(1024*1024);\n    if (rank == 0) printf(\"Gridsize : %ldK and memory size : %fMB \\n\", gridSize/1000, memsize);\n\n    \n\n    if (rank == 0) printf(\"Initializing vector...\\n\");\n    grid[0] = 300.0;\n    for (j = 1; j < gridSize; j++) {\n        ranDif = ((rand() % 10) + 1)/10;\n        grid[j] = grid[j-1] + ((rand()%3)-1)*ranDif;\n    }\n\n    \n\n    FTI_Protect(0, &i, 1*sizeof(int));\n    FTI_Protect(1, &res, 1*sizeof(float));\n    FTI_Protect(2, grid, gridSize*sizeof(float));\n\n    \n\n    t =\n    for (i = 0; i < steps; i++) {\n\n        \n\n        FTI_Snapshot();\n\n        \n\n        res = compute(grid, gridSize, rank);\n\n        \n\n        if (rank == 0 && i%10 == 0) printf(\"Step : %d and res : %f \\n\", i, res);\n\n    }\n    \n\n    if (rank == 0) printf(\"Final result = %f computed in %f seconds\\n\", res,\n\n    \n\n    free(grid);\n\n    FTI_Finalize();\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    FTI_Init(argv[3], MPI_COMM_WORLD);\n\n    int rank, nbProc, steps, i;\n    unsigned long gridSize, j;\n    float *grid, ranDif, res = 1;\n    double t;\n\n    \n\n    MPI_Comm_rank(FTI_COMM_WORLD, &rank);\n    MPI_Comm_size(FTI_COMM_WORLD, &nbProc);\n    srand(time(NULL)*rank);\n\n    \n\n    gridSize = atoi(argv[1])*262150; \n\n    steps = atoi(argv[2]);\n    grid = malloc(gridSize * sizeof(float));\n    float memsize = (float) gridSize*sizeof(float);\n    memsize = memsize /(1024*1024);\n    if (rank == 0) printf(\"Gridsize : %ldK and memory size : %fMB \\n\", gridSize/1000, memsize);\n\n    \n\n    if (rank == 0) printf(\"Initializing vector...\\n\");\n    grid[0] = 300.0;\n    for (j = 1; j < gridSize; j++) {\n        ranDif = ((rand() % 10) + 1)/10;\n        grid[j] = grid[j-1] + ((rand()%3)-1)*ranDif;\n    }\n\n    \n\n    FTI_Protect(0, &i, 1*sizeof(int));\n    FTI_Protect(1, &res, 1*sizeof(float));\n    FTI_Protect(2, grid, gridSize*sizeof(float));\n\n    \n\n    t = MPI_Wtime();\n    for (i = 0; i < steps; i++) {\n\n        \n\n        FTI_Snapshot();\n\n        \n\n        res = compute(grid, gridSize, rank);\n\n        \n\n        if (rank == 0 && i%10 == 0) printf(\"Step : %d and res : %f \\n\", i, res);\n\n    }\n    \n\n    if (rank == 0) printf(\"Final result = %f computed in %f seconds\\n\", res, MPI_Wtime() - t);\n\n    \n\n    free(grid);\n\n    FTI_Finalize();\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "qingu_1078", "code": "int main(int argc, char **argv)\n{\n    int rank, size, ret, i, num_cpus = -1;\n    char str[STRLEN], foo[STRLEN];\n    cpu_set_t mask;\n\n\n    while (++argv && *argv) {\n        if (!strcmp(argv[0], \"--num-cpus\")) {\n            num_cpus = atoi(argv[1]);\n            argv += 2;\n        }\n        else if (!strcmp(argv[0], \"--help\") || !strcmp(argv[0], \"-help\") ||\n                 !strcmp(argv[0], \"-h\")) {\n            fprintf(stderr, \"Usage: ./binding {--num-cpus [CPUs]}\\n\");\n        }\n    }\n    if (num_cpus < 0)\n        num_cpus = NUM_CPUS;\n\n    CPU_ZERO(&mask);\n    ret = sched_getaffinity(getpid(), sizeof(cpu_set_t), &mask);\n    if (ret < 0) {\n        fprintf(stderr, \"sched_getaffinity failure\\n\");\n    }\n\n    snprintf(foo, STRLEN, \"[%d] \", rank);\n    for (i = 0; i < num_cpus; i++) {\n        snprintf(str, STRLEN, \"%s %d \", foo, CPU_ISSET(i, &mask));\n        snprintf(foo, STRLEN, \"%s\", str);\n    }\n    fprintf(stderr, \"%s\\n\", str);\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, ret, i, num_cpus = -1;\n    char str[STRLEN], foo[STRLEN];\n    cpu_set_t mask;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    while (++argv && *argv) {\n        if (!strcmp(argv[0], \"--num-cpus\")) {\n            num_cpus = atoi(argv[1]);\n            argv += 2;\n        }\n        else if (!strcmp(argv[0], \"--help\") || !strcmp(argv[0], \"-help\") ||\n                 !strcmp(argv[0], \"-h\")) {\n            fprintf(stderr, \"Usage: ./binding {--num-cpus [CPUs]}\\n\");\n            MPI_Abort(MPI_COMM_WORLD, -1);\n        }\n    }\n    if (num_cpus < 0)\n        num_cpus = NUM_CPUS;\n\n    CPU_ZERO(&mask);\n    ret = sched_getaffinity(getpid(), sizeof(cpu_set_t), &mask);\n    if (ret < 0) {\n        fprintf(stderr, \"sched_getaffinity failure\\n\");\n        MPI_Abort(MPI_COMM_WORLD, -1);\n    }\n\n    snprintf(foo, STRLEN, \"[%d] \", rank);\n    for (i = 0; i < num_cpus; i++) {\n        snprintf(str, STRLEN, \"%s %d \", foo, CPU_ISSET(i, &mask));\n        snprintf(foo, STRLEN, \"%s\", str);\n    }\n    fprintf(stderr, \"%s\\n\", str);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "syftalent_1082", "code": "nt main(int argc, char **argv)\n{\n    int i, num_posted, num_completed;\n    int wrank, wsize;\n    unsigned int seed = 0x10bc;\n    unsigned int post_seq, complete_seq;\n    struct laundry larr[WINDOW];\n    MPI_Request reqs[WINDOW];\n    int outcount;\n    int indices[WINDOW];\n    MPI_Comm comms[NUM_COMMS];\n    MPI_Comm comm;\n\n\n    \n\n    post_seq = complete_seq = gen_prn(seed);\n\n    num_completed = 0;\n    num_posted = 0;\n\n    \n\n    for (i = 0; i < NUM_COMMS; ++i) {\n    }\n\n    \n\n    for (i = 0; i < WINDOW; ++i) {\n        reqs[i] = MPI_REQUEST_NULL;\n        memset(&larr[i], 0, sizeof(struct laundry));\n        larr[i].case_num = -1;\n\n        \n\n        comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n\n        start_random_nonblocking(comm, post_seq, &reqs[i], &larr[i]);\n        ++num_posted;\n        post_seq = gen_prn(post_seq);\n    }\n\n    \n\n    while (num_completed < MAIN_ITERATIONS) {\n        complete_something_somehow(complete_seq, WINDOW, reqs, &outcount, indices);\n        complete_seq = gen_prn(complete_seq);\n        for (i = 0; i < outcount; ++i) {\n            int idx = indices[i];\n            assert(reqs[idx] == MPI_REQUEST_NULL);\n            if (larr[idx].case_num != -1) {\n                check_after_completion(&larr[idx]);\n                cleanup_laundry(&larr[idx]);\n                ++num_completed;\n                if (num_posted < MAIN_ITERATIONS) {\n                    comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n                    start_random_nonblocking(comm, post_seq, &reqs[idx], &larr[idx]);\n                    ++num_posted;\n                    post_seq = gen_prn(post_seq);\n                }\n            }\n        }\n\n        \n\n        if (0 == rand_range(gen_prn(complete_seq + wrank), 0, CHANCE_OF_SLEEP)) {\n            usleep(JITTER_DELAY); \n\n        }\n    }\n\n    for (i = 0; i < NUM_COMMS; ++i) {\n    }\n\n    if (wrank == 0) {\n        if (errs)\n            printf(\"found %d errors\\n\", errs);\n        else\n            printf(\" No errors\\n\");\n    }\n\n\n    return 0;\n}\n\n", "label": "nt main(int argc, char **argv)\n{\n    int i, num_posted, num_completed;\n    int wrank, wsize;\n    unsigned int seed = 0x10bc;\n    unsigned int post_seq, complete_seq;\n    struct laundry larr[WINDOW];\n    MPI_Request reqs[WINDOW];\n    int outcount;\n    int indices[WINDOW];\n    MPI_Comm comms[NUM_COMMS];\n    MPI_Comm comm;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &wrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &wsize);\n\n    \n\n    post_seq = complete_seq = gen_prn(seed);\n\n    num_completed = 0;\n    num_posted = 0;\n\n    \n\n    for (i = 0; i < NUM_COMMS; ++i) {\n        MPI_Comm_dup(MPI_COMM_WORLD, &comms[i]);\n    }\n\n    \n\n    for (i = 0; i < WINDOW; ++i) {\n        reqs[i] = MPI_REQUEST_NULL;\n        memset(&larr[i], 0, sizeof(struct laundry));\n        larr[i].case_num = -1;\n\n        \n\n        comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n\n        start_random_nonblocking(comm, post_seq, &reqs[i], &larr[i]);\n        ++num_posted;\n        post_seq = gen_prn(post_seq);\n    }\n\n    \n\n    while (num_completed < MAIN_ITERATIONS) {\n        complete_something_somehow(complete_seq, WINDOW, reqs, &outcount, indices);\n        complete_seq = gen_prn(complete_seq);\n        for (i = 0; i < outcount; ++i) {\n            int idx = indices[i];\n            assert(reqs[idx] == MPI_REQUEST_NULL);\n            if (larr[idx].case_num != -1) {\n                check_after_completion(&larr[idx]);\n                cleanup_laundry(&larr[idx]);\n                ++num_completed;\n                if (num_posted < MAIN_ITERATIONS) {\n                    comm = comms[rand_range(gen_prn(post_seq), 0, NUM_COMMS)];\n                    start_random_nonblocking(comm, post_seq, &reqs[idx], &larr[idx]);\n                    ++num_posted;\n                    post_seq = gen_prn(post_seq);\n                }\n            }\n        }\n\n        \n\n        if (0 == rand_range(gen_prn(complete_seq + wrank), 0, CHANCE_OF_SLEEP)) {\n            usleep(JITTER_DELAY); \n\n        }\n    }\n\n    for (i = 0; i < NUM_COMMS; ++i) {\n        MPI_Comm_free(&comms[i]);\n    }\n\n    if (wrank == 0) {\n        if (errs)\n            printf(\"found %d errors\\n\", errs);\n        else\n            printf(\" No errors\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}\n\n"}
{"program": "brian-o_1083", "code": "int main(int argc, char* argv[])\n{\n    int i;\n    int id;\n    int size;\n\n    char s[100];\n    int accum=0;\n    double mytime;\n    char name[100];\n    int  leng=10;\n\n    \n\n\n\n    \n\n\n    \n\n\n    \n\n    sprintf(s,\"sleep %d\",id);\n\n\n    \n\n    printf(\"running on %s\\n\",name);\n\n    printf(\"hello world in %d size %d\\n\",id,size);\n\n    \n\n    for (i=0; i< argc; i++ )\n    {printf(\"%s \\n\",argv[i]);}\n\n    printf(\"hello world out %d\\n\",id);\n\n\n    mytime=\n\n    \n\n\n\n    \n\n\n    \n\n\n    {printf (\"%d says time is %f\\n\",id,mytime);}\n\n    {printf (\"%d says accumulation is %d\\n\",id,accum);}\n\n}", "label": "int main(int argc, char* argv[])\n{\n    int i;\n    int id;\n    int size;\n\n    char s[100];\n    int accum=0;\n    double mytime;\n    char name[100];\n    int  leng=10;\n\n    \n\n\n    MPI_Init(&argc,&argv);\n\n    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    \n\n    sprintf(s,\"sleep %d\",id);\n\n\n    \n\n    MPI_Get_processor_name(name,&leng);\n    printf(\"running on %s\\n\",name);\n\n    printf(\"hello world in %d size %d\\n\",id,size);\n\n    \n\n    for (i=0; i< argc; i++ )\n    {printf(\"%s \\n\",argv[i]);}\n\n    printf(\"hello world out %d\\n\",id);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    mytime=-MPI_Wtime();\n\n    \n\n\n\n    \n\n    MPI_Reduce(&id,&accum,1,MPI_INT,MPI_SUM,3,MPI_COMM_WORLD);\n\n    \n\n    mytime+=MPI_Wtime();\n\n    {printf (\"%d says time is %f\\n\",id,mytime);}\n\n    {printf (\"%d says accumulation is %d\\n\",id,accum);}\n\n    MPI_Finalize();\n}"}
{"program": "mtholder_1085", "code": "int main (int argc, char *argv[])\n{ \n  printf(\"\\nThis is RAxML FINE-GRAIN MPI Process Number: %d\\n\", processID);   \n  {\n    tree  *tr = (tree*)malloc(sizeof(tree));\n  \n    analdef *adef = (analdef*)malloc(sizeof(analdef));   \n\n    \n\n    \n#if ! (defined(__ppc) || defined(__powerpc__) || defined(PPC))\n    _mm_setcsr( _mm_getcsr() | _MM_FLUSH_ZERO_ON);\n#endif   \n\n  \n\n   \n    masterTime = gettime();         \n    \n  \n\n    \n    initAdef(adef);\n\n  \n\n  \n    get_args(argc, argv, adef, tr); \n  \n  \n\n\n    if(processID == 0)\n      makeFileNames();\n\n    initializeTree(tr, adef);                               \n    \n    if(processID == 0)  \n      {\n\tprintModelAndProgramInfo(tr, adef, argc, argv);  \n\tprintBothOpen(\"Memory Saving Option: %s\\n\", (tr->saveMemory == TRUE)?\"ENABLED\":\"DISABLED\");   \t             \n      }  \n                         \n    \n\n  \n   \n\n    if(adef->useCheckpoint)\n      {\n#ifdef _BAYESIAN\n\tassert(0);\n#endif\n      \n\t\n\n\trestart(tr);       \t\n\t\n\t\n\n\tcomputeBIGRAPID(tr, adef, TRUE); \n      }\n    else\n      {\n\t\n\n\t\n\tif(processID == 0)\n\t  accumulatedTime = 0.0;\n\t\n\t\n\n\t\n\t\n\tswitch(tr->startingTree)\n\t  {\n\t  case randomTree:\n\t    assert(0);\n\t    pllMakeRandomTree(tr);\n\t    break;\n\t  case givenTree:\n\t    getStartingTree(tr);     \n\t    break;\n\t  case parsimonyTree:\t     \n\t    \n\n\t    assert(0);\n\t    allocateParsimonyDataStructures(tr);\n\t    pllMakeParsimonyTreeFast(tr);\n\t    pllFreeParsimonyDataStructures(tr);\n\t  break;\n\tdefault:\n\t  assert(0);\n\t}\n\n\t\n    \n      \n\t\n\n      \n\tevaluateGeneric(tr, tr->start, TRUE);\t\n\t\n\t\n\n      \t\n\ttreeEvaluate(tr, 1); \n\n\t\n\n      \n#ifdef _BAYESIAN \n\tif(adef->bayesian)\n\t  {\n\t    \n\n\t    \n\t    allocateParsimonyDataStructures(tr);\n\t    mcmc(tr, adef);\n\t    pllFreeParsimonyDataStructures(tr);\n\t  }\n\telse\n#endif\t \n\t  computeBIGRAPID(tr, adef, TRUE); \t\t\t     \n      }            \n      \n    \n\n  \n    if(processID == 0)\n      finalizeInfoFile(tr, adef);\n  }\n  \n\n\n\n  return 0;\n}", "label": "int main (int argc, char *argv[])\n{ \n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &processID);\n  MPI_Comm_size(MPI_COMM_WORLD, &processes);\n  printf(\"\\nThis is RAxML FINE-GRAIN MPI Process Number: %d\\n\", processID);   \n  MPI_Barrier(MPI_COMM_WORLD);\n  {\n    tree  *tr = (tree*)malloc(sizeof(tree));\n  \n    analdef *adef = (analdef*)malloc(sizeof(analdef));   \n\n    \n\n    \n#if ! (defined(__ppc) || defined(__powerpc__) || defined(PPC))\n    _mm_setcsr( _mm_getcsr() | _MM_FLUSH_ZERO_ON);\n#endif   \n\n  \n\n   \n    masterTime = gettime();         \n    \n  \n\n    \n    initAdef(adef);\n\n  \n\n  \n    get_args(argc, argv, adef, tr); \n  \n  \n\n\n    if(processID == 0)\n      makeFileNames();\n\n    initializeTree(tr, adef);                               \n    \n    if(processID == 0)  \n      {\n\tprintModelAndProgramInfo(tr, adef, argc, argv);  \n\tprintBothOpen(\"Memory Saving Option: %s\\n\", (tr->saveMemory == TRUE)?\"ENABLED\":\"DISABLED\");   \t             \n      }  \n                         \n    \n\n  \n   \n\n    if(adef->useCheckpoint)\n      {\n#ifdef _BAYESIAN\n\tassert(0);\n#endif\n      \n\t\n\n\trestart(tr);       \t\n\t\n\t\n\n\tcomputeBIGRAPID(tr, adef, TRUE); \n      }\n    else\n      {\n\t\n\n\t\n\tif(processID == 0)\n\t  accumulatedTime = 0.0;\n\t\n\t\n\n\t\n\t\n\tswitch(tr->startingTree)\n\t  {\n\t  case randomTree:\n\t    assert(0);\n\t    pllMakeRandomTree(tr);\n\t    break;\n\t  case givenTree:\n\t    getStartingTree(tr);     \n\t    break;\n\t  case parsimonyTree:\t     \n\t    \n\n\t    assert(0);\n\t    allocateParsimonyDataStructures(tr);\n\t    pllMakeParsimonyTreeFast(tr);\n\t    pllFreeParsimonyDataStructures(tr);\n\t  break;\n\tdefault:\n\t  assert(0);\n\t}\n\n\t\n    \n      \n\t\n\n      \n\tevaluateGeneric(tr, tr->start, TRUE);\t\n\t\n\t\n\n      \t\n\ttreeEvaluate(tr, 1); \n\n\t\n\n      \n#ifdef _BAYESIAN \n\tif(adef->bayesian)\n\t  {\n\t    \n\n\t    \n\t    allocateParsimonyDataStructures(tr);\n\t    mcmc(tr, adef);\n\t    pllFreeParsimonyDataStructures(tr);\n\t  }\n\telse\n#endif\t \n\t  computeBIGRAPID(tr, adef, TRUE); \t\t\t     \n      }            \n      \n    \n\n  \n    if(processID == 0)\n      finalizeInfoFile(tr, adef);\n  }\n  \n\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "gnu3ra_1086", "code": "int main( int argc, char** argv )\n{\n    MPE_XGraph graph;\n    int ierr, mp_size, my_rank;\n\tMPE_Color my_color;\n    char ckey;\n    \n\n\n\n    \n\n\n    ierr = MPE_Open_graphics( &graph, MPI_COMM_WORLD, NULL,\n                              -1, -1, 400, 400, 0 );\n    if ( ierr != MPE_SUCCESS ) {\n        fprintf( stderr, \"%d : MPE_Open_graphics() fails\\n\", my_rank );\n        ierr =\n        exit(1);\n    }\n    my_color = (MPE_Color) (my_rank + 1);\n    if ( my_rank == 0 )\n        ierr = MPE_Draw_string( graph, 187, 205, MPE_BLUE, \"Hello\" );\n    ierr = MPE_Draw_circle( graph, 200, 200, 20+my_rank*5, my_color );\n    ierr = MPE_Update( graph );\n\n    if ( my_rank == 0 ) {\n        \n\n        sleep(1);\n    }\n\n    ierr = MPE_Close_graphics( &graph );\n\n    \n    return 0;\n}", "label": "int main( int argc, char** argv )\n{\n    MPE_XGraph graph;\n    int ierr, mp_size, my_rank;\n\tMPE_Color my_color;\n    char ckey;\n    \n\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_size( MPI_COMM_WORLD, &mp_size );\n    MPI_Comm_rank( MPI_COMM_WORLD, &my_rank );\n\n    \n\n\n    ierr = MPE_Open_graphics( &graph, MPI_COMM_WORLD, NULL,\n                              -1, -1, 400, 400, 0 );\n    if ( ierr != MPE_SUCCESS ) {\n        fprintf( stderr, \"%d : MPE_Open_graphics() fails\\n\", my_rank );\n        ierr = MPI_Abort( MPI_COMM_WORLD, 1 );\n        exit(1);\n    }\n    my_color = (MPE_Color) (my_rank + 1);\n    if ( my_rank == 0 )\n        ierr = MPE_Draw_string( graph, 187, 205, MPE_BLUE, \"Hello\" );\n    ierr = MPE_Draw_circle( graph, 200, 200, 20+my_rank*5, my_color );\n    ierr = MPE_Update( graph );\n\n    if ( my_rank == 0 ) {\n        \n\n        sleep(1);\n    }\n    MPI_Barrier( MPI_COMM_WORLD );\n\n    ierr = MPE_Close_graphics( &graph );\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "mpip_1087", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  double *planned_in, *executed_in;\n  pfft_complex *planned_out, *executed_out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  planned_in  = pfft_alloc_real(2 * alloc_local);\n  planned_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      planned_in, planned_out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      planned_out, planned_in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_free(planned_in); pfft_free(planned_out);\n\n  \n\n  executed_in  = pfft_alloc_real(2 * alloc_local);\n  executed_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  pfft_init_input_real(4, n, local_ni, local_i_start,\n      executed_in);\n\n  \n\n  pfft_execute_dft_r2c(plan_forw, executed_in, executed_out);\n\n  \n\n  pfft_clear_input_real(4, n, local_ni, local_i_start,\n      executed_in);\n  \n  \n\n  pfft_execute_dft_c2r(plan_back, executed_out, executed_in);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_no[0] * local_no[1] * local_no[2] * local_no[3]; l++)\n    executed_in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_real(4, n, local_no, local_o_start, executed_in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(executed_in); pfft_free(executed_out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  double *planned_in, *executed_in;\n  pfft_complex *planned_out, *executed_out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  planned_in  = pfft_alloc_real(2 * alloc_local);\n  planned_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      planned_in, planned_out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      planned_out, planned_in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_free(planned_in); pfft_free(planned_out);\n\n  \n\n  executed_in  = pfft_alloc_real(2 * alloc_local);\n  executed_out = pfft_alloc_complex(alloc_local);\n\n  \n\n  pfft_init_input_real(4, n, local_ni, local_i_start,\n      executed_in);\n\n  \n\n  pfft_execute_dft_r2c(plan_forw, executed_in, executed_out);\n\n  \n\n  pfft_clear_input_real(4, n, local_ni, local_i_start,\n      executed_in);\n  \n  \n\n  pfft_execute_dft_c2r(plan_back, executed_out, executed_in);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_no[0] * local_no[1] * local_no[2] * local_no[3]; l++)\n    executed_in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real(4, n, local_no, local_o_start, executed_in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(executed_in); pfft_free(executed_out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "cot_1092", "code": "int main(int argc, char *argv[]) {\n  int        rank, new_rank, sendbuf, recvbuf, numtasks, ranks1[2]={0,1}, ranks2[2]={2,3};\n  MPI_Group  orig_group, new_group;\n  MPI_Comm   new_comm;\n\n\n  if (numtasks != NPROCS) {\n    printf(\"Must specify MP_PROCS= %d. Terminating.\\n\",NPROCS);\n    exit(0);\n  }\n\n  sendbuf = rank;\n\n  \n\n\n  \n\n  if (rank < NPROCS/2) {\n  }\n  else {\n  }\n\n  \n\n\n  int i;\n\n  \n\n  for(i=0; i<NPROCS/2; i++) {\n  }\n\n  for(i=0; i<NPROCS/2; i++) {\n  }\n\n  \n\n  MPI_Request reqs[NPROCS/2];\n  for(i=0; i<NPROCS/2; i++) {\n  }\n\n  for(i=0; i<NPROCS/2; i++) {\n  }\n\n  \n\n  for(i=0; i<NPROCS/2; i++) {\n  }\n\n\n\n  for(i=0; i<NPROCS/2; i++) {\n  }\n\n\n\n\n  printf(\"rank= %d newrank= %d recvbuf= %d\\n\",rank,new_rank,recvbuf);\n\n  sleep(1);\n\n  \n\n  for(i=0; i<NPROCS; i++) {\n  }\n\n  for(i=0; i<NPROCS; i++) {\n  }\n\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  int        rank, new_rank, sendbuf, recvbuf, numtasks, ranks1[2]={0,1}, ranks2[2]={2,3};\n  MPI_Group  orig_group, new_group;\n  MPI_Comm   new_comm;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n  if (numtasks != NPROCS) {\n    printf(\"Must specify MP_PROCS= %d. Terminating.\\n\",NPROCS);\n    MPI_Finalize();\n    exit(0);\n  }\n\n  sendbuf = rank;\n\n  \n\n  MPI_Comm_group(MPI_COMM_WORLD, &orig_group);\n\n  \n\n  if (rank < NPROCS/2) {\n    MPI_Group_incl(orig_group, NPROCS/2, ranks1, &new_group);\n  }\n  else {\n    MPI_Group_incl(orig_group, NPROCS/2, ranks2, &new_group);\n  }\n\n  \n\n  MPI_Comm_create(MPI_COMM_WORLD, new_group, &new_comm);\n\n  int i;\n\n  \n\n  for(i=0; i<NPROCS/2; i++) {\n    MPI_Send(&sendbuf, 1, MPI_INT, i, 0, new_comm);\n  }\n\n  for(i=0; i<NPROCS/2; i++) {\n    MPI_Recv(&recvbuf, 1, MPI_INT, i, 0, new_comm, MPI_STATUS_IGNORE);\n  }\n\n  \n\n  MPI_Request reqs[NPROCS/2];\n  for(i=0; i<NPROCS/2; i++) {\n    MPI_Isend(&sendbuf, 1, MPI_INT, i, 0, new_comm, &reqs[i]);\n  }\n  MPI_Waitall(NPROCS/2, reqs, MPI_STATUSES_IGNORE);\n\n  for(i=0; i<NPROCS/2; i++) {\n    MPI_Irecv(&recvbuf, 1, MPI_INT, i, 0, new_comm, &reqs[i]);\n  }\n  MPI_Waitall(NPROCS/2, reqs, MPI_STATUSES_IGNORE);\n\n  \n\n  for(i=0; i<NPROCS/2; i++) {\n    MPI_Send_init(&sendbuf, 1, MPI_INT, i, 0, new_comm, &reqs[i]);\n  }\n\n  MPI_Startall(NPROCS/2, reqs);\n  MPI_Waitall(NPROCS/2, reqs, MPI_STATUSES_IGNORE);\n\n  MPI_Startall(NPROCS/2, reqs);\n  MPI_Waitall(NPROCS/2, reqs, MPI_STATUSES_IGNORE);\n\n  for(i=0; i<NPROCS/2; i++) {\n    MPI_Recv_init(&recvbuf, 1, MPI_INT, i, 0, new_comm, &reqs[i]);\n  }\n\n  MPI_Startall(NPROCS/2, reqs);\n  MPI_Waitall(NPROCS/2, reqs, MPI_STATUSES_IGNORE);\n\n  MPI_Startall(NPROCS/2, reqs);\n  MPI_Waitall(NPROCS/2, reqs, MPI_STATUSES_IGNORE);\n\n  MPI_Group_rank (new_group, &new_rank);\n\n  printf(\"rank= %d newrank= %d recvbuf= %d\\n\",rank,new_rank,recvbuf);\n\n  sleep(1);\n\n  \n\n  for(i=0; i<NPROCS; i++) {\n    MPI_Send(&sendbuf, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n  }\n\n  for(i=0; i<NPROCS; i++) {\n    MPI_Recv(&recvbuf, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gentryx_1093", "code": "int main(int argc, char* argv[])\n{\n\n    int rank, size;\n\n    int commute = 1;\n    MPI_Op myop;\n\n    int in[8]  = {1,2,3,4,5,6,7,8};\n    int out[8] = {-1,-2,-3,-4,-5,-6,-7,-8};\n\n    printf(\"%d: MPI_IN_PLACE = %p in = %p out = %p \\n\", rank, MPI_IN_PLACE, in, out);\n\n    fflush(stdout);\n\n    if (rank==0) printf(\"out-of-place Allreduce \\n\");\n    fflush(stdout);\n\n    if (rank==0) printf(\"in-place Allreduce \\n\");\n    fflush(stdout);\n\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int commute = 1;\n    MPI_Op myop;\n    MPI_Op_create(printer, commute, &myop);\n\n    int in[8]  = {1,2,3,4,5,6,7,8};\n    int out[8] = {-1,-2,-3,-4,-5,-6,-7,-8};\n\n    printf(\"%d: MPI_IN_PLACE = %p in = %p out = %p \\n\", rank, MPI_IN_PLACE, in, out);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    fflush(stdout);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank==0) printf(\"out-of-place Allreduce \\n\");\n    MPI_Allreduce(in, out, 8, MPI_INT, myop, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n    fflush(stdout);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank==0) printf(\"in-place Allreduce \\n\");\n    MPI_Allreduce(MPI_IN_PLACE, out, 8, MPI_INT, myop, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n    fflush(stdout);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Op_free(&myop);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "germasch_1095", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  mrc_domain_set_param_int(domain, \"bcx\", BC_PERIODIC);\n  mrc_domain_set_param_int(domain, \"bcy\", BC_PERIODIC);\n  mrc_domain_set_param_int(domain, \"bcz\", BC_PERIODIC);\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  mrc_domain_plot(domain);\n\n  struct mrc_fld *m3 = mrc_domain_m3_create(domain);\n  mrc_fld_set_name(m3, \"test_m3\");\n  mrc_fld_set_param_int(m3, \"nr_comps\", 2);\n  mrc_fld_set_param_int(m3, \"nr_ghosts\", 1);\n  mrc_fld_set_from_options(m3);\n  mrc_fld_setup(m3);\n  mrc_fld_set_comp_name(m3, 0, \"fld0\");\n  mrc_fld_set_comp_name(m3, 1, \"fld1\");\n  mrc_fld_view(m3);\n\n  set_m3(m3);\n\n  struct mrc_ddc *ddc = mrc_domain_create_ddc(domain);\n  mrc_ddc_setup(ddc);\n  mrc_ddc_view(ddc);\n  mrc_ddc_fill_ghosts_fld(ddc, 0, 2, m3);\n  mrc_ddc_destroy(ddc);\n\n  check_m3(m3);\n\n  mrc_fld_destroy(m3);\n\n  mrc_domain_destroy(domain);\n\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  mrc_domain_set_param_int(domain, \"bcx\", BC_PERIODIC);\n  mrc_domain_set_param_int(domain, \"bcy\", BC_PERIODIC);\n  mrc_domain_set_param_int(domain, \"bcz\", BC_PERIODIC);\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  mrc_domain_plot(domain);\n\n  struct mrc_fld *m3 = mrc_domain_m3_create(domain);\n  mrc_fld_set_name(m3, \"test_m3\");\n  mrc_fld_set_param_int(m3, \"nr_comps\", 2);\n  mrc_fld_set_param_int(m3, \"nr_ghosts\", 1);\n  mrc_fld_set_from_options(m3);\n  mrc_fld_setup(m3);\n  mrc_fld_set_comp_name(m3, 0, \"fld0\");\n  mrc_fld_set_comp_name(m3, 1, \"fld1\");\n  mrc_fld_view(m3);\n\n  set_m3(m3);\n\n  struct mrc_ddc *ddc = mrc_domain_create_ddc(domain);\n  mrc_ddc_setup(ddc);\n  mrc_ddc_view(ddc);\n  mrc_ddc_fill_ghosts_fld(ddc, 0, 2, m3);\n  mrc_ddc_destroy(ddc);\n\n  check_m3(m3);\n\n  mrc_fld_destroy(m3);\n\n  mrc_domain_destroy(domain);\n\n  MPI_Finalize();\n}"}
{"program": "alexfrolov_1096", "code": "int main(int argc, char* argv[]) {\n  int log_numverts;\n  int size, rank;\n  unsigned long my_edges;\n  unsigned long global_edges;\n  double start, stop;\n  size_t i;\n\n\n  log_numverts = 16; \n\n  if (argc >= 2) log_numverts = atoi(argv[1]);\n\n\n  if (rank == 0) fprintf(stderr, \"Graph size is %\" PRId64 \" vertices and %\" PRId64 \" edges\\n\", INT64_C(1) << log_numverts, INT64_C(16) << log_numverts);\n\n  \n\n  start =\n  int64_t nedges;\n  packed_edge* result;\n  make_graph(log_numverts, INT64_C(16) << log_numverts, 1, 2, &nedges, &result);\n  stop =\n  \n\n\n  my_edges = nedges;\n\n  for (i = 0; i < my_edges; ++i) {\n    assert ((get_v0_from_edge(&result[i]) >> log_numverts) == 0);\n    assert ((get_v1_from_edge(&result[i]) >> log_numverts) == 0);\n  }\n  \n  xfree(result);\n\n  if (rank == 0) {\n    fprintf(stderr, \"%lu edge%s generated in %fs (%f Medges/s on %d processor%s)\\n\", global_edges, (global_edges == 1 ? \"\" : \"s\"), (stop - start), global_edges / (stop - start) * 1.e-6, size, (size == 1 ? \"\" : \"s\"));\n  }\n  return 0;\n}", "label": "int main(int argc, char* argv[]) {\n  int log_numverts;\n  int size, rank;\n  unsigned long my_edges;\n  unsigned long global_edges;\n  double start, stop;\n  size_t i;\n\n  MPI_Init(&argc, &argv);\n\n  log_numverts = 16; \n\n  if (argc >= 2) log_numverts = atoi(argv[1]);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) fprintf(stderr, \"Graph size is %\" PRId64 \" vertices and %\" PRId64 \" edges\\n\", INT64_C(1) << log_numverts, INT64_C(16) << log_numverts);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  start = MPI_Wtime();\n  int64_t nedges;\n  packed_edge* result;\n  make_graph(log_numverts, INT64_C(16) << log_numverts, 1, 2, &nedges, &result);\n  MPI_Barrier(MPI_COMM_WORLD);\n  stop = MPI_Wtime();\n  \n\n\n  my_edges = nedges;\n\n  for (i = 0; i < my_edges; ++i) {\n    assert ((get_v0_from_edge(&result[i]) >> log_numverts) == 0);\n    assert ((get_v1_from_edge(&result[i]) >> log_numverts) == 0);\n  }\n  \n  xfree(result);\n\n  MPI_Reduce(&my_edges, &global_edges, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    fprintf(stderr, \"%lu edge%s generated in %fs (%f Medges/s on %d processor%s)\\n\", global_edges, (global_edges == 1 ? \"\" : \"s\"), (stop - start), global_edges / (stop - start) * 1.e-6, size, (size == 1 ? \"\" : \"s\"));\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "jianfeipan_1097", "code": "main(int argc, char **argv) \n{\nint numtasks,\t\t\t\n\n    taskid,\t\t\t\n\n    numworkers,\t\t\t\n\n    source,\t\t\t\n\n    dest,\t\t\t\n\n    nbytes,\t\t\t\n\n    mtype,\t\t\t\n\n    intsize,\t\t\t\n\n    dbsize,\t\t\t\n\n    rows,                      \t\n\n    averow, extra, offset,      \n\n    i, j, k,\t\t\t\n\n    count;\ndouble a[NRA][NCA], \t\t\n\n       b[NCA][NCB],      \t\n\n       c[NRA][NCB];\t\t\n\n\nintsize = sizeof(int);\ndbsize = sizeof(double);\n\nMPI_Init(&argc, &argv);\nMPI_Comm_rank(MPI_COMM_WORLD, &taskid);\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nnumworkers = numtasks-1;\n\n\n\nif (taskid == MASTER) {\n  printf(\"Number of worker tasks = %d\\n\",numworkers);\n  for (i=0; i<NRA; i++)\n    for (j=0; j<NCA; j++)\n      a[i][j]= i+j;\n  for (i=0; i<NCA; i++)\n    for (j=0; j<NCB; j++)\n      b[i][j]= i*j;\n\n  \n\n  averow = NRA/numworkers;\n  extra = NRA%numworkers;\n  offset = 0;\n  mtype = FROM_MASTER;\n  for (dest=1; dest<=numworkers; dest++) {\t\t\t\n    rows = (dest <= extra) ? averow+1 : averow;   \t\n    printf(\"   sending %d rows to task %d\\n\",rows,dest);\n    count = rows*NCA;\n    count = NCA*NCB;\n\n    offset = offset + rows;\n    }\n\n  \n\n  mtype = FROM_WORKER;\n  for (i=1; i<=numworkers; i++)\t{\t\t\t\n    source = i;\n    count = rows*NCB;\n \n    }\n\n  \n\n  printf(\"Here is the result matrix\\n\");\n  for (i=0; i<NRA; i++) { \n    printf(\"\\n\"); \n    for (j=0; j<NCB; j++) \n      printf(\"%6.2f   \", c[i][j]);\n    }\n  printf (\"\\n\");\n\n  }  \n\n\n\n\n\n\nif (taskid > MASTER) {\n  mtype = FROM_MASTER;\n  source = MASTER;\n  printf (\"Master =%d, mtype=%d\\n\", source, mtype);\n  printf (\"offset =%d\\n\", offset);\n  printf (\"row =%d\\n\", rows);\n  count = rows*NCA;\n  printf (\"a[0][0] =%e\\n\", a[0][0]);\n  count = NCA*NCB;\n  printf (\"b=\\n\");\n  for (k=0; k<NCB; k++)\n    for (i=0; i<rows; i++) {\n      c[i][k] = 0.0;\n      for (j=0; j<NCA; j++)\n        c[i][k] = c[i][k] + a[i][j] * b[j][k];\n      }\n\n  mtype = FROM_WORKER;\n  printf (\"after computer\\n\");\n  printf (\"after send\\n\");\n\n  }  \n\n}", "label": "main(int argc, char **argv) \n{\nint numtasks,\t\t\t\n\n    taskid,\t\t\t\n\n    numworkers,\t\t\t\n\n    source,\t\t\t\n\n    dest,\t\t\t\n\n    nbytes,\t\t\t\n\n    mtype,\t\t\t\n\n    intsize,\t\t\t\n\n    dbsize,\t\t\t\n\n    rows,                      \t\n\n    averow, extra, offset,      \n\n    i, j, k,\t\t\t\n\n    count;\ndouble a[NRA][NCA], \t\t\n\n       b[NCA][NCB],      \t\n\n       c[NRA][NCB];\t\t\n\n\nintsize = sizeof(int);\ndbsize = sizeof(double);\n\nMPI_Init(&argc, &argv);\nMPI_Comm_rank(MPI_COMM_WORLD, &taskid);\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nnumworkers = numtasks-1;\n\n\n\nif (taskid == MASTER) {\n  printf(\"Number of worker tasks = %d\\n\",numworkers);\n  for (i=0; i<NRA; i++)\n    for (j=0; j<NCA; j++)\n      a[i][j]= i+j;\n  for (i=0; i<NCA; i++)\n    for (j=0; j<NCB; j++)\n      b[i][j]= i*j;\n\n  \n\n  averow = NRA/numworkers;\n  extra = NRA%numworkers;\n  offset = 0;\n  mtype = FROM_MASTER;\n  for (dest=1; dest<=numworkers; dest++) {\t\t\t\n    rows = (dest <= extra) ? averow+1 : averow;   \t\n    printf(\"   sending %d rows to task %d\\n\",rows,dest);\n    MPI_Send(&offset, 1, MPI_INT, dest, mtype, MPI_COMM_WORLD);\n    MPI_Send(&rows, 1, MPI_INT, dest, mtype, MPI_COMM_WORLD);\n    count = rows*NCA;\n    MPI_Send(&a[offset][0], count, MPI_DOUBLE, dest, mtype, MPI_COMM_WORLD);\n    count = NCA*NCB;\n    MPI_Send(&b, count, MPI_DOUBLE, dest, mtype, MPI_COMM_WORLD);\n\n    offset = offset + rows;\n    }\n\n  \n\n  mtype = FROM_WORKER;\n  for (i=1; i<=numworkers; i++)\t{\t\t\t\n    source = i;\n    MPI_Recv(&offset, 1, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&rows, 1, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n    count = rows*NCB;\n    MPI_Recv(&c[offset][0], count, MPI_DOUBLE, source, mtype, MPI_COMM_WORLD, \n               &status);\n \n    }\n\n  \n\n  printf(\"Here is the result matrix\\n\");\n  for (i=0; i<NRA; i++) { \n    printf(\"\\n\"); \n    for (j=0; j<NCB; j++) \n      printf(\"%6.2f   \", c[i][j]);\n    }\n  printf (\"\\n\");\n\n  }  \n\n\n\n\n\n\nif (taskid > MASTER) {\n  mtype = FROM_MASTER;\n  source = MASTER;\n  printf (\"Master =%d, mtype=%d\\n\", source, mtype);\n  MPI_Recv(&offset, 1, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n  printf (\"offset =%d\\n\", offset);\n  MPI_Recv(&rows, 1, MPI_INT, source, mtype, MPI_COMM_WORLD, &status);\n  printf (\"row =%d\\n\", rows);\n  count = rows*NCA;\n  MPI_Recv(&a, count, MPI_DOUBLE, source, mtype, MPI_COMM_WORLD, &status);\n  printf (\"a[0][0] =%e\\n\", a[0][0]);\n  count = NCA*NCB;\n  MPI_Recv(&b, count, MPI_DOUBLE, source, mtype, MPI_COMM_WORLD, &status);\n  printf (\"b=\\n\");\n  for (k=0; k<NCB; k++)\n    for (i=0; i<rows; i++) {\n      c[i][k] = 0.0;\n      for (j=0; j<NCA; j++)\n        c[i][k] = c[i][k] + a[i][j] * b[j][k];\n      }\n\n  mtype = FROM_WORKER;\n  printf (\"after computer\\n\");\n  MPI_Send(&offset, 1, MPI_INT, MASTER, mtype, MPI_COMM_WORLD);\n  MPI_Send(&rows, 1, MPI_INT, MASTER, mtype, MPI_COMM_WORLD);\n  MPI_Send(&c, rows*NCB, MPI_DOUBLE, MASTER, mtype, MPI_COMM_WORLD);\n  printf (\"after send\\n\");\n\n  }  \n\n  MPI_Finalize();\n}"}
{"program": "fezproof_1099", "code": "int main(int argc, char *argv[])\n{\n\n    \n\n    int numProcs, rank;\n\n    \n\n    omp_set_dynamic(0);\n\n    OPTIONS *options = createOptionsStruct();\n    readOptions(argc, argv, options);\n\n    srand(time(NULL));\n    srand(0);\n\n    if(rank == 0) {\n\n        FILE *fp = initialiseCSV(options->type, options->probability, options->perlocationType, options->runs, options->maxThreadNum);\n\n        doTests(options->type, options->minSize, options->probability, options->perlocationType, options->runs, options->maxSize, options->minThreadNum, options->maxThreadNum, options->minNodeNum, options->maxNodeNum, rank, fp);\n\n        terminateSlaves(numProcs);\n\n        rewind(fp);\n        printFile(fp);\n        fclose(fp);\n\n    } else {\n        clusterSlave(options->type);\n    }\n\n\n}", "label": "int main(int argc, char *argv[])\n{\n\n    \n\n    int numProcs, rank;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\n    \n\n    omp_set_dynamic(0);\n\n    OPTIONS *options = createOptionsStruct();\n    readOptions(argc, argv, options);\n\n    srand(time(NULL));\n    srand(0);\n\n    if(rank == 0) {\n\n        FILE *fp = initialiseCSV(options->type, options->probability, options->perlocationType, options->runs, options->maxThreadNum);\n\n        doTests(options->type, options->minSize, options->probability, options->perlocationType, options->runs, options->maxSize, options->minThreadNum, options->maxThreadNum, options->minNodeNum, options->maxNodeNum, rank, fp);\n\n        terminateSlaves(numProcs);\n\n        rewind(fp);\n        printFile(fp);\n        fclose(fp);\n\n    } else {\n        clusterSlave(options->type);\n    }\n\n\n    MPI_Finalize();\n}"}
{"program": "cea-hpc_1100", "code": "int main(int argc,char **argv)\n{\n  int rank,size;\n  int tmp;\n  double user_tremain = -1.0 ;\n  size_t maxstrlen = GLOST_STRLEN ;\n\n  \n\n\n  tmp=0;\n  set_sigaction();\n\n  \n\n  if (rank == GLOST_MASTER)\n    tmp=read_options(argc,argv,&user_tremain,&maxstrlen);\n  \n  if (tmp != 0){\n    exit(0);\n  }\n\n  if (rank == GLOST_MASTER) {\n    printf(\"master is %d , nb slaves: %d\\n\", GLOST_MASTER,size-1);\t\n  }\n  \n  \n\n  if (size == 1)\n    read_and_exec(argv[optind],user_tremain,maxstrlen);\n  else if(size != 1 && rank == GLOST_MASTER)\n    read_and_send(size-1,argv[optind],user_tremain,maxstrlen);\n  else\n    recv_and_exec(rank,maxstrlen);\n\n  \n\n}", "label": "int main(int argc,char **argv)\n{\n  int rank,size;\n  int tmp;\n  double user_tremain = -1.0 ;\n  size_t maxstrlen = GLOST_STRLEN ;\n\n  \n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);  \n\n  tmp=0;\n  set_sigaction();\n\n  \n\n  if (rank == GLOST_MASTER)\n    tmp=read_options(argc,argv,&user_tremain,&maxstrlen);\n  \n  MPI_Bcast(&tmp,1,MPI_INT,0,MPI_COMM_WORLD);\n  if (tmp != 0){\n    MPI_Finalize();\n    exit(0);\n  }\n  MPI_Bcast(&maxstrlen,1,MPI_UINT64_T,0,MPI_COMM_WORLD);\n\n  if (rank == GLOST_MASTER) {\n    printf(\"master is %d , nb slaves: %d\\n\", GLOST_MASTER,size-1);\t\n  }\n  \n  \n\n  if (size == 1)\n    read_and_exec(argv[optind],user_tremain,maxstrlen);\n  else if(size != 1 && rank == GLOST_MASTER)\n    read_and_send(size-1,argv[optind],user_tremain,maxstrlen);\n  else\n    recv_and_exec(rank,maxstrlen);\n\n  \n\n  MPI_Finalize();\n}"}
{"program": "furious-luke_1101", "code": "int\nmain( int argc,\n      char** argv )\n{\n   unsigned n_chunks, *chunks;\n   unsigned n_files = 8, *n_file_elems;\n   unsigned n_elems = 0, *idxs, *data;\n   unsigned n_local_elems, elem_offs;\n   char fn[1000];\n   int rank, n_ranks;\n   FILE* file;\n   int ii, jj, kk;\n\n\n   n_file_elems = ALLOC( unsigned, n_files );\n   for( ii = 0; ii < n_files; ++ii )\n   {\n      sprintf( fn, \"test_file.%05d\", ii );\n      file = fopen( fn, \"r\" );\n      assert( file );\n      fscanf( file, \"%d\", n_file_elems + ii );\n      n_elems += n_file_elems[ii];\n   }\n   chunk_files( n_files, n_file_elems, &n_chunks, &chunks, MPI_COMM_WORLD );\n   FREE( n_file_elems );\n\n   n_local_elems = local_size( n_elems, n_ranks, rank );\n   elem_offs -= n_local_elems;\n\n   data = ALLOC( unsigned, n_local_elems );\n   for( ii = 0, kk = 0; ii < n_chunks; ++ii )\n   {\n      sprintf( fn, \"test_file.%05d\", chunks[3*ii + 0] );\n      file = fopen( fn, \"r\" );\n      assert( file );\n      for( jj = 0; jj < chunks[3*ii + 1] + 1; ++jj ) \n\n      {\n         unsigned dummy;\n         fscanf( file, \"%d\", &dummy );\n      }\n      for( jj = 0; jj < chunks[3*ii + 2]; ++jj, ++kk )\n      {\n         assert( kk < n_local_elems );\n         fscanf( file, \"%d\", data + kk );\n      }\n   }\n\n   idxs = ALLOC( unsigned, n_local_elems );\n   for( ii = 0; ii < n_local_elems; ++ii )\n      idxs[ii] = (elem_offs + ii + 10)%n_elems;\n\n   permute( n_elems, n_local_elems, idxs, data, MPI_UNSIGNED, MPI_COMM_WORLD );\n\n   {\n      unsigned *all_sizes, *all_data, *all_displs;\n      all_sizes = ALLOC( unsigned, n_ranks );\n      all_displs = ALLOC( unsigned, n_ranks );\n      all_data = ALLOC( unsigned, n_elems );\n      make_displs( n_ranks, all_sizes, all_displs );\n      if( rank == 0 )\n      {\n         for( ii = 0; ii < n_elems; ++ii )\n            printf( \"%d \", all_data[ii] );\n         printf( \"\\n\" );\n      }\n   }\n\n   FREE( chunks );\n   return EXIT_SUCCESS;\n}", "label": "int\nmain( int argc,\n      char** argv )\n{\n   unsigned n_chunks, *chunks;\n   unsigned n_files = 8, *n_file_elems;\n   unsigned n_elems = 0, *idxs, *data;\n   unsigned n_local_elems, elem_offs;\n   char fn[1000];\n   int rank, n_ranks;\n   FILE* file;\n   int ii, jj, kk;\n\n   MPI_Init( &argc, &argv );\n   MPI_Comm_size( MPI_COMM_WORLD, &n_ranks );\n   MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\n   n_file_elems = ALLOC( unsigned, n_files );\n   for( ii = 0; ii < n_files; ++ii )\n   {\n      sprintf( fn, \"test_file.%05d\", ii );\n      file = fopen( fn, \"r\" );\n      assert( file );\n      fscanf( file, \"%d\", n_file_elems + ii );\n      n_elems += n_file_elems[ii];\n   }\n   chunk_files( n_files, n_file_elems, &n_chunks, &chunks, MPI_COMM_WORLD );\n   FREE( n_file_elems );\n\n   n_local_elems = local_size( n_elems, n_ranks, rank );\n   MPI_OK( MPI_Scan( &n_local_elems, &elem_offs, 1, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD ) );\n   elem_offs -= n_local_elems;\n\n   data = ALLOC( unsigned, n_local_elems );\n   for( ii = 0, kk = 0; ii < n_chunks; ++ii )\n   {\n      sprintf( fn, \"test_file.%05d\", chunks[3*ii + 0] );\n      file = fopen( fn, \"r\" );\n      assert( file );\n      for( jj = 0; jj < chunks[3*ii + 1] + 1; ++jj ) \n\n      {\n         unsigned dummy;\n         fscanf( file, \"%d\", &dummy );\n      }\n      for( jj = 0; jj < chunks[3*ii + 2]; ++jj, ++kk )\n      {\n         assert( kk < n_local_elems );\n         fscanf( file, \"%d\", data + kk );\n      }\n   }\n\n   idxs = ALLOC( unsigned, n_local_elems );\n   for( ii = 0; ii < n_local_elems; ++ii )\n      idxs[ii] = (elem_offs + ii + 10)%n_elems;\n\n   permute( n_elems, n_local_elems, idxs, data, MPI_UNSIGNED, MPI_COMM_WORLD );\n\n   {\n      unsigned *all_sizes, *all_data, *all_displs;\n      all_sizes = ALLOC( unsigned, n_ranks );\n      all_displs = ALLOC( unsigned, n_ranks );\n      all_data = ALLOC( unsigned, n_elems );\n      MPI_OK( MPI_Gather( &n_local_elems, 1, MPI_UNSIGNED, all_sizes, 1, MPI_UNSIGNED, 0, MPI_COMM_WORLD ) );\n      make_displs( n_ranks, all_sizes, all_displs );\n      MPI_OK( MPI_Gatherv( data, n_local_elems, MPI_UNSIGNED, all_data, all_sizes, all_displs, MPI_UNSIGNED, 0, MPI_COMM_WORLD ) );\n      if( rank == 0 )\n      {\n         for( ii = 0; ii < n_elems; ++ii )\n            printf( \"%d \", all_data[ii] );\n         printf( \"\\n\" );\n      }\n   }\n\n   FREE( chunks );\n   MPI_Finalize();\n   return EXIT_SUCCESS;\n}"}
{"program": "JasonRuonanWang_1102", "code": "int\nmain(int argc, char* argv[]) \n{\n    int np, me;\n\n    char master_contact[CONTACTLEN];             \n\n    char *transport = NULL;\n    attr_list listen_list = NULL, contact_list;\n    char *string_list;\n    CManager cm;\n\n\n\n\n  \n    while (argc > 1) {\n\tif (strcmp(argv[1], \"-t\") == 0) {\n\t    transport = argv[2];\n\t    argc--;\n\t    argv++;\n\t} else if (strcmp(argv[1], \"-v\") == 0) {\n\t    quiet--;\n\t} else {\n\t    printf(\"Unknown argument %s\\n\", argv[1]);\n\t}\n\targv++;\n\targc--;\n    }\n    CM_TRANSPORT = attr_atom_from_string(\"CM_TRANSPORT\");\n    if (transport || (transport = getenv(\"CMTransport\")) != NULL) {\n\tif (listen_list == NULL) listen_list = create_attr_list();\n\tadd_attr(listen_list, CM_TRANSPORT, Attr_String,\n\t\t (attr_value) strdup(transport));\n    }\n    cm = CManager_create();\n    CMlisten_specific(cm, listen_list);\n    contact_list = CMget_specific_contact_list(cm, listen_list);\n    \n    if (transport != NULL) {\n      char *actual_transport = NULL;\n      get_string_attr(contact_list, CM_TRANSPORT, &actual_transport);\n      if (!actual_transport || (strcmp(actual_transport, transport) != 0)) {\n\tprintf(\"Failed to load transport \\\"%s\\\"\\n\", transport);\n\texit(1);\n      }\n    }\n    string_list = attr_list_to_string(contact_list);\n    free_attr_list(contact_list);\n    strcpy(master_contact, string_list);\n\n    if (me == 0) {    \n\n\tCMFormat format;\n\tint message_wait_condition;\n\ttime_t start, end;\n\tif (quiet <= 0) {\n\t    printf(\"Master contact is %s\\n\", master_contact);\n\t}\n\tformat = CMregister_format(cm, simple_format_list);\n\tmessage_wait_condition = CMCondition_get(cm, NULL);\n\tCMregister_handler(format, simple_handler, (void*)(long)message_wait_condition);\n \tstart = time(NULL);\n\tCMCondition_wait(cm, message_wait_condition);\n\tend = time(NULL);\n\tprintf(\"Elapsed time was %d\\n\", (int)(end - start));\n    } else { \n\n\tattr_list contact_list;\n\tCMConnection conn = NULL;\n\tCMFormat format;\n\tsimple_rec data;\n\tattr_list attrs;\n\tatom_t CMDEMO_TEST_ATOM;\n\tif (quiet <= 0) {\n\t    printf(\"Node %d thinks master contact is %s\\n\", me, master_contact);\n\t}\n\tcontact_list = attr_list_from_string(master_contact);\n\tconn = CMinitiate_conn(cm, contact_list);\n\tif (conn == NULL) {\n\t    printf(\"No connection, attr list was :\");\n\t    dump_attr_list(contact_list);\n\t    printf(\"\\n\");\n\t    exit(1);\n\t}\n\tfree_attr_list(contact_list);\n\tformat = CMregister_format(cm, simple_format_list);\n\tgenerate_record(&data);\n\tattrs = create_attr_list();\n\tCMDEMO_TEST_ATOM = attr_atom_from_string(\"CMdemo_test_atom\");\n\tadd_attr(attrs, CMDEMO_TEST_ATOM, Attr_Int4, (attr_value)45678);\n\tCMwrite_attr(conn, format, &data, attrs);\n\tCMsleep(cm, 1);\n\tfree_attr_list(attrs);\n    }\n\n    exit(0);\n}", "label": "int\nmain(int argc, char* argv[]) \n{\n    int np, me;\n\n    char master_contact[CONTACTLEN];             \n\n    char *transport = NULL;\n    attr_list listen_list = NULL, contact_list;\n    char *string_list;\n    CManager cm;\n\n    MPI_Init(&argc, &argv);                \n\n    MPI_Comm_size(MPI_COMM_WORLD, &np);    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &me);    \n\n  \n    while (argc > 1) {\n\tif (strcmp(argv[1], \"-t\") == 0) {\n\t    transport = argv[2];\n\t    argc--;\n\t    argv++;\n\t} else if (strcmp(argv[1], \"-v\") == 0) {\n\t    quiet--;\n\t} else {\n\t    printf(\"Unknown argument %s\\n\", argv[1]);\n\t}\n\targv++;\n\targc--;\n    }\n    CM_TRANSPORT = attr_atom_from_string(\"CM_TRANSPORT\");\n    if (transport || (transport = getenv(\"CMTransport\")) != NULL) {\n\tif (listen_list == NULL) listen_list = create_attr_list();\n\tadd_attr(listen_list, CM_TRANSPORT, Attr_String,\n\t\t (attr_value) strdup(transport));\n    }\n    cm = CManager_create();\n    CMlisten_specific(cm, listen_list);\n    contact_list = CMget_specific_contact_list(cm, listen_list);\n    \n    if (transport != NULL) {\n      char *actual_transport = NULL;\n      get_string_attr(contact_list, CM_TRANSPORT, &actual_transport);\n      if (!actual_transport || (strcmp(actual_transport, transport) != 0)) {\n\tprintf(\"Failed to load transport \\\"%s\\\"\\n\", transport);\n\tMPI_Finalize();\n\texit(1);\n      }\n    }\n    string_list = attr_list_to_string(contact_list);\n    free_attr_list(contact_list);\n    strcpy(master_contact, string_list);\n\n    if (me == 0) {    \n\n\tCMFormat format;\n\tint message_wait_condition;\n\ttime_t start, end;\n\tif (quiet <= 0) {\n\t    printf(\"Master contact is %s\\n\", master_contact);\n\t}\n\tformat = CMregister_format(cm, simple_format_list);\n\tmessage_wait_condition = CMCondition_get(cm, NULL);\n\tCMregister_handler(format, simple_handler, (void*)(long)message_wait_condition);\n\tMPI_Bcast(master_contact,CONTACTLEN,MPI_CHAR,0,MPI_COMM_WORLD);\n \tstart = time(NULL);\n\tCMCondition_wait(cm, message_wait_condition);\n\tend = time(NULL);\n\tprintf(\"Elapsed time was %d\\n\", (int)(end - start));\n    } else { \n\n\tattr_list contact_list;\n\tCMConnection conn = NULL;\n\tCMFormat format;\n\tsimple_rec data;\n\tattr_list attrs;\n\tatom_t CMDEMO_TEST_ATOM;\n\tMPI_Bcast(master_contact,CONTACTLEN,MPI_CHAR,0,MPI_COMM_WORLD);\n\tif (quiet <= 0) {\n\t    printf(\"Node %d thinks master contact is %s\\n\", me, master_contact);\n\t}\n\tcontact_list = attr_list_from_string(master_contact);\n\tconn = CMinitiate_conn(cm, contact_list);\n\tif (conn == NULL) {\n\t    printf(\"No connection, attr list was :\");\n\t    dump_attr_list(contact_list);\n\t    printf(\"\\n\");\n\t    exit(1);\n\t}\n\tfree_attr_list(contact_list);\n\tformat = CMregister_format(cm, simple_format_list);\n\tgenerate_record(&data);\n\tattrs = create_attr_list();\n\tCMDEMO_TEST_ATOM = attr_atom_from_string(\"CMdemo_test_atom\");\n\tadd_attr(attrs, CMDEMO_TEST_ATOM, Attr_Int4, (attr_value)45678);\n\tCMwrite_attr(conn, format, &data, attrs);\n\tCMsleep(cm, 1);\n\tfree_attr_list(attrs);\n    }\n\n    MPI_Finalize();\n    exit(0);\n}"}
{"program": "eliask_1105", "code": "main(int argc, char *argv[])\n{\n  double rn;\n  int i, myid;\n\n\n  \n\n            \n\n\n\n  \n\n\n  init_sprng(SEED,SPRNG_DEFAULT);\t\n\n  printf(\"Process %d, print information about stream:\\n\", myid);\n  print_sprng();\n\n  \n\n            \n  for (i=0;i<3;i++)\n  {\n    rn = sprng();\t\t\n\n    printf(\"Process %d, random number %d: %.14f\\n\", myid, i+1, rn);\n  }\n\n\n}", "label": "main(int argc, char *argv[])\n{\n  double rn;\n  int i, myid;\n\n\n  \n\n            \n  MPI_Init(&argc, &argv);       \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\t\n\n\n  \n\n\n  init_sprng(SEED,SPRNG_DEFAULT);\t\n\n  printf(\"Process %d, print information about stream:\\n\", myid);\n  print_sprng();\n\n  \n\n            \n  for (i=0;i<3;i++)\n  {\n    rn = sprng();\t\t\n\n    printf(\"Process %d, random number %d: %.14f\\n\", myid, i+1, rn);\n  }\n\n  MPI_Finalize();\t\t\n\n}"}
{"program": "gyaikhom_1106", "code": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    bc_init(BC_ERR | BC_PLIST_XALL);\n    FILE *f;\n    char fname[128];\n\n    sprintf(fname, \"scatter_bc_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, scatter_bc(1001, 1L << i));\n    }\n    fclose(f);\n\n    sprintf(fname, \"scatter_mpi_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, scatter_mpi(1001, 1L << i));\n    }\n    fclose(f);\n\n    bc_final();\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    MPI_Init(&argc, &argv);\n    bc_init(BC_ERR | BC_PLIST_XALL);\n    FILE *f;\n    char fname[128];\n\n    sprintf(fname, \"scatter_bc_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, scatter_bc(1001, 1L << i));\n    }\n    fclose(f);\n\n    sprintf(fname, \"scatter_mpi_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, scatter_mpi(1001, 1L << i));\n    }\n    fclose(f);\n\n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "htapiagroup_1107", "code": "int main(int argc, char *argv[])\r\n{\r\n\tint rank, size, dato=0;\r\n\tchar host[20];\r\n\tMPI_Status estado;\r\n\n\n\n\t\n\n\tgethostname(host, 20);\r\n\tif(rank > 0 && rank < (size-1))\r\n\t{\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado \n\n\t\t); \r\n\t\tprintf\r\n\t\t(\r\n\t\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\tdato=run_kernel(dato,rank);\r\n\t}else if(rank == size-1){\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado); \n\n\t\tprintf\r\n\t\t(\r\n\t\t\t\u201c\\nProcess[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\t\r\n\t\tdato=run_kernel(dato,rank);\r\n\t}else if(rank == 0){ \r\n\tprintf\r\n\t(\r\n\t\"\\nProceso[ %d ] desde %s : Enviando Dato = %d \\n\",\r\n\trank,\r\n\thost,\r\n\tdato\r\n\t);\r\n\tMPI_Send\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\trank+1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD \n\n\t); \r\n\tMPI_Recv\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\tsize-1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD, \n\n\t\t&estado \n\n\t);\r\n\tprintf\r\n\t(\r\n\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d \\n\\n\",\r\n\t\trank,\r\n\t\thost,\r\n\t\tdato\r\n\t);\r\n}\r\n\treturn 0;\r\n} ", "label": "int main(int argc, char *argv[])\r\n{\r\n\tint rank, size, dato=0;\r\n\tchar host[20];\r\n\tMPI_Status estado;\r\n\tMPI_Init (&argc, &argv); \n\n\tMPI_Comm_size (MPI_COMM_WORLD, &size);\n\n\tMPI_Comm_rank (MPI_COMM_WORLD, &rank); \n\n\t\n\n\tgethostname(host, 20);\r\n\tif(rank > 0 && rank < (size-1))\r\n\t{\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado \n\n\t\t); \r\n\t\tprintf\r\n\t\t(\r\n\t\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\tdato=run_kernel(dato,rank);\r\n\t\tMPI_Send(&dato, 1 ,MPI_INT ,rank+1 , 0 ,MPI_COMM_WORLD); \r\n\t}else if(rank == size-1){\r\n\t\tMPI_Recv\r\n\t\t(\r\n\t\t\t&dato, \n\n\t\t\t1, \n\n\t\t\tMPI_INT, \n\n\t\t\trank-1, \n\n\t\t\t0, \n\n\t\t\tMPI_COMM_WORLD, \n\n\t\t\t&estado); \n\n\t\tprintf\r\n\t\t(\r\n\t\t\t\u201c\\nProcess[ %d ] desde %s : Recibi Dato = %d . Envio %d + %d \\n\",\r\n\t\t\trank,\r\n\t\t\thost,\r\n\t\t\tdato,\r\n\t\t\tdato,\r\n\t\t\trank\r\n\t\t);\r\n\t\t\r\n\t\tdato=run_kernel(dato,rank);\r\n\t\tMPI_Send(&dato, 1 ,MPI_INT ,0 , 0 ,MPI_COMM_WORLD);\r\n\t}else if(rank == 0){ \r\n\tprintf\r\n\t(\r\n\t\"\\nProceso[ %d ] desde %s : Enviando Dato = %d \\n\",\r\n\trank,\r\n\thost,\r\n\tdato\r\n\t);\r\n\tMPI_Send\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\trank+1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD \n\n\t); \r\n\tMPI_Recv\r\n\t(\r\n\t\t&dato, \n\n\t\t1, \n\n\t\tMPI_INT, \n\n\t\tsize-1, \n\n\t\t0, \n\n\t\tMPI_COMM_WORLD, \n\n\t\t&estado \n\n\t);\r\n\tprintf\r\n\t(\r\n\t\t\"\\nProceso[ %d ] desde %s : Recibi Dato = %d \\n\\n\",\r\n\t\trank,\r\n\t\thost,\r\n\t\tdato\r\n\t);\r\n}\r\n\tMPI_Finalize(); \r\n\treturn 0;\r\n} "}
{"program": "pheenyx_1110", "code": "int main(int argc, char* argv[]){\r\n    \r\n\tint myid, numprocs;\r\n\t\r\n\t\n\r\n\tint A[SIZE][SIZE];\r\n\tint x[SIZE];\r\n\tint i,j;\r\n\tif(myid == 0){\r\n\t\tfor(i=0;i<SIZE;++i){\r\n\t\t\tfor(j=0;j<SIZE;++j){\r\n\t\t\t\tA[i][j] = (i*SIZE+j) % 3;\r\n\t\t\t}\r\n\t\t\tx[i] = (SIZE-i) % 3;\r\n\t\t}\r\n\t}\r\n\r\n\t\n\r\n\t\n\t\r\n\t\n\r\n        int result[SIZE];\r\n\t\n\n        if (myid == 0) {\r\n        } else {\r\n            MPI_Status* status = 0;\r\n        }\r\n\r\n\t\n\n        int breadth = SIZE / numprocs;\r\n        for (i = 0; i < SIZE; i++) {\r\n            result[i] = 0;\r\n            for (j = myid * breadth; j < (myid + 1) * breadth; j++) {\r\n                result[i] += A[i][j] * x[j];\r\n            }\r\n        }\r\n\t\r\n\t\n\n        int new_result[SIZE];\r\n        if (myid == 0) {\r\n        }\r\n\t\r\n\t\r\n\t\n\r\n\t\n\r\n\t\n\r\n\r\n\t\n\t\r\n\tif(myid == 0){\r\n\t\tfor(i=0;i<SIZE;++i){\r\n\t\t\tprintf(\"%d \", new_result[i]);\r\n\t\t}\r\n\t\tprintf(\"\\n\");\r\n\t}\r\n\r\n\treturn 0;\r\n}", "label": "int main(int argc, char* argv[]){\r\n    \r\n\tMPI_Init(&argc, &argv);\r\n\tint myid, numprocs;\r\n\t\r\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);\r\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\r\n\tif(numprocs > SIZE){ MPI_Finalize(); return 0; }\r\n\t\n\r\n\tint A[SIZE][SIZE];\r\n\tint x[SIZE];\r\n\tint i,j;\r\n\tif(myid == 0){\r\n\t\tfor(i=0;i<SIZE;++i){\r\n\t\t\tfor(j=0;j<SIZE;++j){\r\n\t\t\t\tA[i][j] = (i*SIZE+j) % 3;\r\n\t\t\t}\r\n\t\t\tx[i] = (SIZE-i) % 3;\r\n\t\t}\r\n\t}\r\n\r\n\t\n\r\n\t\n\t\r\n\t\n\r\n        int result[SIZE];\r\n\t\n\n        if (myid == 0) {\r\n            MPI_Bcast(&A, SIZE * SIZE, MPI_INT, 0, MPI_COMM_WORLD);\r\n            MPI_Bcast(&x, SIZE, MPI_INT, 0, MPI_COMM_WORLD);\r\n        } else {\r\n            MPI_Status* status = 0;\r\n            MPI_Recv(&A, SIZE * SIZE, MPI_INT, 0, 0, MPI_COMM_WORLD, status);\r\n            MPI_Recv(&x, SIZE, MPI_INT, 0, 0, MPI_COMM_WORLD, status);\r\n        }\r\n\r\n\t\n\n        int breadth = SIZE / numprocs;\r\n        for (i = 0; i < SIZE; i++) {\r\n            result[i] = 0;\r\n            for (j = myid * breadth; j < (myid + 1) * breadth; j++) {\r\n                result[i] += A[i][j] * x[j];\r\n            }\r\n        }\r\n\t\r\n\t\n\n        int new_result[SIZE];\r\n        if (myid == 0) {\r\n            MPI_Reduce(&result[i], &new_result[i], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\r\n        }\r\n\t\r\n\t\r\n\t\n\r\n\t\n\r\n\t\n\r\n\r\n\t\n\t\r\n\tif(myid == 0){\r\n\t\tfor(i=0;i<SIZE;++i){\r\n\t\t\tprintf(\"%d \", new_result[i]);\r\n\t\t}\r\n\t\tprintf(\"\\n\");\r\n\t}\r\n\r\n\tMPI_Finalize(); \r\n\treturn 0;\r\n}"}
{"program": "JulianKunkel_1111", "code": "int main( int argc, char * argv[] )\n{\n\t\n\n\t\n\r\n\r\n\tint rank, size;\n\tMPI_File fh;\n\tchar* buf;\n\tMPI_Info info;\n\n\n\n\t\n\tMPI_Info infoUsed;\n\tint nkeys;\n\n   int vallen = MPI_MAX_INFO_VAL;\n   for (int i=0; i<nkeys; i++) {\n   \t  int exists;\n   \t  char key[MPI_MAX_INFO_KEY];\n   \t  char val[MPI_MAX_INFO_VAL + 1];\n \n        vallen = MPI_MAX_INFO_VAL;\n        printf(\"Key: %s=%s\\n\", key, val);\n    }\n\n\n\treturn 0;\n}", "label": "int main( int argc, char * argv[] )\n{\n\t\n\n\t\n\r\n\r\n\tint rank, size;\n\tMPI_File fh;\n\tchar* buf;\n\tMPI_Info info;\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\tMPI_Comm_size( MPI_COMM_WORLD, &size );\n\n\tMPI_Info_create(& info);\n\n\tMPI_File_open( MPI_COMM_WORLD, \"test.dat\", MPI_MODE_CREATE | MPI_MODE_RDWR, info, &fh );\n\t\n\tMPI_Info infoUsed;\n\tint nkeys;\n\n\tMPI_File_get_info( fh, & infoUsed );\n   MPI_Info_get_nkeys( infoUsed, &nkeys );\n   int vallen = MPI_MAX_INFO_VAL;\n   for (int i=0; i<nkeys; i++) {\n   \t  int exists;\n   \t  char key[MPI_MAX_INFO_KEY];\n   \t  char val[MPI_MAX_INFO_VAL + 1];\n        MPI_Info_get_nthkey( infoUsed, i, key );\n \n        vallen = MPI_MAX_INFO_VAL;\n        MPI_Info_get( infoUsed, key, vallen, val, & exists );\n        printf(\"Key: %s=%s\\n\", key, val);\n    }\n\n\tMPI_File_close( &fh );\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "markfasheh_1113", "code": "int main(int argc, char *argv[])\n{\n\tint ret, exec_argc, len;\n\tchar path[PATH_MAX + 1];\n\tchar **exec_argv = NULL;\n\tstruct run_item *item_list = NULL;\n\tstruct run_item *tmp_item;\n\n\tret =\n\tif (ret != MPI_SUCCESS) {\n\t\tfprintf(stderr, \"MPI_Init failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (argc < 2) {\n\t\tusage();\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tlen = strlen(argv[1]);\n\tif (len > PATH_MAX) {\n\t\tfprintf(stderr, \"Path \\\"%s\\\" is too long\\n\", argv[1]);\n\t\tret = ENAMETOOLONG;\n\t\tgoto out;\n\t}\n\n\tstrncpy(path, argv[1], PATH_MAX + 1);\n\n\twhile (len && path[--len] == '/')\n\t\tpath[len] = '\\0';\n\n\texec_argc = argc - 2;\n\tif (exec_argc)\n\t\texec_argv = &argv[2];\n\n\tif (is_runnable(path)) {\n\t\tret = run_executible(path, exec_argc, exec_argv);\n\t\tif (ret)\n\t\t\tfprintf(stderr, \"Error %d executing %s\\n\", ret, path);\n\t\tgoto out;\n\t}\n\n\tret = build_item_list(path, &item_list);\n\tif (ret) {\n\t\tfprintf(stderr, \"Error %d reading directory %s\\n\", ret, path);\n\t\tgoto out;\n\t}\n\n\tret = run_item_list(item_list, exec_argc, exec_argv);\n\tif (ret)\n\t\tfprintf(stderr, \"Error %d executing from directory %s\\n\", ret,\n\t\t\tpath);\n\nout:\n\twhile (item_list) {\n\t\ttmp_item = item_list;\n\t\titem_list = item_list->ri_next;\n\t\tfree(tmp_item);\n\t}\n\n\tif (ret)\n\telse\n\n\treturn ret;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint ret, exec_argc, len;\n\tchar path[PATH_MAX + 1];\n\tchar **exec_argv = NULL;\n\tstruct run_item *item_list = NULL;\n\tstruct run_item *tmp_item;\n\n\tret = MPI_Init(&argc, &argv);\n\tif (ret != MPI_SUCCESS) {\n\t\tfprintf(stderr, \"MPI_Init failed: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tif (argc < 2) {\n\t\tusage();\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tlen = strlen(argv[1]);\n\tif (len > PATH_MAX) {\n\t\tfprintf(stderr, \"Path \\\"%s\\\" is too long\\n\", argv[1]);\n\t\tret = ENAMETOOLONG;\n\t\tgoto out;\n\t}\n\n\tstrncpy(path, argv[1], PATH_MAX + 1);\n\n\twhile (len && path[--len] == '/')\n\t\tpath[len] = '\\0';\n\n\texec_argc = argc - 2;\n\tif (exec_argc)\n\t\texec_argv = &argv[2];\n\n\tif (is_runnable(path)) {\n\t\tret = run_executible(path, exec_argc, exec_argv);\n\t\tif (ret)\n\t\t\tfprintf(stderr, \"Error %d executing %s\\n\", ret, path);\n\t\tgoto out;\n\t}\n\n\tret = build_item_list(path, &item_list);\n\tif (ret) {\n\t\tfprintf(stderr, \"Error %d reading directory %s\\n\", ret, path);\n\t\tgoto out;\n\t}\n\n\tret = run_item_list(item_list, exec_argc, exec_argv);\n\tif (ret)\n\t\tfprintf(stderr, \"Error %d executing from directory %s\\n\", ret,\n\t\t\tpath);\n\nout:\n\twhile (item_list) {\n\t\ttmp_item = item_list;\n\t\titem_list = item_list->ri_next;\n\t\tfree(tmp_item);\n\t}\n\n\tif (ret)\n\t\tMPI_Abort(MPI_COMM_WORLD, 1);\n\telse\n\t\tMPI_Finalize();\n\n\treturn ret;\n}"}
{"program": "qingu_1114", "code": "int main(int argc, char *argv[])\n{\n    int err, errs = 0;\n\n    \n\n    parse_args(argc, argv);\n\n    \n\n\n    err = struct_negdisp_test();\n    if (verbose && err) fprintf(stderr, \"error in struct_negdisp_test\\n\");\n    errs += err;\n\n    err = vector_negstride_test();\n    if (verbose && err) fprintf(stderr, \"error in vector_negstride_test\\n\");\n    errs += err;\n\n    err = indexed_negdisp_test();\n    if (verbose && err) fprintf(stderr, \"error in indexed_negdisp_test\\n\");\n    errs += err;\n\n    err = struct_struct_test();\n    if (verbose && err) fprintf(stderr, \"error in struct_struct_test\\n\");\n    errs += err;\n\n    err = flatten_test();\n    if (verbose && err) fprintf(stderr, \"error in flatten_test\\n\");\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int err, errs = 0;\n\n    \n\n    MPI_Init(&argc, &argv);\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    err = struct_negdisp_test();\n    if (verbose && err) fprintf(stderr, \"error in struct_negdisp_test\\n\");\n    errs += err;\n\n    err = vector_negstride_test();\n    if (verbose && err) fprintf(stderr, \"error in vector_negstride_test\\n\");\n    errs += err;\n\n    err = indexed_negdisp_test();\n    if (verbose && err) fprintf(stderr, \"error in indexed_negdisp_test\\n\");\n    errs += err;\n\n    err = struct_struct_test();\n    if (verbose && err) fprintf(stderr, \"error in struct_struct_test\\n\");\n    errs += err;\n\n    err = flatten_test();\n    if (verbose && err) fprintf(stderr, \"error in flatten_test\\n\");\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "mpip_1119", "code": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3], N[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  fftw_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_r2r(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_r2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = fftw_alloc_real(alloc_local);\n  out = fftw_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_r2r(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_r2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real_3d(ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real_3d(ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_real_3d(ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  fftw_free(in); fftw_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3], N[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  fftw_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_r2r(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_r2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = fftw_alloc_real(alloc_local);\n  out = fftw_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_r2r(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_r2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real_3d(ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real_3d(ni, local_ni, local_i_start,\n      in);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real_3d(ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  fftw_free(in); fftw_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "syftalent_1122", "code": "int main(int argc, char *argv[])\n{\n    int tasks = 0, i, j;\n    MPI_Comm parent;\n#ifdef USE_THREADS\n    int provided;\n    pthread_t * threads = NULL;\n#else\n    MPI_Comm * child;\n#endif \n\n    int can_spawn, errs = 0;\n\n#ifdef USE_THREADS\n    CHECK_SUCCESS(MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided));\n    if (provided != MPI_THREAD_MULTIPLE) {\n\tfprintf(stderr, \"MPI does not provide THREAD_MULTIPLE support\\n\");\n    }\n#else\n    CHECK_SUCCESS(MPI_Init(&argc, &argv));\n#endif\n\n    errs += MTestSpawnPossible(&can_spawn);\n\n    if (!can_spawn) {\n        if (errs)\n            printf( \" Found %d errors\\n\", errs );\n        else\n            printf( \" No Errors\\n\" );\n        fflush( stdout );\n        goto fn_exit;\n    }\n\n    CHECK_SUCCESS(MPI_Comm_get_parent(&parent));\n\n    if (parent == MPI_COMM_NULL) { \n\n\tif (argc == 2) {\n\t    tasks = atoi(argv[1]);\n\t}\n\telse if (argc == 1) {\n\t    tasks = DEFAULT_TASKS;\n\t}\n\telse {\n\t    fprintf(stderr, \"Usage: %s {number_of_tasks}\\n\", argv[0]);\n\t}\n\n\tCHECK_SUCCESS(MPI_Comm_rank(MPI_COMM_WORLD, &comm_world_rank));\n\tCHECK_SUCCESS(MPI_Comm_size(MPI_COMM_WORLD, &comm_world_size));\n\n#ifdef USE_THREADS\n\tthreads = (pthread_t *) malloc(tasks * sizeof(pthread_t));\n\tif (!threads) {\n\t    fprintf(stderr, \"Unable to allocate memory for threads\\n\");\n\t}\n#else\n\tchild = (MPI_Comm *) malloc(tasks * sizeof(MPI_Comm));\n\tif (!child) {\n\t    fprintf(stderr, \"Unable to allocate memory for child communicators\\n\");\n\t}\n#endif \n\n\n#ifdef USE_THREADS\n\t\n\n\tfor (i = 0; i < tasks;) {\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tpthread_create(&threads[j], NULL, main_thread, &j);\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tpthread_join(threads[j], NULL);\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#else\n\t\n\n\tfor (i = 0; i < tasks;) {\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_spawn(&child[j], -1);\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_disconnect(&child[j], -1);\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#endif \n\n\n\tCHECK_SUCCESS(MPI_Barrier(MPI_COMM_WORLD));\n\n\tif (comm_world_rank == 0)\n\t    printf(\" No Errors\\n\");\n    }\n    else { \n\n\t\n\n\tCHECK_SUCCESS(MPI_Send(NULL, 0, MPI_CHAR, 0, 1, parent));\n\tCHECK_SUCCESS(MPI_Recv(NULL, 0, MPI_CHAR, 0, 1, parent, MPI_STATUS_IGNORE));\n\tCHECK_SUCCESS(MPI_Comm_disconnect(&parent));\n    }\n\nfn_exit:\n#ifdef USE_THREADS\n    if (threads) free(threads);\n#endif\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int tasks = 0, i, j;\n    MPI_Comm parent;\n#ifdef USE_THREADS\n    int provided;\n    pthread_t * threads = NULL;\n#else\n    MPI_Comm * child;\n#endif \n\n    int can_spawn, errs = 0;\n\n#ifdef USE_THREADS\n    CHECK_SUCCESS(MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided));\n    if (provided != MPI_THREAD_MULTIPLE) {\n\tfprintf(stderr, \"MPI does not provide THREAD_MULTIPLE support\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, -1);\n    }\n#else\n    CHECK_SUCCESS(MPI_Init(&argc, &argv));\n#endif\n\n    errs += MTestSpawnPossible(&can_spawn);\n\n    if (!can_spawn) {\n        if (errs)\n            printf( \" Found %d errors\\n\", errs );\n        else\n            printf( \" No Errors\\n\" );\n        fflush( stdout );\n        goto fn_exit;\n    }\n\n    CHECK_SUCCESS(MPI_Comm_get_parent(&parent));\n\n    if (parent == MPI_COMM_NULL) { \n\n\tif (argc == 2) {\n\t    tasks = atoi(argv[1]);\n\t}\n\telse if (argc == 1) {\n\t    tasks = DEFAULT_TASKS;\n\t}\n\telse {\n\t    fprintf(stderr, \"Usage: %s {number_of_tasks}\\n\", argv[0]);\n\t    MPI_Abort(MPI_COMM_WORLD, -1);\n\t}\n\n\tCHECK_SUCCESS(MPI_Comm_rank(MPI_COMM_WORLD, &comm_world_rank));\n\tCHECK_SUCCESS(MPI_Comm_size(MPI_COMM_WORLD, &comm_world_size));\n\n#ifdef USE_THREADS\n\tthreads = (pthread_t *) malloc(tasks * sizeof(pthread_t));\n\tif (!threads) {\n\t    fprintf(stderr, \"Unable to allocate memory for threads\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, -1);\n\t}\n#else\n\tchild = (MPI_Comm *) malloc(tasks * sizeof(MPI_Comm));\n\tif (!child) {\n\t    fprintf(stderr, \"Unable to allocate memory for child communicators\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, -1);\n\t}\n#endif \n\n\n#ifdef USE_THREADS\n\t\n\n\tfor (i = 0; i < tasks;) {\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tpthread_create(&threads[j], NULL, main_thread, &j);\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tpthread_join(threads[j], NULL);\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#else\n\t\n\n\tfor (i = 0; i < tasks;) {\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_spawn(&child[j], -1);\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_disconnect(&child[j], -1);\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#endif \n\n\n\tCHECK_SUCCESS(MPI_Barrier(MPI_COMM_WORLD));\n\n\tif (comm_world_rank == 0)\n\t    printf(\" No Errors\\n\");\n    }\n    else { \n\n\t\n\n\tCHECK_SUCCESS(MPI_Send(NULL, 0, MPI_CHAR, 0, 1, parent));\n\tCHECK_SUCCESS(MPI_Recv(NULL, 0, MPI_CHAR, 0, 1, parent, MPI_STATUS_IGNORE));\n\tCHECK_SUCCESS(MPI_Comm_disconnect(&parent));\n    }\n\nfn_exit:\n#ifdef USE_THREADS\n    if (threads) free(threads);\n#endif\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ahmadia_1125", "code": "static int\nPyMPI_Main(int argc, char **argv)\n{\n  int sts=0, flag=1, finalize=0;\n  double tstart,tend,elapsed;\n  int err,verbosity,rank,size;\n\n  verbosity = 0;\n  int c;\n     \n  while ((c = getopt (argc, argv, \":v\")) != -1)\n    switch (c)\n      {\n      case 'v':\n        verbosity = 2;\n        break;\n      default:\n        break;\n      }\n\n  \n\n  (void)MPI_Initialized(&flag);\n  if (!flag) {\n#if (defined(MPI_VERSION) && (MPI_VERSION > 1))\n    int required = MPI_THREAD_MULTIPLE;\n    int provided = MPI_THREAD_SINGLE;\n    (void)MPI_Init_thread(&argc, &argv, required, &provided);\n#else\n    (void)MPI_Init(&argc, &argv);\n#endif\n    finalize = 1;\n  }\n\n\n  \n\n  collfs_initialize(verbosity, NULL);\n  tstart =\n  collfs_comm_push(MPI_COMM_WORLD);\n\n  \n\n#if PY_MAJOR_VERSION >= 3\n  sts = Py3_Main(argc, argv);\n#else\n  sts = Py2_Main(argc, argv);\n#endif\n\n  collfs_comm_pop();\n  tend =\n  elapsed = tend - tstart;\n\n  if (verbosity > 1) printf(\"[%d] elapsed = %g\\n\",rank,elapsed);\n  {\n    struct {double time; int rank; } loc,gmax,gmin;\n    double gsum;\n    loc.time = elapsed;\n    loc.rank = rank;\n    if (!rank) printf(\"NumProcs %d  Min %g@%d  Max %g@%d  Ratio %g  Ave %g\\n\",size,gmin.time,gmin.rank,gmax.time,gmax.rank,gmax.time/gmin.time,gsum/size);\n  }\n\n  \n\n  collfs_finalize();\n\n  \n\n  (void)MPI_Finalized(&flag);\n  if (!flag) {\n    if (sts != 0) (void)MPI_Abort(MPI_COMM_WORLD, sts);\n    if (finalize) (void)MPI_Finalize();\n  }\n\n  return sts;\n}", "label": "static int\nPyMPI_Main(int argc, char **argv)\n{\n  int sts=0, flag=1, finalize=0;\n  double tstart,tend,elapsed;\n  int err,verbosity,rank,size;\n\n  verbosity = 0;\n  int c;\n     \n  while ((c = getopt (argc, argv, \":v\")) != -1)\n    switch (c)\n      {\n      case 'v':\n        verbosity = 2;\n        break;\n      default:\n        break;\n      }\n\n  \n\n  (void)MPI_Initialized(&flag);\n  if (!flag) {\n#if (defined(MPI_VERSION) && (MPI_VERSION > 1))\n    int required = MPI_THREAD_MULTIPLE;\n    int provided = MPI_THREAD_SINGLE;\n    (void)MPI_Init_thread(&argc, &argv, required, &provided);\n#else\n    (void)MPI_Init(&argc, &argv);\n#endif\n    finalize = 1;\n  }\n\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n\n  \n\n  collfs_initialize(verbosity, NULL);\n  tstart = MPI_Wtime();\n  collfs_comm_push(MPI_COMM_WORLD);\n\n  \n\n#if PY_MAJOR_VERSION >= 3\n  sts = Py3_Main(argc, argv);\n#else\n  sts = Py2_Main(argc, argv);\n#endif\n\n  collfs_comm_pop();\n  tend = MPI_Wtime();\n  elapsed = tend - tstart;\n\n  if (verbosity > 1) printf(\"[%d] elapsed = %g\\n\",rank,elapsed);\n  {\n    struct {double time; int rank; } loc,gmax,gmin;\n    double gsum;\n    loc.time = elapsed;\n    loc.rank = rank;\n    MPI_Reduce(&loc,&gmax,1,MPI_DOUBLE_INT,MPI_MAXLOC,0,MPI_COMM_WORLD);\n    MPI_Reduce(&loc,&gmin,1,MPI_DOUBLE_INT,MPI_MINLOC,0,MPI_COMM_WORLD);\n    MPI_Reduce(&elapsed,&gsum,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n    if (!rank) printf(\"NumProcs %d  Min %g@%d  Max %g@%d  Ratio %g  Ave %g\\n\",size,gmin.time,gmin.rank,gmax.time,gmax.rank,gmax.time/gmin.time,gsum/size);\n  }\n\n  \n\n  collfs_finalize();\n\n  \n\n  (void)MPI_Finalized(&flag);\n  if (!flag) {\n    if (sts != 0) (void)MPI_Abort(MPI_COMM_WORLD, sts);\n    if (finalize) (void)MPI_Finalize();\n  }\n\n  return sts;\n}"}
{"program": "Unidata_1126", "code": "int\nmain(int argc, char **argv)\n{\n   int i;\n   char testfile[NC_MAX_NAME + 1];\n\n#ifdef USE_PNETCDF\n#endif\n\n   printf(\"\\n*** Testing small files.\\n\");\n   \n\n\n   \n\n   for (i = NUM_FORMATS; i >= 1; i--)\n   {\n      switch (i)\n      {\n\t case NC_FORMAT_CLASSIC:\n\t    nc_set_default_format(NC_FORMAT_CLASSIC, NULL);\n\t    printf(\"Switching to netCDF classic format.\\n\");\n\t    strcpy(testfile, \"tst_small_classic.nc\");\n\t    break;\n\t case NC_FORMAT_64BIT_OFFSET:\n\t    nc_set_default_format(NC_FORMAT_64BIT_OFFSET, NULL);\n\t    printf(\"Switching to 64-bit offset format.\\n\");\n\t    strcpy(testfile, \"tst_small_64bit.nc\");\n\t    break;\n#ifdef ENABLE_CDF5\n\t case NC_FORMAT_CDF5:\n\t    nc_set_default_format(NC_FORMAT_CDF5, NULL);\n\t    printf(\"Switching to 64-bit data format.\\n\");\n\t    strcpy(testfile, \"tst_small_cdf5.nc\");\n\t    break;\n#else\n      case NC_FORMAT_CDF5:\n        continue;\n#endif\n#ifdef USE_HDF5\n\t case NC_FORMAT_NETCDF4_CLASSIC:\n\t    nc_set_default_format(NC_FORMAT_NETCDF4_CLASSIC, NULL);\n\t    strcpy(testfile, \"tst_small_netcdf4_classic.nc\");\n\t    printf(\"Switching to netCDF-4 format (with NC_CLASSIC_MODEL).\\n\");\n\t    break;\n\t case NC_FORMAT_NETCDF4: \n\n\t    nc_set_default_format(NC_FORMAT_NETCDF4, NULL);\n\t    strcpy(testfile, \"tst_small_netcdf4.nc\");\n\t    printf(\"Switching to netCDF-4 format.\\n\");\n\t    break;\n#else\n\t case NC_FORMAT_NETCDF4_CLASSIC:\n\t case NC_FORMAT_NETCDF4:\n\t    continue; \n\n#endif\n\t default:\n\t    printf(\"Unexpected format!\\n\");\n\t    return 2;\n      }\n\n      printf(\"*** testing simple small file with a global attribute...\");\n      test_small_atts(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing simple small file with fixed dimensions...\");\n      test_small_fixed(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing simple small file with an unlimited dimension...\");\n      test_small_unlim(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one variable...\");\n      test_small_one(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one variable and one att...\");\n      test_one_with_att(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one record variable, which grows...\");\n      test_one_growing(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one growing record \"\n\t     \"variable, with attributes added...\");\n      test_one_growing_with_att(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with two growing record \"\n\t     \"variables, with attributes added...\");\n      test_two_growing_with_att(testfile);\n      SUMMARIZE_ERR;\n   }\n\n#ifdef USE_PNETCDF\n#endif\n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char **argv)\n{\n   int i;\n   char testfile[NC_MAX_NAME + 1];\n\n#ifdef USE_PNETCDF\n   MPI_Init(&argc, &argv);\n#endif\n\n   printf(\"\\n*** Testing small files.\\n\");\n   \n\n\n   \n\n   for (i = NUM_FORMATS; i >= 1; i--)\n   {\n      switch (i)\n      {\n\t case NC_FORMAT_CLASSIC:\n\t    nc_set_default_format(NC_FORMAT_CLASSIC, NULL);\n\t    printf(\"Switching to netCDF classic format.\\n\");\n\t    strcpy(testfile, \"tst_small_classic.nc\");\n\t    break;\n\t case NC_FORMAT_64BIT_OFFSET:\n\t    nc_set_default_format(NC_FORMAT_64BIT_OFFSET, NULL);\n\t    printf(\"Switching to 64-bit offset format.\\n\");\n\t    strcpy(testfile, \"tst_small_64bit.nc\");\n\t    break;\n#ifdef ENABLE_CDF5\n\t case NC_FORMAT_CDF5:\n\t    nc_set_default_format(NC_FORMAT_CDF5, NULL);\n\t    printf(\"Switching to 64-bit data format.\\n\");\n\t    strcpy(testfile, \"tst_small_cdf5.nc\");\n\t    break;\n#else\n      case NC_FORMAT_CDF5:\n        continue;\n#endif\n#ifdef USE_HDF5\n\t case NC_FORMAT_NETCDF4_CLASSIC:\n\t    nc_set_default_format(NC_FORMAT_NETCDF4_CLASSIC, NULL);\n\t    strcpy(testfile, \"tst_small_netcdf4_classic.nc\");\n\t    printf(\"Switching to netCDF-4 format (with NC_CLASSIC_MODEL).\\n\");\n\t    break;\n\t case NC_FORMAT_NETCDF4: \n\n\t    nc_set_default_format(NC_FORMAT_NETCDF4, NULL);\n\t    strcpy(testfile, \"tst_small_netcdf4.nc\");\n\t    printf(\"Switching to netCDF-4 format.\\n\");\n\t    break;\n#else\n\t case NC_FORMAT_NETCDF4_CLASSIC:\n\t case NC_FORMAT_NETCDF4:\n\t    continue; \n\n#endif\n\t default:\n\t    printf(\"Unexpected format!\\n\");\n\t    return 2;\n      }\n\n      printf(\"*** testing simple small file with a global attribute...\");\n      test_small_atts(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing simple small file with fixed dimensions...\");\n      test_small_fixed(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing simple small file with an unlimited dimension...\");\n      test_small_unlim(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one variable...\");\n      test_small_one(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one variable and one att...\");\n      test_one_with_att(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one record variable, which grows...\");\n      test_one_growing(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with one growing record \"\n\t     \"variable, with attributes added...\");\n      test_one_growing_with_att(testfile);\n      SUMMARIZE_ERR;\n\n      printf(\"*** testing small file with two growing record \"\n\t     \"variables, with attributes added...\");\n      test_two_growing_with_att(testfile);\n      SUMMARIZE_ERR;\n   }\n\n#ifdef USE_PNETCDF\n   MPI_Finalize();\n#endif\n   FINAL_RESULTS;\n}"}
{"program": "germasch_1127", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  mrc_crds_set_type(crds, \"uniform\");\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  \n  if (strcmp(mrc_crds_type(crds), \"rectilinear\") == 0) {\n    mrctest_set_crds_rectilinear_1(domain);\n  }\n\n  struct mrc_fld *fld = mrc_domain_fld_create(domain, 2, \"fld0:fld1\");\n  mrc_fld_set_name(fld, \"test_fld\");\n  mrc_fld_set_from_options(fld);\n  mrc_fld_setup(fld);\n  mrc_fld_view(fld);\n  \n  set_fld(fld);\n  check_fld(fld);\n\n  switch (testcase) {\n  case 1:\n    test_write_fld(fld);\n    break;\n  case 2:\n    test_write_read_fld(fld);\n    break;\n  }\n  mrc_fld_destroy(fld);\n  mrc_domain_destroy(domain);\n\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  int testcase = 1;\n  mrc_params_get_option_int(\"case\", &testcase);\n\n  struct mrc_domain *domain = mrc_domain_create(MPI_COMM_WORLD);\n  mrc_domain_set_type(domain, \"multi\");\n  struct mrc_crds *crds = mrc_domain_get_crds(domain);\n  mrc_crds_set_type(crds, \"uniform\");\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  \n  if (strcmp(mrc_crds_type(crds), \"rectilinear\") == 0) {\n    mrctest_set_crds_rectilinear_1(domain);\n  }\n\n  struct mrc_fld *fld = mrc_domain_fld_create(domain, 2, \"fld0:fld1\");\n  mrc_fld_set_name(fld, \"test_fld\");\n  mrc_fld_set_from_options(fld);\n  mrc_fld_setup(fld);\n  mrc_fld_view(fld);\n  \n  set_fld(fld);\n  check_fld(fld);\n\n  switch (testcase) {\n  case 1:\n    test_write_fld(fld);\n    break;\n  case 2:\n    test_write_read_fld(fld);\n    break;\n  }\n  mrc_fld_destroy(fld);\n  mrc_domain_destroy(domain);\n\n  MPI_Finalize();\n}"}
{"program": "wayne927_1129", "code": "int main(int argc, char* argv[])\n{\n    if(argc < 3)\n    {\n        printf(\"scf n_max l_max\\n\");\n        exit(0);\n    }\n\n\n    \n\n    decompose_nmax = atoi(argv[1]);\n    decompose_lmax = atoi(argv[2]);\n    \n    \n\n    int n_elements = (decompose_nmax+1)*(decompose_lmax+1)*(decompose_lmax+1);\n\t\n    \n    \n\n\n    read_ascii();\n\n    printf(\"Task %d: NumPart = %lld\\n\", ThisTask, NumPart);\n\n\n    double* mat_cos = (double*)malloc(n_elements*sizeof(double));\n    double* mat_sin = (double*)malloc(n_elements*sizeof(double));\n\n    \n\n    scf_integral(mat_cos, mat_sin);\n\n\n    double* mat_cos_reduced = (double*)malloc(n_elements*sizeof(double));\n    double* mat_sin_reduced = (double*)malloc(n_elements*sizeof(double));\n\n    \n\n  \n   \n    \n\n  \n  \n    char out_filename[200];\n    FILE* fout;\n  \n    \n\n    if(ThisTask == 0)\n    {\n\n\n\n        sprintf(out_filename, \"scfmatrix_%d_%d.scf\", decompose_nmax, decompose_lmax);\n\n        fout = fopen(out_filename, \"w\");\n        \n        if(fout == NULL)\n        {\n            perror(\"Can't write to matrix file!\\n\");\n            exit(1);\n        }\n\n        fwrite(&decompose_nmax, sizeof(decompose_nmax), 1, fout);\n        fwrite(&decompose_lmax, sizeof(decompose_lmax), 1, fout);\n        fwrite(mat_cos, sizeof(double), n_elements, fout);\n        fwrite(mat_sin, sizeof(double), n_elements, fout);\n        fwrite(mat_cos_reduced, sizeof(double), n_elements, fout);\n        fwrite(mat_sin_reduced, sizeof(double), n_elements, fout);\n\n        fclose(fout);\n\n    }\n\n\n\n\tfree(xx);\n\tfree(yy);\n\tfree(zz);\n\tfree(mm);\n\tfree(mat_cos);\n\tfree(mat_sin);\n\tfree(mat_cos_reduced);\n\tfree(mat_sin_reduced);\n\t\n\t\n\treturn 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    if(argc < 3)\n    {\n        printf(\"scf n_max l_max\\n\");\n        exit(0);\n    }\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &ThisTask);\n    MPI_Comm_size(MPI_COMM_WORLD, &NTask);\n\n    \n\n    decompose_nmax = atoi(argv[1]);\n    decompose_lmax = atoi(argv[2]);\n    \n    \n\n    int n_elements = (decompose_nmax+1)*(decompose_lmax+1)*(decompose_lmax+1);\n\t\n    \n    \n\n\n    read_ascii();\n\n    printf(\"Task %d: NumPart = %lld\\n\", ThisTask, NumPart);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    double* mat_cos = (double*)malloc(n_elements*sizeof(double));\n    double* mat_sin = (double*)malloc(n_elements*sizeof(double));\n\n    \n\n    scf_integral(mat_cos, mat_sin);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    double* mat_cos_reduced = (double*)malloc(n_elements*sizeof(double));\n    double* mat_sin_reduced = (double*)malloc(n_elements*sizeof(double));\n\n    \n\n    MPI_Reduce(mat_cos, mat_cos_reduced, n_elements, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  \n    MPI_Barrier(MPI_COMM_WORLD);\n   \n    \n\n    MPI_Reduce(mat_sin, mat_sin_reduced, n_elements, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  \n  \n    char out_filename[200];\n    FILE* fout;\n  \n    \n\n    if(ThisTask == 0)\n    {\n\n\n\n        sprintf(out_filename, \"scfmatrix_%d_%d.scf\", decompose_nmax, decompose_lmax);\n\n        fout = fopen(out_filename, \"w\");\n        \n        if(fout == NULL)\n        {\n            perror(\"Can't write to matrix file!\\n\");\n            MPI_Finalize();\n            exit(1);\n        }\n\n        fwrite(&decompose_nmax, sizeof(decompose_nmax), 1, fout);\n        fwrite(&decompose_lmax, sizeof(decompose_lmax), 1, fout);\n        fwrite(mat_cos, sizeof(double), n_elements, fout);\n        fwrite(mat_sin, sizeof(double), n_elements, fout);\n        fwrite(mat_cos_reduced, sizeof(double), n_elements, fout);\n        fwrite(mat_sin_reduced, sizeof(double), n_elements, fout);\n\n        fclose(fout);\n\n    }\n\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n\tfree(xx);\n\tfree(yy);\n\tfree(zz);\n\tfree(mm);\n\tfree(mat_cos);\n\tfree(mat_sin);\n\tfree(mat_cos_reduced);\n\tfree(mat_sin_reduced);\n\t\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "ivanamihalek_1130", "code": "int main ( int argc, char * argv[]) {\n\n    int myrank, size;\n    int rank;\n    char  name[MPI_MAX_PROCESSOR_NAME];\n    char msg[MSG_LEN]; \n    int  length;\n    MPI_Status status;\n    \n    \n\n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    if ( myrank == 0) { \n\n\tint have_message = 0;\n\tint done = 0, ctr = 0;\n\t\n\t\n\n\twhile ( ! done ) {\n\t    for ( rank=1; rank < size && ! done; rank++) {\n\t\t\n\t\t\n\t\tif ( have_message ) {\n\n\t\t    strcpy(msg, pdb_name[ctr]); \n\t\t    ctr ++;\n\t\t    done = ( ctr== nr_of_names);\n\t\t    printf(\"rank: %d   name: %s  collected %d msgs\\n\", myrank, name, ctr);\n\t\t    fflush(stdout);\n\t\t    \n\t\t}\n\t    }\n\t}\n        \n\n\tfor (rank=1; rank < size; rank++) {\n\t    \n\n\t    strcpy (msg,\"done\"); \n\t}\n\t\n    \n\n    \n\n    \n\n    } else {\n\tint done = 0;\n\tsrand48 ( time(NULL)*myrank );\n\t\n \n\tmemset   (msg, 0, MSG_LEN);\n\tstrcpy   (msg, \"I'm up\"); \n\t\n\n\twhile ( ! done ) {\n\t    \n\n\t    memset (msg, 0, MSG_LEN);\n\t    printf(\"rank: %d   name: %s   received: %s\\n\", myrank, name, msg);\n\t    fflush(stdout);\n\t    if ( !strcmp(msg, \"done\") ){\n\t\tdone = 1;\n\t    } else {\n\t\t\n\n\t\t\n\t\tpid_t pid;\n\t\tint retval;\n\t\tchar cmd[250];\n\t\tchar local_name[50];\n\n\t\tmemset  (local_name, 0, 50);\n\t\tsprintf (local_name, \"%s\", msg);\n\t\tmemset  (msg, 0, MSG_LEN);\n\t\t\n\t\tswitch(pid=fork()) {\n\t\tcase -1: \n\n\t\t    strcpy   (msg, \"fail fork\");\n\t\t    break;\n\n\t\tcase 0:  \n\n\t\t    memset (cmd, 0, 250);\n\t\t    sprintf ( cmd,\n\t\t\t      \"/home/imihalek/projects/rate4site/src_18_10_04/rate4site -s %s.hssp.pruned.98_15.ph -a %s  -o %s.r4s \",\n\t\t\t      local_name, local_name,  local_name);\n\t\t    retval = system (cmd);\n\t\t    exit(retval);\n\n\t\tdefault: \n\n\t\t    wait(&retval);\n\t\t    if ( retval ) {\n\t\t\tstrcpy   (msg, \"fail r4s\");\n\t\t    } else {\n\t\t\tstrcpy   (msg, \"success\");\n\t\t    }\n\t\t    \n\t\t}\n\n\t\t\n\t\t\n\n\t\tprintf(\"rank: %d   name: %s   %s\\n\", myrank, name, msg);\n\t\tfflush(stdout);\n\t    }\n\t}\n    }\n\n    \n\n    return 0;\n\n}", "label": "int main ( int argc, char * argv[]) {\n\n    int myrank, size;\n    int rank;\n    char  name[MPI_MAX_PROCESSOR_NAME];\n    char msg[MSG_LEN]; \n    int  length;\n    MPI_Status status;\n    \n    \n\n    MPI_Init (&argc, &argv);\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n\n    MPI_Comm_rank (MPI_COMM_WORLD, &myrank);\n    MPI_Get_processor_name(name, &length);\n\n    \n\n    \n\n    \n\n    if ( myrank == 0) { \n\n\tint have_message = 0;\n\tint done = 0, ctr = 0;\n\t\n\t\n\n\twhile ( ! done ) {\n\t    for ( rank=1; rank < size && ! done; rank++) {\n\t\t\n\t\tMPI_Iprobe(rank, 99, MPI_COMM_WORLD, &have_message, &status);\n\t\t\n\t\tif ( have_message ) {\n\t\t    MPI_Recv(msg, MSG_LEN, MPI_CHAR, rank, 99, MPI_COMM_WORLD, &status);\n\n\t\t    strcpy(msg, pdb_name[ctr]); \n\t\t    MPI_Send(msg, strlen(msg), MPI_CHAR, rank, 99, MPI_COMM_WORLD);\n\t\t    ctr ++;\n\t\t    done = ( ctr== nr_of_names);\n\t\t    printf(\"rank: %d   name: %s  collected %d msgs\\n\", myrank, name, ctr);\n\t\t    fflush(stdout);\n\t\t    \n\t\t}\n\t    }\n\t}\n        \n\n\tfor (rank=1; rank < size; rank++) {\n\t    \n\n\t    strcpy (msg,\"done\"); \n\t    MPI_Send(msg, strlen(msg), MPI_CHAR, rank, 99, MPI_COMM_WORLD); \n\t}\n\t\n    \n\n    \n\n    \n\n    } else {\n\tint done = 0;\n\tsrand48 ( time(NULL)*myrank );\n\t\n \n\tmemset   (msg, 0, MSG_LEN);\n\tstrcpy   (msg, \"I'm up\"); \n\tMPI_Send (msg, strlen(msg), MPI_CHAR, 0, 99, MPI_COMM_WORLD); \n\t\n\n\twhile ( ! done ) {\n\t    \n\n\t    memset (msg, 0, MSG_LEN);\n\t    MPI_Recv(msg, MSG_LEN, MPI_CHAR, 0, 99, MPI_COMM_WORLD, &status); \n\t    printf(\"rank: %d   name: %s   received: %s\\n\", myrank, name, msg);\n\t    fflush(stdout);\n\t    if ( !strcmp(msg, \"done\") ){\n\t\tdone = 1;\n\t    } else {\n\t\t\n\n\t\t\n\t\tpid_t pid;\n\t\tint retval;\n\t\tchar cmd[250];\n\t\tchar local_name[50];\n\n\t\tmemset  (local_name, 0, 50);\n\t\tsprintf (local_name, \"%s\", msg);\n\t\tmemset  (msg, 0, MSG_LEN);\n\t\t\n\t\tswitch(pid=fork()) {\n\t\tcase -1: \n\n\t\t    strcpy   (msg, \"fail fork\");\n\t\t    break;\n\n\t\tcase 0:  \n\n\t\t    memset (cmd, 0, 250);\n\t\t    sprintf ( cmd,\n\t\t\t      \"/home/imihalek/projects/rate4site/src_18_10_04/rate4site -s %s.hssp.pruned.98_15.ph -a %s  -o %s.r4s \",\n\t\t\t      local_name, local_name,  local_name);\n\t\t    retval = system (cmd);\n\t\t    exit(retval);\n\n\t\tdefault: \n\n\t\t    wait(&retval);\n\t\t    if ( retval ) {\n\t\t\tstrcpy   (msg, \"fail r4s\");\n\t\t    } else {\n\t\t\tstrcpy   (msg, \"success\");\n\t\t    }\n\t\t    \n\t\t}\n\n\t\t\n\t\t\n\n\t\tprintf(\"rank: %d   name: %s   %s\\n\", myrank, name, msg);\n\t\tfflush(stdout);\n\t\tMPI_Send (msg, strlen(msg), MPI_CHAR, 0, 99, MPI_COMM_WORLD); \n\t    }\n\t}\n    }\n\n    \n\n    MPI_Finalize();   \n    return 0;\n\n}"}
{"program": "linhbngo_1131", "code": "int main(int argc, char * argv[] ) {\n  int rank;     \n\n  int size;     \n\n  int i;        \n\n  int distance; \n\n    \n    \n  \n\n  \n\n  distance = (int)(size / 2);\n    \n  i = 1;\n  while (distance >= 1){      \n    if ((rank >= distance) && (rank < distance * 2)){\n      printf (\"At time step %d, sender %d sends to %d\\n\", i, rank, rank - distance);\n    }\n    if (rank < distance) {\n      printf (\"At time step %d, receiver %d receives from %d\\n\", i, rank, rank + distance);\n    }\n    distance = distance / 2;\n    i += 1;\n  }\n    \n  return 0;  \n}", "label": "int main(int argc, char * argv[] ) {\n  int rank;     \n\n  int size;     \n\n  int i;        \n\n  int distance; \n\n    \n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n    \n  \n\n  \n\n  distance = (int)(size / 2);\n    \n  i = 1;\n  while (distance >= 1){      \n    if ((rank >= distance) && (rank < distance * 2)){\n      printf (\"At time step %d, sender %d sends to %d\\n\", i, rank, rank - distance);\n    }\n    if (rank < distance) {\n      printf (\"At time step %d, receiver %d receives from %d\\n\", i, rank, rank + distance);\n    }\n    distance = distance / 2;\n    i += 1;\n  }\n    \n  MPI_Finalize();\n  return 0;  \n}"}
{"program": "cbries_1132", "code": "t main (int argc, char **argv)\n{\n\tint i, m;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\tdouble messungTime = 0.0;\n\tdouble messungTimeSumme = 0.0;\n\tint datalen = 0;\n\tdouble *data = NULL;\n\tdouble *dataLocal = NULL;\n\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tif(argc<2) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Usage: hw10b bytes\\n\");\t\n\t\t}\n\t\treturn 1;\n\t}\n\n\t\n\n  datalen = atoi(argv[1]);\n\n\tm = datalen / nprocs;\n\n\tif(datalen % nprocs > 0) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"datalen should be divided by processors.\\n\");\n\t\t}\n\t\treturn 1;\n\t}\n\n\tsrand(time(NULL));\n\n\tdata = (double*) malloc(datalen*sizeof(double));\n\tif(myrank==0) {\n\t\tfor(i=0; i<datalen; i++) {\n\t\t\tdata[i] = 0.0;\n\t\t}\n\t}\n\n\tdataLocal = (double*) malloc(m*sizeof(double));\n\tfor(i=0; i<m; i++) {\n\t\tdataLocal[i] = (double) i + (myrank/100.0);\n\t}\n\n\tmessungTime =\n\n\n\n\tmessungTime = MPI_Wtime() - messungTime;\n\n\n\tif(myrank==0) {\n\t\tprintf(\"The operation took %.10lf seconds.\\n\", messungTimeSumme);\n\t}\n\n\t\n\n\n\treturn 0;\n}\n/*", "label": "t main (int argc, char **argv)\n{\n\tint i, m;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\tdouble messungTime = 0.0;\n\tdouble messungTimeSumme = 0.0;\n\tint datalen = 0;\n\tdouble *data = NULL;\n\tdouble *dataLocal = NULL;\n\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tif(argc<2) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"Usage: hw10b bytes\\n\");\t\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\t\n\n  datalen = atoi(argv[1]);\n\n\tm = datalen / nprocs;\n\n\tif(datalen % nprocs > 0) {\n\t\tif(myrank==0) {\n\t\t\tprintf(\"datalen should be divided by processors.\\n\");\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\tsrand(time(NULL));\n\n\tdata = (double*) malloc(datalen*sizeof(double));\n\tif(myrank==0) {\n\t\tfor(i=0; i<datalen; i++) {\n\t\t\tdata[i] = 0.0;\n\t\t}\n\t}\n\n\tdataLocal = (double*) malloc(m*sizeof(double));\n\tfor(i=0; i<m; i++) {\n\t\tdataLocal[i] = (double) i + (myrank/100.0);\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tmessungTime = MPI_Wtime();\n\n\tMPI_Gather(dataLocal, m, MPI_DOUBLE, \n\t\t\t\t\t\t data, m, MPI_DOUBLE, \n\t\t\t\t\t\t 0, MPI_COMM_WORLD);\n\n\tMPI_Bcast(data, datalen, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\tmessungTime = MPI_Wtime() - messungTime;\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tMPI_Reduce(&messungTime, &messungTimeSumme, 1, MPI_DOUBLE, MPI_SUM,\n\t\t\t\t\t\t 0, MPI_COMM_WORLD);\n\n\tif(myrank==0) {\n\t\tprintf(\"The operation took %.10lf seconds.\\n\", messungTimeSumme);\n\t}\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n/*"}
{"program": "syftalent_1133", "code": "int main(int argc, char **argv)\n{\n    int rank, size, namelen;\n    FILE *fp;\n    char path[PATH_MAX];\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n\n    char shell_cmd[100];\n    sprintf(shell_cmd, \"cat /proc/%d/stat | awk '{print $39}'\", getpid());\n\n    if (rank == 0) {\n        printf(\"----------------------\\n\");\n        printf(\" PROCESS/CPU BINDING\\n\");\n        printf(\"----------------------\\n\");\n    }\n\n    fp = popen(shell_cmd, \"r\");\n\n    while (fgets(path, PATH_MAX, fp) != NULL) {\n        printf(\"%s[%d]: running on CPU %s\", processor_name, rank, path);\n    }\n\n    pclose(fp);\n\n    fflush(stdout);\n\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, namelen;\n    FILE *fp;\n    char path[PATH_MAX];\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    char shell_cmd[100];\n    sprintf(shell_cmd, \"cat /proc/%d/stat | awk '{print $39}'\", getpid());\n\n    if (rank == 0) {\n        printf(\"----------------------\\n\");\n        printf(\" PROCESS/CPU BINDING\\n\");\n        printf(\"----------------------\\n\");\n    }\n\n    fp = popen(shell_cmd, \"r\");\n\n    while (fgets(path, PATH_MAX, fp) != NULL) {\n        printf(\"%s[%d]: running on CPU %s\", processor_name, rank, path);\n    }\n\n    pclose(fp);\n\n    fflush(stdout);\n\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "BlueFern_1134", "code": "int main(int argc, char **argv)\n{\n\n\t\n\n    ode_workspace *odews;\n\n\n    odews = malloc(sizeof *odews); \t\n\n\n    int verbose = 1;\n\n    \n\n    odews->dt  \t  = 1e-5; \t\t\t\n\n    odews->t0     = 0;   \t\t\t\n\n    odews->tf     = T_FINAL;  \t\t\n\n    odews->ftol   = 1e-3; \t\t\t\n\n    odews->ytol   = 1e-3; \t\t\t\n\n    odews->nconv  = 5;    \t\t\t\n\n    odews->maxits = 100;   \t\t\t\n\n    odews->dtwrite = (double) 1/DT_PSEC; \t\t\n\n\n    \n\n    if (argc > 2)\n    {\n    \todews->tf = atoi(argv[2]);\n    }\n    if (argc > 3)\n    {\n    \todews->dtwrite = (double) 1/atoi(argv[3]);\n    }\n    \n    \n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\n\n\n\n    \n\n    solver_init(odews, argc, argv);\n\n    if (odews->W->rank == 0)\n    {\n    \tprintf(\"Reminder that usage: mpirun -np <number of cores> %s <Number of levels in tree> <Final time> <Number of outputs per second> <Theta map filename (if using)>\\n\", argv[0]);\n    \tprintf(\"ECS diffusion: %1.f, Gap junctions: %1.f\\n\", DIFFUSION_SWITCH, GJ_SWITCH);\n    \tprintf(\"SPATIAL_CHOICE %d\\n\", SPATIAL_CHOICE);\n    }\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n    double t0 =\n    back_euler(odews); \n\n    double tf =\n\n    odews->W->ntimestamps = (odews->tf-odews->t0)/odews->dt;\n\n    \n\n    if (odews->W->rank == 0)\n    {\n        if (verbose)\n        {\n            printf(\"Levels: %d Subtree size: %d N procs: %d\\n\", odews->W->N, odews->W->Nsub, odews->W->n_procs);\n            printf(\"Solution time:                %g seconds\\n\", tf - t0);\n            printf(\"    # fevals:                 %d\\n\", odews->W->fevals);\n            printf(\"    # Jacobians:              %d\\n\", odews->W->jacupdates);\n            printf(\"    # feval time:             %g seconds\\n\", odews->W->tfeval);\n            printf(\"    # Jacobian update time:   %g seconds\\n\", odews->W->tjacupdate);\n            printf(\"    # Jacobian symbolic time: %g seconds\\n\", odews->W->tjacfactorize);\n        }\n        else\n        {\n            printf(\"%4d%4d%4d%12.4e%4d%4d%12.4e%12.4e%12.4e\\n\", odews->W->N, odews->W->Nsub, odews->W->n_procs, tf - t0, odews->W->fevals, odews->W->jacupdates, odews->W->tfeval, odews->W->tjacupdate, odews->W->tjacfactorize);\n        }\n\n        printf(\"ECS diffusion: %1.f, Gap junctions: %1.f\\n\", DIFFUSION_SWITCH, GJ_SWITCH);\n        printf(\"SPATIAL_CHOICE %d\\n\", SPATIAL_CHOICE);\n        printf(\"Directory: %s, ECS diffusion: %1.f, Gap junctions: %1.f\\n\", odews->W->dirName, DIFFUSION_SWITCH, GJ_SWITCH);\n    }\n\n    \n\n    close_io(odews->W);\n    free_var(odews);\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n\t\n\n    ode_workspace *odews;\n\n    MPI_Init(&argc, &argv);\t\t\t\n\n    odews = malloc(sizeof *odews); \t\n\n\n    int verbose = 1;\n\n    \n\n    odews->dt  \t  = 1e-5; \t\t\t\n\n    odews->t0     = 0;   \t\t\t\n\n    odews->tf     = T_FINAL;  \t\t\n\n    odews->ftol   = 1e-3; \t\t\t\n\n    odews->ytol   = 1e-3; \t\t\t\n\n    odews->nconv  = 5;    \t\t\t\n\n    odews->maxits = 100;   \t\t\t\n\n    odews->dtwrite = (double) 1/DT_PSEC; \t\t\n\n\n    \n\n    if (argc > 2)\n    {\n    \todews->tf = atoi(argv[2]);\n    }\n    if (argc > 3)\n    {\n    \todews->dtwrite = (double) 1/atoi(argv[3]);\n    }\n    \n    \n\t\n\t\n\n\n\n\n\n\n\n\n\n\n\n\t\n\t\n\t\n\t\n\n\n\n    \n\n    solver_init(odews, argc, argv);\n\n    if (odews->W->rank == 0)\n    {\n    \tprintf(\"Reminder that usage: mpirun -np <number of cores> %s <Number of levels in tree> <Final time> <Number of outputs per second> <Theta map filename (if using)>\\n\", argv[0]);\n    \tprintf(\"ECS diffusion: %1.f, Gap junctions: %1.f\\n\", DIFFUSION_SWITCH, GJ_SWITCH);\n    \tprintf(\"SPATIAL_CHOICE %d\\n\", SPATIAL_CHOICE);\n    }\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n    double t0 = MPI_Wtime();\n    back_euler(odews); \n\n    double tf = MPI_Wtime();\n\n    odews->W->ntimestamps = (odews->tf-odews->t0)/odews->dt;\n\n    \n\n    if (odews->W->rank == 0)\n    {\n        if (verbose)\n        {\n            printf(\"Levels: %d Subtree size: %d N procs: %d\\n\", odews->W->N, odews->W->Nsub, odews->W->n_procs);\n            printf(\"Solution time:                %g seconds\\n\", tf - t0);\n            printf(\"    # fevals:                 %d\\n\", odews->W->fevals);\n            printf(\"    # Jacobians:              %d\\n\", odews->W->jacupdates);\n            printf(\"    # feval time:             %g seconds\\n\", odews->W->tfeval);\n            printf(\"    # Jacobian update time:   %g seconds\\n\", odews->W->tjacupdate);\n            printf(\"    # Jacobian symbolic time: %g seconds\\n\", odews->W->tjacfactorize);\n        }\n        else\n        {\n            printf(\"%4d%4d%4d%12.4e%4d%4d%12.4e%12.4e%12.4e\\n\", odews->W->N, odews->W->Nsub, odews->W->n_procs, tf - t0, odews->W->fevals, odews->W->jacupdates, odews->W->tfeval, odews->W->tjacupdate, odews->W->tjacfactorize);\n        }\n\n        printf(\"ECS diffusion: %1.f, Gap junctions: %1.f\\n\", DIFFUSION_SWITCH, GJ_SWITCH);\n        printf(\"SPATIAL_CHOICE %d\\n\", SPATIAL_CHOICE);\n        printf(\"Directory: %s, ECS diffusion: %1.f, Gap junctions: %1.f\\n\", odews->W->dirName, DIFFUSION_SWITCH, GJ_SWITCH);\n    }\n\n    \n\n    close_io(odews->W);\n    free_var(odews);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "scafacos_1135", "code": "int main (int argc, char** argv) {\n  fcs_int num_particles = TEST_N_PARTICLES;\n  fcs_float box_size = TEST_BOX_SIZE;\n  fcs_int i, px, py, pz;\n  fcs_float positions[3*TEST_N_PARTICLES];\n  fcs_float charges[TEST_N_PARTICLES];\n  fcs_float field[3*TEST_N_PARTICLES];\n  fcs_float potentials[TEST_N_PARTICLES];\n  MPI_Comm comm = MPI_COMM_WORLD;\n\n  i = 0;\n  for (px = 0; px < TEST_BOX_SIZE; ++px) {\n    for (py = 0; py < TEST_BOX_SIZE; ++py) {\n      for (pz = 0; pz < TEST_BOX_SIZE; ++pz) {\n        positions[3*i] = px + 0.5;\n        positions[3*i + 1] = py + 0.5;\n        positions[3*i + 2] = pz + 0.5;\n        charges[i] = 1.0-((px + py + pz) % 2)*2;\n        ++i;\n      }\n    }\n  }\n\n  \n\n  fcs_int near_field_flag = 1;\n  fcs_float box_a[] = { box_size, 0.0, 0.0 };\n  fcs_float box_b[] = { 0.0, box_size, 0.0 };\n  fcs_float box_c[] = { 0.0, 0.0, box_size };\n  fcs_float offset[] = {0.0, 0.0, 0.0};\n  fcs_int periodicity[] = {1, 1, 1};\n\n  FCS fcs_handle;\n  FCSResult fcs_result;\n\n  fcs_result = fcs_init(&fcs_handle, \"DIRECT\", comm);\n  assert_fcs(fcs_result);\n  fcs_result = fcs_set_common(fcs_handle, near_field_flag, box_a, box_b, box_c, offset, periodicity, num_particles);\n  assert_fcs(fcs_result);\n  fcs_result = fcs_tune(fcs_handle, num_particles, positions, charges);\n  assert_fcs(fcs_result);\n  fcs_result = fcs_run(fcs_handle, num_particles, positions, charges, field, potentials);\n  assert_fcs(fcs_result);\n\n  printf(\"Potentials via FCS DIRECT:\\n\");\n  printf(\"%\" FCS_LMOD_FLOAT \"f\\n\", potentials[0]);\n\n\n\n\n  printf(\"\\n\");\n  fcs_destroy(fcs_handle);\n\n  \n\n  fcs_int k;\n  fcs_float tolerance = 1.0;\n  void* rd = NULL;\n  ifcs_p2nfft_init(&rd, comm);\n  for (k = 0; k < 8; ++k) {\n    printf(\"===================================\\n\");\n    printf(\"Trying tolerance %\" FCS_LMOD_FLOAT \"f.\\n\", tolerance);\n    fcs_p2nfft_set_tolerance(rd, FCS_TOLERANCE_TYPE_FIELD, tolerance);\n    ifcs_p2nfft_tune(rd, periodicity, num_particles, positions, charges, box_a, box_b, box_c, offset, near_field_flag);\n    ifcs_p2nfft_run(rd, num_particles, num_particles, positions, charges, potentials, field);\n\n    printf(\"Potentials via P2NFFT\\n\");\n\n\n      printf(\"%.10\" FCS_LMOD_FLOAT \"f\\n\", potentials[0]);\n\n\n    tolerance /= 10.0;\n  }\n  ifcs_p2nfft_destroy(rd);\n  return 0;\n}", "label": "int main (int argc, char** argv) {\n  fcs_int num_particles = TEST_N_PARTICLES;\n  fcs_float box_size = TEST_BOX_SIZE;\n  fcs_int i, px, py, pz;\n  fcs_float positions[3*TEST_N_PARTICLES];\n  fcs_float charges[TEST_N_PARTICLES];\n  fcs_float field[3*TEST_N_PARTICLES];\n  fcs_float potentials[TEST_N_PARTICLES];\n  MPI_Init(&argc, &argv);\n  MPI_Comm comm = MPI_COMM_WORLD;\n\n  i = 0;\n  for (px = 0; px < TEST_BOX_SIZE; ++px) {\n    for (py = 0; py < TEST_BOX_SIZE; ++py) {\n      for (pz = 0; pz < TEST_BOX_SIZE; ++pz) {\n        positions[3*i] = px + 0.5;\n        positions[3*i + 1] = py + 0.5;\n        positions[3*i + 2] = pz + 0.5;\n        charges[i] = 1.0-((px + py + pz) % 2)*2;\n        ++i;\n      }\n    }\n  }\n\n  \n\n  fcs_int near_field_flag = 1;\n  fcs_float box_a[] = { box_size, 0.0, 0.0 };\n  fcs_float box_b[] = { 0.0, box_size, 0.0 };\n  fcs_float box_c[] = { 0.0, 0.0, box_size };\n  fcs_float offset[] = {0.0, 0.0, 0.0};\n  fcs_int periodicity[] = {1, 1, 1};\n\n  FCS fcs_handle;\n  FCSResult fcs_result;\n\n  fcs_result = fcs_init(&fcs_handle, \"DIRECT\", comm);\n  assert_fcs(fcs_result);\n  fcs_result = fcs_set_common(fcs_handle, near_field_flag, box_a, box_b, box_c, offset, periodicity, num_particles);\n  assert_fcs(fcs_result);\n  fcs_result = fcs_tune(fcs_handle, num_particles, positions, charges);\n  assert_fcs(fcs_result);\n  fcs_result = fcs_run(fcs_handle, num_particles, positions, charges, field, potentials);\n  assert_fcs(fcs_result);\n\n  printf(\"Potentials via FCS DIRECT:\\n\");\n  printf(\"%\" FCS_LMOD_FLOAT \"f\\n\", potentials[0]);\n\n\n\n\n  printf(\"\\n\");\n  fcs_destroy(fcs_handle);\n\n  \n\n  fcs_int k;\n  fcs_float tolerance = 1.0;\n  void* rd = NULL;\n  ifcs_p2nfft_init(&rd, comm);\n  for (k = 0; k < 8; ++k) {\n    printf(\"===================================\\n\");\n    printf(\"Trying tolerance %\" FCS_LMOD_FLOAT \"f.\\n\", tolerance);\n    fcs_p2nfft_set_tolerance(rd, FCS_TOLERANCE_TYPE_FIELD, tolerance);\n    ifcs_p2nfft_tune(rd, periodicity, num_particles, positions, charges, box_a, box_b, box_c, offset, near_field_flag);\n    ifcs_p2nfft_run(rd, num_particles, num_particles, positions, charges, potentials, field);\n\n    printf(\"Potentials via P2NFFT\\n\");\n\n\n      printf(\"%.10\" FCS_LMOD_FLOAT \"f\\n\", potentials[0]);\n\n\n    tolerance /= 10.0;\n  }\n  ifcs_p2nfft_destroy(rd);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gnu3ra_1138", "code": "int main(int argc, char **argv) {\n    int i, j, rank, nranks, peer, bufsize, errors;\n    double **buffer, *src_buf;\n    int count[2], src_stride, trg_stride, stride_level;\n\n    ARMCI_Init();\n\n\n    buffer = (double **) malloc(sizeof(double *) * nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    ARMCI_Malloc((void **) buffer, bufsize);\n    src_buf = ARMCI_Malloc_local(bufsize);\n\n    if (rank == 0)\n        printf(\"ARMCI Strided Put Test:\\n\");\n\n    src_stride = XDIM * sizeof(double);\n    trg_stride = XDIM * sizeof(double);\n    stride_level = 1;\n\n    count[1] = YDIM;\n    count[0] = XDIM * sizeof(double);\n\n    ARMCI_Barrier();\n\n    peer = (rank+1) % nranks;\n\n    for (i = 0; i < ITERATIONS; i++) {\n\n      for (j = 0; j < XDIM*YDIM; j++) {\n        *(src_buf + j) = rank + i;\n      }\n\n      ARMCI_PutS(\n          src_buf,\n          &src_stride,\n          (void *) buffer[peer],\n          &trg_stride,\n          count,\n          stride_level,\n          peer);\n    }\n\n    ARMCI_Barrier();\n\n    ARMCI_Access_begin(buffer[rank]);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer[rank] + i + j*XDIM);\n        const double expected = (1.0 + rank) + (1.0 + ((rank+nranks-1)%nranks)) + (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    ARMCI_Access_end(buffer[rank]);\n\n    ARMCI_Free((void *) buffer[rank]);\n    ARMCI_Free_local(src_buf);\n\n    ARMCI_Finalize();\n\n    if (errors == 0) {\n      printf(\"%d: Success\\n\", rank);\n      return 0;\n    } else {\n      printf(\"%d: Fail\\n\", rank);\n      return 1;\n    }\n}", "label": "int main(int argc, char **argv) {\n    int i, j, rank, nranks, peer, bufsize, errors;\n    double **buffer, *src_buf;\n    int count[2], src_stride, trg_stride, stride_level;\n\n    MPI_Init(&argc, &argv);\n    ARMCI_Init();\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    buffer = (double **) malloc(sizeof(double *) * nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    ARMCI_Malloc((void **) buffer, bufsize);\n    src_buf = ARMCI_Malloc_local(bufsize);\n\n    if (rank == 0)\n        printf(\"ARMCI Strided Put Test:\\n\");\n\n    src_stride = XDIM * sizeof(double);\n    trg_stride = XDIM * sizeof(double);\n    stride_level = 1;\n\n    count[1] = YDIM;\n    count[0] = XDIM * sizeof(double);\n\n    ARMCI_Barrier();\n\n    peer = (rank+1) % nranks;\n\n    for (i = 0; i < ITERATIONS; i++) {\n\n      for (j = 0; j < XDIM*YDIM; j++) {\n        *(src_buf + j) = rank + i;\n      }\n\n      ARMCI_PutS(\n          src_buf,\n          &src_stride,\n          (void *) buffer[peer],\n          &trg_stride,\n          count,\n          stride_level,\n          peer);\n    }\n\n    ARMCI_Barrier();\n\n    ARMCI_Access_begin(buffer[rank]);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer[rank] + i + j*XDIM);\n        const double expected = (1.0 + rank) + (1.0 + ((rank+nranks-1)%nranks)) + (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    ARMCI_Access_end(buffer[rank]);\n\n    ARMCI_Free((void *) buffer[rank]);\n    ARMCI_Free_local(src_buf);\n\n    ARMCI_Finalize();\n    MPI_Finalize();\n\n    if (errors == 0) {\n      printf(\"%d: Success\\n\", rank);\n      return 0;\n    } else {\n      printf(\"%d: Fail\\n\", rank);\n      return 1;\n    }\n}"}
{"program": "ClaudioNahmad_1139", "code": "int main(int argc, char *argv[])\n{\n  int i, j, nerrors=0, total_errors=0;\n\n  int rank, size;\n  int bpos;\n\n  MPI_Datatype darray;\n  MPI_Status status;\n  MPI_File mpi_fh;\n\n  \n\n  int distrib[2] = { MPI_DISTRIBUTE_CYCLIC, MPI_DISTRIBUTE_CYCLIC };\n  int bsize[2] = { NBLOCK, NBLOCK };\n  int gsize[2] = { NSIDE, NSIDE };\n  int psize[2] = { NPROC, NPROC };\n\n  double data[NSIDE*NSIDE];\n  double *ldata, *pdata;\n\n  int tsize, nelem;\n\n  MPI_File dfile;\n\n\n\n  \n\n  CHECK(MPI_Type_create_darray(size, rank, 2, gsize, distrib,\n\t\t\t bsize, psize, MPI_ORDER_FORTRAN, MPI_DOUBLE, &darray));\n  CHECK(MPI_Type_commit(&darray));\n  CHECK(MPI_Type_size(darray, &tsize));\n  nelem = tsize / sizeof(double);\n\n  for(i = 0; i < (NSIDE*NSIDE); i++) data[i] = i;\n\n  if (rank == 0) {\n    CHECK(MPI_File_open(MPI_COMM_SELF, argv[1],\n\t\tMPI_MODE_CREATE|MPI_MODE_WRONLY, MPI_INFO_NULL, &dfile));\n    CHECK(MPI_File_write(dfile, data, NSIDE*NSIDE, MPI_DOUBLE, &status));\n    CHECK(MPI_File_close(&dfile));\n  }\n\n  \n\n  ldata = (double *)malloc(tsize);\n  pdata = (double *)malloc(tsize);\n\n  \n\n  bpos = 0;\n  CHECK(MPI_Pack(data, 1, darray, pdata, tsize, &bpos, MPI_COMM_WORLD));\n\n\n  \n\n  CHECK(MPI_File_open(MPI_COMM_WORLD, argv[1], MPI_MODE_RDONLY, MPI_INFO_NULL, &mpi_fh));\n  CHECK(MPI_File_set_view(mpi_fh, 0, MPI_DOUBLE, darray, \"native\", MPI_INFO_NULL));\n  CHECK(MPI_File_read_all(mpi_fh, ldata, nelem, MPI_DOUBLE, &status));\n  CHECK(MPI_File_close(&mpi_fh));\n\n  for(i = 0; i < size; i++) {\n#ifdef VERBOSE\n    if(rank == i) {\n      printf(\"=== Rank %i === (%i elements) \\nPacked: \", rank, nelem);\n      for(j = 0; j < nelem; j++) {\n        printf(\"%4.1f \", pdata[j]);\n        fflush(stdout);\n      }\n      printf(\"\\nRead:   \");\n      for(j = 0; j < nelem; j++) {\n        printf(\"%4.1f \", ldata[j]);\n        fflush(stdout);\n      }\n      printf(\"\\n\\n\");\n      fflush(stdout);\n    }\n#endif\n    if(rank == i) {\n\tfor (j=0; j< nelem; j++) {\n\t    if (pdata[j] != ldata[j]) {\n\t\tfprintf(stderr, \"rank %d at index %d: packbuf %4.1f filebuf %4.1f\\n\",\n\t\t\trank, j, pdata[j], ldata[j]);\n\t\tnerrors++;\n\t    }\n\t}\n    }\n  }\n  if (rank == 0 && total_errors == 0)\n      printf(\" No Errors\\n\");\n\n  free(ldata);\n  free(pdata);\n\n  exit(total_errors);\n\n}", "label": "int main(int argc, char *argv[])\n{\n  int i, j, nerrors=0, total_errors=0;\n\n  int rank, size;\n  int bpos;\n\n  MPI_Datatype darray;\n  MPI_Status status;\n  MPI_File mpi_fh;\n\n  \n\n  int distrib[2] = { MPI_DISTRIBUTE_CYCLIC, MPI_DISTRIBUTE_CYCLIC };\n  int bsize[2] = { NBLOCK, NBLOCK };\n  int gsize[2] = { NSIDE, NSIDE };\n  int psize[2] = { NPROC, NPROC };\n\n  double data[NSIDE*NSIDE];\n  double *ldata, *pdata;\n\n  int tsize, nelem;\n\n  MPI_File dfile;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  CHECK(MPI_Type_create_darray(size, rank, 2, gsize, distrib,\n\t\t\t bsize, psize, MPI_ORDER_FORTRAN, MPI_DOUBLE, &darray));\n  CHECK(MPI_Type_commit(&darray));\n  CHECK(MPI_Type_size(darray, &tsize));\n  nelem = tsize / sizeof(double);\n\n  for(i = 0; i < (NSIDE*NSIDE); i++) data[i] = i;\n\n  if (rank == 0) {\n    CHECK(MPI_File_open(MPI_COMM_SELF, argv[1],\n\t\tMPI_MODE_CREATE|MPI_MODE_WRONLY, MPI_INFO_NULL, &dfile));\n    CHECK(MPI_File_write(dfile, data, NSIDE*NSIDE, MPI_DOUBLE, &status));\n    CHECK(MPI_File_close(&dfile));\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  ldata = (double *)malloc(tsize);\n  pdata = (double *)malloc(tsize);\n\n  \n\n  bpos = 0;\n  CHECK(MPI_Pack(data, 1, darray, pdata, tsize, &bpos, MPI_COMM_WORLD));\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  \n\n  CHECK(MPI_File_open(MPI_COMM_WORLD, argv[1], MPI_MODE_RDONLY, MPI_INFO_NULL, &mpi_fh));\n  CHECK(MPI_File_set_view(mpi_fh, 0, MPI_DOUBLE, darray, \"native\", MPI_INFO_NULL));\n  CHECK(MPI_File_read_all(mpi_fh, ldata, nelem, MPI_DOUBLE, &status));\n  CHECK(MPI_File_close(&mpi_fh));\n\n  for(i = 0; i < size; i++) {\n#ifdef VERBOSE\n    MPI_Barrier(MPI_COMM_WORLD);\n    if(rank == i) {\n      printf(\"=== Rank %i === (%i elements) \\nPacked: \", rank, nelem);\n      for(j = 0; j < nelem; j++) {\n        printf(\"%4.1f \", pdata[j]);\n        fflush(stdout);\n      }\n      printf(\"\\nRead:   \");\n      for(j = 0; j < nelem; j++) {\n        printf(\"%4.1f \", ldata[j]);\n        fflush(stdout);\n      }\n      printf(\"\\n\\n\");\n      fflush(stdout);\n    }\n#endif\n    if(rank == i) {\n\tfor (j=0; j< nelem; j++) {\n\t    if (pdata[j] != ldata[j]) {\n\t\tfprintf(stderr, \"rank %d at index %d: packbuf %4.1f filebuf %4.1f\\n\",\n\t\t\trank, j, pdata[j], ldata[j]);\n\t\tnerrors++;\n\t    }\n\t}\n    }\n  }\n  MPI_Allreduce(&nerrors, &total_errors, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  if (rank == 0 && total_errors == 0)\n      printf(\" No Errors\\n\");\n\n  free(ldata);\n  free(pdata);\n  MPI_Type_free(&darray);\n  MPI_Finalize();\n\n  exit(total_errors);\n\n}"}
{"program": "qingu_1140", "code": "int main(int argc, char *argv[])\n{\n    int tasks = 0, provided, i, j;\n    MPI_Comm parent;\n#ifndef USE_THREADS\n    MPI_Comm * child;\n#endif \n\n\n#ifdef USE_THREADS\n    CHECK_SUCCESS(MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided));\n    if (provided != MPI_THREAD_MULTIPLE) {\n\tfprintf(stderr, \"MPI does not provide THREAD_MULTIPLE support\\n\");\n    }\n#else\n    CHECK_SUCCESS(MPI_Init(&argc, &argv));\n#endif\n\n    CHECK_SUCCESS(MPI_Comm_get_parent(&parent));\n\n    if (parent == MPI_COMM_NULL) { \n\n\tif (argc == 2) {\n\t    tasks = atoi(argv[1]);\n\t}\n\telse if (argc == 1) {\n\t    tasks = DEFAULT_TASKS;\n\t}\n\telse {\n\t    fprintf(stderr, \"Usage: %s {number_of_tasks}\\n\", argv[0]);\n\t}\n\n\tCHECK_SUCCESS(MPI_Comm_rank(MPI_COMM_WORLD, &comm_world_rank));\n\tCHECK_SUCCESS(MPI_Comm_size(MPI_COMM_WORLD, &comm_world_size));\n\n#ifndef USE_THREADS\n\tchild = (MPI_Comm *) malloc(tasks * sizeof(MPI_Comm));\n\tif (!child) {\n\t    fprintf(stderr, \"Unable to allocate memory for child communicators\\n\");\n\t}\n#endif \n\n\n#ifdef USE_THREADS\n\t\n\n\tfor (i = 0; i < tasks;) {\n        for (j = 0; j < DEFAULT_TASK_WINDOW; j++){\n            MTest_Start_thread(main_thread, (void*)(size_t)j);\n        }\n        MTest_Join_threads();\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#else\n\t\n\n\tfor (i = 0; i < tasks;) {\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_spawn(&child[j], -1);\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_disconnect(&child[j], -1);\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#endif \n\n\n\tCHECK_SUCCESS(MPI_Barrier(MPI_COMM_WORLD));\n\n\tif (comm_world_rank == 0)\n\t    printf(\" No Errors\\n\");\n    }\n    else { \n\n\t\n\n\tCHECK_SUCCESS(MPI_Send(NULL, 0, MPI_CHAR, 0, 1, parent));\n\tCHECK_SUCCESS(MPI_Recv(NULL, 0, MPI_CHAR, 0, 1, parent, MPI_STATUS_IGNORE));\n\tCHECK_SUCCESS(MPI_Comm_disconnect(&parent));\n    }\n\nfn_exit:\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int tasks = 0, provided, i, j;\n    MPI_Comm parent;\n#ifndef USE_THREADS\n    MPI_Comm * child;\n#endif \n\n\n#ifdef USE_THREADS\n    CHECK_SUCCESS(MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided));\n    if (provided != MPI_THREAD_MULTIPLE) {\n\tfprintf(stderr, \"MPI does not provide THREAD_MULTIPLE support\\n\");\n\tMPI_Abort(MPI_COMM_WORLD, -1);\n    }\n#else\n    CHECK_SUCCESS(MPI_Init(&argc, &argv));\n#endif\n\n    CHECK_SUCCESS(MPI_Comm_get_parent(&parent));\n\n    if (parent == MPI_COMM_NULL) { \n\n\tif (argc == 2) {\n\t    tasks = atoi(argv[1]);\n\t}\n\telse if (argc == 1) {\n\t    tasks = DEFAULT_TASKS;\n\t}\n\telse {\n\t    fprintf(stderr, \"Usage: %s {number_of_tasks}\\n\", argv[0]);\n\t    MPI_Abort(MPI_COMM_WORLD, -1);\n\t}\n\n\tCHECK_SUCCESS(MPI_Comm_rank(MPI_COMM_WORLD, &comm_world_rank));\n\tCHECK_SUCCESS(MPI_Comm_size(MPI_COMM_WORLD, &comm_world_size));\n\n#ifndef USE_THREADS\n\tchild = (MPI_Comm *) malloc(tasks * sizeof(MPI_Comm));\n\tif (!child) {\n\t    fprintf(stderr, \"Unable to allocate memory for child communicators\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, -1);\n\t}\n#endif \n\n\n#ifdef USE_THREADS\n\t\n\n\tfor (i = 0; i < tasks;) {\n        for (j = 0; j < DEFAULT_TASK_WINDOW; j++){\n            MTest_Start_thread(main_thread, (void*)(size_t)j);\n        }\n        MTest_Join_threads();\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#else\n\t\n\n\tfor (i = 0; i < tasks;) {\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_spawn(&child[j], -1);\n\t    for (j = 0; j < DEFAULT_TASK_WINDOW; j++)\n\t\tprocess_disconnect(&child[j], -1);\n\t    i += DEFAULT_TASK_WINDOW;\n\t}\n#endif \n\n\n\tCHECK_SUCCESS(MPI_Barrier(MPI_COMM_WORLD));\n\n\tif (comm_world_rank == 0)\n\t    printf(\" No Errors\\n\");\n    }\n    else { \n\n\t\n\n\tCHECK_SUCCESS(MPI_Send(NULL, 0, MPI_CHAR, 0, 1, parent));\n\tCHECK_SUCCESS(MPI_Recv(NULL, 0, MPI_CHAR, 0, 1, parent, MPI_STATUS_IGNORE));\n\tCHECK_SUCCESS(MPI_Comm_disconnect(&parent));\n    }\n\nfn_exit:\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "qingu_1141", "code": "int main( int argc, char *argv[] )\n{\n    MPI_Request rreq, sreq, rreq2;\n    int wrank, wsize;\n    int buf = -1, sbuf = 2, rbuf=-1;;\n    int vbuf[10];\n    MPI_Comm dupworld;\n\n\n    \n\n    init_dbr();\n\n    \n\n    \n\n    \n\n\n    \n\n    printf( \"Should see pending recv with tag 17, 19 on dupworld and send with tag 18 on world\\n\" );\n    showQueues( 3, 3 );\n\n    \n\n\n    \n\n    printf( \"\\nAfter a few send/receives\\n\" );\n    printf( \"Should see recv with tag 19 on dupworld\\n\" );\n    showQueues( 3, 1 );\n\n\n    \n\n    printf( \"\\nAfter a few send/receives (all now matched)\\n\" );\n    showQueues( 3, 0 );\n\n    \n\n\n    \n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    MPI_Request rreq, sreq, rreq2;\n    int wrank, wsize;\n    int buf = -1, sbuf = 2, rbuf=-1;;\n    int vbuf[10];\n    MPI_Comm dupworld;\n\n    MPI_Init( &argc, &argv );\n\n    \n\n    init_dbr();\n\n    \n\n    MPI_Comm_dup( MPI_COMM_WORLD, &dupworld );\n    MPI_Comm_set_name( dupworld, \"Dup of comm world\" );\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n    MPI_Comm_size( MPI_COMM_WORLD, &wsize );\n    MPI_Irecv( &buf, 1, MPI_INT, (wrank + 1) % wsize, 17, dupworld, &rreq );\n    MPI_Irecv( vbuf, 10, MPI_INT, (wrank + 1) % wsize, 19, dupworld, &rreq2 );\n    MPI_Isend( &sbuf, 1, MPI_INT, (wrank + wsize - 1) % wsize, 18, \n\t       MPI_COMM_WORLD, &sreq );\n    \n\n    \n\n\n    \n\n    printf( \"Should see pending recv with tag 17, 19 on dupworld and send with tag 18 on world\\n\" );\n    showQueues( 3, 3 );\n    MPI_Barrier( MPI_COMM_WORLD );\n\n    \n\n    MPI_Send( &sbuf, 1, MPI_INT, (wrank + wsize - 1) % wsize, 17, dupworld );\n    MPI_Recv( &rbuf, 1, MPI_INT, (wrank + 1) % wsize, 18, MPI_COMM_WORLD, \n    \t      MPI_STATUS_IGNORE );\n    MPI_Wait( &rreq, MPI_STATUS_IGNORE );\n    MPI_Wait( &sreq, MPI_STATUS_IGNORE );\n\n    \n\n    printf( \"\\nAfter a few send/receives\\n\" );\n    printf( \"Should see recv with tag 19 on dupworld\\n\" );\n    showQueues( 3, 1 );\n\n    MPI_Barrier( MPI_COMM_WORLD );\n    MPI_Send( &sbuf, 1, MPI_INT, (wrank + wsize - 1) % wsize, 19, dupworld );\n\n    \n\n    printf( \"\\nAfter a few send/receives (all now matched)\\n\" );\n    showQueues( 3, 0 );\n\n    \n\n    MPI_Cancel(&rreq2);\n    MPI_Wait(&rreq2, MPI_STATUS_IGNORE);\n    MPI_Comm_free(&dupworld);\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "qingu_1143", "code": "int main( int argc, char *argv[] ) {\n  int np = NUM_SPAWNS;\n  int my_rank, size;\n  int errcodes[NUM_SPAWNS];\n  MPI_Comm allcomm;\n  MPI_Comm intercomm;\n\n\n\n\n  if ( intercomm == MPI_COMM_NULL ) {\n      fprintf(stdout, \"intercomm is null\\n\");\n  }\n\n\n\n  \n\n  \n\n\n  fprintf(stdout, \"%s:%d: Sleep starting; children should exit\\n\",\n          __FILE__, __LINE__ );fflush(stdout);\n  sleep(30);\n  fprintf(stdout, \n          \"%s:%d: Sleep done; all children should have already exited\\n\", \n          __FILE__, __LINE__ );fflush(stdout);\n\n  return 0;\n}", "label": "int main( int argc, char *argv[] ) {\n  int np = NUM_SPAWNS;\n  int my_rank, size;\n  int errcodes[NUM_SPAWNS];\n  MPI_Comm allcomm;\n  MPI_Comm intercomm;\n\n  MPI_Init( &argc, &argv );\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\n  MPI_Comm_spawn( (char*)\"./spawntest_child\", MPI_ARGV_NULL, np,\n                  MPI_INFO_NULL, 0, MPI_COMM_WORLD, &intercomm, errcodes );\n\n  if ( intercomm == MPI_COMM_NULL ) {\n      fprintf(stdout, \"intercomm is null\\n\");\n  }\n\n  MPI_Intercomm_merge(intercomm, 0, &allcomm);\n\n  MPI_Comm_rank(allcomm, &my_rank);\n  MPI_Comm_size(allcomm, &size);\n\n  \n\n  MPI_Barrier( allcomm );\n  \n\n  MPI_Comm_free( &allcomm );\n  MPI_Comm_disconnect( &intercomm );\n\n  fprintf(stdout, \"%s:%d: Sleep starting; children should exit\\n\",\n          __FILE__, __LINE__ );fflush(stdout);\n  sleep(30);\n  fprintf(stdout, \n          \"%s:%d: Sleep done; all children should have already exited\\n\", \n          __FILE__, __LINE__ );fflush(stdout);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "alucas_1145", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif ((loop % 2) == rank)\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_isend(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(tab_handle, other_rank, loop, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif ((loop % 2) == rank)\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_isend(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(tab_handle, other_rank, loop, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "jeffhammond_1146", "code": "int main(int argc, char * argv[])\n{\n\n    int rank, size;\n\n    int *   shptr = NULL;\n    MPI_Win shwin;\n\n    \n\n    MPI_Aint rsize = 0;\n    int rdisp;\n    int * rptr = NULL;\n    int lint = -999;\n    if (rptr==NULL || rsize!=sizeof(int)) {\n        printf(\"rptr=%p rsize=%zu \\n\", rptr, (size_t)rsize);\n    }\n\n    \n\n\n    if (rank==0) {\n        *shptr = 42; \n\n    }\n\n    \n\n\n    lint = *rptr;\n\n    \n\n\n    if (1==coll_check_equal(lint,MPI_COMM_WORLD)) {\n        if (rank==0) {\n            printf(\"SUCCESS!\\n\");\n        }\n    } else {\n        printf(\"rank %d: lint = %d \\n\", rank, lint);\n    }\n\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *   shptr = NULL;\n    MPI_Win shwin;\n    MPI_Win_allocate_shared(rank==0 ? sizeof(int) : 0,sizeof(int),\n                            MPI_INFO_NULL, MPI_COMM_WORLD,\n                            &shptr, &shwin);\n\n    \n\n    MPI_Aint rsize = 0;\n    int rdisp;\n    int * rptr = NULL;\n    int lint = -999;\n    MPI_Win_shared_query(shwin, 0, &rsize, &rdisp, &rptr);\n    if (rptr==NULL || rsize!=sizeof(int)) {\n        printf(\"rptr=%p rsize=%zu \\n\", rptr, (size_t)rsize);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    \n\n\n    MPI_Win_fence(MPI_MODE_NOPUT | MPI_MODE_NOPRECEDE, shwin);\n    if (rank==0) {\n        *shptr = 42; \n\n    }\n    MPI_Win_fence(MPI_MODE_NOPUT | MPI_MODE_NOSUCCEED, shwin);\n\n    \n\n\n    MPI_Win_fence(MPI_MODE_NOPUT | MPI_MODE_NOPRECEDE, shwin);\n    lint = *rptr;\n    MPI_Win_fence(MPI_MODE_NOPUT | MPI_MODE_NOSUCCEED, shwin);\n\n    \n\n\n    if (1==coll_check_equal(lint,MPI_COMM_WORLD)) {\n        if (rank==0) {\n            printf(\"SUCCESS!\\n\");\n        }\n    } else {\n        printf(\"rank %d: lint = %d \\n\", rank, lint);\n    }\n\n    MPI_Win_free(&shwin);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_1147", "code": "int main( int argc, char* argv[] ) \n{\n\tDictionary*\t\t\tdictionary;\n\tXML_IO_Handler*\t\t\tioHandler;\n\tStream*\t\t\t\tstream;\n\tStream*\t\t\t\terror;\n\tDictionary_Entry_Value*\t\tpluginList;\n\n\tIndex plugin_I, count;\n\n\n\tif( !Base_Init( &argc, &argv ) ) {\n\t\tfprintf( stderr, \"Error initialising StGermain, exiting.\\n\" );\n\t\texit( EXIT_FAILURE );\n\t}\n\tstream = Journal_Register( Info_Type, __FILE__ );\n\terror = Journal_Register( Info_Type, __FILE__ );\n\t\n\tdictionary = Dictionary_New();\n\n\t\n\n\tioHandler = XML_IO_Handler_New();\n\tIO_Handler_ReadAllFromCommandLine( ioHandler, argc, argv, dictionary );\n\n\tStream_Enable( stream, True );\n\tStream_Enable( error, True );\n\n\tpluginList = Dictionary_Get( dictionary, \"plugins\" );\n\tcount = Dictionary_Entry_Value_GetCount( pluginList );\n\tfor ( plugin_I = 0; plugin_I < count; ++plugin_I ) {\n\t\tJournal_Printf( stream, \"%s\\n\", Dictionary_Entry_Value_AsString( Dictionary_Entry_Value_GetElement( pluginList, plugin_I ) ) );\n\t}\n\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( ioHandler );\n\n\tBase_Finalise();\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) \n{\n\tDictionary*\t\t\tdictionary;\n\tXML_IO_Handler*\t\t\tioHandler;\n\tStream*\t\t\t\tstream;\n\tStream*\t\t\t\terror;\n\tDictionary_Entry_Value*\t\tpluginList;\n\n\tIndex plugin_I, count;\n\n\n\tMPI_Init( &argc, &argv );\n\tif( !Base_Init( &argc, &argv ) ) {\n\t\tfprintf( stderr, \"Error initialising StGermain, exiting.\\n\" );\n\t\texit( EXIT_FAILURE );\n\t}\n\tstream = Journal_Register( Info_Type, __FILE__ );\n\terror = Journal_Register( Info_Type, __FILE__ );\n\t\n\tdictionary = Dictionary_New();\n\n\t\n\n\tioHandler = XML_IO_Handler_New();\n\tIO_Handler_ReadAllFromCommandLine( ioHandler, argc, argv, dictionary );\n\n\tStream_Enable( stream, True );\n\tStream_Enable( error, True );\n\n\tpluginList = Dictionary_Get( dictionary, \"plugins\" );\n\tcount = Dictionary_Entry_Value_GetCount( pluginList );\n\tfor ( plugin_I = 0; plugin_I < count; ++plugin_I ) {\n\t\tJournal_Printf( stream, \"%s\\n\", Dictionary_Entry_Value_AsString( Dictionary_Entry_Value_GetElement( pluginList, plugin_I ) ) );\n\t}\n\n\tStg_Class_Delete( dictionary );\n\tStg_Class_Delete( ioHandler );\n\n\tBase_Finalise();\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "sg0_1148", "code": "int main(int argc, char **argv)\n{\n  int rank, nprocs, i, j;\n  int g_A, g_B, g_C, *local_C=NULL, dims[DIM]={SIZE,SIZE};\n  double val1=5., val2=4., alpha=3., beta=2.;\n\n#if defined(USE_ELEMENTAL)\n  \n\n  ElInitialize( &argc, &argv );\n  ElMPICommRank( MPI_COMM_WORLD, &rank );\n  ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n  \n\n  ElGlobalArraysConstruct_d( &eldga );\n  \n\n  ElGlobalArraysInitialize_d( eldga );\n#else\n\n\n  MA_init(C_INT, 1000, 1000);\n\n  GA_Initialize();\n#endif\n\n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysCreate_d( eldga, DIM, dims, \"array_A\", NULL, &g_A );\n#else\n  g_A = NGA_Create(C_DBL, DIM, dims, \"array_A\", NULL);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDuplicate_d( eldga, g_A, \"array_B\", &g_B );\n  ElGlobalArraysDuplicate_d( eldga, g_A, \"array_C\", &g_C );\n#else  \n  g_B = GA_Duplicate(g_A, \"array_B\");\n  g_C = GA_Duplicate(g_A, \"array_C\");\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysFill_d( eldga, g_A, &val1 );\n  ElGlobalArraysFill_d( eldga, g_B, &val2 );\n#else\n  GA_Fill(g_A, &val1);\n  GA_Fill(g_B, &val2);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDgemm_d( eldga, 'N', 'T', SIZE, SIZE, SIZE, alpha, g_A, g_B, beta, g_C );\n#else\n  GA_Dgemm('N', 'T', SIZE, SIZE, SIZE, alpha, g_A, g_B, beta, g_C);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysSync_d( eldga );\n#else\n  GA_Sync();\n#endif\n\n  if (rank == 0)\n      printf (\"alpha = %f and beta = %f\\n\", alpha, beta);\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysPrint_d( eldga, g_A );\n  ElGlobalArraysPrint_d( eldga, g_B );\n  ElGlobalArraysPrint_d( eldga, g_C );\n#else \n  GA_Print(g_A);\n  GA_Print(g_B);\n  GA_Print(g_C);\n#endif\n \n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysSync_d( eldga );\n#else\n  GA_Sync();\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDestroy_d( eldga, g_A );\n  ElGlobalArraysDestroy_d( eldga, g_B );\n  ElGlobalArraysDestroy_d( eldga, g_C );\n#else\n  GA_Destroy(g_A);\n  GA_Destroy(g_B);\n  GA_Destroy(g_C);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysTerminate_d( eldga );\n  \n\n  ElGlobalArraysDestruct_d( eldga );\n  ElFinalize();\n#else\n  GA_Terminate();\n#endif\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int rank, nprocs, i, j;\n  int g_A, g_B, g_C, *local_C=NULL, dims[DIM]={SIZE,SIZE};\n  double val1=5., val2=4., alpha=3., beta=2.;\n\n#if defined(USE_ELEMENTAL)\n  \n\n  ElInitialize( &argc, &argv );\n  ElMPICommRank( MPI_COMM_WORLD, &rank );\n  ElMPICommSize( MPI_COMM_WORLD, &nprocs );\n  \n\n  ElGlobalArraysConstruct_d( &eldga );\n  \n\n  ElGlobalArraysInitialize_d( eldga );\n#else\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  MA_init(C_INT, 1000, 1000);\n\n  GA_Initialize();\n#endif\n\n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysCreate_d( eldga, DIM, dims, \"array_A\", NULL, &g_A );\n#else\n  g_A = NGA_Create(C_DBL, DIM, dims, \"array_A\", NULL);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDuplicate_d( eldga, g_A, \"array_B\", &g_B );\n  ElGlobalArraysDuplicate_d( eldga, g_A, \"array_C\", &g_C );\n#else  \n  g_B = GA_Duplicate(g_A, \"array_B\");\n  g_C = GA_Duplicate(g_A, \"array_C\");\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysFill_d( eldga, g_A, &val1 );\n  ElGlobalArraysFill_d( eldga, g_B, &val2 );\n#else\n  GA_Fill(g_A, &val1);\n  GA_Fill(g_B, &val2);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDgemm_d( eldga, 'N', 'T', SIZE, SIZE, SIZE, alpha, g_A, g_B, beta, g_C );\n#else\n  GA_Dgemm('N', 'T', SIZE, SIZE, SIZE, alpha, g_A, g_B, beta, g_C);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysSync_d( eldga );\n#else\n  GA_Sync();\n#endif\n\n  if (rank == 0)\n      printf (\"alpha = %f and beta = %f\\n\", alpha, beta);\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysPrint_d( eldga, g_A );\n  ElGlobalArraysPrint_d( eldga, g_B );\n  ElGlobalArraysPrint_d( eldga, g_C );\n#else \n  GA_Print(g_A);\n  GA_Print(g_B);\n  GA_Print(g_C);\n#endif\n \n  \n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysSync_d( eldga );\n#else\n  GA_Sync();\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysDestroy_d( eldga, g_A );\n  ElGlobalArraysDestroy_d( eldga, g_B );\n  ElGlobalArraysDestroy_d( eldga, g_C );\n#else\n  GA_Destroy(g_A);\n  GA_Destroy(g_B);\n  GA_Destroy(g_C);\n#endif\n\n#if defined(USE_ELEMENTAL)\n  ElGlobalArraysTerminate_d( eldga );\n  \n\n  ElGlobalArraysDestruct_d( eldga );\n  ElFinalize();\n#else\n  GA_Terminate();\n  MPI_Finalize();\n#endif\n\n  return 0;\n}"}
{"program": "syftalent_1149", "code": "int main(int argc, char *argv[])\n{\n    int n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    MPI_Win nwin, piwin;\n\n\n    if (myid == 0) {\n    }\n    else {\n    }\n    while (1) {\n        if (myid == 0) {\n            fprintf(stdout, \"Enter the number of intervals: (0 quits) \");\n            fflush(stdout);\n            scanf(\"%d\",&n);\n            pi = 0.0;\n        }\n        if (myid != 0)\n        if (n == 0)\n            break;\n        else {\n            h = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs) {\n                x = h * ((double)i - 0.5);\n                sum += (4.0 / (1.0 + x*x));\n            }\n            mypi = h * sum;\n            if (myid == 0) {\n                fprintf(stdout, \"pi is approximately %.16f, Error is %.16f\\n\",\n                        pi, fabs(pi - PI25DT));\n                fflush(stdout);\n            }\n        }\n    }\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    MPI_Win nwin, piwin;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n    if (myid == 0) {\n        MPI_Win_create(&n, sizeof(int), 1, MPI_INFO_NULL,\n                       MPI_COMM_WORLD, &nwin);\n        MPI_Win_create(&pi, sizeof(double), 1, MPI_INFO_NULL,\n                       MPI_COMM_WORLD, &piwin);\n    }\n    else {\n        MPI_Win_create(MPI_BOTTOM, 0, 1, MPI_INFO_NULL,\n                       MPI_COMM_WORLD, &nwin);\n        MPI_Win_create(MPI_BOTTOM, 0, 1, MPI_INFO_NULL,\n                       MPI_COMM_WORLD, &piwin);\n    }\n    while (1) {\n        if (myid == 0) {\n            fprintf(stdout, \"Enter the number of intervals: (0 quits) \");\n            fflush(stdout);\n            scanf(\"%d\",&n);\n            pi = 0.0;\n        }\n        MPI_Win_fence(0, nwin);\n        if (myid != 0)\n            MPI_Get(&n, 1, MPI_INT, 0, 0, 1, MPI_INT, nwin);\n        MPI_Win_fence(0, nwin);\n        if (n == 0)\n            break;\n        else {\n            h = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs) {\n                x = h * ((double)i - 0.5);\n                sum += (4.0 / (1.0 + x*x));\n            }\n            mypi = h * sum;\n            MPI_Win_fence( 0, piwin);\n            MPI_Accumulate(&mypi, 1, MPI_DOUBLE, 0, 0, 1, MPI_DOUBLE,\n                           MPI_SUM, piwin);\n            MPI_Win_fence(0, piwin);\n            if (myid == 0) {\n                fprintf(stdout, \"pi is approximately %.16f, Error is %.16f\\n\",\n                        pi, fabs(pi - PI25DT));\n                fflush(stdout);\n            }\n        }\n    }\n    MPI_Win_free(&nwin);\n    MPI_Win_free(&piwin);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "peoronoob_1150", "code": "int main( int argc, char **argv )\n{\n\tint i, n;\n\tint var;\n\t\n\t\n\tn = GET_N();\n\tif( n < 2 ) {\n\t\tTESTS_ERROR( 1, \"Use this with at least 2 processes!\" );\n\t}\n\t\n\tif( GET_ID() == 0 ) {\n\t\tvar = 666;\n\t\tTESTS_MPI_SEND( &var, 1, MPI_INT, GET_ID()+1 );\n\t}\n\telse if( GET_ID() < GET_N()-1 ) {\n\t\tint size;\n\t\tMPI_Status status;\n\t\tSPD_DEBUG( \"Gonna receive %d bytes\", size );\n\t\t\n\t\tTESTS_MPI_RECEIVE( &var, 1, MPI_INT, GET_ID()-1 );\n\t\tTESTS_MPI_SEND( &var, 1, MPI_INT, GET_ID()+1 );\n\t}\n\telse \n {\n\t\tint size;\n\t\tMPI_Status status;\n\t\tSPD_DEBUG( \"Gonna receive %d bytes\", size );\n\t\t\n\t\tTESTS_MPI_RECEIVE( &var, 1, MPI_INT, GET_ID()-1 );\n\t}\n\t\n\tSPD_DEBUG( \"Got %d\", var );\n\t\n\treturn 0;\n}", "label": "int main( int argc, char **argv )\n{\n\tint i, n;\n\tint var;\n\t\n\tMPI_Init( &argc, &argv );\n\t\n\tn = GET_N();\n\tif( n < 2 ) {\n\t\tTESTS_ERROR( 1, \"Use this with at least 2 processes!\" );\n\t}\n\t\n\tif( GET_ID() == 0 ) {\n\t\tvar = 666;\n\t\tTESTS_MPI_SEND( &var, 1, MPI_INT, GET_ID()+1 );\n\t}\n\telse if( GET_ID() < GET_N()-1 ) {\n\t\tint size;\n\t\tMPI_Status status;\n\t\tMPI_Probe( GET_ID()-1, MPI_ANY_TAG, MPI_COMM_WORLD, &status );\n\t\tMPI_Get_count( &status, MPI_BYTE, &size );\n\t\tSPD_DEBUG( \"Gonna receive %d bytes\", size );\n\t\t\n\t\tTESTS_MPI_RECEIVE( &var, 1, MPI_INT, GET_ID()-1 );\n\t\tTESTS_MPI_SEND( &var, 1, MPI_INT, GET_ID()+1 );\n\t}\n\telse \n {\n\t\tint size;\n\t\tMPI_Status status;\n\t\tMPI_Probe( GET_ID()-1, MPI_ANY_TAG, MPI_COMM_WORLD, &status );\n\t\tMPI_Get_count( &status, MPI_BYTE, &size );\n\t\tSPD_DEBUG( \"Gonna receive %d bytes\", size );\n\t\t\n\t\tTESTS_MPI_RECEIVE( &var, 1, MPI_INT, GET_ID()-1 );\n\t}\n\t\n\tSPD_DEBUG( \"Got %d\", var );\n\t\n\tMPI_Finalize( );\n\treturn 0;\n}"}
{"program": "NLeSC_1152", "code": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Request *requests;\n    MPI_Status *statusses;\n\n\n\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n\n       requests = malloc(size * sizeof(MPI_Request));\n       statusses = malloc(size * sizeof(MPI_Status));\n\n       if (requests == 0) {\n          fprintf(stderr, \"Failed to allocate requests!\");\n          return 1;\n       }\n\n       if (statusses == 0) {\n          fprintf(stderr, \"Failed to allocate statusses!\");\n          return 1;\n       }\n\n       for (i=0;i<size;i++) {\n          error =\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Failed to post ireceive! %d\\n\", error);\n             return 1;\n          }\n       }\n    }\n\n    error =\n\n    if (error != MPI_SUCCESS) {\n       fprintf(stderr, \"Send failed on %d! %d\\n\", rank, error);\n       return 1;\n    }\n\n    if (rank == 0) {\n       error =\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Waitall failed on %d! %d\\n\", rank, error);\n          return 1;\n       }\n\n       free(requests);\n       free(statusses);\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int  namelen, rank, size, i, error;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int buffer[10] = { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };\n\n    MPI_Request *requests;\n    MPI_Status *statusses;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    fprintf(stderr, \"Process %d of %d on %s\\n\", rank, size, processor_name);\n\n    if (rank == 0) {\n\n       requests = malloc(size * sizeof(MPI_Request));\n       statusses = malloc(size * sizeof(MPI_Status));\n\n       if (requests == 0) {\n          fprintf(stderr, \"Failed to allocate requests!\");\n          return 1;\n       }\n\n       if (statusses == 0) {\n          fprintf(stderr, \"Failed to allocate statusses!\");\n          return 1;\n       }\n\n       for (i=0;i<size;i++) {\n          error = MPI_Irecv(buffer, 10, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &requests[i]);\n\n          if (error != MPI_SUCCESS) {\n             fprintf(stderr, \"Failed to post ireceive! %d\\n\", error);\n             return 1;\n          }\n       }\n    }\n\n    error = MPI_Send(buffer, 10, MPI_INTEGER, 0, 0, MPI_COMM_WORLD);\n\n    if (error != MPI_SUCCESS) {\n       fprintf(stderr, \"Send failed on %d! %d\\n\", rank, error);\n       return 1;\n    }\n\n    if (rank == 0) {\n       error = MPI_Waitall(size, requests, statusses);\n\n       if (error != MPI_SUCCESS) {\n          fprintf(stderr, \"Waitall failed on %d! %d\\n\", rank, error);\n          return 1;\n       }\n\n       free(requests);\n       free(statusses);\n    }\n\n    fprintf(stderr, \"Done!\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "RafaelDexter_1155", "code": "int main(int argc, char *argv[]) \n{\n  int i,j;\n  int numprocs0,myid0,ID0;\n  int numprocs1,myid1;\n  int num;\n  int Num_Comm_World1;\n  int myworld0,myworld1;\n  int *NPROCS1_ID,*NPROCS1_WD;\n  int *Comm_World1;\n  int *Comm_World_StartID;\n  MPI_Comm *MPI_CommWD;\n  MPI_Comm comm;\n\n  myworld0 = 0;\n\n  Num_Comm_World1 = 2;\n\n  \n\n\n  NPROCS1_ID = (int*)malloc(sizeof(int)*numprocs0); \n  Comm_World1 = (int*)malloc(sizeof(int)*numprocs0); \n  NPROCS1_WD = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  Comm_World_StartID = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  MPI_CommWD = (MPI_Comm*)malloc(sizeof(MPI_Comm)*Num_Comm_World1); \n\n  \n\n\n  Make_Comm_Worlds(MPI_COMM_WORLD, myid0, numprocs0, Num_Comm_World1, &myworld1, MPI_CommWD, \n                   NPROCS1_ID, Comm_World1, NPROCS1_WD, Comm_World_StartID);\n\n  \n\n\n\n  printf(\"numprocs0=%2d myid0=%2d myworld1=%2d numprocs1=%2d myid1=%2d\\n\",\n         numprocs0,myid0,myworld1,numprocs1,myid1);\n\n\n\n\n\n\n\n  \n\n\n  \n\n\n\n  \n\n\n  \n\n  free(NPROCS1_ID);\n  free(Comm_World1);\n  free(NPROCS1_WD);\n  free(Comm_World_StartID);\n  free(MPI_CommWD);\n\n  \n\n\n\n}", "label": "int main(int argc, char *argv[]) \n{\n  int i,j;\n  int numprocs0,myid0,ID0;\n  int numprocs1,myid1;\n  int num;\n  int Num_Comm_World1;\n  int myworld0,myworld1;\n  int *NPROCS1_ID,*NPROCS1_WD;\n  int *Comm_World1;\n  int *Comm_World_StartID;\n  MPI_Comm *MPI_CommWD;\n  MPI_Comm comm;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs0);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid0);\n  myworld0 = 0;\n\n  Num_Comm_World1 = 2;\n\n  \n\n\n  NPROCS1_ID = (int*)malloc(sizeof(int)*numprocs0); \n  Comm_World1 = (int*)malloc(sizeof(int)*numprocs0); \n  NPROCS1_WD = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  Comm_World_StartID = (int*)malloc(sizeof(int)*Num_Comm_World1); \n  MPI_CommWD = (MPI_Comm*)malloc(sizeof(MPI_Comm)*Num_Comm_World1); \n\n  \n\n\n  Make_Comm_Worlds(MPI_COMM_WORLD, myid0, numprocs0, Num_Comm_World1, &myworld1, MPI_CommWD, \n                   NPROCS1_ID, Comm_World1, NPROCS1_WD, Comm_World_StartID);\n\n  \n\n\n  MPI_Comm_size(MPI_CommWD[myworld1],&numprocs1);\n  MPI_Comm_rank(MPI_CommWD[myworld1],&myid1);\n\n  printf(\"numprocs0=%2d myid0=%2d myworld1=%2d numprocs1=%2d myid1=%2d\\n\",\n         numprocs0,myid0,myworld1,numprocs1,myid1);\n\n\n\n\n  MPI_Comm_rank(MPI_CommWD[myworld1],&myid1);\n\n  MPI_Comm_free(&MPI_CommWD[myworld1]);\n\n\n  \n\n\n  \n\n\n\n  \n\n\n  \n\n  free(NPROCS1_ID);\n  free(Comm_World1);\n  free(NPROCS1_WD);\n  free(Comm_World_StartID);\n  free(MPI_CommWD);\n\n  \n\n\n  MPI_Finalize();\n\n}"}
{"program": "blue42u_1156", "code": "int\nmain()\n{\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n  int types[2] = {0, 1};\n  int nservers = 1;\n  int am_server;\n  MPI_Comm adlb_comm = MPI_COMM_WORLD;\n  MPI_Comm worker_comm;\n  ADLB_Init(nservers, 2, types, &am_server, adlb_comm, &worker_comm);\n\n  int tasks_per_worker = 1;\n\n  int BUFFER_LENGTH = 128;\n\n  int rank;\n\n  if (am_server)\n  {\n    ADLB_Server(1);\n  }\n  else\n  {\n    char buffer[BUFFER_LENGTH];\n    for (int i = 0; i < tasks_per_worker; i++)\n    {\n      sprintf(buffer, \"PARALLEL_STRING from: %i #%i\", rank, i);\n      adlb_put_opts opts = ADLB_DEFAULT_PUT_OPTS;\n      opts.parallelism = 2;\n      ADLB_Put(buffer, (int)strlen(buffer)+1, ADLB_RANK_ANY, rank, 0, opts);\n    }\n    while (true)\n    {\n      int length = BUFFER_LENGTH;\n      int answer;\n      int type;\n      MPI_Comm task_comm;\n      void*  b = &buffer[0];\n      void** p = &b;\n      adlb_code rc =\n          ADLB_Get(0, p, &length, length, &answer, &type, &task_comm);\n      if (rc == ADLB_SHUTDOWN)\n        break;\n      if (task_comm == MPI_COMM_SELF)\n      {\n        printf(\"SELF\\n\");\n      }\n      else\n      {\n        printf(\"PARALLEL TASK\\n\");\n        task(buffer, task_comm);\n      }\n    }\n  }\n\n  ADLB_Finalize();\n  return 0;\n}", "label": "int\nmain()\n{\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n  MPI_Init(&mpi_argc, &mpi_argv);\n  int types[2] = {0, 1};\n  int nservers = 1;\n  int am_server;\n  MPI_Comm adlb_comm = MPI_COMM_WORLD;\n  MPI_Comm worker_comm;\n  ADLB_Init(nservers, 2, types, &am_server, adlb_comm, &worker_comm);\n\n  int tasks_per_worker = 1;\n\n  int BUFFER_LENGTH = 128;\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (am_server)\n  {\n    ADLB_Server(1);\n  }\n  else\n  {\n    char buffer[BUFFER_LENGTH];\n    for (int i = 0; i < tasks_per_worker; i++)\n    {\n      sprintf(buffer, \"PARALLEL_STRING from: %i #%i\", rank, i);\n      adlb_put_opts opts = ADLB_DEFAULT_PUT_OPTS;\n      opts.parallelism = 2;\n      ADLB_Put(buffer, (int)strlen(buffer)+1, ADLB_RANK_ANY, rank, 0, opts);\n    }\n    while (true)\n    {\n      int length = BUFFER_LENGTH;\n      int answer;\n      int type;\n      MPI_Comm task_comm;\n      void*  b = &buffer[0];\n      void** p = &b;\n      adlb_code rc =\n          ADLB_Get(0, p, &length, length, &answer, &type, &task_comm);\n      if (rc == ADLB_SHUTDOWN)\n        break;\n      if (task_comm == MPI_COMM_SELF)\n      {\n        printf(\"SELF\\n\");\n      }\n      else\n      {\n        printf(\"PARALLEL TASK\\n\");\n        task(buffer, task_comm);\n      }\n    }\n  }\n\n  ADLB_Finalize();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "lstorchi_1157", "code": "int main (int argc, char ** argv)\n{\n  char myname[MPI_MAX_PROCESSOR_NAME]; \n  int size, rank, rlen, N, i;\n  double * buf, stime, etime;\n\n\n  fprintf (stdout, \"I am proces %3d of %3d my name is %s (%s)\\n\", \n    rank, size, myname, argv[0]);\n  fflush(stdout);\n\n  N = -1;\n  if (argc == 2)\n    N = atoi(argv[1]);\n\n  if (N < 0)\n    N = 100;\n\n  buf = malloc (N * sizeof (double));\n  if (buf == NULL)\n  { \n     fprintf (stderr, \"Malloc error\\n\");\n     return EXIT_FAILURE;\n  }\n\n  if (rank == 0) \n    for (i=0; i<N; i++)\n      buf[i] = (double) i;\n\n  stime =\n  bcast (rank, 0, size, buf, N);\n  etime =\n\n  if (rank == 0)\n    fprintf (stdout, \"Bcast time: %f s\\n\", etime-stime);\n\n  stime =\n  bcast_tree (rank, size, buf, N);\n  etime =\n\n  if (rank == 0)\n    fprintf (stdout, \"Bcast btree time: %f s\\n\", etime-stime);\n  \n\n\n  free (buf);\n\n \n  return EXIT_SUCCESS;\n}", "label": "int main (int argc, char ** argv)\n{\n  char myname[MPI_MAX_PROCESSOR_NAME]; \n  int size, rank, rlen, N, i;\n  double * buf, stime, etime;\n\n  MPI_Init (&argc, &argv);\n  MPI_Comm_size (MPI_COMM_WORLD, &size);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  MPI_Get_processor_name (myname, &rlen);\n\n  fprintf (stdout, \"I am proces %3d of %3d my name is %s (%s)\\n\", \n    rank, size, myname, argv[0]);\n  fflush(stdout);\n\n  N = -1;\n  if (argc == 2)\n    N = atoi(argv[1]);\n\n  if (N < 0)\n    N = 100;\n\n  buf = malloc (N * sizeof (double));\n  if (buf == NULL)\n  { \n     fprintf (stderr, \"Malloc error\\n\");\n     return EXIT_FAILURE;\n  }\n\n  if (rank == 0) \n    for (i=0; i<N; i++)\n      buf[i] = (double) i;\n\n  MPI_Barrier (MPI_COMM_WORLD);\n  stime = MPI_Wtime();\n  bcast (rank, 0, size, buf, N);\n  MPI_Barrier (MPI_COMM_WORLD);\n  etime = MPI_Wtime();\n\n  if (rank == 0)\n    fprintf (stdout, \"Bcast time: %f s\\n\", etime-stime);\n\n  MPI_Barrier (MPI_COMM_WORLD);\n  stime = MPI_Wtime();\n  bcast_tree (rank, size, buf, N);\n  MPI_Barrier (MPI_COMM_WORLD);\n  etime = MPI_Wtime();\n\n  if (rank == 0)\n    fprintf (stdout, \"Bcast btree time: %f s\\n\", etime-stime);\n  \n\n\n  free (buf);\n\n  MPI_Finalize ();\n \n  return EXIT_SUCCESS;\n}"}
{"program": "PhilippParis_1158", "code": "int main(int argc, char* argv[]) {\n    int my_rank, nprocs, p;\n    reprompib_st_opts_t opts;\n    int master_rank;\n    reprompib_st_error_t ret;\n\n    double *all_rdtsc_times = NULL;\n    double *all_wtime_times = NULL;\n    double *rdtsc_times;\n    double *wtime_times;\n\n    int step;\n    int n_wait_steps = 11;\n    double wait_time_s = 0.1;\n\n    \n\n    master_rank = 0;\n\n    ret = parse_test_options(&opts, argc, argv);\n    validate_test_options_or_abort(ret, &opts);\n\n    n_wait_steps = opts.n_rep + 1;\n\n    wtime_times = (double*) calloc(n_wait_steps, sizeof(double));\n    rdtsc_times = (double*) calloc(n_wait_steps, sizeof(double));\n\n    if (my_rank == master_rank) {\n        all_wtime_times = (double*) calloc(nprocs * n_wait_steps,\n                sizeof(double));\n\n\n        all_rdtsc_times = (double*) calloc(nprocs * n_wait_steps,\n                sizeof(double));\n    }\n\n    for (step = 0; step < n_wait_steps; step++) {\n        rdtsc_times[step] = get_time();\n        wtime_times[step] =\n\n        \n\n        struct timespec sleep_time;\n        sleep_time.tv_sec = 0;\n        sleep_time.tv_nsec = wait_time_s * 1e9;\n\n        nanosleep(&sleep_time, NULL);\n    }\n\n    if (my_rank == master_rank) {\n        printf(\"wait_time_s p wtime rdtsc\\n\");\n    }\n\n    \n\n\n\n    if (my_rank == master_rank) {\n        for (p = 0; p < nprocs; p++) {\n            for (step = 0; step < n_wait_steps; step++) {\n                printf(\"%14.9f %3d %14.9f %14.9f\\n\", step * wait_time_s, p,\n                        all_wtime_times[p * n_wait_steps + step],\n                        all_rdtsc_times[p * n_wait_steps + step]);\n            }\n        }\n    }\n\n    free(wtime_times);\n    free(rdtsc_times);\n    if (my_rank == master_rank) {\n        free(all_rdtsc_times);\n        free(all_wtime_times);\n\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char* argv[]) {\n    int my_rank, nprocs, p;\n    reprompib_st_opts_t opts;\n    int master_rank;\n    reprompib_st_error_t ret;\n\n    double *all_rdtsc_times = NULL;\n    double *all_wtime_times = NULL;\n    double *rdtsc_times;\n    double *wtime_times;\n\n    int step;\n    int n_wait_steps = 11;\n    double wait_time_s = 0.1;\n\n    \n\n    MPI_Init(&argc, &argv);\n    master_rank = 0;\n\n    ret = parse_test_options(&opts, argc, argv);\n    validate_test_options_or_abort(ret, &opts);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    n_wait_steps = opts.n_rep + 1;\n\n    wtime_times = (double*) calloc(n_wait_steps, sizeof(double));\n    rdtsc_times = (double*) calloc(n_wait_steps, sizeof(double));\n\n    if (my_rank == master_rank) {\n        all_wtime_times = (double*) calloc(nprocs * n_wait_steps,\n                sizeof(double));\n\n\n        all_rdtsc_times = (double*) calloc(nprocs * n_wait_steps,\n                sizeof(double));\n    }\n\n    for (step = 0; step < n_wait_steps; step++) {\n        rdtsc_times[step] = get_time();\n        wtime_times[step] = MPI_Wtime();\n\n        \n\n        struct timespec sleep_time;\n        sleep_time.tv_sec = 0;\n        sleep_time.tv_nsec = wait_time_s * 1e9;\n\n        nanosleep(&sleep_time, NULL);\n    }\n\n    if (my_rank == master_rank) {\n        printf(\"wait_time_s p wtime rdtsc\\n\");\n    }\n\n    \n\n    MPI_Gather(rdtsc_times, n_wait_steps, MPI_DOUBLE, all_rdtsc_times,\n            n_wait_steps, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    MPI_Gather(wtime_times, n_wait_steps, MPI_DOUBLE, all_wtime_times,\n            n_wait_steps, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (my_rank == master_rank) {\n        for (p = 0; p < nprocs; p++) {\n            for (step = 0; step < n_wait_steps; step++) {\n                printf(\"%14.9f %3d %14.9f %14.9f\\n\", step * wait_time_s, p,\n                        all_wtime_times[p * n_wait_steps + step],\n                        all_rdtsc_times[p * n_wait_steps + step]);\n            }\n        }\n    }\n\n    free(wtime_times);\n    free(rdtsc_times);\n    if (my_rank == master_rank) {\n        free(all_rdtsc_times);\n        free(all_wtime_times);\n\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "anushkrish_1160", "code": "int main(int argc, char *argv[])\n{\n\tint rank, numprocs, i;\n\tint array[3], result[3];\n\n\t\n\n\n\t\n\n\t\n\t\n\n\n\t\n\n\tarray[0] = 3*rank;\n\tarray[1] = 3*rank+1;\n\tarray[2] = 3*rank+2;\n\n\t\n\n\tif (rank==0)\n\t{\n\t\tprintf(\"\\nBefore reduce:\\n\");\n\t}\n\n\tprintf(\"Process %d: \", rank);\n\tfor (i=0; i<3; i++)\n\t{\n\t\tprintf(\"%d, \", array[i]);\n\t}\n\tprintf(\"\\n\");\n\t\n\n\t\n\n\n\t\n\n\tif (rank==0) printf(\"\\nAfter reduce:\\n\");\n\tprintf(\"Process %d: \", rank);\n\tfor (i=0; i<3; i++)\n\t{\n\t\tprintf(\"%d, \", result[i]);\n\t}\n\tprintf(\"\\n\");\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint rank, numprocs, i;\n\tint array[3], result[3];\n\n\t\n\n\tMPI_Init(&argc,&argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\t\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t\n\n\tarray[0] = 3*rank;\n\tarray[1] = 3*rank+1;\n\tarray[2] = 3*rank+2;\n\n\t\n\n\tif (rank==0)\n\t{\n\t\tprintf(\"\\nBefore reduce:\\n\");\n\t}\n\tMPI_Barrier(MPI_COMM_WORLD); \n\n\tprintf(\"Process %d: \", rank);\n\tfor (i=0; i<3; i++)\n\t{\n\t\tprintf(\"%d, \", array[i]);\n\t}\n\tprintf(\"\\n\");\n\t\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\t\n\n\tMPI_Reduce(array, result, 3, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t\n\n\tif (rank==0) printf(\"\\nAfter reduce:\\n\");\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tprintf(\"Process %d: \", rank);\n\tfor (i=0; i<3; i++)\n\t{\n\t\tprintf(\"%d, \", result[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "chaos_1164", "code": "int main(int argc, char **argv)\n{\n        int i;\n        struct count_log_data log = {0, 0, NULL};\n\n        \n\n        for (i = 1; i < argc; i++) {\n                if (!strcmp(argv[i], \"-h\") || !strcmp(argv[i], \"--help\")) {\n                        usage();\n                }\n        }\n\n\n        if (mpi_rank == 0) {\n                printf(\"createabunch is running with %d process(es)\\n\", mpi_size);\n                fflush(stdout);\n        }\n\n\n        parse_command_line(argc, argv);\n        if (mpi_rank == 0 && recursive_mkdir(args.test_directory_name) != 0) {\n        }\n        enlarge_log(&log);\n        createabunch(&log);\n        dump_aggregate_log_data(&log);\n        if (args.dump_all_tasks_data)\n                dump_all_log_data(&log);\n\n        exit(0);\n}", "label": "int main(int argc, char **argv)\n{\n        int i;\n        struct count_log_data log = {0, 0, NULL};\n\n        \n\n        for (i = 1; i < argc; i++) {\n                if (!strcmp(argv[i], \"-h\") || !strcmp(argv[i], \"--help\")) {\n                        usage();\n                }\n        }\n\n        MPI_Init(&argc, &argv);\n        MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n        if (mpi_rank == 0) {\n                printf(\"createabunch is running with %d process(es)\\n\", mpi_size);\n                fflush(stdout);\n        }\n\n\n        parse_command_line(argc, argv);\n        if (mpi_rank == 0 && recursive_mkdir(args.test_directory_name) != 0) {\n                MPI_Abort(MPI_COMM_WORLD, 3);\n        }\n        enlarge_log(&log);\n        createabunch(&log);\n        dump_aggregate_log_data(&log);\n        if (args.dump_all_tasks_data)\n                dump_all_log_data(&log);\n\n        MPI_Finalize();\n        exit(0);\n}"}
{"program": "MrTheodor_1165", "code": "int main(int argc, char *argv[])\n{\n  int i,n=5;\n\n\n  for (i=0; i<n; i++) {\n    Py_Initialize();\n    PyRun_SimpleString(helloworld);\n    Py_Finalize();\n  }\n\n  Py_Initialize();\n  PyRun_SimpleString(helloworld);\n\n  Py_Finalize();  \n\n\n  Py_Initialize();\n  PyRun_SimpleString(\"from mpi4py import MPI\\n\");\n  Py_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n  int i,n=5;\n\n  MPI_Init(&argc, &argv);\n\n  for (i=0; i<n; i++) {\n    Py_Initialize();\n    PyRun_SimpleString(helloworld);\n    Py_Finalize();\n  }\n\n  Py_Initialize();\n  PyRun_SimpleString(helloworld);\n  MPI_Finalize(); \n\n  Py_Finalize();  \n\n\n  Py_Initialize();\n  PyRun_SimpleString(\"from mpi4py import MPI\\n\");\n  Py_Finalize();\n\n  return 0;\n}"}
{"program": "aeslaughter_1167", "code": "int main(int argc, char **argv)\n{\n   MPI_Comm comm = MPI_COMM_WORLD;\n   MPI_Info info = MPI_INFO_NULL;\n   int mpi_size, mpi_rank;\n   int mpi_mode[NUM_MODES] = {NC_MPIIO, NC_MPIPOSIX};\n   int facc_type[NUM_FACC] = {NC_INDEPENDENT, NC_COLLECTIVE};\n   size_t chunk_size_2d[NUM_CHUNK_COMBOS_2D][NDIMS1] = {{0, 0},\n\t\t\t\t\t\t\t{DIMSIZE2, DIMSIZE1},\n\t\t\t\t\t\t\t{DIMSIZE2/2 + 1, DIMSIZE1 / 2}};\n   size_t chunk_size_4d[NUM_CHUNK_COMBOS_4D][NDIMS2] = {{0, 0, 0, 0},\n\t\t\t\t\t\t\t{1, DIMSIZE3, DIMSIZE2, DIMSIZE1},\n\t\t\t\t\t\t\t{TIMELEN / 2, DIMSIZE3 / 2 + 1, DIMSIZE2 / 2 + 1, DIMSIZE1 / 2},\n\t\t\t\t\t\t\t{TIMELEN, DIMSIZE3, DIMSIZE2, DIMSIZE1}};\n   size_t cache_size[NUM_CACHE_SIZES] = {MEGABYTE, 32 * MEGABYTE, 64 * MEGABYTE};\n   int m, f, c, i;\n\n   \n\n\n   \n\n   if ((float)DIMSIZE1 / mpi_size != (int)(DIMSIZE1 / mpi_size))\n   {\n      printf(\"%d divided by number of processors must be a whole number!\\n\",\n\t     DIMSIZE1);\n      return -1;\n   }\n\n   if (!mpi_rank)\n   {\n      printf(\"*** Testing parallel IO for NASA...\\n\");\n      printf(\"num_proc\\tMPI mode\\taccess\\t\\tcache (MB)\\tgrid size\\tchunks\\tavg. write time(s)\\t\"\n\t     \"avg. write bandwidth(MB/s)\\tnum_tries\\n\");\n   }\n\n   for (i = 0; i < NUM_CACHE_SIZES; i++)\n      for (m = 0; m < NUM_MODES; m++)\n\t for (f = 0; f < NUM_FACC; f++)\n\t    for (c = 0; c < NUM_CHUNK_COMBOS_2D; c++)\n\t       if (test_pio_2d(cache_size[i], mpi_mode[m], facc_type[f], comm,\n\t\t\t       info, mpi_size, mpi_rank, chunk_size_2d[c])) ERR;\n\n   for (i = 0; i < NUM_CACHE_SIZES; i++)\n      for (m = 0; m < NUM_MODES; m++)\n\t for (f = 0; f < NUM_FACC; f++)\n\t    for (c = 0; c < NUM_CHUNK_COMBOS_4D; c++)\n\t       if (test_pio_4d(cache_size[i], mpi_mode[m], facc_type[f], comm,\n\t\t\t       info, mpi_size, mpi_rank, chunk_size_4d[c])) ERR;\n\n   if (!mpi_rank)\n      SUMMARIZE_ERR;\n\n   if (!mpi_rank)\n      FINAL_RESULTS;\n\n   return 0;\n}", "label": "int main(int argc, char **argv)\n{\n   MPI_Comm comm = MPI_COMM_WORLD;\n   MPI_Info info = MPI_INFO_NULL;\n   int mpi_size, mpi_rank;\n   int mpi_mode[NUM_MODES] = {NC_MPIIO, NC_MPIPOSIX};\n   int facc_type[NUM_FACC] = {NC_INDEPENDENT, NC_COLLECTIVE};\n   size_t chunk_size_2d[NUM_CHUNK_COMBOS_2D][NDIMS1] = {{0, 0},\n\t\t\t\t\t\t\t{DIMSIZE2, DIMSIZE1},\n\t\t\t\t\t\t\t{DIMSIZE2/2 + 1, DIMSIZE1 / 2}};\n   size_t chunk_size_4d[NUM_CHUNK_COMBOS_4D][NDIMS2] = {{0, 0, 0, 0},\n\t\t\t\t\t\t\t{1, DIMSIZE3, DIMSIZE2, DIMSIZE1},\n\t\t\t\t\t\t\t{TIMELEN / 2, DIMSIZE3 / 2 + 1, DIMSIZE2 / 2 + 1, DIMSIZE1 / 2},\n\t\t\t\t\t\t\t{TIMELEN, DIMSIZE3, DIMSIZE2, DIMSIZE1}};\n   size_t cache_size[NUM_CACHE_SIZES] = {MEGABYTE, 32 * MEGABYTE, 64 * MEGABYTE};\n   int m, f, c, i;\n\n   \n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n   \n\n   if ((float)DIMSIZE1 / mpi_size != (int)(DIMSIZE1 / mpi_size))\n   {\n      printf(\"%d divided by number of processors must be a whole number!\\n\",\n\t     DIMSIZE1);\n      return -1;\n   }\n\n   if (!mpi_rank)\n   {\n      printf(\"*** Testing parallel IO for NASA...\\n\");\n      printf(\"num_proc\\tMPI mode\\taccess\\t\\tcache (MB)\\tgrid size\\tchunks\\tavg. write time(s)\\t\"\n\t     \"avg. write bandwidth(MB/s)\\tnum_tries\\n\");\n   }\n\n   for (i = 0; i < NUM_CACHE_SIZES; i++)\n      for (m = 0; m < NUM_MODES; m++)\n\t for (f = 0; f < NUM_FACC; f++)\n\t    for (c = 0; c < NUM_CHUNK_COMBOS_2D; c++)\n\t       if (test_pio_2d(cache_size[i], mpi_mode[m], facc_type[f], comm,\n\t\t\t       info, mpi_size, mpi_rank, chunk_size_2d[c])) ERR;\n\n   for (i = 0; i < NUM_CACHE_SIZES; i++)\n      for (m = 0; m < NUM_MODES; m++)\n\t for (f = 0; f < NUM_FACC; f++)\n\t    for (c = 0; c < NUM_CHUNK_COMBOS_4D; c++)\n\t       if (test_pio_4d(cache_size[i], mpi_mode[m], facc_type[f], comm,\n\t\t\t       info, mpi_size, mpi_rank, chunk_size_4d[c])) ERR;\n\n   if (!mpi_rank)\n      SUMMARIZE_ERR;\n   MPI_Finalize();\n\n   if (!mpi_rank)\n      FINAL_RESULTS;\n\n   return 0;\n}"}
{"program": "ManyBodyPhysics_1168", "code": "int main(int argc, char *argv[])\n{\n  char     inputFile[ONE_LINE]; \n  SHELL    model;\n  TID      wallTime, cpuTime;\n  int      calcCM;\n\n\n\n  \n\n\n\n  \n\n\n  if(argc >= 2) { \n\n    strcpy(inputFile, argv[1]); \n\n  }\n  else  { \n\n    printf(\"\\n Type input filename for basic shellModelData = \");\n    scanf(\"%s\",inputFile);\n  } \n\n\n  \n\n\n  wallClock(1,0); \n\n  cpuClock(1,0);\n  wallClock(1,1); \n\n  cpuClock(1,0);\n\n  if(input_process(inputFile, &model) == YES) {    \n\n\n    pn_LancIterateCalc(&model);\n\n    \n\n\n    wallClock(1,2); \n\n    cpuClock(1,2);\n    wallTime = wallClock(1,3);  \n\n    cpuTime  = cpuClock(1,3);\n\n    if(Rank == MASTER) {\n      char  filename[ONE_LINE]; \n      FILE  *file_ptr;\n      \n      sprintf(filename,\"%s%s\",model.title,RESULT_OUTPUT); \n      if((file_ptr = fopen(filename,\"a\"))== NULL) {\n\tprintf(\"\\n\\nRank%d: Error in function lanc-main.c():\",Rank);\n\tprintf(\"\\nWrong file = %s for the output data\\n\\n\", filename);\n      }\n      fprintf(file_ptr, \"\\n\\nTotal shell model process wall time used\");  \n      fprintf(file_ptr, \" %llu hour %llu min %llu sec\",\n\t      wallTime.hour, wallTime.min, wallTime.sec);\n      fprintf(file_ptr, \"\\nTotal shell model process cpu time used\");  \n      fprintf(file_ptr, \" %llu hour %llu min %llu sec\\n\",\n\t      cpuTime.hour, cpuTime.min, cpuTime.sec);\n      fclose(file_ptr);\n\n    } \n\n\n  }  \n\n\n  printf(\"\\n\\n\");\n\n\n  return 0; \n\n\n}", "label": "int main(int argc, char *argv[])\n{\n  char     inputFile[ONE_LINE]; \n  SHELL    model;\n  TID      wallTime, cpuTime;\n  int      calcCM;\n\n\n\n  \n\n\n  MPI_Init(&argc, &argv);      \n  MPI_Comm_size(MPI_COMM_WORLD,&NumProcs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &Rank);\n\n  \n\n\n  if(argc >= 2) { \n\n    strcpy(inputFile, argv[1]); \n\n  }\n  else  { \n\n    printf(\"\\n Type input filename for basic shellModelData = \");\n    scanf(\"%s\",inputFile);\n  } \n\n\n  \n\n\n  wallClock(1,0); \n\n  cpuClock(1,0);\n  wallClock(1,1); \n\n  cpuClock(1,0);\n\n  if(input_process(inputFile, &model) == YES) {    \n\n\n    pn_LancIterateCalc(&model);\n\n    \n\n\n    wallClock(1,2); \n\n    cpuClock(1,2);\n    wallTime = wallClock(1,3);  \n\n    cpuTime  = cpuClock(1,3);\n\n    if(Rank == MASTER) {\n      char  filename[ONE_LINE]; \n      FILE  *file_ptr;\n      \n      sprintf(filename,\"%s%s\",model.title,RESULT_OUTPUT); \n      if((file_ptr = fopen(filename,\"a\"))== NULL) {\n\tprintf(\"\\n\\nRank%d: Error in function lanc-main.c():\",Rank);\n\tprintf(\"\\nWrong file = %s for the output data\\n\\n\", filename);\n\tMPI_Abort(MPI_COMM_WORLD,Rank);\n      }\n      fprintf(file_ptr, \"\\n\\nTotal shell model process wall time used\");  \n      fprintf(file_ptr, \" %llu hour %llu min %llu sec\",\n\t      wallTime.hour, wallTime.min, wallTime.sec);\n      fprintf(file_ptr, \"\\nTotal shell model process cpu time used\");  \n      fprintf(file_ptr, \" %llu hour %llu min %llu sec\\n\",\n\t      cpuTime.hour, cpuTime.min, cpuTime.sec);\n      fclose(file_ptr);\n\n    } \n\n\n  }  \n\n\n  printf(\"\\n\\n\");\n\n  MPI_Finalize();\n\n  return 0; \n\n\n}"}
{"program": "bmi-forum_1169", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* entryPoint;\n\tStream* stream;\n\tdouble  result;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, \"myStream\" );\n\tStream_SetPrintingRank( stream, procToWatch );\n\t\n\tJournal_Printf( stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_Maximum_VoidPtr_CastType );\n\tEP_Append( entryPoint, Return1 );\n\tEP_Append( entryPoint, Return89 );\n\tEP_Append( entryPoint, ReturnNeg43 );\n\tEP_Append( entryPoint, ReturnZero );\n\tresult = ((EntryPoint_Maximum_VoidPtr_CallCast*) entryPoint->run)( entryPoint, stream );\n\tJournal_PrintDouble( stream, result );\n\tStg_Class_Delete( entryPoint );\n\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_Minimum_VoidPtr_CastType );\n\tEP_Append( entryPoint, Return1 );\n\tEP_Append( entryPoint, Return89 );\n\tEP_Append( entryPoint, ReturnNeg43 );\n\tEP_Append( entryPoint, ReturnZero );\n\tresult = ((EntryPoint_Minimum_VoidPtr_CallCast*) entryPoint->run)( entryPoint, stream );\n\tJournal_PrintDouble( stream, result );\n\tStg_Class_Delete( entryPoint );\t\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* entryPoint;\n\tStream* stream;\n\tdouble  result;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\t\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, \"myStream\" );\n\tStream_SetPrintingRank( stream, procToWatch );\n\t\n\tJournal_Printf( stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_Maximum_VoidPtr_CastType );\n\tEP_Append( entryPoint, Return1 );\n\tEP_Append( entryPoint, Return89 );\n\tEP_Append( entryPoint, ReturnNeg43 );\n\tEP_Append( entryPoint, ReturnZero );\n\tresult = ((EntryPoint_Maximum_VoidPtr_CallCast*) entryPoint->run)( entryPoint, stream );\n\tJournal_PrintDouble( stream, result );\n\tStg_Class_Delete( entryPoint );\n\n\t\n\n\tentryPoint = EntryPoint_New( testEpName, EntryPoint_Minimum_VoidPtr_CastType );\n\tEP_Append( entryPoint, Return1 );\n\tEP_Append( entryPoint, Return89 );\n\tEP_Append( entryPoint, ReturnNeg43 );\n\tEP_Append( entryPoint, ReturnZero );\n\tresult = ((EntryPoint_Minimum_VoidPtr_CallCast*) entryPoint->run)( entryPoint, stream );\n\tJournal_PrintDouble( stream, result );\n\tStg_Class_Delete( entryPoint );\t\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "qingu_1170", "code": "int main(int argc, char **argv) {\n  int          me, nproc, grp_me, grp_nproc;\n  ARMCI_Group  g_world, g_new;\n  void       **base_ptrs;\n\n  ARMCI_Init();\n\n\n  base_ptrs = malloc(sizeof(void*)*nproc);\n\n  if (me == 0) printf(\"ARMCI Group test starting on %d procs\\n\", nproc);\n\n  ARMCI_Group_get_world(&g_world);\n  \n  if (me == 0) printf(\" + Creating odd/even groups\\n\");\n\n  ARMCIX_Group_split(&g_world, me%2, me, &g_new);\n\n  ARMCI_Group_rank(&g_new, &grp_me);\n  ARMCI_Group_size(&g_new, &grp_nproc);\n\n  if (me == 0) printf(\" + Performing group allocation\\n\");\n  ARMCI_Malloc_group(base_ptrs, DATA_SZ, &g_new);\n  ARMCI_Barrier();\n\n  if (me == 0) printf(\" + Freeing group allocation\\n\");\n\n  ARMCI_Free_group(base_ptrs[grp_me], &g_new);\n  ARMCI_Barrier();\n\n  if (me == 0) printf(\" + Freeing group\\n\");\n\n  ARMCI_Group_free(&g_new);\n\n  if (me == 0) printf(\" + done\\n\");\n\n  free(base_ptrs);\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  int          me, nproc, grp_me, grp_nproc;\n  ARMCI_Group  g_world, g_new;\n  void       **base_ptrs;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  base_ptrs = malloc(sizeof(void*)*nproc);\n\n  if (me == 0) printf(\"ARMCI Group test starting on %d procs\\n\", nproc);\n\n  ARMCI_Group_get_world(&g_world);\n  \n  if (me == 0) printf(\" + Creating odd/even groups\\n\");\n\n  ARMCIX_Group_split(&g_world, me%2, me, &g_new);\n\n  ARMCI_Group_rank(&g_new, &grp_me);\n  ARMCI_Group_size(&g_new, &grp_nproc);\n\n  if (me == 0) printf(\" + Performing group allocation\\n\");\n  ARMCI_Malloc_group(base_ptrs, DATA_SZ, &g_new);\n  ARMCI_Barrier();\n\n  if (me == 0) printf(\" + Freeing group allocation\\n\");\n\n  ARMCI_Free_group(base_ptrs[grp_me], &g_new);\n  ARMCI_Barrier();\n\n  if (me == 0) printf(\" + Freeing group\\n\");\n\n  ARMCI_Group_free(&g_new);\n\n  if (me == 0) printf(\" + done\\n\");\n\n  free(base_ptrs);\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "rahlk_1171", "code": "int main (int argc, char *argv[])\n{\n\n\n\nint reps, tag, numtasks, rank, rank0, rank1, to, dest, source, rc, n, i, j;\nfloat avgT, stdev;\n\n\n\ndouble Tstart, Tend, delT, sumT, stdT;\nprintf(\"Data_Size Mean Stdev\\n\");\n\nMPI_Status status;\nMPI_Request rq;\n\n\n\nMPI_Init(&argc,&argv);\n\n\n\nMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\nMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\n\n\nMPI_Barrier(MPI_COMM_WORLD);\n\n\n\n\ntag = 1;\nreps = NUMBER_REPS; \n\ndouble Stdarr[numtasks];\ndouble tdarr[numtasks];\ndouble udarr[numtasks];\n\n\n\n\nfor (i=2;i<11;i++) {\n  int n_char = pow(2,i);\n  char msg[n_char];\n  memset(msg, 'x', n_char*sizeof(char));\n  int chunksize=sizeof(msg);\n  if (rank==0) {\n    double statArr0[9][numtasks-1];\n    double statArr1[9][numtasks-1];\n\n    for (rank1=1; rank1<numtasks; rank1++) {\n      \n\n      sumT=0;\n      int rep;\n      \n\n        Tstart =\n        rc =\n        rc =\n        delT = Tend-Tstart;\n        Stdarr[rep]=delT;\n        sumT+=delT;\n      \n\n    statArr0[i-2][rank1-1]=std(Stdarr, reps);\n    statArr1[i-2][rank1-1]=sumT/reps;\n    \n\n    \n\n    }\n  }\n  else {\n    dest=rank0;\n    source=rank0;\n    int chunksize = sizeof(msg);\n    rc =\n    rc =\n    }\n  }\nprintf(\"\\n\");\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n\n\n\nMPI_Finalize();\nexit(0);\n}", "label": "int main (int argc, char *argv[])\n{\n\n\n\nint reps, tag, numtasks, rank, rank0, rank1, to, dest, source, rc, n, i, j;\nfloat avgT, stdev;\n\n\n\ndouble Tstart, Tend, delT, sumT, stdT;\nprintf(\"Data_Size Mean Stdev\\n\");\n\nMPI_Status status;\nMPI_Request rq;\n\n\n\nMPI_Init(&argc,&argv);\n\n\n\nMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\nMPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\n\n\nMPI_Barrier(MPI_COMM_WORLD);\n\n\n\n\ntag = 1;\nreps = NUMBER_REPS; \n\ndouble Stdarr[numtasks];\ndouble tdarr[numtasks];\ndouble udarr[numtasks];\n\n\n\n\nfor (i=2;i<11;i++) {\n  int n_char = pow(2,i);\n  char msg[n_char];\n  memset(msg, 'x', n_char*sizeof(char));\n  int chunksize=sizeof(msg);\n  if (rank==0) {\n    double statArr0[9][numtasks-1];\n    double statArr1[9][numtasks-1];\n\n    for (rank1=1; rank1<numtasks; rank1++) {\n      \n\n      sumT=0;\n      int rep;\n      \n\n        Tstart = MPI_Wtime();\n        rc = MPI_Send(&msg, chunksize, MPI_CHAR, rank1, tag, MPI_COMM_WORLD);\n        rc = MPI_Recv(&msg, chunksize, MPI_CHAR, rank1, tag, MPI_COMM_WORLD, &status);\n        Tend=MPI_Wtime();\n        delT = Tend-Tstart;\n        Stdarr[rep]=delT;\n        sumT+=delT;\n      \n\n    statArr0[i-2][rank1-1]=std(Stdarr, reps);\n    statArr1[i-2][rank1-1]=sumT/reps;\n    \n\n    \n\n    }\n  }\n  else {\n    dest=rank0;\n    source=rank0;\n    int chunksize = sizeof(msg);\n    rc = MPI_Send(&msg, chunksize, MPI_CHAR, 0, rank, MPI_COMM_WORLD);\n    rc = MPI_Recv(&msg, chunksize, MPI_CHAR, 0, rank, MPI_COMM_WORLD, &status);\n    }\n  }\nprintf(\"\\n\");\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n\n\n\nMPI_Finalize();\nexit(0);\n}"}
{"program": "gnu3ra_1174", "code": "int\nmain(int argc, char* argv[])\n{\n    hid_t fapl1, fapl2;\n    H5E_auto2_t func;\n\n    char\tname[1024];\n    const char *envval = NULL;\n\n    int mpi_size, mpi_rank;\n    MPI_Comm comm  = MPI_COMM_WORLD;\n    MPI_Info info  = MPI_INFO_NULL;\n\n\n    fapl1 = H5Pcreate(H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl1, comm, info);\n\n    fapl2 = H5Pcreate(H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl2, comm, info);\n\n\n    if(mpi_rank == 0)\n\tTESTING(\"H5Fflush (part2 with flush)\");\n\n    \n\n    envval = HDgetenv(\"HDF5_DRIVER\");\n    if (envval == NULL)\n        envval = \"nomatch\";\n    if (HDstrcmp(envval, \"core\") && HDstrcmp(envval, \"split\")) {\n\t\n\n\th5_fixname(FILENAME[0], fapl1, name, sizeof name);\n\tif(check_file(name, fapl1))\n\t{\n\t    H5_FAILED()\n\t    goto error;\n\t}\n\telse if(mpi_rank == 0)\n\t{\n\t    PASSED()\n\t}\n\n\t\n\n\tif(mpi_rank == 0)\n\t    TESTING(\"H5Fflush (part2 without flush)\");\n\tH5Eget_auto2(H5E_DEFAULT,&func,NULL);\n\tH5Eset_auto2(H5E_DEFAULT, NULL, NULL);\n\n\th5_fixname(FILENAME[1], fapl2, name, sizeof name);\n\tif(check_file(name, fapl2))\n\t{\n\t    if(mpi_rank == 0)\n\t    {\n\t\tPASSED()\n\t    }\n\t}\n\telse\n\t{\n\t    H5_FAILED()\n\t    goto error;\n\t}\n\tH5Eset_auto2(H5E_DEFAULT, func, NULL);\n\n\n\th5_cleanup(&FILENAME[0], fapl1);\n\th5_cleanup(&FILENAME[1], fapl2);\n    }\n    else\n    {\n        SKIPPED();\n        puts(\"    Test not compatible with current Virtual File Driver\");\n    }\n\n    return 0;\n\n    error:\n        return 1;\n}", "label": "int\nmain(int argc, char* argv[])\n{\n    hid_t fapl1, fapl2;\n    H5E_auto2_t func;\n\n    char\tname[1024];\n    const char *envval = NULL;\n\n    int mpi_size, mpi_rank;\n    MPI_Comm comm  = MPI_COMM_WORLD;\n    MPI_Info info  = MPI_INFO_NULL;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(comm, &mpi_size);\n    MPI_Comm_rank(comm, &mpi_rank);\n\n    fapl1 = H5Pcreate(H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl1, comm, info);\n\n    fapl2 = H5Pcreate(H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl2, comm, info);\n\n\n    if(mpi_rank == 0)\n\tTESTING(\"H5Fflush (part2 with flush)\");\n\n    \n\n    envval = HDgetenv(\"HDF5_DRIVER\");\n    if (envval == NULL)\n        envval = \"nomatch\";\n    if (HDstrcmp(envval, \"core\") && HDstrcmp(envval, \"split\")) {\n\t\n\n\th5_fixname(FILENAME[0], fapl1, name, sizeof name);\n\tif(check_file(name, fapl1))\n\t{\n\t    H5_FAILED()\n\t    goto error;\n\t}\n\telse if(mpi_rank == 0)\n\t{\n\t    PASSED()\n\t}\n\n\t\n\n\tif(mpi_rank == 0)\n\t    TESTING(\"H5Fflush (part2 without flush)\");\n\tH5Eget_auto2(H5E_DEFAULT,&func,NULL);\n\tH5Eset_auto2(H5E_DEFAULT, NULL, NULL);\n\n\th5_fixname(FILENAME[1], fapl2, name, sizeof name);\n\tif(check_file(name, fapl2))\n\t{\n\t    if(mpi_rank == 0)\n\t    {\n\t\tPASSED()\n\t    }\n\t}\n\telse\n\t{\n\t    H5_FAILED()\n\t    goto error;\n\t}\n\tH5Eset_auto2(H5E_DEFAULT, func, NULL);\n\n\n\th5_cleanup(&FILENAME[0], fapl1);\n\th5_cleanup(&FILENAME[1], fapl2);\n    }\n    else\n    {\n        SKIPPED();\n        puts(\"    Test not compatible with current Virtual File Driver\");\n    }\n\n    MPI_Finalize();\n    return 0;\n\n    error:\n        return 1;\n}"}
{"program": "bmi-forum_1175", "code": "int main( int argc, char* argv[] ) {\n\tunsigned\trank;\n\tunsigned\tnProcs;\n\tunsigned\twatch;\n\tBool\t\tresult;\n\n\t\n\n\n\t\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\twatch = (argc >= 2) ? atoi( argv[1] ) : 0;\n\n\t\n\n\tresult = testBcast( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array broadcast... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testArrayConv( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array conversion... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testGather( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array gather... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testAllgather( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array gather to all... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testAlltoall( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array all to all... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\t\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\n\treturn MPI_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tunsigned\trank;\n\tunsigned\tnProcs;\n\tunsigned\twatch;\n\tBool\t\tresult;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_size( MPI_COMM_WORLD, (int*)&nProcs );\n\tMPI_Comm_rank( MPI_COMM_WORLD, (int*)&rank );\n\n\t\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\twatch = (argc >= 2) ? atoi( argv[1] ) : 0;\n\n\t\n\n\tresult = testBcast( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array broadcast... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testArrayConv( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array conversion... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testGather( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array gather... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testAllgather( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array gather to all... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\tresult = testAlltoall( rank, nProcs, watch );\n\tif( rank == watch )\n\t\tprintf( \"Testing array all to all... %s\\n\", result ? \"passed\" : \"failed\" );\n\n\t\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\n\treturn MPI_SUCCESS;\n}"}
{"program": "andresf01_1177", "code": "int main(int argc,char *argv[]){\n\tint size, rank, dest, source, count, tag=1;\n\tint inmsg, outmsg=5;\n\tMPI_Status Stat;\n\n\n\tif (rank == 0) {\n\t  dest = 1;\n\t  source = 1;\n\t}else if (rank == 1) {\n\t  dest = 0;\n\t  source = 0;\n\t }\n\n\tif (rank == 0 || rank == 1)\n\t\tprintf(\"Task %d: Received %d (number) from task %d with tag %d \\n\",\n\t\t   rank, inmsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\n}", "label": "int main(int argc,char *argv[]){\n\tint size, rank, dest, source, count, tag=1;\n\tint inmsg, outmsg=5;\n\tMPI_Status Stat;\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t  dest = 1;\n\t  source = 1;\n\t  MPI_Send(&outmsg, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n\t  MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n\t}else if (rank == 1) {\n\t  dest = 0;\n\t  source = 0;\n\t  MPI_Recv(&inmsg, 1, MPI_INT, source, tag, MPI_COMM_WORLD, &Stat);\n\t  MPI_Send(&outmsg, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);\n\t }\n\n\tMPI_Get_count(&Stat, MPI_CHAR, &count);\n\tif (rank == 0 || rank == 1)\n\t\tprintf(\"Task %d: Received %d (number) from task %d with tag %d \\n\",\n\t\t   rank, inmsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\n\tMPI_Finalize();\n}"}
{"program": "vincent-noel_1178", "code": "int main(int argc, char **argv )\n{\n\n\n#ifdef MPI\n\t\n\n\tint nnodes, myid;\n\n\tint rc = \t     \n\n\tif (rc != MPI_SUCCESS)\n\t\tprintf (\" > Error starting MPI program. \\n\");\n\n\n\n\n\n\n#endif\t\n\n\n\t\n\n\t\n\n\tinit_models();\n\tlist_of_models * t_models = getListOfModels();\n\tModelDefinition * working_model = t_models->models[0];\n\n\t\n\n\tinit_data();\n\tExperiment * experiments = getListOfExperiments();\n\tint nb_experiments = getNbExperiments();\n\n    ScoreSettings * score_settings = init_score_settings();\n\n    \n\n\tinit_params(working_model);\n\n\t\n\n\tInitializeModelVsDataScoreFunction(working_model, experiments, nb_experiments, score_settings, getOptimParameters());\n\n#ifdef MPI\n\tSAType * settings = InitPLSA(nnodes, myid);\n\n#else\n\tSAType * settings = InitPLSA();\n\n#endif\n\n\n\t\n\n    init_settings(settings);\n\n\tsettings->scoreFunction = &computeScore;\n\tsettings->printFunction = &saveBestResult;\n\tsettings->logs->best_score = 1;\n\tsettings->logs->best_res = 1;\n\tsettings->logs->params = 1;\n\tsettings->logs->res = 1;\n\tsettings->logs->score = 1;\n\tsettings->logs->pid = 1;\n\n\t\n\n\tInitializeLogs(settings->logs);\n\n\n\n#ifdef MPI\n\tif (myid == 0)\n\t{\n#endif\n\n\tPrintReferenceData(getLogDir());\n\n#ifdef MPI\n\t}\n#endif\n\n\t\n\n\trunPLSA();\n\n\n\tFinalizeScoreFunction();\n\tfinalize_data();\n\tfinalize_models();\n\n#ifdef MPI\n\t\n\n#endif\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv )\n{\n\n\n#ifdef MPI\n\t\n\n\tint nnodes, myid;\n\n\tint rc = MPI_Init(NULL, NULL); \t     \n\n\tif (rc != MPI_SUCCESS)\n\t\tprintf (\" > Error starting MPI program. \\n\");\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nnodes);        \n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);         \n\n\n\n\n#endif\t\n\n\n\t\n\n\t\n\n\tinit_models();\n\tlist_of_models * t_models = getListOfModels();\n\tModelDefinition * working_model = t_models->models[0];\n\n\t\n\n\tinit_data();\n\tExperiment * experiments = getListOfExperiments();\n\tint nb_experiments = getNbExperiments();\n\n    ScoreSettings * score_settings = init_score_settings();\n\n    \n\n\tinit_params(working_model);\n\n\t\n\n\tInitializeModelVsDataScoreFunction(working_model, experiments, nb_experiments, score_settings, getOptimParameters());\n\n#ifdef MPI\n\tSAType * settings = InitPLSA(nnodes, myid);\n\n#else\n\tSAType * settings = InitPLSA();\n\n#endif\n\n\n\t\n\n    init_settings(settings);\n\n\tsettings->scoreFunction = &computeScore;\n\tsettings->printFunction = &saveBestResult;\n\tsettings->logs->best_score = 1;\n\tsettings->logs->best_res = 1;\n\tsettings->logs->params = 1;\n\tsettings->logs->res = 1;\n\tsettings->logs->score = 1;\n\tsettings->logs->pid = 1;\n\n\t\n\n\tInitializeLogs(settings->logs);\n\n\n\n#ifdef MPI\n\tif (myid == 0)\n\t{\n#endif\n\n\tPrintReferenceData(getLogDir());\n\n#ifdef MPI\n\t}\n#endif\n\n\t\n\n\trunPLSA();\n\n\n\tFinalizeScoreFunction();\n\tfinalize_data();\n\tfinalize_models();\n\n#ifdef MPI\n\t\n\n\tMPI_Finalize();\n#endif\n\n\treturn 0;\n}"}
{"program": "ljdursi_1179", "code": "int main(int argc, char **argv) {\n    int ierr, rank, size;\n    MPI_Offset offset;\n    MPI_File   file;\n    MPI_Status status;\n    const int datasize=15;\n    int gridsize[2];\n    int globalsize[2] = {datasize, datasize+1};\n    int subsize[2], start[2];\n    int  myrow, mycol, locnrows, locncols;\n    int startr, endr, startc, endc;\n    char **mydata;\n    MPI_Datatype viewtype;\n    int i, j;\n\n    ierr =\n    ierr|=\n    ierr|=\n    \n    gridsize[0]=0; gridsize[1] = 0;\n    ierr =\n    \n    printf(\"Rank = %d, globalsize = (%d, %d), gridsize = (%d, %d)\\n\", rank, globalsize[0], globalsize[1], gridsize[0], gridsize[1]);\n\n    mycol = rank % gridsize[1];\n    myrow = rank / gridsize[1];\n  \n    locncols = globalsize[1] / gridsize[1];\n    startc = mycol * locncols;\n    endc   = startc + locncols - 1;\n    if (mycol == gridsize[1] - 1) {\n        endc = datasize;\n        locncols = endc - startc + 1;\n    }\n  \n    locnrows = globalsize[0] / gridsize[0];\n    startr = myrow * locnrows;\n    endr   = startr + locnrows - 1;\n    if (myrow == gridsize[0] - 1) {\n        endr = datasize - 1;\n        locnrows = endr - startr + 1;\n    }\n  \n    printf(\"Rank = %d, size = (%d, %d), starts = (%d, %d, %d, %d)\\n\", rank, locnrows, locncols, startr, endr, startc, endc);\n\n    mydata = chararray2d_alloc(locnrows, locncols);\n    for (i=0; i<locnrows; i++)\n        for (j=0; j<locncols; j++) \n            mydata[i][j] = (char)('0' + rank);\n    \n    if (mycol == gridsize[1] - 1) \n        for (i=0; i<locnrows; i++) \n            mydata[i][locncols-1] = '\\n';\n\n    subsize[0] = locnrows; subsize[1] = locncols;\n    start[0]   = startr;   start[1]   = startc;\n\n\n    offset = 0;\n\n\n\n   \n    chararray2d_free(locnrows, locncols, mydata);\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int ierr, rank, size;\n    MPI_Offset offset;\n    MPI_File   file;\n    MPI_Status status;\n    const int datasize=15;\n    int gridsize[2];\n    int globalsize[2] = {datasize, datasize+1};\n    int subsize[2], start[2];\n    int  myrow, mycol, locnrows, locncols;\n    int startr, endr, startc, endc;\n    char **mydata;\n    MPI_Datatype viewtype;\n    int i, j;\n\n    ierr = MPI_Init(&argc, &argv);\n    ierr|= MPI_Comm_size(MPI_COMM_WORLD, &size);\n    ierr|= MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    gridsize[0]=0; gridsize[1] = 0;\n    ierr = MPI_Dims_create(size, 2, gridsize);\n    \n    printf(\"Rank = %d, globalsize = (%d, %d), gridsize = (%d, %d)\\n\", rank, globalsize[0], globalsize[1], gridsize[0], gridsize[1]);\n\n    mycol = rank % gridsize[1];\n    myrow = rank / gridsize[1];\n  \n    locncols = globalsize[1] / gridsize[1];\n    startc = mycol * locncols;\n    endc   = startc + locncols - 1;\n    if (mycol == gridsize[1] - 1) {\n        endc = datasize;\n        locncols = endc - startc + 1;\n    }\n  \n    locnrows = globalsize[0] / gridsize[0];\n    startr = myrow * locnrows;\n    endr   = startr + locnrows - 1;\n    if (myrow == gridsize[0] - 1) {\n        endr = datasize - 1;\n        locnrows = endr - startr + 1;\n    }\n  \n    printf(\"Rank = %d, size = (%d, %d), starts = (%d, %d, %d, %d)\\n\", rank, locnrows, locncols, startr, endr, startc, endc);\n\n    mydata = chararray2d_alloc(locnrows, locncols);\n    for (i=0; i<locnrows; i++)\n        for (j=0; j<locncols; j++) \n            mydata[i][j] = (char)('0' + rank);\n    \n    if (mycol == gridsize[1] - 1) \n        for (i=0; i<locnrows; i++) \n            mydata[i][locncols-1] = '\\n';\n\n    subsize[0] = locnrows; subsize[1] = locncols;\n    start[0]   = startr;   start[1]   = startc;\n\n    MPI_Type_create_subarray(2, globalsize, subsize, start, MPI_ORDER_C,\n                                 MPI_CHAR, &viewtype);\n    MPI_Type_commit(&viewtype);\n\n    offset = 0;\n\n    MPI_File_open(MPI_COMM_WORLD, \"viewtype.txt\", \n                  MPI_MODE_CREATE|MPI_MODE_WRONLY,\n                  MPI_INFO_NULL, &file);\n\n    MPI_File_set_view(file, offset,  MPI_CHAR, viewtype, \n                           \"native\", MPI_INFO_NULL);\n\n    MPI_File_write_all(file, &(mydata[0][0]), locnrows*locncols, MPI_CHAR, &status);\n    MPI_File_close(&file);\n   \n    MPI_Type_free(&viewtype);\n    chararray2d_free(locnrows, locncols, mydata);\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "priyankaganesan_1180", "code": "int main(int argc, char **argv)\n{\n  int my_id, my_dst, my_src, num_processes;\n  int i,j,k;\n  int test;\n  char ch;\n\n  int count;\n  int myrank;\n  struct timeval tv;\n  double curr_time_s;\n  double curr_time_us;\n  struct timeval tv1, tv2;\n  int N=100000;\n  double total_time;\n  \n  tournament_barrier_init();\n  \n  if(rank == 0)\n  {\n    gettimeofday(&tv1, NULL);\n  }\n\n  for(k=0;k<N;k++)\n  {\n    \n  \n\n    tournament_barrier();\n\n  \n\n\n    tournament_barrier();\n  \n  \n    tournament_barrier();\n  \n  \n\n    tournament_barrier();\n\n    tournament_barrier();\n\n  }\n\n  if(rank==0)\n  {\n    gettimeofday(&tv2, NULL);\n\n    total_time = (double) (tv2.tv_usec - tv1.tv_usec) + (double) (tv2.tv_sec - tv1.tv_sec)*1000000;\n    printf(\"\\nSUMMARY:\\nNumber of processes: %d\\n Total run-time for %d \"\n            \"loops with 5 barriers per loop: %fs\\n\"\n            \"The average time per barrier: %fus\\n\",\n            P, N, total_time/1000000, (double)(total_time/(N*5)));\n  }\n  \n tournament_barrier_finish();\n\n\n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int my_id, my_dst, my_src, num_processes;\n  int i,j,k;\n  int test;\n  char ch;\n\n  MPI_Init(&argc, &argv);\n  int count;\n  int myrank;\n  struct timeval tv;\n  double curr_time_s;\n  double curr_time_us;\n  struct timeval tv1, tv2;\n  int N=100000;\n  double total_time;\n  \n  tournament_barrier_init();\n  MPI_Comm_rank(MPI_COMM_WORLD,&myrank);\n  \n  if(rank == 0)\n  {\n    gettimeofday(&tv1, NULL);\n  }\n\n  for(k=0;k<N;k++)\n  {\n    \n  \n\n    tournament_barrier();\n\n  \n\n\n    tournament_barrier();\n  \n  \n    tournament_barrier();\n  \n  \n\n    tournament_barrier();\n\n    tournament_barrier();\n\n  }\n\n  if(rank==0)\n  {\n    gettimeofday(&tv2, NULL);\n\n    total_time = (double) (tv2.tv_usec - tv1.tv_usec) + (double) (tv2.tv_sec - tv1.tv_sec)*1000000;\n    printf(\"\\nSUMMARY:\\nNumber of processes: %d\\n Total run-time for %d \"\n            \"loops with 5 barriers per loop: %fs\\n\"\n            \"The average time per barrier: %fus\\n\",\n            P, N, total_time/1000000, (double)(total_time/(N*5)));\n  }\n  \n tournament_barrier_finish();\n\n  MPI_Finalize();\n\n\n  return 0;\n}"}
{"program": "byu-vv-lab_1181", "code": "void main(int Argc, char *Argv[])\n{\n\tpthread_t  Thread1, Thread2;\n\n\tMPI_Status  Status;\n\n\t\n\n\n\n\n\tpthread_create(&Thread1, NULL, (void *(*) (void *)) Work, (void *) 1);\n\n\tpthread_create(&Thread2, NULL, (void *(*) (void *)) Work, (void *) 2);\n\n\tpthread_join(Thread1, NULL);\n\tpthread_join(Thread2, NULL);\n\n\n\treturn;\n\n}", "label": "void main(int Argc, char *Argv[])\n{\n\tpthread_t  Thread1, Thread2;\n\n\tMPI_Status  Status;\n\n\t\n\n\n\tMPI_Init(&Argc, &Argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &Numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &Myrank);\n\n\n\tpthread_create(&Thread1, NULL, (void *(*) (void *)) Work, (void *) 1);\n\n\tpthread_create(&Thread2, NULL, (void *(*) (void *)) Work, (void *) 2);\n\n\tpthread_join(Thread1, NULL);\n\tpthread_join(Thread2, NULL);\n\n\tMPI_Finalize();\n\n\treturn;\n\n}"}
{"program": "qingu_1182", "code": "int main( int argc, char *argv[] )\n{\n    int wrank, wsize, rank, size, color;\n    int tmp, errs = 0;\n    MPI_Comm newcomm;\n\n\n\n    \n\n    \n\n    color = (wrank > 0) && (wrank <= wsize/2);\n\n\n    \n\n\n    if (color) {\n\t\n\n\texit(1);\n    }\n    \n    \n\n    if (tmp != (size*(size+1)) / 2) {\n\tprintf( \"Allreduce gave %d but expected %d\\n\", tmp, (size*(size+1))/2);\n\terrs ++;\n    }\n\n\n    printf( \" No Errors\\n\" );\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int wrank, wsize, rank, size, color;\n    int tmp, errs = 0;\n    MPI_Comm newcomm;\n\n    MPI_Init( &argc, &argv );\n\n    MPI_Comm_size( MPI_COMM_WORLD, &wsize );\n    MPI_Comm_rank( MPI_COMM_WORLD, &wrank );\n\n    \n\n    \n\n    color = (wrank > 0) && (wrank <= wsize/2);\n    MPI_Comm_split( MPI_COMM_WORLD, color, wrank, &newcomm );\n\n    MPI_Comm_size( newcomm, &size );\n    MPI_Comm_rank( newcomm, &rank );\n\n    \n\n    MPI_Comm_set_errhandler( MPI_ERRORS_RETURN, MPI_COMM_WORLD );\n    MPI_Comm_set_errhandler( MPI_ERRORS_RETURN, newcomm );\n\n    MPI_Barrier( MPI_COMM_WORLD );\n    if (color) {\n\t\n\n\texit(1);\n    }\n    \n    \n\n    MPI_Allreduce( &rank, &tmp, 1, MPI_INT, MPI_SUM, newcomm );\n    if (tmp != (size*(size+1)) / 2) {\n\tprintf( \"Allreduce gave %d but expected %d\\n\", tmp, (size*(size+1))/2);\n\terrs ++;\n    }\n\n    MPI_Comm_free( &newcomm );\n    MPI_Finalize();\n\n    printf( \" No Errors\\n\" );\n\n    return 0;\n}"}
{"program": "ghisvail_1183", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_3d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(3, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  err = pfft_check_output_c2r(3, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_3d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(3, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2r(3, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "mpip_1185", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c(4, n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c(\n      4, n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r(\n      4, n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(4, n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_real(4, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c(4, n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c(\n      4, n, in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r(\n      4, n, out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(4, n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_real(4, n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_1187", "code": "int main(int argc,char *argv[])\n{\n    int done = 0, n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int  namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n\n    \n\n\n    while (!done) {\n        if (myid == 0) {\n            fprintf(stdout, \"Enter the number of intervals: (0 quits) \");\n\t    fflush(stdout);\n            if (scanf(\"%d\",&n) != 1) {\n\t\tfprintf( stdout, \"No number entered; quitting\\n\" );\n\t\tn = 0;\n\t    }\n\t    startwtime =\n        }\n        if (n == 0)\n            done = 1;\n        else {\n            h   = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs) {\n                x = h * ((double)i - 0.5);\n                sum += f(x);\n            }\n            mypi = h * sum;\n\n            if (myid == 0) {\n                printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n                       pi, fabs(pi - PI25DT));\n\t\tendwtime =\n\t\tprintf(\"wall clock time = %f\\n\", endwtime-startwtime);\t       \n\t\tfflush( stdout );\n\t    }\n        }\n    }\n    return 0;\n}", "label": "int main(int argc,char *argv[])\n{\n    int done = 0, n, myid, numprocs, i;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int  namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n    MPI_Get_processor_name(processor_name,&namelen);\n\n    \n\n\n    while (!done) {\n        if (myid == 0) {\n            fprintf(stdout, \"Enter the number of intervals: (0 quits) \");\n\t    fflush(stdout);\n            if (scanf(\"%d\",&n) != 1) {\n\t\tfprintf( stdout, \"No number entered; quitting\\n\" );\n\t\tn = 0;\n\t    }\n\t    startwtime = MPI_Wtime();\n        }\n        MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        if (n == 0)\n            done = 1;\n        else {\n            h   = 1.0 / (double) n;\n            sum = 0.0;\n            for (i = myid + 1; i <= n; i += numprocs) {\n                x = h * ((double)i - 0.5);\n                sum += f(x);\n            }\n            mypi = h * sum;\n            MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n            if (myid == 0) {\n                printf(\"pi is approximately %.16f, Error is %.16f\\n\",\n                       pi, fabs(pi - PI25DT));\n\t\tendwtime = MPI_Wtime();\n\t\tprintf(\"wall clock time = %f\\n\", endwtime-startwtime);\t       \n\t\tfflush( stdout );\n\t    }\n        }\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "kgourgou_1188", "code": "void manyMatrixSuchMultiply(){\n\n  double **A, **B, **C, *tmp;\n  double startTime, endTime;\n  int numElements, offset, stripSize, myrank, numnodes, N, i, j, k;\n  int argc;\n  char **argv;\n  \n  \n  \n  N = 1000;\n  \n  \n\n  \n\n  \n  if (myrank == 0) {\n    tmp = (double *) malloc (sizeof(double ) * N * N);\n    A = (double **) malloc (sizeof(double *) * N);\n    for (i = 0; i < N; i++)\n      A[i] = &tmp[i * N];\n  }\n  else {\n    tmp = (double *) malloc (sizeof(double ) * N * N / numnodes);\n    A = (double **) malloc (sizeof(double *) * N / numnodes);\n    for (i = 0; i < N / numnodes; i++)\n      A[i] = &tmp[i * N];\n  }\n  \n  \n  tmp = (double *) malloc (sizeof(double ) * N * N);\n  B = (double **) malloc (sizeof(double *) * N);\n  for (i = 0; i < N; i++)\n    B[i] = &tmp[i * N];\n  \n  \n  if (myrank == 0) {\n    tmp = (double *) malloc (sizeof(double ) * N * N);\n    C = (double **) malloc (sizeof(double *) * N);\n    for (i = 0; i < N; i++)\n      C[i] = &tmp[i * N];\n  }\n  else {\n    tmp = (double *) malloc (sizeof(double ) * N * N / numnodes);\n    C = (double **) malloc (sizeof(double *) * N / numnodes);\n    for (i = 0; i < N / numnodes; i++)\n      C[i] = &tmp[i * N];\n  }\n\n  if (myrank == 0) {\n    \n\n    for (i=0; i<N; i++) {\n      for (j=0; j<N; j++) {\n        A[i][j] = 1.0;\n        B[i][j] = 1.0;\n      }\n    }\n  }\n  \n  \n\n  if (myrank == 0) {\n    startTime =\n  }\n  \n  stripSize = N/numnodes;\n\n  \n\n  if (myrank == 0) {\n    offset = stripSize;\n    numElements = stripSize * N;\n    for (i=1; i<numnodes; i++) {\n      offset += stripSize;\n    }\n  }\n  else {  \n\n  }\n  \n  \n\n\n  \n\n  for (i=0; i<stripSize; i++) {\n    for (j=0; j<N; j++) {\n      C[i][j] = 0.0;\n    }\n  }\n\n  \n\n  for (i=0; i<stripSize; i++) {\n    for (j=0; j<N; j++) {\n      for (k=0; k<N; k++) {\n\tC[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n\n  \n\n  if (myrank == 0) {\n    offset = stripSize; \n    numElements = stripSize * N;\n    for (i=1; i<numnodes; i++) {\n      offset += stripSize;\n    }\n  }\n  else { \n\n  }\n\n  \n\n  if (myrank == 0) {\n    endTime =\n    printf(\"Time is %f\\n\", endTime-startTime);\n  }\n  \n  \n\n  if (myrank == 0 && N < 10) {\n    for (i=0; i<N; i++) {\n      for (j=0; j<N; j++) {\n        printf(\"%f \", C[i][j]);\n      }\n      printf(\"\\n\");\n    }\n  }\n  \n  if(myrank==0) printf(\"Succesfully ran mpi through python with %d threads\\n\",numnodes);\n\t\n}", "label": "void manyMatrixSuchMultiply(){\n\n  double **A, **B, **C, *tmp;\n  double startTime, endTime;\n  int numElements, offset, stripSize, myrank, numnodes, N, i, j, k;\n  int argc;\n  char **argv;\n  \n  MPI_Init(&argc, &argv);\n  \n  MPI_Comm_rank(MPIC, &myrank);\n  MPI_Comm_size(MPIC, &numnodes);\n  \n  N = 1000;\n  \n  \n\n  \n\n  \n  if (myrank == 0) {\n    tmp = (double *) malloc (sizeof(double ) * N * N);\n    A = (double **) malloc (sizeof(double *) * N);\n    for (i = 0; i < N; i++)\n      A[i] = &tmp[i * N];\n  }\n  else {\n    tmp = (double *) malloc (sizeof(double ) * N * N / numnodes);\n    A = (double **) malloc (sizeof(double *) * N / numnodes);\n    for (i = 0; i < N / numnodes; i++)\n      A[i] = &tmp[i * N];\n  }\n  \n  \n  tmp = (double *) malloc (sizeof(double ) * N * N);\n  B = (double **) malloc (sizeof(double *) * N);\n  for (i = 0; i < N; i++)\n    B[i] = &tmp[i * N];\n  \n  \n  if (myrank == 0) {\n    tmp = (double *) malloc (sizeof(double ) * N * N);\n    C = (double **) malloc (sizeof(double *) * N);\n    for (i = 0; i < N; i++)\n      C[i] = &tmp[i * N];\n  }\n  else {\n    tmp = (double *) malloc (sizeof(double ) * N * N / numnodes);\n    C = (double **) malloc (sizeof(double *) * N / numnodes);\n    for (i = 0; i < N / numnodes; i++)\n      C[i] = &tmp[i * N];\n  }\n\n  if (myrank == 0) {\n    \n\n    for (i=0; i<N; i++) {\n      for (j=0; j<N; j++) {\n        A[i][j] = 1.0;\n        B[i][j] = 1.0;\n      }\n    }\n  }\n  \n  \n\n  if (myrank == 0) {\n    startTime = MPI_Wtime();\n  }\n  \n  stripSize = N/numnodes;\n\n  \n\n  if (myrank == 0) {\n    offset = stripSize;\n    numElements = stripSize * N;\n    for (i=1; i<numnodes; i++) {\n      MPI_Send(A[offset], numElements, MPI_DOUBLE, i, TAG, MPI_COMM_WORLD);\n      offset += stripSize;\n    }\n  }\n  else {  \n\n    MPI_Recv(A[0], stripSize * N, MPI_DOUBLE, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  \n  \n\n  MPI_Bcast(B[0], N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  \n\n  for (i=0; i<stripSize; i++) {\n    for (j=0; j<N; j++) {\n      C[i][j] = 0.0;\n    }\n  }\n\n  \n\n  for (i=0; i<stripSize; i++) {\n    for (j=0; j<N; j++) {\n      for (k=0; k<N; k++) {\n\tC[i][j] += A[i][k] * B[k][j];\n      }\n    }\n  }\n\n  \n\n  if (myrank == 0) {\n    offset = stripSize; \n    numElements = stripSize * N;\n    for (i=1; i<numnodes; i++) {\n      MPI_Recv(C[offset], numElements, MPI_DOUBLE, i, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      offset += stripSize;\n    }\n  }\n  else { \n\n    MPI_Send(C[0], stripSize * N, MPI_DOUBLE, 0, TAG, MPI_COMM_WORLD);\n  }\n\n  \n\n  if (myrank == 0) {\n    endTime = MPI_Wtime();\n    printf(\"Time is %f\\n\", endTime-startTime);\n  }\n  \n  \n\n  if (myrank == 0 && N < 10) {\n    for (i=0; i<N; i++) {\n      for (j=0; j<N; j++) {\n        printf(\"%f \", C[i][j]);\n      }\n      printf(\"\\n\");\n    }\n  }\n  \n  MPI_Finalize();\n  if(myrank==0) printf(\"Succesfully ran mpi through python with %d threads\\n\",numnodes);\n\t\n}"}
{"program": "anushkrish_1190", "code": "int main(int argc, char *argv[])\n{\n\tint         numtasks, rank, dest, source, count;\n\tchar        inmsg[20], outmsg[20];\n\tMPI_Status  Stat;\n\n\t\n\n\n\t\n\n\t\n\t\n\n\n\tif (rank == 0) \n\n\t{\n\t\tstrcpy(outmsg, \"We're number one.\");\n\n\t\t\n\n\t\tdest = 1;\n\n\t\t\n\n\t\tsource = 1;\n\t}\n\telse if(rank == 1) \n\n\t{\n\t\tstrcpy(outmsg, \"We try harder.\");\n\n\t\t\n\n\t\tsource = 0;\n\n\t\t\n\n\t\tdest = 0;\n\t}\n\n\tprintf(\"Process %d received: \\\"%s\\\"  from Process %d with Tag %d \\n\", rank, inmsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint         numtasks, rank, dest, source, count;\n\tchar        inmsg[20], outmsg[20];\n\tMPI_Status  Stat;\n\n\t\n\n\tMPI_Init(&argc,&argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\t\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) \n\n\t{\n\t\tstrcpy(outmsg, \"We're number one.\");\n\n\t\t\n\n\t\tdest = 1;\n\t\tMPI_Send(&outmsg, 18, MPI_CHAR, dest, 0, MPI_COMM_WORLD);\n\n\t\t\n\n\t\tsource = 1;\n\t\tMPI_Recv(&inmsg, 15, MPI_CHAR, source, 1, MPI_COMM_WORLD, &Stat);\n\t}\n\telse if(rank == 1) \n\n\t{\n\t\tstrcpy(outmsg, \"We try harder.\");\n\n\t\t\n\n\t\tsource = 0;\n\t\tMPI_Recv(&inmsg, 18, MPI_CHAR, source, 0, MPI_COMM_WORLD, &Stat);\n\n\t\t\n\n\t\tdest = 0;\n\t\tMPI_Send(&outmsg, 15, MPI_CHAR, dest, 1, MPI_COMM_WORLD);\n\t}\n\n\tMPI_Get_count(&Stat, MPI_CHAR, &count);\n\tprintf(\"Process %d received: \\\"%s\\\"  from Process %d with Tag %d \\n\", rank, inmsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "bjoern-leder_1191", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,iu,iud;\n   double phi[2],phi_prime[2];\n   su3 *usv;\n   su3_dble *udsv;\n   FILE *flog=NULL;\n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n      printf(\"\\n\");\n      printf(\"Check of openbc(), openbcd(), sfbc() and sfbcd()\\n\");\n      printf(\"------------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n   }\n\n   start_ranlux(0,12345);\n   geometry();\n\n   usv=amalloc(4*VOLUME*sizeof(*usv),ALIGN);\n   udsv=amalloc(4*VOLUME*sizeof(*udsv),ALIGN);\n   error((usv==NULL)||(udsv==NULL),1,\"main [check2.c]\",\n         \"Unable to allocate auxiliary arrays\");\n\n   new_flds();\n   save_u(usv);\n   save_ud(udsv);\n\n   if (my_rank==0)\n   {\n      printf(\"Before imposing open boundary conditions:\\n\");\n      print_flags();\n   }\n      \n   openbc();\n   openbcd();\n\n   if (my_rank==0)\n   {\n      printf(\"After imposing open boundary conditions:\\n\");\n      print_flags();\n   }\n\n   cmp_flds(my_rank,usv,udsv,&iu,&iud);\n      \n   error(iu!=0,1,\"main [check2.c]\",\"Action of openbc() is incorrect\");\n   error(iud!=0,1,\"main [check2.c]\",\"Action of openbcd() is incorrect\");\n\n   iu=check_bc();\n   iud=check_bcd();\n\n   error(iu!=1,1,\"main [check2.c]\",\"Action of openbc() is incorrect\");\n   error(iud!=1,1,\"main [check2.c]\",\"Action of openbcd() is incorrect\");   \n\n   new_flds();\n   save_u(usv);\n   save_ud(udsv);\n\n   if (my_rank==0)\n   {\n      printf(\"Before imposing SF boundary conditions:\\n\");\n      print_flags();\n   }\n\n   phi[0]=0.1;\n   phi[1]=0.2;\n   phi_prime[0]=-0.3;\n   phi_prime[1]=0.4;\n   set_sf_parms(phi,phi_prime);\n   \n   sfbc();\n   sfbcd();\n\n   if (my_rank==0)\n   {\n      printf(\"After imposing SF boundary conditions:\\n\");\n      print_flags();\n   }\n\n   iu=check_sfbc();\n   iud=check_sfbcd();\n\n   error(iu!=1,1,\"main [check2.c]\",\"Action of sfbc() is incorrect\");\n   error(iud!=1,1,\"main [check2.c]\",\"Action of sfbcd() is incorrect\");   \n   \n   error_chk();\n\n   if (my_rank==0)\n   {\n      printf(\"No errors detected --- all programs work correctly\\n\\n\");  \n      fclose(flog);\n   }\n   \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,iu,iud;\n   double phi[2],phi_prime[2];\n   su3 *usv;\n   su3_dble *udsv;\n   FILE *flog=NULL;\n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n      printf(\"\\n\");\n      printf(\"Check of openbc(), openbcd(), sfbc() and sfbcd()\\n\");\n      printf(\"------------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n   }\n\n   start_ranlux(0,12345);\n   geometry();\n\n   usv=amalloc(4*VOLUME*sizeof(*usv),ALIGN);\n   udsv=amalloc(4*VOLUME*sizeof(*udsv),ALIGN);\n   error((usv==NULL)||(udsv==NULL),1,\"main [check2.c]\",\n         \"Unable to allocate auxiliary arrays\");\n\n   new_flds();\n   save_u(usv);\n   save_ud(udsv);\n\n   if (my_rank==0)\n   {\n      printf(\"Before imposing open boundary conditions:\\n\");\n      print_flags();\n   }\n      \n   openbc();\n   openbcd();\n\n   if (my_rank==0)\n   {\n      printf(\"After imposing open boundary conditions:\\n\");\n      print_flags();\n   }\n\n   cmp_flds(my_rank,usv,udsv,&iu,&iud);\n      \n   error(iu!=0,1,\"main [check2.c]\",\"Action of openbc() is incorrect\");\n   error(iud!=0,1,\"main [check2.c]\",\"Action of openbcd() is incorrect\");\n\n   iu=check_bc();\n   iud=check_bcd();\n\n   error(iu!=1,1,\"main [check2.c]\",\"Action of openbc() is incorrect\");\n   error(iud!=1,1,\"main [check2.c]\",\"Action of openbcd() is incorrect\");   \n\n   new_flds();\n   save_u(usv);\n   save_ud(udsv);\n\n   if (my_rank==0)\n   {\n      printf(\"Before imposing SF boundary conditions:\\n\");\n      print_flags();\n   }\n\n   phi[0]=0.1;\n   phi[1]=0.2;\n   phi_prime[0]=-0.3;\n   phi_prime[1]=0.4;\n   set_sf_parms(phi,phi_prime);\n   \n   sfbc();\n   sfbcd();\n\n   if (my_rank==0)\n   {\n      printf(\"After imposing SF boundary conditions:\\n\");\n      print_flags();\n   }\n\n   iu=check_sfbc();\n   iud=check_sfbcd();\n\n   error(iu!=1,1,\"main [check2.c]\",\"Action of sfbc() is incorrect\");\n   error(iud!=1,1,\"main [check2.c]\",\"Action of sfbcd() is incorrect\");   \n   \n   error_chk();\n\n   if (my_rank==0)\n   {\n      printf(\"No errors detected --- all programs work correctly\\n\\n\");  \n      fclose(flog);\n   }\n   \n   MPI_Finalize();\n   exit(0);\n}"}
{"program": "ParBLiSS_1194", "code": "int main(int argc, char** argv) {\n  const int64_t n = 200000;\n  int64_t* result = NULL;\n  int64_t result_size;\n  mrg_state st;\n  uint_fast32_t seed[5] = {1, 2, 3, 4, 5};\n  mrg_seed(&st, seed);\n  double start =\n  rand_sort_mpi(MPI_COMM_WORLD, &st, n, &result_size, &result);\n  double time = MPI_Wtime() - start;\n#if 0\n  int64_t i;\n  printf(\"My count = %\" PRId64 \"\\n\", result_size);\n  for (i = 0; i < result_size; ++i) printf(\"%\" PRId64 \"\\n\", result[i]);\n#endif\n  int rank;\n  if (rank == 0) {\n    printf(\"Shuffle of %\" PRId64 \" element(s) took %f second(s).\\n\", n, time);\n  }\n  free(result); result = NULL;\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n  MPI_Init(&argc, &argv);\n  const int64_t n = 200000;\n  int64_t* result = NULL;\n  int64_t result_size;\n  mrg_state st;\n  uint_fast32_t seed[5] = {1, 2, 3, 4, 5};\n  mrg_seed(&st, seed);\n  MPI_Barrier(MPI_COMM_WORLD);\n  double start = MPI_Wtime();\n  rand_sort_mpi(MPI_COMM_WORLD, &st, n, &result_size, &result);\n  MPI_Barrier(MPI_COMM_WORLD);\n  double time = MPI_Wtime() - start;\n#if 0\n  int64_t i;\n  printf(\"My count = %\" PRId64 \"\\n\", result_size);\n  for (i = 0; i < result_size; ++i) printf(\"%\" PRId64 \"\\n\", result[i]);\n#endif\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    printf(\"Shuffle of %\" PRId64 \" element(s) took %f second(s).\\n\", n, time);\n  }\n  free(result); result = NULL;\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bmi-forum_1195", "code": "int main( int argc, char* argv[] ) {\n\tTestSuite*\tsuite;\n\tTestSuite_Test\ttests[nTests] = {{\"set locals\", testLocals}, \n\t\t\t\t\t {\"mappings\", testMaps}};\n\n\t\n\n\n\t\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\tsuite = TestSuite_New();\n\tTestSuite_SetProcToWatch( suite, (argc >= 2) ? atoi( argv[1] ) : 0 );\n\tTestSuite_SetTests( suite, nTests, tests );\n\n\t\n\n\tTestSuite_Run( suite );\n\n\t\n\n\tFreeObject( suite );\n\n\t\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\n\treturn MPI_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tTestSuite*\tsuite;\n\tTestSuite_Test\ttests[nTests] = {{\"set locals\", testLocals}, \n\t\t\t\t\t {\"mappings\", testMaps}};\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\n\t\n\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\n\t\n\n\tsuite = TestSuite_New();\n\tTestSuite_SetProcToWatch( suite, (argc >= 2) ? atoi( argv[1] ) : 0 );\n\tTestSuite_SetTests( suite, nTests, tests );\n\n\t\n\n\tTestSuite_Run( suite );\n\n\t\n\n\tFreeObject( suite );\n\n\t\n\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\n\treturn MPI_SUCCESS;\n}"}
{"program": "melanj_1197", "code": "int main(int argc, char *argv[]) \n{\n  const int MASTER = 0;\n  const int TAG_GENERAL = 1;\n\t\n  int numTasks;\n  int rank;\n  int source;\n  int dest;\n  int rc;\n  int count;\n  int dataWaitingFlag;\n\n  char inMsg;\n  char outMsg;\n\t\n  MPI_Status Stat;\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n  if (rank == MASTER) {\n\t\n    \n\n    for (dest = 1; dest < numTasks; dest++) {\n      outMsg = rand() % 256;\t\n\n\n      \n\n      rc =\t\t\t\n      printf(\"Task %d: Sent message %d to task %d with tag %d\\n\",\n             rank, outMsg, dest, TAG_GENERAL);\n    }\n\t\t\n  } \n\n  \n\n  else  {\n    \n\n    do {\n      printf(\"Waiting\\n\");\n    } while (!dataWaitingFlag);\n\n    \n\n    rc =\n\n    \n\n    rc =\n    printf(\"Task %d: Received %d char(s) (%d) from task %d with tag %d \\n\",\n            rank, count, inMsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\t\t\n  }\n\n}", "label": "int main(int argc, char *argv[]) \n{\n  const int MASTER = 0;\n  const int TAG_GENERAL = 1;\n\t\n  int numTasks;\n  int rank;\n  int source;\n  int dest;\n  int rc;\n  int count;\n  int dataWaitingFlag;\n\n  char inMsg;\n  char outMsg;\n\t\n  MPI_Status Stat;\n\n  \n\n  MPI_Init(&argc,&argv);\n\n  \n\n  MPI_Comm_size(MPI_COMM_WORLD, &numTasks);\n\n  \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n  if (rank == MASTER) {\n\t\n    \n\n    for (dest = 1; dest < numTasks; dest++) {\n      outMsg = rand() % 256;\t\n\n\n      \n\n      rc = MPI_Send(&outMsg, 1, MPI_CHAR, dest, TAG_GENERAL, MPI_COMM_WORLD);\t\t\t\n      printf(\"Task %d: Sent message %d to task %d with tag %d\\n\",\n             rank, outMsg, dest, TAG_GENERAL);\n    }\n\t\t\n  } \n\n  \n\n  else  {\n    \n\n    do {\n      MPI_Iprobe(MASTER, 1, MPI_COMM_WORLD, &dataWaitingFlag, MPI_STATUS_IGNORE);\n      printf(\"Waiting\\n\");\n    } while (!dataWaitingFlag);\n\n    \n\n    rc = MPI_Recv(&inMsg, 1, MPI_CHAR, MASTER, TAG_GENERAL, MPI_COMM_WORLD, &Stat);\n\n    \n\n    rc = MPI_Get_count(&Stat, MPI_CHAR, &count);\n    printf(\"Task %d: Received %d char(s) (%d) from task %d with tag %d \\n\",\n            rank, count, inMsg, Stat.MPI_SOURCE, Stat.MPI_TAG);\n\t\t\n  }\n\n  MPI_Finalize();\n}"}
{"program": "aman-devy_1198", "code": "int main (int argc, char *argv[])\n{\nint   numtasks, taskid, rc, dest, offset, i, j, tag1,\n      tag2, source, chunksize;\nfloat mysum, sum;\nfloat update(int myoffset, int chunk, int myid);\nMPI_Status status;\n\n\n\nMPI_Init(&argc, &argv);\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nif (numtasks % 4 != 0) {\n   printf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n   exit(0);\n   }\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nprintf (\"MPI task %d has started...\\n\", taskid);\nchunksize = (ARRAYSIZE / numtasks);\ntag2 = 1;\ntag1 = 2;\n\n\n\nif (taskid == MASTER){\n\n  \n\n  sum = 0;\n  for(i=0; i<ARRAYSIZE; i++) {\n    data[i] =  i * 1.0;\n    sum = sum + data[i];\n    }\n  printf(\"Initialized array sum = %e\\n\",sum);\n\n  \n\n  offset = chunksize;\n  for (dest=1; dest<numtasks; dest++) {\n    printf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n    offset = offset + chunksize;\n    }\n\n  \n\n  offset = 0;\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  for (i=1; i<numtasks; i++) {\n    source = i;\n    }\n\n  \n\n  printf(\"Sample results: \\n\");\n  offset = 0;\n  for (i=0; i<numtasks; i++) {\n    for (j=0; j<5; j++)\n      printf(\"  %e\",data[offset+j]);\n    printf(\"\\n\");\n    offset = offset + chunksize;\n    }\n  printf(\"*** Final sum= %e ***\\n\",sum);\n\n  }  \n\n\n\n\n\n\n\nif (taskid > MASTER) {\n\n  \n\n  source = MASTER;\n\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  dest = MASTER;\n\n\n  } \n\n\n\nMPI_Finalize();\n\n}", "label": "int main (int argc, char *argv[])\n{\nint   numtasks, taskid, rc, dest, offset, i, j, tag1,\n      tag2, source, chunksize;\nfloat mysum, sum;\nfloat update(int myoffset, int chunk, int myid);\nMPI_Status status;\n\n\n\nMPI_Init(&argc, &argv);\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nif (numtasks % 4 != 0) {\n   printf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n   MPI_Abort(MPI_COMM_WORLD, rc);\n   exit(0);\n   }\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nprintf (\"MPI task %d has started...\\n\", taskid);\nchunksize = (ARRAYSIZE / numtasks);\ntag2 = 1;\ntag1 = 2;\n\n\n\nif (taskid == MASTER){\n\n  \n\n  sum = 0;\n  for(i=0; i<ARRAYSIZE; i++) {\n    data[i] =  i * 1.0;\n    sum = sum + data[i];\n    }\n  printf(\"Initialized array sum = %e\\n\",sum);\n\n  \n\n  offset = chunksize;\n  for (dest=1; dest<numtasks; dest++) {\n    MPI_Send(&offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n    MPI_Send(&data[offset], chunksize, MPI_FLOAT, dest, tag2, MPI_COMM_WORLD);\n    printf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n    offset = offset + chunksize;\n    }\n\n  \n\n  offset = 0;\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  for (i=1; i<numtasks; i++) {\n    source = i;\n    MPI_Recv(&offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);\n    MPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n      MPI_COMM_WORLD, &status);\n    }\n\n  \n\n  MPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n  printf(\"Sample results: \\n\");\n  offset = 0;\n  for (i=0; i<numtasks; i++) {\n    for (j=0; j<5; j++)\n      printf(\"  %e\",data[offset+j]);\n    printf(\"\\n\");\n    offset = offset + chunksize;\n    }\n  printf(\"*** Final sum= %e ***\\n\",sum);\n\n  }  \n\n\n\n\n\n\n\nif (taskid > MASTER) {\n\n  \n\n  source = MASTER;\n  MPI_Recv(&offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);\n  MPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n    MPI_COMM_WORLD, &status);\n\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  dest = MASTER;\n  MPI_Send(&offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n  MPI_Send(&data[offset], chunksize, MPI_FLOAT, MASTER, tag2, MPI_COMM_WORLD);\n\n  MPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\n  } \n\n\n\nMPI_Finalize();\n\n}"}
{"program": "linhbngo_1199", "code": "main(int argc, char **argv)\n{\n    int rank, size;\n    MPI_File infile;\n    MPI_Status status;\n    int nbytes, myarray[array_size], mode, i;\n    double start, stop;\n\n    \n\n\n    \n\n    mode = MPI_MODE_RDONLY;\n\n\n    \n\n\n    \n\n\n\n    \n\n\n    \n\n    for (i=0; i < array_size; i++) \n      printf(\"%2d%c\", myarray[i], i%4==3 ? '\\n' : ' ');\n\n    \n\n}", "label": "main(int argc, char **argv)\n{\n    int rank, size;\n    MPI_File infile;\n    MPI_Status status;\n    int nbytes, myarray[array_size], mode, i;\n    double start, stop;\n\n    \n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n\n    \n\n    mode = MPI_MODE_RDONLY;\n\n    MPI_File_open( MPI_COMM_WORLD, filename, mode, MPI_INFO_NULL, &infile );\n\n    \n\n    MPI_File_set_view( infile, rank*array_size*sizeof(MPI_INT), MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL );\n\n    \n\n    MPI_File_read( infile, &myarray[0], array_size, MPI_INT, &status );\n\n\n    \n\n    MPI_File_close( &infile );\n\n    \n\n    for (i=0; i < array_size; i++) \n      printf(\"%2d%c\", myarray[i], i%4==3 ? '\\n' : ' ');\n\n    \n\n    MPI_Finalize();\n}"}
{"program": "MengbinZhu_1202", "code": "int\nmain(int argc, char **argv)\n{\n\n#ifdef USE_PARALLEL\n#endif\n\n   printf(\"\\n*** Testing netcdf-4 large files.\\n\");\n   printf(\"**** testing simple fill value attribute creation...\");\n   {\n      int ncid, varid, dimids[NUMDIMS];\n      size_t index[NUMDIMS] = {0, 0};\n      signed char vals[DIM2];\n      signed char char_val_in;\n      size_t start[NUMDIMS] = {0, 0}, count[NUMDIMS] = {1, DIM2};\n      int j;\n\n      \n\n      for (j = 0; j < DIM2; j++) \n\t vals[j] = 9 * (j + 11); \n\n\n      \n\n      if (nc_create(FILE_NAME, NC_NETCDF4, &ncid)) ERR;\n      if (nc_set_fill(ncid, NC_NOFILL, NULL)) ERR;\n      if (nc_def_dim(ncid, \"dim1\", DIM1, &dimids[0])) ERR;\n      if (nc_def_dim(ncid, \"dim2\", DIM2, &dimids[1])) ERR;\n      if (nc_def_var(ncid, \"var\", NC_BYTE, NUMDIMS, dimids, &varid)) ERR;\n      if (nc_enddef(ncid)) ERR;\n\n      \n\n      if (nc_put_vara_schar(ncid, varid, start, count, vals)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n   }\n   SUMMARIZE_ERR;\n\n#ifdef USE_PARALLEL\n#endif   \n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char **argv)\n{\n\n#ifdef USE_PARALLEL\n   MPI_Init(&argc, &argv);\n#endif\n\n   printf(\"\\n*** Testing netcdf-4 large files.\\n\");\n   printf(\"**** testing simple fill value attribute creation...\");\n   {\n      int ncid, varid, dimids[NUMDIMS];\n      size_t index[NUMDIMS] = {0, 0};\n      signed char vals[DIM2];\n      signed char char_val_in;\n      size_t start[NUMDIMS] = {0, 0}, count[NUMDIMS] = {1, DIM2};\n      int j;\n\n      \n\n      for (j = 0; j < DIM2; j++) \n\t vals[j] = 9 * (j + 11); \n\n\n      \n\n      if (nc_create(FILE_NAME, NC_NETCDF4, &ncid)) ERR;\n      if (nc_set_fill(ncid, NC_NOFILL, NULL)) ERR;\n      if (nc_def_dim(ncid, \"dim1\", DIM1, &dimids[0])) ERR;\n      if (nc_def_dim(ncid, \"dim2\", DIM2, &dimids[1])) ERR;\n      if (nc_def_var(ncid, \"var\", NC_BYTE, NUMDIMS, dimids, &varid)) ERR;\n      if (nc_enddef(ncid)) ERR;\n\n      \n\n      if (nc_put_vara_schar(ncid, varid, start, count, vals)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n   }\n   SUMMARIZE_ERR;\n\n#ifdef USE_PARALLEL\n   MPI_Finalize();\n#endif   \n   FINAL_RESULTS;\n}"}
{"program": "nci-australia_1205", "code": "int main (int argc, char *argv[]) {\n int \t\tfd,size,rank,idaho=1;\n FILE \t\t*file;\n INDEX_T\ti,ndata,ndata_min,ndata_max;\n size_t\t\tnbyte;\n DATA_T\t\t*data;\n double\t\tt0,t1;\n char\t\tfname[BUFLEN],ffname[BUFLEN];\n struct timeval tv;\n\n#ifdef MPI\n#else \n  size = 1;\n  rank = 0;\n#endif\n\n ndata_min = 1*MB/sizeof(DATA_T);\n ndata_max = 2*MB/sizeof(DATA_T);\n\n while(--argc && argv++) {\n  if(!strcmp(\"-n\",*argv)) {\n    --argc; argv++;\n    ndata_min = atoi(*argv);\n    ndata_max = ndata_min;\n  } else if(!strcmp(\"-min\",*argv)) {\n    --argc; argv++;\n    ndata_min = atoi(*argv);\n  } else if(!strcmp(\"-max\",*argv)) {\n    --argc; argv++;\n    ndata_max = atoi(*argv);\n  }\n }\n\n\n for(ndata=ndata_min;ndata<=ndata_max;ndata+=((ndata/4>0)?(ndata/2):(1))) { \n\n  nbyte = ndata*sizeof(DATA_T);\n  data = (DATA_T *)malloc(nbyte);\n  for(i=0;i<ndata;i++) data[i] = TRUTH(i);\n  sprintf(fname,\"tmpdata_%d_%d_%lld\",size,rank,ndata);\n  sprintf(ffname,\"fort.%d\",rank+11);\n\n\n\n\n\n\n\n\n  if(idaho) {\n   HARNESS(\"priv_mdir\", \n    rank_dir(1,rank);\n   );\n  }\n\n\n\n  HARNESS (\"fwrite3\", \n   file = fopen(fname,\"w\");\n   fwrite(data,nbyte,1,file);\n   fclose(file);\n  );\n\n\n\n\n  HARNESS(\"fread3\", \n   file = fopen(fname,\"w\");\n   fread(data,nbyte,1,file);\n   fclose(file);\n  );\n  VERIFY(\"fread3\");\n  HARNESS(\"unlink3\", \n   unlink(fname);\n  );\n\n\n\n  HARNESS(\"write2\", \n   fd = open(fname,O_CREAT|O_WRONLY,FMODE);\n   write(fd,data,nbyte);\n   close(fd);\n  );\n\n\n\n  HARNESS(\"read2\", \n   fd = open(fname,O_RDONLY,FMODE);\n   read(fd,data,nbyte);\n   close(fd);\n  );\n  VERIFY(\"read2\");\n  HARNESS(\"unlink2\", \n   unlink(fname);\n  );\n\n\n\n\n  HARNESS(\"write_uf\", \n   fort_write_da_(&size,&rank,&ndata,data);\n  );\n\n\n\n  HARNESS(\"read_uf\", \n   fort_read_da_(&size,&rank,&ndata,data);\n  );\n  VERIFY(\"read_uf\");\n  HARNESS(\"unlinkf\", \n   unlink(ffname);\n  );\n\n  if(idaho) {\n   HARNESS(\"priv_rdir\", \n    rank_dir(-1,rank);\n   );\n  }\n\n\n\n\n\n\n\n\n  if(!rank) {\n\n  }\n\n  free(data);\n }\n\n#ifdef MPI\n#endif\n}", "label": "int main (int argc, char *argv[]) {\n int \t\tfd,size,rank,idaho=1;\n FILE \t\t*file;\n INDEX_T\ti,ndata,ndata_min,ndata_max;\n size_t\t\tnbyte;\n DATA_T\t\t*data;\n double\t\tt0,t1;\n char\t\tfname[BUFLEN],ffname[BUFLEN];\n struct timeval tv;\n\n#ifdef MPI\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n#else \n  size = 1;\n  rank = 0;\n#endif\n\n ndata_min = 1*MB/sizeof(DATA_T);\n ndata_max = 2*MB/sizeof(DATA_T);\n\n while(--argc && argv++) {\n  if(!strcmp(\"-n\",*argv)) {\n    --argc; argv++;\n    ndata_min = atoi(*argv);\n    ndata_max = ndata_min;\n  } else if(!strcmp(\"-min\",*argv)) {\n    --argc; argv++;\n    ndata_min = atoi(*argv);\n  } else if(!strcmp(\"-max\",*argv)) {\n    --argc; argv++;\n    ndata_max = atoi(*argv);\n  }\n }\n\n\n for(ndata=ndata_min;ndata<=ndata_max;ndata+=((ndata/4>0)?(ndata/2):(1))) { \n\n  nbyte = ndata*sizeof(DATA_T);\n  data = (DATA_T *)malloc(nbyte);\n  for(i=0;i<ndata;i++) data[i] = TRUTH(i);\n  sprintf(fname,\"tmpdata_%d_%d_%lld\",size,rank,ndata);\n  sprintf(ffname,\"fort.%d\",rank+11);\n\n\n\n\n\n\n\n\n  if(idaho) {\n   HARNESS(\"priv_mdir\", \n    rank_dir(1,rank);\n   );\n  }\n\n\n\n  HARNESS (\"fwrite3\", \n   file = fopen(fname,\"w\");\n   fwrite(data,nbyte,1,file);\n   fclose(file);\n  );\n\n\n\n\n  HARNESS(\"fread3\", \n   file = fopen(fname,\"w\");\n   fread(data,nbyte,1,file);\n   fclose(file);\n  );\n  VERIFY(\"fread3\");\n  HARNESS(\"unlink3\", \n   unlink(fname);\n  );\n\n\n\n  HARNESS(\"write2\", \n   fd = open(fname,O_CREAT|O_WRONLY,FMODE);\n   write(fd,data,nbyte);\n   close(fd);\n  );\n\n\n\n  HARNESS(\"read2\", \n   fd = open(fname,O_RDONLY,FMODE);\n   read(fd,data,nbyte);\n   close(fd);\n  );\n  VERIFY(\"read2\");\n  HARNESS(\"unlink2\", \n   unlink(fname);\n  );\n\n\n\n\n  HARNESS(\"write_uf\", \n   fort_write_da_(&size,&rank,&ndata,data);\n  );\n\n\n\n  HARNESS(\"read_uf\", \n   fort_read_da_(&size,&rank,&ndata,data);\n  );\n  VERIFY(\"read_uf\");\n  HARNESS(\"unlinkf\", \n   unlink(ffname);\n  );\n\n  if(idaho) {\n   HARNESS(\"priv_rdir\", \n    rank_dir(-1,rank);\n   );\n  }\n\n\n\n\n\n\n\n\n  if(!rank) {\n\n  }\n\n  free(data);\n }\n\n#ifdef MPI\n  MPI_Finalize();\n#endif\n}"}
{"program": "bmi-forum_1206", "code": "int main( int argc, char* argv[] ) \n{\n\t\n\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tDictionary*\t\t\tdictionary;\n\tXML_IO_Handler*\t\t\tioHandler;\n\n\tStream* \t\t\tinfoStream;\n\t\n\t\n\n\tAbstractContext*\t\tcontext = NULL;\n\t\n\n\tAbstractContext*\t\treplacedContext = NULL;\n\tEntryPoint*\t\t\tapplicationsFinalise_EP;\n\n\t\n\n\t\n\tStGermain_Init( &argc, &argv );\n\t#ifdef HAVE_PYTHON\n\t\tPy_Initialize();\n\t#endif\t\n\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\n\t\n\n\tioHandler = XML_IO_Handler_New();\n\tIO_Handler_ReadAllFromCommandLine( ioHandler, argc, argv, dictionary );\n\n\tJournal_ReadFromDictionary( dictionary );\n\t\n\n\t\n\n\tcontext = _AbstractContext_New( \n\t\t\tsizeof(AbstractContext),\n\t       \t        AbstractContext_Type,\n\t                _AbstractContext_Delete,\n\t                _AbstractContext_Print,\n\t                NULL,\n\t                NULL,\n\t                _AbstractContext_Construct,\n\t                _AbstractContext_Build,\n\t                _AbstractContext_Initialise,\n\t                _AbstractContext_Execute,\n\t                _AbstractContext_Destroy,\n\t                \"context\",\n\t                True,\n\t                NULL,\n\t                0,\n\t                10,\n\t                CommWorld,\n\t                dictionary );\n\n\t\n\n\tStg_Component_Construct( context, 0 \n, &context, True );\n\t\n\t\n\n\tStg_Component_Build( context, 0 \n, False );\n\t\n\t\n\n\tStg_Component_Initialise( context, 0 \n, False );\n\t\n\t\n\n\tAbstractContext_Dump( context );\n\tStg_Component_Execute( context, 0 \n, False );\n\n\t\n\n\n\t\n\n\tapplicationsFinalise_EP = Context_GetEntryPoint( context, EP_APPLICATIONS_FINALISE );\n\t\n\n\tif(applicationsFinalise_EP != NULL)\n\t\t((EntryPoint_VoidPtr_CallCast*) applicationsFinalise_EP->run)( applicationsFinalise_EP, NULL);\n\n\t\n\n\tStg_Component_Destroy( context, 0 \n, False );\n\tif(replacedContext != NULL)\n\t\tStg_Class_Delete( replacedContext );\n\tStg_Class_Delete( context );\n\tStg_Class_Delete( dictionary );\n\n\t\n\n\t#ifdef HAVE_PYTHON\n\t\tPy_Finalize();\n\t#endif\n\n\tinfoStream = Journal_Register(Info_Type, \"StGermainFinalise\");\n\tJournal_Printf( infoStream, \"Finalised: StGermain Framework.\\n\");\n\tStGermain_Finalise();\n\t\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) \n{\n\t\n\n\tMPI_Comm\t\t\tCommWorld;\n\tint\t\t\t\trank;\n\tint\t\t\t\tnumProcessors;\n\tDictionary*\t\t\tdictionary;\n\tXML_IO_Handler*\t\t\tioHandler;\n\n\tStream* \t\t\tinfoStream;\n\t\n\t\n\n\tAbstractContext*\t\tcontext = NULL;\n\t\n\n\tAbstractContext*\t\treplacedContext = NULL;\n\tEntryPoint*\t\t\tapplicationsFinalise_EP;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tStGermain_Init( &argc, &argv );\n\t#ifdef HAVE_PYTHON\n\t\tPy_Initialize();\n\t#endif\t\n\tMPI_Barrier( CommWorld ); \n\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\n\t\n\n\tioHandler = XML_IO_Handler_New();\n\tIO_Handler_ReadAllFromCommandLine( ioHandler, argc, argv, dictionary );\n\n\tJournal_ReadFromDictionary( dictionary );\n\t\n\n\t\n\n\tcontext = _AbstractContext_New( \n\t\t\tsizeof(AbstractContext),\n\t       \t        AbstractContext_Type,\n\t                _AbstractContext_Delete,\n\t                _AbstractContext_Print,\n\t                NULL,\n\t                NULL,\n\t                _AbstractContext_Construct,\n\t                _AbstractContext_Build,\n\t                _AbstractContext_Initialise,\n\t                _AbstractContext_Execute,\n\t                _AbstractContext_Destroy,\n\t                \"context\",\n\t                True,\n\t                NULL,\n\t                0,\n\t                10,\n\t                CommWorld,\n\t                dictionary );\n\n\t\n\n\tStg_Component_Construct( context, 0 \n, &context, True );\n\t\n\t\n\n\tStg_Component_Build( context, 0 \n, False );\n\t\n\t\n\n\tStg_Component_Initialise( context, 0 \n, False );\n\t\n\t\n\n\tAbstractContext_Dump( context );\n\tStg_Component_Execute( context, 0 \n, False );\n\n\t\n\n\n\t\n\n\tapplicationsFinalise_EP = Context_GetEntryPoint( context, EP_APPLICATIONS_FINALISE );\n\t\n\n\tif(applicationsFinalise_EP != NULL)\n\t\t((EntryPoint_VoidPtr_CallCast*) applicationsFinalise_EP->run)( applicationsFinalise_EP, NULL);\n\n\t\n\n\tStg_Component_Destroy( context, 0 \n, False );\n\tif(replacedContext != NULL)\n\t\tStg_Class_Delete( replacedContext );\n\tStg_Class_Delete( context );\n\tStg_Class_Delete( dictionary );\n\n\t\n\n\t#ifdef HAVE_PYTHON\n\t\tPy_Finalize();\n\t#endif\n\n\tinfoStream = Journal_Register(Info_Type, \"StGermainFinalise\");\n\tJournal_Printf( infoStream, \"Finalised: StGermain Framework.\\n\");\n\tStGermain_Finalise();\n\t\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "gnu3ra_1207", "code": "int main(int argc, char ** argv) {\n  int    rank, nproc, i, test_iter;\n  int   *my_data, *buf;\n  void **base_ptrs;\n  void **buf_shared;\n\n  ARMCI_Init();\n\n\n  if (rank == 0) printf(\"Starting ARMCI test with %d processes\\n\", nproc);\n\n  base_ptrs  = malloc(sizeof(void*)*nproc);\n  buf_shared = malloc(sizeof(void*)*nproc);\n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + iteration %d\\n\", test_iter);\n\n    if (rank == 0 && VERBOSE) printf(\"   - Allocating shared buffers\\n\");\n\n    \n\n    ARMCI_Malloc(base_ptrs,  DATA_SZ);\n    ARMCI_Malloc(buf_shared, DATA_SZ);\n\n    buf     = buf_shared[rank];\n    my_data = base_ptrs[rank];\n\n    if (rank == 0 && VERBOSE) printf(\"   - Testing one-sided get\\n\");\n\n    \n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank*test_iter;\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Get(base_ptrs[(rank+1) % nproc], buf, DATA_SZ, (rank+1) % nproc);\n\n    ARMCI_Access_begin(buf);\n\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (buf[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: GET expected %d, got %d\\n\", rank, (rank+1) % nproc, buf[i]);\n      }\n    }\n\n    ARMCI_Access_end(buf);\n\n    ARMCI_Barrier(); \n\n\n    if (rank == 0 && VERBOSE) printf(\"   - Testing one-sided put\\n\");\n\n    \n\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank*test_iter;\n    ARMCI_Put(buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: PUT expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    if (rank == 0 && VERBOSE) printf(\"   - Testing one-sided accumlate\\n\");\n\n    \n\n    ARMCI_Access_begin(buf);\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank;\n    ARMCI_Access_end(buf);\n    \n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank;\n    ARMCI_Access_end(my_data);\n    ARMCI_Barrier();\n\n    int scale = test_iter;\n    ARMCI_Acc(ARMCI_ACC_INT, &scale, buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != rank + ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: ACC expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    if (rank == 0 && VERBOSE) printf(\"   - Freeing shared buffers\\n\");\n\n    ARMCI_Free(my_data);\n    ARMCI_Free(buf);\n  }\n\n  free(base_ptrs);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int    rank, nproc, i, test_iter;\n  int   *my_data, *buf;\n  void **base_ptrs;\n  void **buf_shared;\n\n  MPI_Init(&argc, &argv);\n  ARMCI_Init();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (rank == 0) printf(\"Starting ARMCI test with %d processes\\n\", nproc);\n\n  base_ptrs  = malloc(sizeof(void*)*nproc);\n  buf_shared = malloc(sizeof(void*)*nproc);\n\n  for (test_iter = 0; test_iter < NUM_ITERATIONS; test_iter++) {\n    if (rank == 0) printf(\" + iteration %d\\n\", test_iter);\n\n    if (rank == 0 && VERBOSE) printf(\"   - Allocating shared buffers\\n\");\n\n    \n\n    ARMCI_Malloc(base_ptrs,  DATA_SZ);\n    ARMCI_Malloc(buf_shared, DATA_SZ);\n\n    buf     = buf_shared[rank];\n    my_data = base_ptrs[rank];\n\n    if (rank == 0 && VERBOSE) printf(\"   - Testing one-sided get\\n\");\n\n    \n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank*test_iter;\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Get(base_ptrs[(rank+1) % nproc], buf, DATA_SZ, (rank+1) % nproc);\n\n    ARMCI_Access_begin(buf);\n\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (buf[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: GET expected %d, got %d\\n\", rank, (rank+1) % nproc, buf[i]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n      }\n    }\n\n    ARMCI_Access_end(buf);\n\n    ARMCI_Barrier(); \n\n\n    if (rank == 0 && VERBOSE) printf(\"   - Testing one-sided put\\n\");\n\n    \n\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank*test_iter;\n    ARMCI_Put(buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: PUT expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    ARMCI_Barrier(); \n\n\n    if (rank == 0 && VERBOSE) printf(\"   - Testing one-sided accumlate\\n\");\n\n    \n\n    ARMCI_Access_begin(buf);\n    for (i = 0; i < DATA_NELTS; i++) buf[i] = rank;\n    ARMCI_Access_end(buf);\n    \n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) my_data[i] = rank;\n    ARMCI_Access_end(my_data);\n    ARMCI_Barrier();\n\n    int scale = test_iter;\n    ARMCI_Acc(ARMCI_ACC_INT, &scale, buf, base_ptrs[(rank+nproc-1) % nproc], DATA_SZ, (rank+nproc-1) % nproc);\n\n    ARMCI_Barrier(); \n\n\n    ARMCI_Access_begin(my_data);\n    for (i = 0; i < DATA_NELTS; i++) {\n      if (my_data[i] != rank + ((rank+1) % nproc)*test_iter) {\n        printf(\"%d: ACC expected %d, got %d\\n\", rank, (rank+1) % nproc, my_data[i]);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n      }\n    }\n    ARMCI_Access_end(my_data);\n\n    if (rank == 0 && VERBOSE) printf(\"   - Freeing shared buffers\\n\");\n\n    ARMCI_Free(my_data);\n    ARMCI_Free(buf);\n  }\n\n  free(base_ptrs);\n\n  if (rank == 0) printf(\"Test complete: PASS.\\n\");\n\n  ARMCI_Finalize();\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "likesmusik_1208", "code": "int main(int argc, char* argv[])\n{\n\t\n\n\tint rank, size;\n\t\n\t\n\n\tchar receive[BUF_SIZE];\n\t\n\tif(rank == 0) {\n\t\t\n\n\t\tint i;\n\t\tfor(i=1; i<size; ++i) {\n\t\t\t\n\n\t\t\t\n\n\t\t\tprintf(\"%s\\n\", receive);\n\t\t}\n\t} else {\n\t\t\n\n\t\tchar message[BUF_SIZE];\n\t\n\t\t\n\n\t\tif(gethostname(message, BUF_SIZE) != 0) {\n\t\t\tprintf(\"Error in gethostname\\n\");\n\t\t}\n\t\tstrcat(message, \": \");\n\t\t\n\t\t\n\n\t\t\n\n\t\ttime_t date = time(NULL);\n\t\tstruct tm *tmptr = localtime(&date);\n\t\tif(tmptr == NULL) {\n\t\t\tprintf(\"Error in localtime\\n\");\n\t\t}\n\t\tstruct timeval time;\n\t\tif(gettimeofday(&time, NULL) != 0) {\n\t\t\tprintf(\"Error in gettimeofday\\n\");\n\t\t}\n\t\t\n\n\t\tstrftime(message+strlen(message), BUF_SIZE-strlen(message), \"%Y-%m-%d %H:%M:%S.\", tmptr);\n\t\t\n\n\t\tsnprintf(message+strlen(message), BUF_SIZE-strlen(message), \"%06ld\", time.tv_usec);\n\t\t\n\t\t\n\n\t}\n\t\n\t\n\n\t\n\n\tif(MPI_Barrier(MPI_COMM_WORLD) != MPI_SUCCESS) {\n\t\tprintf(\"Error in MPI_Barrier\\n\");\n\t}\n\t\n\t\n\n\tprintf(\"Rank %i beendet jetzt!\\n\", rank);\n\t\n\treturn 0;\n}", "label": "int main(int argc, char* argv[])\n{\n\t\n\n\tint rank, size;\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\t\n\t\n\n\tchar receive[BUF_SIZE];\n\t\n\tif(rank == 0) {\n\t\t\n\n\t\tint i;\n\t\tfor(i=1; i<size; ++i) {\n\t\t\t\n\n\t\t\tMPI_Recv(receive, BUF_SIZE, MPI_CHAR, i, 0, MPI_COMM_WORLD, NULL);\n\t\t\t\n\n\t\t\tprintf(\"%s\\n\", receive);\n\t\t}\n\t} else {\n\t\t\n\n\t\tchar message[BUF_SIZE];\n\t\n\t\t\n\n\t\tif(gethostname(message, BUF_SIZE) != 0) {\n\t\t\tprintf(\"Error in gethostname\\n\");\n\t\t}\n\t\tstrcat(message, \": \");\n\t\t\n\t\t\n\n\t\t\n\n\t\ttime_t date = time(NULL);\n\t\tstruct tm *tmptr = localtime(&date);\n\t\tif(tmptr == NULL) {\n\t\t\tprintf(\"Error in localtime\\n\");\n\t\t}\n\t\tstruct timeval time;\n\t\tif(gettimeofday(&time, NULL) != 0) {\n\t\t\tprintf(\"Error in gettimeofday\\n\");\n\t\t}\n\t\t\n\n\t\tstrftime(message+strlen(message), BUF_SIZE-strlen(message), \"%Y-%m-%d %H:%M:%S.\", tmptr);\n\t\t\n\n\t\tsnprintf(message+strlen(message), BUF_SIZE-strlen(message), \"%06ld\", time.tv_usec);\n\t\t\n\t\t\n\n\t\tMPI_Send(message, BUF_SIZE, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n\t}\n\t\n\t\n\n\t\n\n\tif(MPI_Barrier(MPI_COMM_WORLD) != MPI_SUCCESS) {\n\t\tprintf(\"Error in MPI_Barrier\\n\");\n\t}\n\t\n\t\n\n\tprintf(\"Rank %i beendet jetzt!\\n\", rank);\n\tMPI_Finalize();\n\t\n\treturn 0;\n}"}
{"program": "tcsiwula_1209", "code": "int main(void) {\n   int p, my_rank;\n   MPI_Comm comm;\n   int x;\n   int total;\n\n   comm = MPI_COMM_WORLD;\n\n   srandom(my_rank+1);\n   x = random() % MAX_CONTRIB;\n   printf(\"Proc %d > x = %d\\n\", my_rank, x);\n\n   total = Global_sum(x, my_rank, p, comm);\n\n   if (my_rank == 0)\n      printf(\"The total is %d\\n\", total);\n\n   return 0;\n}", "label": "int main(void) {\n   int p, my_rank;\n   MPI_Comm comm;\n   int x;\n   int total;\n\n   MPI_Init(NULL, NULL);\n   comm = MPI_COMM_WORLD;\n   MPI_Comm_size(comm, &p);\n   MPI_Comm_rank(comm, &my_rank);\n\n   srandom(my_rank+1);\n   x = random() % MAX_CONTRIB;\n   printf(\"Proc %d > x = %d\\n\", my_rank, x);\n\n   total = Global_sum(x, my_rank, p, comm);\n\n   if (my_rank == 0)\n      printf(\"The total is %d\\n\", total);\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "lapesd_1210", "code": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing Dataset test...\\n\", stdout);\n  ctx_init ();\n  trec_init ();\n  prng_pool_init ();\n\n  if (ctx.num_procs > MAX_PROCESSES)\n    {\n      MPI_Comm new_comm;\n      MPI_Group grp, new_grp;\n      int range[3] =\n\t{ 0, MAX_PROCESSES - 1, 1 };\n      ctx.comm = new_comm;\n    }\n\n  if (ctx.comm != MPI_COMM_NULL)\n    {\n      const int num_tests = 6;\n      int i;\n      int rerr;\n      for (i = 1; i <= num_tests; i++)\n\t{\n\t  trec_exp_start (1);\n\t  trec_exp_repl_start (1, 1);\n\t  trec_run_start (1, 1);\n\t  trec_run_rept_start (1);\n\t  switch (i)\n\t    {\n\t    case 1:\n\t      rerr = test_01 ();\n\t      break;\n\t    case 2:\n\t      rerr = test_02 ();\n\t      break;\n\t    case 3:\n\t      rerr = test_03 ();\n\t      break;\n\t    case 4:\n\t      rerr = test_04 ();\n\t      break;\n\t    case 5:\n\t      rerr = test_05 ();\n\t      break;\n\t    case 6:\n\t      rerr = test_06 ();\n\t      break;\n\t    }\n\t  if (rerr)\n\t    fprintf (stdout, \"[Task %d] Test %d: ...FAIL!\\n\", ctx.task_id, i);\n\t  else\n\t    fprintf (stdout, \"[Task %d] Test %d: ...SUCCESS!\\n\", ctx.task_id,\n\t\t     i);\n\t  trec_run_rept_stop ();\n\t  trec_run_stop ();\n\t  trec_exp_repl_stop ();\n\t  trec_exp_stop ();\n\t}\n    }\n\n  fputs (\"Finalizing Dataset test.\\n\", stdout);\n}", "label": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing Dataset test...\\n\", stdout);\n  MPI_Init (&argc, &argv);\n  ctx_init ();\n  trec_init ();\n  prng_pool_init ();\n\n  if (ctx.num_procs > MAX_PROCESSES)\n    {\n      MPI_Comm new_comm;\n      MPI_Group grp, new_grp;\n      int range[3] =\n\t{ 0, MAX_PROCESSES - 1, 1 };\n      MPI_Comm_group (MPI_COMM_WORLD, &grp);\n      MPI_Group_range_incl (grp, 1, &range, &new_grp);\n      MPI_Comm_create (MPI_COMM_WORLD, new_grp, &new_comm);\n      ctx.comm = new_comm;\n    }\n\n  if (ctx.comm != MPI_COMM_NULL)\n    {\n      const int num_tests = 6;\n      int i;\n      int rerr;\n      for (i = 1; i <= num_tests; i++)\n\t{\n\t  trec_exp_start (1);\n\t  trec_exp_repl_start (1, 1);\n\t  trec_run_start (1, 1);\n\t  trec_run_rept_start (1);\n\t  switch (i)\n\t    {\n\t    case 1:\n\t      rerr = test_01 ();\n\t      break;\n\t    case 2:\n\t      rerr = test_02 ();\n\t      break;\n\t    case 3:\n\t      rerr = test_03 ();\n\t      break;\n\t    case 4:\n\t      rerr = test_04 ();\n\t      break;\n\t    case 5:\n\t      rerr = test_05 ();\n\t      break;\n\t    case 6:\n\t      rerr = test_06 ();\n\t      break;\n\t    }\n\t  if (rerr)\n\t    fprintf (stdout, \"[Task %d] Test %d: ...FAIL!\\n\", ctx.task_id, i);\n\t  else\n\t    fprintf (stdout, \"[Task %d] Test %d: ...SUCCESS!\\n\", ctx.task_id,\n\t\t     i);\n\t  trec_run_rept_stop ();\n\t  trec_run_stop ();\n\t  trec_exp_repl_stop ();\n\t  trec_exp_stop ();\n\t}\n    }\n\n  MPI_Finalize ();\n  fputs (\"Finalizing Dataset test.\\n\", stdout);\n}"}
{"program": "rpereira-dev_1211", "code": "int main(int argc, char **argv)\n{\n    int rank, P, val_to_send, i;\n\n\n    \n\n    int all_vals[P];\n\n    val_to_send = rank;\n\n    for(i = 0 ; i < P ; i++)\n    {\n        if (i == rank)\n        {\n            all_vals[i] = val_to_send;\n        }\n        else\n        {\n        }\n    }\n\n    printf(\"P%02d = \", rank);\n    for(i = 0 ; i < P ; i++)\n    {\n        printf(\"%d \", all_vals[i]);\n    }\n    printf(\"\\n\"); fflush(stdout);\n\n\n    return 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n    int rank, P, val_to_send, i;\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &P);\n\n    \n\n    int all_vals[P];\n\n    val_to_send = rank;\n\n    for(i = 0 ; i < P ; i++)\n    {\n        if (i == rank)\n        {\n            MPI_Bcast(&val_to_send, 1, MPI_INT, rank, MPI_COMM_WORLD);\n            all_vals[i] = val_to_send;\n        }\n        else\n        {\n            MPI_Bcast(all_vals+i, 1, MPI_INT, rank, MPI_COMM_WORLD);\n        }\n    }\n\n    printf(\"P%02d = \", rank);\n    for(i = 0 ; i < P ; i++)\n    {\n        printf(\"%d \", all_vals[i]);\n    }\n    printf(\"\\n\"); fflush(stdout);\n\n    MPI_Finalize();\n\n    return 0;\n}\n"}
{"program": "duaneloh_1212", "code": "int main(int argc, char *argv[]) {\n\tint num_iter, continue_flag = 0, num_threads = omp_get_max_threads() ;\n\tchar config_fname[1024] ;\n\t\n\tgettimeofday(&tr1, NULL) ;\n\t\n\tnum_iter = parse_arguments(argc, argv, &continue_flag, &num_threads, config_fname) ;\n\tif (num_iter < 0) {\n\t\treturn 1 ;\n\t}\n\t\n\tif (setup(config_fname, continue_flag)) {\n\t\treturn 1 ;\n\t}\n\tparam->num_iter = num_iter ;\n\t\n\tif (!param->rank && !continue_flag)\n\t\twrite_log_file_header(num_threads) ;\n\t\n\temc() ;\n\t\n\n\t\n\t\n\treturn 0 ;\n}", "label": "int main(int argc, char *argv[]) {\n\tint num_iter, continue_flag = 0, num_threads = omp_get_max_threads() ;\n\tchar config_fname[1024] ;\n\t\n\tMPI_Init(&argc, &argv) ;\n\tgettimeofday(&tr1, NULL) ;\n\t\n\tnum_iter = parse_arguments(argc, argv, &continue_flag, &num_threads, config_fname) ;\n\tif (num_iter < 0) {\n\t\tMPI_Finalize() ;\n\t\treturn 1 ;\n\t}\n\t\n\tif (setup(config_fname, continue_flag)) {\n\t\tMPI_Finalize() ;\n\t\treturn 1 ;\n\t}\n\tparam->num_iter = num_iter ;\n\t\n\tif (!param->rank && !continue_flag)\n\t\twrite_log_file_header(num_threads) ;\n\t\n\temc() ;\n\t\n\n\t\n\tMPI_Finalize() ;\n\t\n\treturn 0 ;\n}"}
{"program": "ghisvail_1218", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  unsigned parallel, verbose;\n  MPI_Comm comm_cart_1d, comm_cart_2d, comm_cart_3d;\n  \n  \n\n  n[0] = 128; n[1] = 128; n[2] = 128;\n\n\n  np[0] = 1; np[1] = 1; np[2] = 1;\n  verbose = 0;\n\n  \n\n  pfft_init();\n \n  \n\n  init_parameters(argc, argv, n, np, &verbose);\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel forward and backward FFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for n[0] x n[1] x n[2] = %td x %td x %td Fourier coefficients (change with -pfft_n * * *)\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* on  np[0] x np[1] x np[2] = %td x %td x %td processes (change with -pfft_np * * *)\\n\", np[0], np[1], np[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: Procmesh of size %d x %d x %d does not fit to number of allocated processes.\\n\", np[0], np[1], np[2]);\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"       Please allocate %d processes (mpiexec -np %d ...) or change the procmesh (with -pfft_np * * *).\\n\", np[0]*np[1]*np[2], np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  pfft_printf(MPI_COMM_WORLD, \"PFFT runtimes (3d data decomposition):\\n\");\n  loop_pfft_tests(n, comm_cart_3d, verbose);\n\n  \n\n  if( ((np[0]==1) + (np[1]==1) + (np[2]==1)) >= 1 ){\n    if(np[0]==1){ np[0] = np[1]; np[1] = np[2]; np[2] = 1; }\n\n    pfft_create_procmesh(2, MPI_COMM_WORLD, np, &comm_cart_2d);\n    pfft_printf(MPI_COMM_WORLD, \"\\nPFFT runtimes (2d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_2d, verbose);\n  }\n\n  \n\n  if( ((np[0]==1) + (np[1]==1) + (np[2]==1)) >= 2 ){\n    if(np[0]==1){ np[0] = np[1]; np[1] = np[2]; np[2] = 1; }\n\n    pfft_create_procmesh(1, MPI_COMM_WORLD, np, &comm_cart_1d);\n    pfft_printf(MPI_COMM_WORLD, \"\\nPFFT runtimes (1d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_1d, verbose);\n\n    pfft_printf(MPI_COMM_WORLD, \"\\nFFTW_MPI runtimes (1d data decomposition):\\n\");\n    loop_fftw_tests(n, parallel=1, verbose);\n  }\n\n  \n\n  if( np[0]*np[1]*np[2] == 1 ){\n    pfft_printf(MPI_COMM_WORLD, \"\\nserial FFTW runtimes (no data decomposition at all):\\n\");\n    loop_fftw_tests(n, parallel=0, verbose);\n  }\n\n  \n\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3];\n  unsigned parallel, verbose;\n  MPI_Comm comm_cart_1d, comm_cart_2d, comm_cart_3d;\n  \n  \n\n  n[0] = 128; n[1] = 128; n[2] = 128;\n\n\n  np[0] = 1; np[1] = 1; np[2] = 1;\n  verbose = 0;\n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n \n  \n\n  init_parameters(argc, argv, n, np, &verbose);\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel forward and backward FFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for n[0] x n[1] x n[2] = %td x %td x %td Fourier coefficients (change with -pfft_n * * *)\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* on  np[0] x np[1] x np[2] = %td x %td x %td processes (change with -pfft_np * * *)\\n\", np[0], np[1], np[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: Procmesh of size %d x %d x %d does not fit to number of allocated processes.\\n\", np[0], np[1], np[2]);\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"       Please allocate %d processes (mpiexec -np %d ...) or change the procmesh (with -pfft_np * * *).\\n\", np[0]*np[1]*np[2], np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  pfft_printf(MPI_COMM_WORLD, \"PFFT runtimes (3d data decomposition):\\n\");\n  loop_pfft_tests(n, comm_cart_3d, verbose);\n  MPI_Comm_free(&comm_cart_3d);\n\n  \n\n  if( ((np[0]==1) + (np[1]==1) + (np[2]==1)) >= 1 ){\n    if(np[0]==1){ np[0] = np[1]; np[1] = np[2]; np[2] = 1; }\n\n    pfft_create_procmesh(2, MPI_COMM_WORLD, np, &comm_cart_2d);\n    pfft_printf(MPI_COMM_WORLD, \"\\nPFFT runtimes (2d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_2d, verbose);\n    MPI_Comm_free(&comm_cart_2d);\n  }\n\n  \n\n  if( ((np[0]==1) + (np[1]==1) + (np[2]==1)) >= 2 ){\n    if(np[0]==1){ np[0] = np[1]; np[1] = np[2]; np[2] = 1; }\n\n    pfft_create_procmesh(1, MPI_COMM_WORLD, np, &comm_cart_1d);\n    pfft_printf(MPI_COMM_WORLD, \"\\nPFFT runtimes (1d data decomposition):\\n\");\n    loop_pfft_tests(n, comm_cart_1d, verbose);\n    MPI_Comm_free(&comm_cart_1d);\n\n    pfft_printf(MPI_COMM_WORLD, \"\\nFFTW_MPI runtimes (1d data decomposition):\\n\");\n    loop_fftw_tests(n, parallel=1, verbose);\n  }\n\n  \n\n  if( np[0]*np[1]*np[2] == 1 ){\n    pfft_printf(MPI_COMM_WORLD, \"\\nserial FFTW runtimes (no data decomposition at all):\\n\");\n    loop_fftw_tests(n, parallel=0, verbose);\n  }\n\n  \n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "derElektrobesen_1219", "code": "int main(int argc, char **argv) {\n\n\tint world_rank = 0;\n\n\tint world_size = 0;\n\n\tset_rank(world_rank);\n\n\tif (world_size != NODES_COUNT) {\n\t\twarn(\"Must specify MP_PROCS=%d. Terminating.\\n\", NODES_COUNT);\n\t\treturn 0;\n\t}\n\n\tif (world_rank == MANAGER_RANK) {\n\t\tdo_manage();\n\t} else {\n\t\tdo_test(world_rank);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv) {\n\tMPI_Init(&argc, &argv);\n\n\tint world_rank = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n\tint world_size = 0;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n\tset_rank(world_rank);\n\n\tif (world_size != NODES_COUNT) {\n\t\twarn(\"Must specify MP_PROCS=%d. Terminating.\\n\", NODES_COUNT);\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tif (world_rank == MANAGER_RANK) {\n\t\tdo_manage();\n\t} else {\n\t\tdo_test(world_rank);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "indiependente_1221", "code": "int main(int argc, char** argv)\n{\n\tint my_rank, p;\n\tint *buffer;\n\tint *array, local_max, max;\n\tint *locals;\n\tint chunk;\n\n\n\tchunk = SIZE / p;\n\n\tif (my_rank == MASTER)\n\t{\n\t\tarray = malloc(SIZE * sizeof(int));\n\t\trand_fill_array(array, SIZE);\n\t\tlocals = malloc(p * sizeof(int));\n\t}\n\n\tbuffer = malloc(chunk * sizeof(int));\n\n\tlocal_max = find_max(buffer, chunk);\n\n\tif (my_rank == MASTER)\n\t{\n\t\tmax = find_max(locals, p);\n\t\tprintf(\"Il massimo \u00e8: %d\\n\", max);\n\t\tfree(locals);\n\t\tfree(array);\n\t}\n\n\tfree(buffer);\n\treturn 0;\n\n}\n", "label": "int main(int argc, char** argv)\n{\n\tint my_rank, p;\n\tint *buffer;\n\tint *array, local_max, max;\n\tint *locals;\n\tint chunk;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\n\tchunk = SIZE / p;\n\n\tif (my_rank == MASTER)\n\t{\n\t\tarray = malloc(SIZE * sizeof(int));\n\t\trand_fill_array(array, SIZE);\n\t\tlocals = malloc(p * sizeof(int));\n\t}\n\n\tbuffer = malloc(chunk * sizeof(int));\n\n\tMPI_Scatter(array, chunk, MPI_INT, buffer, chunk, MPI_INT, MASTER, MPI_COMM_WORLD);\n\tlocal_max = find_max(buffer, chunk);\n\tMPI_Gather(&local_max, 1, MPI_INT, locals, 1, MPI_INT, MASTER, MPI_COMM_WORLD);\n\n\tif (my_rank == MASTER)\n\t{\n\t\tmax = find_max(locals, p);\n\t\tprintf(\"Il massimo \u00e8: %d\\n\", max);\n\t\tfree(locals);\n\t\tfree(array);\n\t}\n\n\tfree(buffer);\n\tMPI_Finalize();\n\treturn 0;\n\n}\n"}
{"program": "gyaikhom_1222", "code": "int main(int argc, char *argv[])\n{\n\tiskel_all2all_t *all2all;\n\tbc_plist_t *pl;\n\tint i;\n\tfloat in, out[4];\n\n\tbc_init(BC_ERR|BC_PLIST_ALL);\n\n \tpl = bc_plist_create(3, 0, 1, 2);\n  \tall2all = iskel_all2all_create(pl, bc_float);\n\n\tin = bc_rank*1.83;\n  \tiskel_all2all_exec(all2all, &in, out, float);\n\n\tif (bc_plist_iselem(pl, bc_rank)) {\n\t\tprintf(\"[%d] \", bc_rank);\n\t\tfor (i = 0; i < bc_plist_nelem(pl); i++)\n\t\t\tprintf(\"%f \", out[i]);\n\t\tprintf(\"\\n\"); \n\t}\n\n\tin = bc_rank*3.98;\n \tiskel_all2all_exec(all2all, &in, out, float);\n\n\tif (bc_plist_iselem(pl, bc_rank)) {\n\t\tprintf(\"[%d] \", bc_rank);\n\t\tfor (i = 0; i < bc_plist_nelem(pl); i++)\n\t\t\tprintf(\"%f \", out[i]);\n\t\tprintf(\"\\n\"); \n\t}\n\n   \tiskel_all2all_destroy(all2all);\n  \tbc_plist_destroy(pl);\n\n\t\n\n\tbc_final();\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tiskel_all2all_t *all2all;\n\tbc_plist_t *pl;\n\tint i;\n\tfloat in, out[4];\n\n\tMPI_Init(&argc, &argv);\n\tbc_init(BC_ERR|BC_PLIST_ALL);\n\n \tpl = bc_plist_create(3, 0, 1, 2);\n  \tall2all = iskel_all2all_create(pl, bc_float);\n\n\tin = bc_rank*1.83;\n  \tiskel_all2all_exec(all2all, &in, out, float);\n\n\tif (bc_plist_iselem(pl, bc_rank)) {\n\t\tprintf(\"[%d] \", bc_rank);\n\t\tfor (i = 0; i < bc_plist_nelem(pl); i++)\n\t\t\tprintf(\"%f \", out[i]);\n\t\tprintf(\"\\n\"); \n\t}\n\n\tin = bc_rank*3.98;\n \tiskel_all2all_exec(all2all, &in, out, float);\n\n\tif (bc_plist_iselem(pl, bc_rank)) {\n\t\tprintf(\"[%d] \", bc_rank);\n\t\tfor (i = 0; i < bc_plist_nelem(pl); i++)\n\t\t\tprintf(\"%f \", out[i]);\n\t\tprintf(\"\\n\"); \n\t}\n\n   \tiskel_all2all_destroy(all2all);\n  \tbc_plist_destroy(pl);\n\n\t\n\n\tbc_final();\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "VulpesCorsac_1223", "code": "int main(int argc, char** argv) {\n\tint num, rnk;\n\tprintf(\"%d from %d\\n\", rnk, num);\n\n\tconst int N = 1000000;\n\tif (rnk == 0) {\n\t\tint nr = 0;\n\t\tfloat summ = 0;\n\t\twhile (nr < num-1) {\n\t\t\tfloat r;\n\t\t\tsumm += r;\n\t\t\tnr++;\n\t\t }\n\t\tprintf(\"%.8f\", summ*4);\n\t} else {\n\t\tint si = rnk;\n\t\tfloat summ = 0;\n\t\twhile (si < N) {\n\t\t\tdouble x1 = si / ( (double) N);\n\t\t\tdouble x2 = (si+1) / ( (double) N);\n\t\t\tdouble y1 = sqrt(1 - x1*x1);\n\t\t\tdouble y2 = sqrt(1 - x2*x2);\n\t\t\tsumm += (x2-x1) * (y1+y2) * 0.5;\n\t\t\tsi += num - 1;\n\t\t  }\n\t}\n\n}", "label": "int main(int argc, char** argv) {\n\tMPI_Init(&argc, &argv);\n\tint num, rnk;\n\tMPI_Comm_size(MPI_COMM_WORLD, &num);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rnk);\n\tprintf(\"%d from %d\\n\", rnk, num);\n\n\tconst int N = 1000000;\n\tif (rnk == 0) {\n\t\tint nr = 0;\n\t\tfloat summ = 0;\n\t\twhile (nr < num-1) {\n\t\t\tfloat r;\n\t\t\tMPI_Recv(&r, 1, MPI_FLOAT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tsumm += r;\n\t\t\tnr++;\n\t\t }\n\t\tprintf(\"%.8f\", summ*4);\n\t} else {\n\t\tint si = rnk;\n\t\tfloat summ = 0;\n\t\twhile (si < N) {\n\t\t\tdouble x1 = si / ( (double) N);\n\t\t\tdouble x2 = (si+1) / ( (double) N);\n\t\t\tdouble y1 = sqrt(1 - x1*x1);\n\t\t\tdouble y2 = sqrt(1 - x2*x2);\n\t\t\tsumm += (x2-x1) * (y1+y2) * 0.5;\n\t\t\tsi += num - 1;\n\t\t  }\n\t\tMPI_Send(&summ, 1, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\tMPI_Finalize();\n}"}
{"program": "jianfeipan_1225", "code": "int main(int argc, char *argv[])\n{\t\n  int messageNumber=0;\n  int blockSize = 10;\n  int sleepTime = 0;\n  if(argc>1){\n  \tblockSize = atoi(argv[1]);\n\tif(argc==3){\n\t\tsleepTime = atoi(argv[2]);\n\t}\n  }\n  double starttime, endtime;\n  MPI_Status status;\n  int i,j, num, rank, size, nbslaves,loop, receivedsize, nbline;\n  char inputstr [100],outstr [100];\n\n  loop = 1;\n  \n\n\n\n  nbslaves = size -1;\n  \n\n  if (rank == 0) {\n\tstarttime =\n\t\n\n\tfor(i = 1; i<=nbslaves;i++){\n\t\tnbline = -MAXY+(i-1)*blockSize;\n  \t}\n\n\tint lineBuff[(2*MAXX+1)*blockSize+1];\n\twhile(loop){\n\t\tmessageNumber++;\n\t\tif(nbline > MAXY){\n\t\t\tloop=0;\n\t\t}\t\n\t    nbline=nbline+blockSize;\n\t\tj = lineBuff[0];\t\n\t\tfor(j = lineBuff[0];j< MIN(lineBuff[0]+blockSize,MAXY+1);j++){\n\t\t\tfor(i = -MAXX; i <= MAXX; i++) {\n\t\t\t\tcases[i+MAXX][j+MAXY] = lineBuff[MAXX+i+1+(j-lineBuff[0])*(2*MAXX+1)];\n\t\t   \t}\n\t\t}\t\n\t\t\n\t}\n    for( i=1 ; i <=nbslaves ; i++ ) {\n\t}\n\tendtime =\n\tprintf(\"using time : %lf, total message number : %d\\n\",endtime-starttime,messageNumber);\n    dump_ppm(\"mandel.ppm\", cases);\n    printf(\"Fini.\\n\");\n\t\n  }\n\n  else {\n\tdouble x, y;\n\tint lineBuff[(2*MAXX+1)*blockSize+1];\n\tint i, j, res, rc;\n\twhile(loop){\n\t\t\n\n\t\tif ( receivedsize== 0) {\n\t\t\tloop = 0;\n\t\t}else{\n\t\t\tlineBuff[0] = res;\n\t\t\tfor(j=res;j< MIN(res+blockSize,MAXY+1);j++){\n\t\t\t\t\n\n\t\t\t\ty = 1.5 * j / (double)MAXY;\n\t\t\t\tfor(i = -MAXX; i <= MAXX; i++) {     \n\t\t\t\t\tx = 2 * i / (double)MAXX;   \t\t\n\t\t\t\t\tlineBuff[i+MAXX+1+(j-res)*(2*MAXX+1)] = mandel(x, y);\t\t\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\n\t\t}\n\t}\n  }\n  \n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\t\n  int messageNumber=0;\n  int blockSize = 10;\n  int sleepTime = 0;\n  if(argc>1){\n  \tblockSize = atoi(argv[1]);\n\tif(argc==3){\n\t\tsleepTime = atoi(argv[2]);\n\t}\n  }\n  double starttime, endtime;\n  MPI_Status status;\n  int i,j, num, rank, size, nbslaves,loop, receivedsize, nbline;\n  char inputstr [100],outstr [100];\n\n  loop = 1;\n  \n\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  nbslaves = size -1;\n  \n\n  if (rank == 0) {\n\tstarttime = MPI_Wtime();\n\t\n\n\tfor(i = 1; i<=nbslaves;i++){\n\t\tnbline = -MAXY+(i-1)*blockSize;\n\t\tMPI_Send(&nbline, 1 , MPI_INT, i, DATATAG+1, MPI_COMM_WORLD);\n  \t}\n\n\tint lineBuff[(2*MAXX+1)*blockSize+1];\n\twhile(loop){\n\t\tmessageNumber++;\n\t\tMPI_Recv(lineBuff, (2*MAXX+1)*blockSize+1, MPI_INT, MPI_ANY_SOURCE, DATATAG, MPI_COMM_WORLD, &status);\n\t\tif(nbline > MAXY){\n\t\t\tloop=0;\n\t\t}\t\n\t    nbline=nbline+blockSize;\n\t\tj = lineBuff[0];\t\n\t\tfor(j = lineBuff[0];j< MIN(lineBuff[0]+blockSize,MAXY+1);j++){\n\t\t\tfor(i = -MAXX; i <= MAXX; i++) {\n\t\t\t\tcases[i+MAXX][j+MAXY] = lineBuff[MAXX+i+1+(j-lineBuff[0])*(2*MAXX+1)];\n\t\t   \t}\n\t\t}\t\n\t\tMPI_Send(&nbline, 1 , MPI_INT, status.MPI_SOURCE, DATATAG+1, MPI_COMM_WORLD);\n\t\t\n\t}\n    for( i=1 ; i <=nbslaves ; i++ ) {\n\t  MPI_Send(&nbline, 0, MPI_CHAR, i, DATATAG+1, MPI_COMM_WORLD); \n\t}\n\tendtime = MPI_Wtime();\n\tprintf(\"using time : %lf, total message number : %d\\n\",endtime-starttime,messageNumber);\n    dump_ppm(\"mandel.ppm\", cases);\n    printf(\"Fini.\\n\");\n\t\n  }\n\n  else {\n\tdouble x, y;\n\tint lineBuff[(2*MAXX+1)*blockSize+1];\n\tint i, j, res, rc;\n\twhile(loop){\n\t\t\n\n\t\tMPI_Recv(&res, 1, MPI_INT, 0, DATATAG+1, MPI_COMM_WORLD, &status);\n\t\tMPI_Get_count (&status, MPI_INT, &receivedsize);\n\t\tif ( receivedsize== 0) {\n\t\t\tloop = 0;\n\t\t}else{\n\t\t\tlineBuff[0] = res;\n\t\t\tfor(j=res;j< MIN(res+blockSize,MAXY+1);j++){\n\t\t\t\t\n\n\t\t\t\ty = 1.5 * j / (double)MAXY;\n\t\t\t\tfor(i = -MAXX; i <= MAXX; i++) {     \n\t\t\t\t\tx = 2 * i / (double)MAXX;   \t\t\n\t\t\t\t\tlineBuff[i+MAXX+1+(j-res)*(2*MAXX+1)] = mandel(x, y);\t\t\n\t\t\t\t}\n\t\t\t}\n\t\t\tMPI_Send(lineBuff,(2*MAXX+1)*blockSize+1, MPI_INT, 0, DATATAG, MPI_COMM_WORLD); \n\t\t\t\n\n\t\t}\n\t}\n  }\n  \n  MPI_Finalize();\n  return 0;\n}"}
{"program": "rahlk_1226", "code": "main (int argc, char **argv)\n{\n  char processor_name[128];\n  int namelen = 128;\n\n  \n\n  printf (\"Initializing (%d of %d)\\n\", rank, nprocs);\n  printf (\"(%d) is alive on %s\\n\", rank, processor_name);\n  fflush (stdout);\n\n  exchangeData ();\n\n\n  printf (\"(%d) Finished normally\\n\", rank);\n}", "label": "main (int argc, char **argv)\n{\n  char processor_name[128];\n  int namelen = 128;\n\n  \n\n  MPI_Init (&argc, &argv);\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  printf (\"Initializing (%d of %d)\\n\", rank, nprocs);\n  MPI_Get_processor_name (processor_name, &namelen);\n  printf (\"(%d) is alive on %s\\n\", rank, processor_name);\n  fflush (stdout);\n\n  exchangeData ();\n\n  MPI_Barrier (MPI_COMM_WORLD);\n\n  MPI_Finalize ();\n  printf (\"(%d) Finished normally\\n\", rank);\n}"}
{"program": "mpip_1229", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "rahlk_1230", "code": "main (int argc, char **argv)\n{\n  int nprocs = -1;\n  int rank = -1;\n  char buf[256];\n\n\n  gethostname (buf, 256);\n  printf (\"MPI comm size is %d with rank %d executing on %s\\n\",\n\t  nprocs, rank, buf);\n\n  sleep (1);\n\n  exit (1);\n#if 0\n  sleep (1);\n\n  sleep (1);\n  sleep (1);\n#endif\n}", "label": "main (int argc, char **argv)\n{\n  int nprocs = -1;\n  int rank = -1;\n  char buf[256];\n\n  MPI_Init (&argc, &argv);\t\n\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  gethostname (buf, 256);\n  printf (\"MPI comm size is %d with rank %d executing on %s\\n\",\n\t  nprocs, rank, buf);\n\n  MPI_Barrier (MPI_COMM_WORLD);\n  sleep (1);\n  MPI_Barrier (MPI_COMM_WORLD);\n  MPI_Pcontrol (2);\t\t\n\n  MPI_Barrier (MPI_COMM_WORLD);\n  exit (1);\n#if 0\n  sleep (1);\n  MPI_Barrier (MPI_COMM_WORLD);\n  MPI_Pcontrol (-1);\t\t\n\n  sleep (1);\n  MPI_Barrier (MPI_COMM_WORLD);\n  sleep (1);\n  MPI_Finalize ();\n#endif\n}"}
{"program": "CFDEMproject_1231", "code": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *kmem;\n  UserData data;\n  N_Vector cc, sc, constraints;\n  int globalstrategy;\n  long int Nlocal;\n  realtype fnormtol, scsteptol, dq_rel_uu;\n  int flag, maxl, maxlrst;\n  long int mudq, mldq, mukeep, mlkeep;\n  int my_pe, npes, npelast = NPEX*NPEY-1;\n\n  data = NULL;\n  kmem = NULL;\n  cc = sc = constraints = NULL;\n\n  \n\n  comm = MPI_COMM_WORLD;\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      printf(\"\\nMPI_ERROR(0): npes=%d is not equal to NPEX*NPEY=%d\\n\", npes,\n             NPEX*NPEY);\n    return(1);\n  }\n\n  \n \n\n  \n\n  Nlocal = NUM_SPECIES*MXSUB*MYSUB;\n\n  \n\n  data = AllocUserData();\n  InitUserData(my_pe, Nlocal, comm, data);\n\n  \n\n  globalstrategy = KIN_NONE;\n\n  \n\n  cc = N_VNew_Parallel(comm, Nlocal, NEQ);\n  sc = N_VNew_Parallel(comm, Nlocal, NEQ);\n  data->rates = N_VNew_Parallel(comm, Nlocal, NEQ);\n  if (check_flag((void *)data->rates, \"N_VNew_Parallel\", 0, my_pe))\n  constraints = N_VNew_Parallel(comm, Nlocal, NEQ);\n  if (check_flag((void *)constraints, \"N_VNew_Parallel\", 0, my_pe))\n  N_VConst(ZERO, constraints);\n  \n  SetInitialProfiles(cc, sc);\n\n  fnormtol = FTOL; scsteptol = STOL;\n\n  \n\n  kmem = KINCreate();\n\n  \n\n  flag = KINInit(kmem, func, cc);\n\n  flag = KINSetUserData(kmem, data);\n\n  flag = KINSetConstraints(kmem, constraints);\n\n  \n\n  N_VDestroy_Parallel(constraints);\n\n  flag = KINSetFuncNormTol(kmem, fnormtol);\n\n  flag = KINSetScaledStepTol(kmem, scsteptol);\n  \n  \n\n  dq_rel_uu = ZERO;\n  mudq = mldq = 2*NUM_SPECIES - 1;\n  mukeep = mlkeep = NUM_SPECIES;\n\n  \n\n  maxl = 20; maxlrst = 2;\n  flag = KINSpgmr(kmem, maxl);\n\n  \n\n  flag = KINBBDPrecInit(kmem, Nlocal, mudq, mldq, mukeep, mlkeep,\n                        dq_rel_uu, func_local, NULL);\n\n\n  flag = KINSpilsSetMaxRestarts(kmem, maxlrst);\n  if (check_flag(&flag, \"KINSpilsSetMaxRestarts\", 1, my_pe)) \n\n  \n\n  if (my_pe == 0)\n    PrintHeader(globalstrategy, maxl, maxlrst, mudq, mldq, mukeep,\n\t\tmlkeep, fnormtol, scsteptol);\n\n  \n\n  flag = KINSol(kmem,           \n\n                cc,             \n\n                globalstrategy, \n\n                sc,             \n\n                sc);            \n\n\n  if (my_pe == 0) printf(\"\\n\\nComputed equilibrium species concentrations:\\n\");\n  if (my_pe == 0 || my_pe==npelast) PrintOutput(my_pe, comm, cc);\n  \n  \n\n  if (my_pe == 0) \n    PrintFinalStats(kmem);\n\n  N_VDestroy_Parallel(cc);\n  N_VDestroy_Parallel(sc);\n\n  KINFree(&kmem);\n  FreeUserData(data);\n\n\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *kmem;\n  UserData data;\n  N_Vector cc, sc, constraints;\n  int globalstrategy;\n  long int Nlocal;\n  realtype fnormtol, scsteptol, dq_rel_uu;\n  int flag, maxl, maxlrst;\n  long int mudq, mldq, mukeep, mlkeep;\n  int my_pe, npes, npelast = NPEX*NPEY-1;\n\n  data = NULL;\n  kmem = NULL;\n  cc = sc = constraints = NULL;\n\n  \n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &my_pe);\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      printf(\"\\nMPI_ERROR(0): npes=%d is not equal to NPEX*NPEY=%d\\n\", npes,\n             NPEX*NPEY);\n    return(1);\n  }\n\n  \n \n\n  \n\n  Nlocal = NUM_SPECIES*MXSUB*MYSUB;\n\n  \n\n  data = AllocUserData();\n  if (check_flag((void *)data, \"AllocUserData\", 2, my_pe)) MPI_Abort(comm, 1);\n  InitUserData(my_pe, Nlocal, comm, data);\n\n  \n\n  globalstrategy = KIN_NONE;\n\n  \n\n  cc = N_VNew_Parallel(comm, Nlocal, NEQ);\n  if (check_flag((void *)cc, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  sc = N_VNew_Parallel(comm, Nlocal, NEQ);\n  if (check_flag((void *)sc, \"N_VNew_Parallel\", 0, my_pe)) MPI_Abort(comm, 1);\n  data->rates = N_VNew_Parallel(comm, Nlocal, NEQ);\n  if (check_flag((void *)data->rates, \"N_VNew_Parallel\", 0, my_pe))\n      MPI_Abort(comm, 1);\n  constraints = N_VNew_Parallel(comm, Nlocal, NEQ);\n  if (check_flag((void *)constraints, \"N_VNew_Parallel\", 0, my_pe))\n      MPI_Abort(comm, 1);\n  N_VConst(ZERO, constraints);\n  \n  SetInitialProfiles(cc, sc);\n\n  fnormtol = FTOL; scsteptol = STOL;\n\n  \n\n  kmem = KINCreate();\n  if (check_flag((void *)kmem, \"KINCreate\", 0, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  flag = KINInit(kmem, func, cc);\n  if (check_flag(&flag, \"KINInit\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  flag = KINSetUserData(kmem, data);\n  if (check_flag(&flag, \"KINSetUserData\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  flag = KINSetConstraints(kmem, constraints);\n  if (check_flag(&flag, \"KINSetConstraints\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  N_VDestroy_Parallel(constraints);\n\n  flag = KINSetFuncNormTol(kmem, fnormtol);\n  if (check_flag(&flag, \"KINSetFuncNormTol\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  flag = KINSetScaledStepTol(kmem, scsteptol);\n  if (check_flag(&flag, \"KINSetScaledStepTol\", 1, my_pe)) MPI_Abort(comm, 1);\n  \n  \n\n  dq_rel_uu = ZERO;\n  mudq = mldq = 2*NUM_SPECIES - 1;\n  mukeep = mlkeep = NUM_SPECIES;\n\n  \n\n  maxl = 20; maxlrst = 2;\n  flag = KINSpgmr(kmem, maxl);\n  if (check_flag(&flag, \"KINSpgmr\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  flag = KINBBDPrecInit(kmem, Nlocal, mudq, mldq, mukeep, mlkeep,\n                        dq_rel_uu, func_local, NULL);\n  if (check_flag(&flag, \"KINBBDPrecInit\", 1, my_pe)) MPI_Abort(comm, 1);\n\n\n  flag = KINSpilsSetMaxRestarts(kmem, maxlrst);\n  if (check_flag(&flag, \"KINSpilsSetMaxRestarts\", 1, my_pe)) \n    MPI_Abort(comm, 1);\n\n  \n\n  if (my_pe == 0)\n    PrintHeader(globalstrategy, maxl, maxlrst, mudq, mldq, mukeep,\n\t\tmlkeep, fnormtol, scsteptol);\n\n  \n\n  flag = KINSol(kmem,           \n\n                cc,             \n\n                globalstrategy, \n\n                sc,             \n\n                sc);            \n\n  if (check_flag(&flag, \"KINSol\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  if (my_pe == 0) printf(\"\\n\\nComputed equilibrium species concentrations:\\n\");\n  if (my_pe == 0 || my_pe==npelast) PrintOutput(my_pe, comm, cc);\n  \n  \n\n  if (my_pe == 0) \n    PrintFinalStats(kmem);\n\n  N_VDestroy_Parallel(cc);\n  N_VDestroy_Parallel(sc);\n\n  KINFree(&kmem);\n  FreeUserData(data);\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "SalvatoreDiGirolamo_1233", "code": "int main(int argc, char * argv[]){\n\n    CMPI_Win win;\n    int rank;\n    int msg_size = 8; \n\n    MMPI_INIT(&argc, &argv);\n    LSB_Init(\"test_clampi\", 0);\n\n    int seed = 12445678;  \n    srand(seed);\n    \n\n    cl_init();\n\n    \n    int overlap = argc==2;\n\n    if (!overlap) printf(\"Test: costs breakdown (%i)\\n\", argc);\n    else printf(\"Test: overlap\\n\");\n\n    if (rank==0){\n#ifdef USE_FOMPI \n        printf(\"CLAMPI: Using foMPI.\\n\");\n#else \n        printf(\"CLAMPI: Not using foMPI\\n\");\n#endif\n    }\n    void * loc_mem;\n    posix_memalign(&loc_mem, 128, WINSIZE);\n    flushbuff = malloc(FBSIZE);\n\n    \n\n    \n\n\n\n#ifdef SCORE_NOSPACE\n    scoretype = \"NOSPACE\";\n#elif defined(SCORE_NOTIME)\n    scoretype = \"NOTIME\";\n#else\n    scoretype = \"FULL\";\n#endif\n\n\n    int v,s;\n    printf(\"MPI version: %i; subversion: %i\\n\", v, s);\n\n    int n = 1;\n    \n\n    if(*(char *)&n == 1) { printf(\"LITTLE ENDIAN\\n\");}\n    else printf(\"BIG ENDIAN\\n\");\n    MPI_Info info;\n\n    \n\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n\n\n    int start[NPOS];\n    int sizes[NPOS];\n    int gets[NGETS];\n\n    int npsizes = 15;\n    int psizes[] = {4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536};\n\n\n    for (int i=0; i<NPOS; i++){\n        start[i] = i;\n        sizes[i] = psizes[rand() % npsizes] / 4 \n;\n    }\n\n\n    int stop=0, count=0, i;\n    \n\n#ifdef NORMAL_SAMPLING\n    int mu = NPOS/2;\n    int si = mu/4;\n#endif\n\n    \n\n    for (int i=0; i<NGETS; i++){\n#ifdef NORMAL_SAMPLING\n\tgets[i] = i4_normal_ab(mu, si, &seed) % NPOS;\n#else\n        gets[i] = rand() % NPOS;\n#endif\n    }\n\n\n    \n\n    for (int i=0; i<WINCOUNT; i++){\n        ((int32_t *) win_mem)[i] = (int32_t) rand();\n    }\n\n\n    char * benchid = getenv(\"BENCH_ID\");\n    if (benchid!=NULL) LSB_Set_Rparam_string(\"bench_id\", benchid);\n\n    char * htsize_str = getenv(\"CL_HT_ENTRIES\");\n    int htsize = CL_HT_ENTRIES_VAL;\n    if (htsize_str!=NULL) htsize=atoi(htsize_str); \n\n    LSB_Set_Rparam_int(\"htsize\", htsize);\n    LSB_Set_Rparam_int(\"sample_size\", CL_MAX_SCAN);\n\n\n    \n    if (rank==0){\n        if (overlap) bench_overlap_main(loc_mem, start, sizes, gets, NGETS, win);\n        else bench_time_main(loc_mem, start, sizes, gets, NGETS, win);\n    }\n\n    if (rank==0) printf(\"END\\n\");\n    CMPI_Win_free(&win);\n \n    \n\n    \n\n    LSB_Finalize();\n    MMPI_FINALIZE();\n\n}", "label": "int main(int argc, char * argv[]){\n\n    CMPI_Win win;\n    int rank;\n    int msg_size = 8; \n\n    MMPI_INIT(&argc, &argv);\n    LSB_Init(\"test_clampi\", 0);\n\n    int seed = 12445678;  \n    srand(seed);\n    \n\n    cl_init();\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    int overlap = argc==2;\n\n    if (!overlap) printf(\"Test: costs breakdown (%i)\\n\", argc);\n    else printf(\"Test: overlap\\n\");\n\n    if (rank==0){\n#ifdef USE_FOMPI \n        printf(\"CLAMPI: Using foMPI.\\n\");\n#else \n        printf(\"CLAMPI: Not using foMPI\\n\");\n#endif\n    }\n    void * loc_mem;\n    posix_memalign(&loc_mem, 128, WINSIZE);\n    flushbuff = malloc(FBSIZE);\n\n    \n\n    \n\n\n\n#ifdef SCORE_NOSPACE\n    scoretype = \"NOSPACE\";\n#elif defined(SCORE_NOTIME)\n    scoretype = \"NOTIME\";\n#else\n    scoretype = \"FULL\";\n#endif\n\n\n    int v,s;\n    MPI_Get_version(&v, &s);\n    printf(\"MPI version: %i; subversion: %i\\n\", v, s);\n\n    int n = 1;\n    \n\n    if(*(char *)&n == 1) { printf(\"LITTLE ENDIAN\\n\");}\n    else printf(\"BIG ENDIAN\\n\");\n    MPI_Info info;\n    MPI_Info_create(&info);\n    MPI_Info_set(info, CLAMPI_MODE, CLAMPI_USER_DEFINED);\n\n    \n\n    MPICHECK(CMPI_Win_allocate(WINSIZE, WINDISPL, info, MPI_COMM_WORLD, &win_mem, &win));\n\n\n    \n\n\n\n    \n\n    \n\n    \n\n\n\n    int start[NPOS];\n    int sizes[NPOS];\n    int gets[NGETS];\n\n    int npsizes = 15;\n    int psizes[] = {4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536};\n\n\n    for (int i=0; i<NPOS; i++){\n        start[i] = i;\n        sizes[i] = psizes[rand() % npsizes] / 4 \n;\n    }\n\n\n    int stop=0, count=0, i;\n    \n\n#ifdef NORMAL_SAMPLING\n    int mu = NPOS/2;\n    int si = mu/4;\n#endif\n\n    \n\n    for (int i=0; i<NGETS; i++){\n#ifdef NORMAL_SAMPLING\n\tgets[i] = i4_normal_ab(mu, si, &seed) % NPOS;\n#else\n        gets[i] = rand() % NPOS;\n#endif\n    }\n\n\n    \n\n    for (int i=0; i<WINCOUNT; i++){\n        ((int32_t *) win_mem)[i] = (int32_t) rand();\n    }\n\n\n    char * benchid = getenv(\"BENCH_ID\");\n    if (benchid!=NULL) LSB_Set_Rparam_string(\"bench_id\", benchid);\n\n    char * htsize_str = getenv(\"CL_HT_ENTRIES\");\n    int htsize = CL_HT_ENTRIES_VAL;\n    if (htsize_str!=NULL) htsize=atoi(htsize_str); \n\n    LSB_Set_Rparam_int(\"htsize\", htsize);\n    LSB_Set_Rparam_int(\"sample_size\", CL_MAX_SCAN);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n\n    \n    if (rank==0){\n        if (overlap) bench_overlap_main(loc_mem, start, sizes, gets, NGETS, win);\n        else bench_time_main(loc_mem, start, sizes, gets, NGETS, win);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank==0) printf(\"END\\n\");\n    CMPI_Win_free(&win);\n \n    \n\n    \n\n    LSB_Finalize();\n    MMPI_FINALIZE();\n\n}"}
{"program": "annavicente_1234", "code": "int main(int argc, char *argv[])\r\n{\r\n\r\n  int i, rank, size, total_works=10, *result;\r\n\r\n\r\n  if(argc > 1)\r\n    total_works = atoi(argv[1]);\r\n\r\n\r\n  if(rank == 0)\r\n  {\r\n      result = (int*) calloc(total_works, sizeof(int));\r\n\r\n      \n\r\n      for(i=0; i < total_works; i++){\r\n        result[i] = i+1;\r\n      }\r\n\r\n      \n\r\n      master(rank, size, total_works, result);\r\n  }\r\n  else\r\n  {\r\n      slave(rank, size, total_works, ELEMENT);\r\n  }\r\n\r\n  \n\n  return 0;\r\n\r\n}", "label": "int main(int argc, char *argv[])\r\n{\r\n\r\n  int i, rank, size, total_works=10, *result;\r\n\r\n  MPI_Init(&argc, &argv);\r\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\r\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n\r\n  if(argc > 1)\r\n    total_works = atoi(argv[1]);\r\n\r\n\r\n  if(rank == 0)\r\n  {\r\n      result = (int*) calloc(total_works, sizeof(int));\r\n\r\n      \n\r\n      for(i=0; i < total_works; i++){\r\n        result[i] = i+1;\r\n      }\r\n\r\n      \n\r\n      master(rank, size, total_works, result);\r\n  }\r\n  else\r\n  {\r\n      slave(rank, size, total_works, ELEMENT);\r\n  }\r\n\r\n  MPI_Finalize();\r\n  \n\n  return 0;\r\n\r\n}"}
{"program": "bmi-forum_1236", "code": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tStream*\t\tstream;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\tstream = Journal_Register( Info_Type, \"test\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tVariable*           orig;\n\t\tVariable*           compare;\n\t\tdouble*             data;\n\t\tdouble*             data2;\n\t\tIndex               arrayCount      = 150;\n\t\tIndex               componentCount  = 4;\n\t\tIndex               index;\n\t\tdouble              amp             = 0.1;\n\t\tdouble              tolerance       = 0.04;\n\t\tVariable_Register*\tvr;\n\n\t\tdata = Memory_Alloc_Array( double, arrayCount * componentCount, \"test\" );\n\t\tdata2 = Memory_Alloc_Array( double, arrayCount * componentCount, \"test\" );\n\t\tfor( index = 0; index < arrayCount * componentCount; index++ ) {\n\t\t\tdata[index] = 1.0f / (arrayCount+2) * (index+1); \n\t\t\tdata2[ index ] = data[ index ] + amp * cos( index );\n\t\t}\t\t\n\t\t\n\t\tvr = Variable_Register_New();\n\t\torig = Variable_NewVector( \n\t\t\t\t\"orig\", \n\t\t\t\tVariable_DataType_Double, \n\t\t\t\tcomponentCount,\n\t\t\t\t&arrayCount,\n\t\t\t\t(void**)&data,\n\t\t\t\tvr,\n\t\t\t\t\"orig1\",\n\t\t\t\t\"orig2\",\n\t\t\t\t\"orig3\",\n\t\t\t\t\"orig4\" );\n\t\tcompare = Variable_NewVector( \n\t\t\t\t\"compare\", \n\t\t\t\tVariable_DataType_Double, \n\t\t\t\tcomponentCount,\n\t\t\t\t&arrayCount,\n\t\t\t\t(void**)&data2,\n\t\t\t\tvr,\n\t\t\t\t\"compare1\",\n\t\t\t\t\"compare2\",\n\t\t\t\t\"compare3\",\n\t\t\t\t\"compare4\" );\n\t\tBuild( orig, 0, False );\n\t\tBuild( compare, 0, False );\n\n\t\tJournal_PrintDouble( stream, Variable_ValueCompare( orig, compare ) );\n\t\tJournal_PrintBool( stream, Variable_ValueCompareWithinTolerance( orig, compare, tolerance ) );\n\n\t\tMemory_Free( data );\n\t\tMemory_Free( data2 );\n\t}\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tStream*\t\tstream;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_size( MPI_COMM_WORLD, &procCount );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\tstream = Journal_Register( Info_Type, \"test\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tVariable*           orig;\n\t\tVariable*           compare;\n\t\tdouble*             data;\n\t\tdouble*             data2;\n\t\tIndex               arrayCount      = 150;\n\t\tIndex               componentCount  = 4;\n\t\tIndex               index;\n\t\tdouble              amp             = 0.1;\n\t\tdouble              tolerance       = 0.04;\n\t\tVariable_Register*\tvr;\n\n\t\tdata = Memory_Alloc_Array( double, arrayCount * componentCount, \"test\" );\n\t\tdata2 = Memory_Alloc_Array( double, arrayCount * componentCount, \"test\" );\n\t\tfor( index = 0; index < arrayCount * componentCount; index++ ) {\n\t\t\tdata[index] = 1.0f / (arrayCount+2) * (index+1); \n\t\t\tdata2[ index ] = data[ index ] + amp * cos( index );\n\t\t}\t\t\n\t\t\n\t\tvr = Variable_Register_New();\n\t\torig = Variable_NewVector( \n\t\t\t\t\"orig\", \n\t\t\t\tVariable_DataType_Double, \n\t\t\t\tcomponentCount,\n\t\t\t\t&arrayCount,\n\t\t\t\t(void**)&data,\n\t\t\t\tvr,\n\t\t\t\t\"orig1\",\n\t\t\t\t\"orig2\",\n\t\t\t\t\"orig3\",\n\t\t\t\t\"orig4\" );\n\t\tcompare = Variable_NewVector( \n\t\t\t\t\"compare\", \n\t\t\t\tVariable_DataType_Double, \n\t\t\t\tcomponentCount,\n\t\t\t\t&arrayCount,\n\t\t\t\t(void**)&data2,\n\t\t\t\tvr,\n\t\t\t\t\"compare1\",\n\t\t\t\t\"compare2\",\n\t\t\t\t\"compare3\",\n\t\t\t\t\"compare4\" );\n\t\tBuild( orig, 0, False );\n\t\tBuild( compare, 0, False );\n\n\t\tJournal_PrintDouble( stream, Variable_ValueCompare( orig, compare ) );\n\t\tJournal_PrintBool( stream, Variable_ValueCompareWithinTolerance( orig, compare, tolerance ) );\n\n\t\tMemory_Free( data );\n\t\tMemory_Free( data2 );\n\t}\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "CFDEMproject_1237", "code": "int main(int narg, char **arg)\n{\n  \n\n\n\n  if (narg != 3) {\n    printf(\"Syntax: c_driver P in.lammps\\n\");\n    exit(1);\n  }\n\n  int me,nprocs;\n\n  int nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n  }\n\n  int lammps;\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm comm_lammps;\n\n  \n\n\n  FILE *fp;\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n    }\n  }\n\n  \n\n\n  void *ptr;\n  if (lammps == 1) lammps_open(0,NULL,comm_lammps,&ptr);\n\n  int n;\n  char line[1024];\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    if (n == 0) break;\n    if (lammps == 1) lammps_command(ptr,line);\n  }\n\n  \n\n\n  if (lammps == 1) {\n    lammps_command(ptr,\"run 10\");\n\n    int natoms = lammps_get_natoms(ptr);\n    double *x = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(lmp,\"x\",1,3,x);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    lammps_scatter_atoms(lmp,\"x\",1,3,x);\n    free(x);\n\n    lammps_command(ptr,\"run 1\");\n  }\n\n  if (lammps == 1) lammps_close(ptr);\n\n  \n\n\n}", "label": "int main(int narg, char **arg)\n{\n  \n\n\n  MPI_Init(&narg,&arg);\n\n  if (narg != 3) {\n    printf(\"Syntax: c_driver P in.lammps\\n\");\n    exit(1);\n  }\n\n  int me,nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD,&me);\n  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n\n  int nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  int lammps;\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm comm_lammps;\n  MPI_Comm_split(MPI_COMM_WORLD,lammps,0,&comm_lammps);\n\n  \n\n\n  FILE *fp;\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n      MPI_Abort(MPI_COMM_WORLD,1);\n    }\n  }\n\n  \n\n\n  void *ptr;\n  if (lammps == 1) lammps_open(0,NULL,comm_lammps,&ptr);\n\n  int n;\n  char line[1024];\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);\n    if (n == 0) break;\n    MPI_Bcast(line,n,MPI_CHAR,0,MPI_COMM_WORLD);\n    if (lammps == 1) lammps_command(ptr,line);\n  }\n\n  \n\n\n  if (lammps == 1) {\n    lammps_command(ptr,\"run 10\");\n\n    int natoms = lammps_get_natoms(ptr);\n    double *x = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(lmp,\"x\",1,3,x);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    lammps_scatter_atoms(lmp,\"x\",1,3,x);\n    free(x);\n\n    lammps_command(ptr,\"run 1\");\n  }\n\n  if (lammps == 1) lammps_close(ptr);\n\n  \n\n\n  MPI_Finalize();\n}"}
{"program": "CFDEMproject_1238", "code": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem;\n  UserData data;\n  int iout, thispe, ier, npes;\n  long int Neq, local_N;\n  realtype rtol, atol, t0, t1, tout, tret;\n  N_Vector uu, up, constraints, id, res;\n\n  mem = NULL;\n  data = NULL;\n  uu = up = constraints = id = res = NULL;\n\n  \n\n\n  comm = MPI_COMM_WORLD;\n  \n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\", \n              npes,NPEX*NPEY);\n    return(1);\n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB;\n  Neq     = MX * MY;\n  \n  \n\n\n  data = (UserData) malloc(sizeof *data);\n  if(check_flag((void *)data, \"malloc\", 2, thispe)) \n  data->pp = NULL;\n\n  uu = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)uu, \"N_VNew_Parallel\", 0, thispe)) \n\n  up = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)up, \"N_VNew_Parallel\", 0, thispe)) \n\n  res = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)res, \"N_VNew_Parallel\", 0, thispe)) \n\n  constraints = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)constraints, \"N_VNew_Parallel\", 0, thispe)) \n\n  id = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)id, \"N_VNew_Parallel\", 0, thispe)) \n\n  \n\n  data->pp = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)data->pp, \"N_VNew_Parallel\", 0, thispe)) \n\n  InitUserData(thispe, comm, data);\n  \n  \n\n\n  SetInitialProfile(uu, up, id, res, data);\n  \n  \n\n\n  N_VConst(ONE, constraints);\n  \n  t0 = ZERO; t1 = RCONST(0.01);\n  \n  \n\n\n  rtol = ZERO;\n  atol = RCONST(1.0e-3);\n\n  \n\n\n  mem = IDACreate();\n\n  ier = IDASetUserData(mem, data);\n\n  ier = IDASetSuppressAlg(mem, TRUE);\n\n  ier = IDASetId(mem, id);\n\n  ier = IDASetConstraints(mem, constraints);\n  N_VDestroy_Parallel(constraints);  \n\n  ier = IDAInit(mem, resHeat, t0, uu, up);\n  \n  ier = IDASStolerances(mem, rtol, atol);\n\n  \n\n\n  ier = IDASpgmr(mem, 0);\n\n  ier = IDASpilsSetPreconditioner(mem, PsetupHeat, PsolveHeat);\n\n  \n\n  \n  if (thispe == 0) PrintHeader(Neq, rtol, atol);\n  PrintOutput(thispe, mem, t0, uu); \n  \n  \n\n\n  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) {\n\n    ier = IDASolve(mem, tout, &tret, uu, up, IDA_NORMAL);\n\n    PrintOutput(thispe, mem, tret, uu);\n\n  }\n  \n  \n\n\n  if (thispe == 0) PrintFinalStats(mem);\n\n  \n\n\n  IDAFree(&mem);\n\n  N_VDestroy_Parallel(id);\n  N_VDestroy_Parallel(res);\n  N_VDestroy_Parallel(up);\n  N_VDestroy_Parallel(uu);\n\n  N_VDestroy_Parallel(data->pp);\n  free(data);\n\n\n  return(0);\n\n}", "label": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem;\n  UserData data;\n  int iout, thispe, ier, npes;\n  long int Neq, local_N;\n  realtype rtol, atol, t0, t1, tout, tret;\n  N_Vector uu, up, constraints, id, res;\n\n  mem = NULL;\n  data = NULL;\n  uu = up = constraints = id = res = NULL;\n\n  \n\n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &thispe);\n  \n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\", \n              npes,NPEX*NPEY);\n    MPI_Finalize();\n    return(1);\n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB;\n  Neq     = MX * MY;\n  \n  \n\n\n  data = (UserData) malloc(sizeof *data);\n  if(check_flag((void *)data, \"malloc\", 2, thispe)) \n    MPI_Abort(comm, 1);\n  data->pp = NULL;\n\n  uu = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)uu, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  up = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)up, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  res = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)res, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  constraints = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)constraints, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  id = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)id, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  \n\n  data->pp = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)data->pp, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  InitUserData(thispe, comm, data);\n  \n  \n\n\n  SetInitialProfile(uu, up, id, res, data);\n  \n  \n\n\n  N_VConst(ONE, constraints);\n  \n  t0 = ZERO; t1 = RCONST(0.01);\n  \n  \n\n\n  rtol = ZERO;\n  atol = RCONST(1.0e-3);\n\n  \n\n\n  mem = IDACreate();\n  if(check_flag((void *)mem, \"IDACreate\", 0, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetUserData(mem, data);\n  if(check_flag(&ier, \"IDASetUserData\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetSuppressAlg(mem, TRUE);\n  if(check_flag(&ier, \"IDASetSuppressAlg\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetId(mem, id);\n  if(check_flag(&ier, \"IDASetId\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetConstraints(mem, constraints);\n  if(check_flag(&ier, \"IDASetConstraints\", 1, thispe)) MPI_Abort(comm, 1);\n  N_VDestroy_Parallel(constraints);  \n\n  ier = IDAInit(mem, resHeat, t0, uu, up);\n  if(check_flag(&ier, \"IDAInit\", 1, thispe)) MPI_Abort(comm, 1);\n  \n  ier = IDASStolerances(mem, rtol, atol);\n  if(check_flag(&ier, \"IDASStolerances\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n\n  ier = IDASpgmr(mem, 0);\n  if(check_flag(&ier, \"IDASpgmr\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASpilsSetPreconditioner(mem, PsetupHeat, PsolveHeat);\n  if(check_flag(&ier, \"IDASpilsSetPreconditioner\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n  \n  if (thispe == 0) PrintHeader(Neq, rtol, atol);\n  PrintOutput(thispe, mem, t0, uu); \n  \n  \n\n\n  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) {\n\n    ier = IDASolve(mem, tout, &tret, uu, up, IDA_NORMAL);\n    if(check_flag(&ier, \"IDASolve\", 1, thispe)) MPI_Abort(comm, 1);\n\n    PrintOutput(thispe, mem, tret, uu);\n\n  }\n  \n  \n\n\n  if (thispe == 0) PrintFinalStats(mem);\n\n  \n\n\n  IDAFree(&mem);\n\n  N_VDestroy_Parallel(id);\n  N_VDestroy_Parallel(res);\n  N_VDestroy_Parallel(up);\n  N_VDestroy_Parallel(uu);\n\n  N_VDestroy_Parallel(data->pp);\n  free(data);\n\n  MPI_Finalize();\n\n  return(0);\n\n}"}
{"program": "CFDEMproject_1241", "code": "int main(int narg, char **arg)\n{\n  \n\n\n\n  if (narg != 3) {\n    printf(\"Syntax: c_driver P in.lammps\\n\");\n    exit(1);\n  }\n\n  int me,nprocs;\n\n  int nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n  }\n\n  int lammps;\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm comm_lammps;\n\n  \n\n\n  FILE *fp;\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n    }\n  }\n\n  \n\n\n  void *ptr;\n  if (lammps == 1) lammps_open(0,NULL,comm_lammps,&ptr);\n\n  int n;\n  char line[1024];\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    if (n == 0) break;\n    if (lammps == 1) lammps_command(ptr,line);\n  }\n\n  \n\n\n  if (lammps == 1) {\n    lammps_command(ptr,\"run 10\");\n\n    int natoms = lammps_get_natoms(ptr);\n    double *x = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(ptr,\"x\",1,3,x);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    lammps_scatter_atoms(ptr,\"x\",1,3,x);\n    free(x);\n\n    lammps_command(ptr,\"run 1\");\n  }\n\n  if (lammps == 1) lammps_close(ptr);\n\n  \n\n\n}", "label": "int main(int narg, char **arg)\n{\n  \n\n\n  MPI_Init(&narg,&arg);\n\n  if (narg != 3) {\n    printf(\"Syntax: c_driver P in.lammps\\n\");\n    exit(1);\n  }\n\n  int me,nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD,&me);\n  MPI_Comm_size(MPI_COMM_WORLD,&nprocs);\n\n  int nprocs_lammps = atoi(arg[1]);\n  if (nprocs_lammps > nprocs) {\n    if (me == 0)\n      printf(\"ERROR: LAMMPS cannot use more procs than available\\n\");\n    MPI_Abort(MPI_COMM_WORLD,1);\n  }\n\n  int lammps;\n  if (me < nprocs_lammps) lammps = 1;\n  else lammps = MPI_UNDEFINED;\n  MPI_Comm comm_lammps;\n  MPI_Comm_split(MPI_COMM_WORLD,lammps,0,&comm_lammps);\n\n  \n\n\n  FILE *fp;\n  if (me == 0) {\n    fp = fopen(arg[2],\"r\");\n    if (fp == NULL) {\n      printf(\"ERROR: Could not open LAMMPS input script\\n\");\n      MPI_Abort(MPI_COMM_WORLD,1);\n    }\n  }\n\n  \n\n\n  void *ptr;\n  if (lammps == 1) lammps_open(0,NULL,comm_lammps,&ptr);\n\n  int n;\n  char line[1024];\n  while (1) {\n    if (me == 0) {\n      if (fgets(line,1024,fp) == NULL) n = 0;\n      else n = strlen(line) + 1;\n      if (n == 0) fclose(fp);\n    }\n    MPI_Bcast(&n,1,MPI_INT,0,MPI_COMM_WORLD);\n    if (n == 0) break;\n    MPI_Bcast(line,n,MPI_CHAR,0,MPI_COMM_WORLD);\n    if (lammps == 1) lammps_command(ptr,line);\n  }\n\n  \n\n\n  if (lammps == 1) {\n    lammps_command(ptr,\"run 10\");\n\n    int natoms = lammps_get_natoms(ptr);\n    double *x = (double *) malloc(3*natoms*sizeof(double));\n    lammps_gather_atoms(ptr,\"x\",1,3,x);\n    double epsilon = 0.1;\n    x[0] += epsilon;\n    lammps_scatter_atoms(ptr,\"x\",1,3,x);\n    free(x);\n\n    lammps_command(ptr,\"run 1\");\n  }\n\n  if (lammps == 1) lammps_close(ptr);\n\n  \n\n\n  MPI_Finalize();\n}"}
{"program": "joao-lima_1242", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tstarpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(token));\n\n\tint nloops = NITER;\n\tint loop;\n\n\tint last_loop = nloops - 1;\n\tint last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (loop == 0 && rank == 0)\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tFPRINTF(stdout, \"Start with token value %u\\n\", token);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(token_handle, (rank+size-1)%size, tag, MPI_COMM_WORLD, &status);\n\t\t}\n\n\t\tincrement_token();\n\n\t\tif (loop == last_loop && rank == last_rank)\n\t\t{\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tFPRINTF(stdout, \"Finished : token value %u\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstarpu_mpi_req req;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_issend(token_handle, &req, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\n\tstarpu_data_unregister(token_handle);\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tstarpu_vector_data_register(&token_handle, 0, (uintptr_t)&token, 1, sizeof(token));\n\n\tint nloops = NITER;\n\tint loop;\n\n\tint last_loop = nloops - 1;\n\tint last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (loop == 0 && rank == 0)\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tFPRINTF(stdout, \"Start with token value %u\\n\", token);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_recv(token_handle, (rank+size-1)%size, tag, MPI_COMM_WORLD, &status);\n\t\t}\n\n\t\tincrement_token();\n\n\t\tif (loop == last_loop && rank == last_rank)\n\t\t{\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tFPRINTF(stdout, \"Finished : token value %u\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstarpu_mpi_req req;\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_issend(token_handle, &req, (rank+1)%size, tag+1, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\n\tstarpu_data_unregister(token_handle);\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\tif (rank == last_rank)\n\t{\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n"}
{"program": "xyuan_1243", "code": "int\nmain (int argc, char **argv)\n{\n  MPI_Comm            mpicomm;\n  int                 mpiret, retval;\n  int                 level;\n  const char         *filename;\n  p4est_connectivity_t *conn;\n  sc_options_t       *opt;\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n\n  sc_init (MPI_COMM_WORLD, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  opt = sc_options_new (argv[0]);\n  sc_options_add_int (opt, 'l', \"level\", &level, 0,\n                      \"Upfront refinement level\");\n  retval = sc_options_parse (p4est_package_id, SC_LP_ERROR, opt, argc, argv);\n  if (retval == -1 || retval + 1 != argc) {\n    sc_options_print_usage (p4est_package_id, SC_LP_PRODUCTION, opt, NULL);\n    sc_abort_collective (\"Usage error\");\n  }\n  filename = argv[retval];\n  P4EST_LDEBUGF (\"Loading %s\\n\", filename);\n  conn = p4est_connectivity_load (filename, NULL);\n\n  run_load (mpicomm, conn, level);\n\n  p4est_connectivity_destroy (conn);\n  sc_options_destroy (opt);\n\n  sc_finalize ();\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  MPI_Comm            mpicomm;\n  int                 mpiret, retval;\n  int                 level;\n  const char         *filename;\n  p4est_connectivity_t *conn;\n  sc_options_t       *opt;\n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n\n  sc_init (MPI_COMM_WORLD, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  opt = sc_options_new (argv[0]);\n  sc_options_add_int (opt, 'l', \"level\", &level, 0,\n                      \"Upfront refinement level\");\n  retval = sc_options_parse (p4est_package_id, SC_LP_ERROR, opt, argc, argv);\n  if (retval == -1 || retval + 1 != argc) {\n    sc_options_print_usage (p4est_package_id, SC_LP_PRODUCTION, opt, NULL);\n    sc_abort_collective (\"Usage error\");\n  }\n  filename = argv[retval];\n  P4EST_LDEBUGF (\"Loading %s\\n\", filename);\n  conn = p4est_connectivity_load (filename, NULL);\n\n  run_load (mpicomm, conn, level);\n\n  p4est_connectivity_destroy (conn);\n  sc_options_destroy (opt);\n\n  sc_finalize ();\n\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "scafacos_1244", "code": "int\nmain(int argc, char **argv, char **envp)\n{\nint i,ret,*npes;\nint num_interfaces;\nptl_handle_ni_t nih;\nptl_handle_eq_t eqh;\nptl_ni_limits_t ptl_limits;\npid_t child;\nptl_process_id_t rnk,*procid_map;\nint spv,*spawned=&spv;\n\n\n    if ((ret=PtlInit(&num_interfaces)) != PTL_OK) {\n      printf(\"%s: PtlInit failed: %d\\n\", FUNCTION_NAME, ret);\n      exit(1);\n    }\n    printf(\"%s: PtlInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n#ifdef FORK_BEFORE_NI_INIT\n    child = fork();\n#endif\n\n    if((ret=PtlNIInit(IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n                                    PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n      printf(\"%s: PtlNIInit failed: %d\\n\", FUNCTION_NAME, ret);\n      \n\n    }\n    else\n      printf(\"%s: PtlNIInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n#ifdef FORK_AFTER_NI_INIT\n    child = fork();\n#endif\n\n    if ((ret=PtlEQAlloc(nih, 4096, NULL, &eqh)) != PTL_OK) {\n      printf(\"%s: PtlEQAlloc failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n      exit(1);\n    }\n    printf(\"%s: PtlEQAlloc succeeds (%d:%d)\\n\", FUNCTION_NAME, child, ret);\n\n#if 1\n    if(child){\n    }\n \n    if(child){\n      PMI_Init(spawned);\n      printf(\"\\n%d:spanwned=%d\",child,*spawned);\n      if ((ret=PMI_Get_size(npes)) != PMI_SUCCESS) {\n        printf(\"%s: PMI_Get_size failed: %d\\n\", FUNCTION_NAME, ret);\n        \n\n      }\n      else\n        printf(\"%s: PMI_Get_size succeeds (%d)\\n\", FUNCTION_NAME,*npes);\n      \n\n      if((ret = PMI_CNOS_Get_nidpid_map(&procid_map)) != PMI_SUCCESS) {\n        printf(\"Getting proc map failed (npes=%d)\\n\", *npes);\n      }\n      for(i=0;i<*npes;i++){\n        printf(\"\\npid=%d nid=%d npes=%d(%d)\",procid_map[i].pid,procid_map[i].nid,*npes,child);\n      }\n    }\n#endif\n\n    if((ret=PtlGetId(nih,&rnk)) !=PTL_OK) {\n      printf(\"%s: PtlGetId failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n      exit(1);\n    }\n    printf(\"%s: nid=%d pid=%d(%d)\\n\",FUNCTION_NAME,rnk.nid,rnk.pid,child);\n    if(child){\n      printf(\"%s: mpi_init and finalize succeed(%d)\\n\",FUNCTION_NAME,child);\n    }\n         \n}", "label": "int\nmain(int argc, char **argv, char **envp)\n{\nint i,ret,*npes;\nint num_interfaces;\nptl_handle_ni_t nih;\nptl_handle_eq_t eqh;\nptl_ni_limits_t ptl_limits;\npid_t child;\nptl_process_id_t rnk,*procid_map;\nint spv,*spawned=&spv;\n\n\n    if ((ret=PtlInit(&num_interfaces)) != PTL_OK) {\n      printf(\"%s: PtlInit failed: %d\\n\", FUNCTION_NAME, ret);\n      exit(1);\n    }\n    printf(\"%s: PtlInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n#ifdef FORK_BEFORE_NI_INIT\n    child = fork();\n#endif\n\n    if((ret=PtlNIInit(IFACE_FROM_BRIDGE_AND_NALID(PTL_BRIDGE_UK, PTL_IFACE_SS),\n                                    PTL_PID_ANY, NULL, &ptl_limits, &nih)) != PTL_OK) {\n      printf(\"%s: PtlNIInit failed: %d\\n\", FUNCTION_NAME, ret);\n      \n\n    }\n    else\n      printf(\"%s: PtlNIInit succeeds (%d)\\n\", FUNCTION_NAME, ret);\n\n#ifdef FORK_AFTER_NI_INIT\n    child = fork();\n#endif\n\n    if ((ret=PtlEQAlloc(nih, 4096, NULL, &eqh)) != PTL_OK) {\n      printf(\"%s: PtlEQAlloc failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n      exit(1);\n    }\n    printf(\"%s: PtlEQAlloc succeeds (%d:%d)\\n\", FUNCTION_NAME, child, ret);\n\n#if 1\n    if(child){\n      MPI_Init(&argc,&argv);\n    }\n \n    if(child){\n      PMI_Init(spawned);\n      printf(\"\\n%d:spanwned=%d\",child,*spawned);\n      if ((ret=PMI_Get_size(npes)) != PMI_SUCCESS) {\n        printf(\"%s: PMI_Get_size failed: %d\\n\", FUNCTION_NAME, ret);\n        \n\n      }\n      else\n        printf(\"%s: PMI_Get_size succeeds (%d)\\n\", FUNCTION_NAME,*npes);\n      \n\n      if((ret = PMI_CNOS_Get_nidpid_map(&procid_map)) != PMI_SUCCESS) {\n        printf(\"Getting proc map failed (npes=%d)\\n\", *npes);\n      }\n      for(i=0;i<*npes;i++){\n        printf(\"\\npid=%d nid=%d npes=%d(%d)\",procid_map[i].pid,procid_map[i].nid,*npes,child);\n      }\n    }\n#endif\n\n    if((ret=PtlGetId(nih,&rnk)) !=PTL_OK) {\n      printf(\"%s: PtlGetId failed: %d(%d)\\n\",\n                         FUNCTION_NAME, ret, child);\n      exit(1);\n    }\n    printf(\"%s: nid=%d pid=%d(%d)\\n\",FUNCTION_NAME,rnk.nid,rnk.pid,child);\n    if(child){\n      MPI_Finalize();\n      printf(\"%s: mpi_init and finalize succeed(%d)\\n\",FUNCTION_NAME,child);\n    }\n         \n}"}
{"program": "nerscadmin_1245", "code": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double t1, t2;\n  \n  int req, prov;\n  req = MPI_THREAD_SINGLE;\n\n\n  PMPI_Type_size(DATATYPE, &dsize);\n  \n  buf=(char*)malloc(SIZE*dsize);\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds with IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  \n\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      PMPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds without IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n\n  fopen(\"/dev/null\", \"r\");  \n  return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double t1, t2;\n  \n  int req, prov;\n  req = MPI_THREAD_SINGLE;\n\n  MPI_Init( &argc, &argv);\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  PMPI_Type_size(DATATYPE, &dsize);\n  \n  buf=(char*)malloc(SIZE*dsize);\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds with IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  \n\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      PMPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds without IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  MPI_Finalize();\n\n  fopen(\"/dev/null\", \"r\");  \n  return 0;\n}"}
{"program": "daidong_1247", "code": "int main(int argc, char **argv) \n{\n    int nprocs;\n    int mynod;\n    int i;\n    int ret;\n    int fd;\n    int iters;\n    double time1, time2, write1, write2, host1, host2;\n    char onechar;\n    char host[256];\n\n\n    \n\n    if(nprocs > 1)\n    {\n        if(mynod == 0)\n        {\n            fprintf(stderr, \"Error: this benchmark should be run with exactly one process.\\n\");\n        }\n        return(-1);\n    }\n\n    \n\n    if(argc != 2)\n    {\n        fprintf(stderr, \"Usage: %s <number of iterations>\\n\", argv[0]);\n        return(-1);\n    }\n\n    ret = sscanf(argv[1], \"%d\", &iters);\n    if(ret != 1)\n    {\n        fprintf(stderr, \"Usage: %s <number of iterations>\\n\", argv[0]);\n        return(-1);\n    }\n\n    \n\n    time1 =\n    for(i=0; i<iters; i++)\n    {\n        write1 =\n    }\n    time2 =\n\n    sleep(1);\n\n    \n\n    fd = open(\"/dev/null\", O_RDWR);\n    if(fd < 0)\n    {\n        perror(\"open\");\n        return(-1);\n    }\n\n    \n\n    write1 =\n    for(i=0; i<iters; i++)\n    {\n        ret = write(fd, &onechar, 1);\n        if(ret < 0)\n        {\n            perror(\"write\");\n            return(-1);\n        }\n    }\n    write2 =\n\n    close(fd);\n \n    sleep(1);\n \n    \n\n    host1 =\n    for(i=0; i<iters; i++)\n    {\n        ret = gethostname(host, 256);\n        if(ret < 0)\n        {\n            perror(\"gethostname\");\n            return(-1);\n        }\n    }\n    host2 =\n\n\n    \n\n    printf(\"#<op>\\t<iters>\\t<total (s)>\\t<per op (s)>\\n\");\n    printf(\"wtime\\t%d\\t%.9f\\t%.9f\\n\", iters, time2-time1, (time2-time1)/(double)iters);\n    printf(\"write\\t%d\\t%.9f\\t%.9f\\n\", iters, write2-write1, (write2-write1)/(double)iters);\n    printf(\"gethostname\\t%d\\t%.9f\\t%.9f\\n\", iters, host2-host1, (host2-host1)/(double)iters);\n\n    return(0);\n}", "label": "int main(int argc, char **argv) \n{\n    int nprocs;\n    int mynod;\n    int i;\n    int ret;\n    int fd;\n    int iters;\n    double time1, time2, write1, write2, host1, host2;\n    char onechar;\n    char host[256];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    \n\n    if(nprocs > 1)\n    {\n        if(mynod == 0)\n        {\n            fprintf(stderr, \"Error: this benchmark should be run with exactly one process.\\n\");\n        }\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    if(argc != 2)\n    {\n        fprintf(stderr, \"Usage: %s <number of iterations>\\n\", argv[0]);\n        MPI_Finalize();\n        return(-1);\n    }\n\n    ret = sscanf(argv[1], \"%d\", &iters);\n    if(ret != 1)\n    {\n        fprintf(stderr, \"Usage: %s <number of iterations>\\n\", argv[0]);\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    time1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        write1 = MPI_Wtime();\n    }\n    time2 = MPI_Wtime();\n\n    sleep(1);\n\n    \n\n    fd = open(\"/dev/null\", O_RDWR);\n    if(fd < 0)\n    {\n        perror(\"open\");\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    write1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        ret = write(fd, &onechar, 1);\n        if(ret < 0)\n        {\n            perror(\"write\");\n            MPI_Finalize();\n            return(-1);\n        }\n    }\n    write2 = MPI_Wtime();\n\n    close(fd);\n \n    sleep(1);\n \n    \n\n    host1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        ret = gethostname(host, 256);\n        if(ret < 0)\n        {\n            perror(\"gethostname\");\n            MPI_Finalize();\n            return(-1);\n        }\n    }\n    host2 = MPI_Wtime();\n\n\n    \n\n    printf(\"#<op>\\t<iters>\\t<total (s)>\\t<per op (s)>\\n\");\n    printf(\"wtime\\t%d\\t%.9f\\t%.9f\\n\", iters, time2-time1, (time2-time1)/(double)iters);\n    printf(\"write\\t%d\\t%.9f\\t%.9f\\n\", iters, write2-write1, (write2-write1)/(double)iters);\n    printf(\"gethostname\\t%d\\t%.9f\\t%.9f\\n\", iters, host2-host1, (host2-host1)/(double)iters);\n\n    MPI_Finalize();\n    return(0);\n}"}
{"program": "joeladams_1248", "code": "int main(int argc, char** argv) {\n    const int REPS = 8;                      \n\n    int id = -1, numProcesses = -1;\n\n\n    \n\n    \n\n    \n\n    if ((REPS % numProcesses) == 0 && numProcesses <= REPS) {\n\n      int chunkSize = REPS / numProcesses;      \n\n      int start = id * chunkSize;               \n\n      int stop = start + chunkSize;             \n\n\n      for (int i = start; i < stop; i++) {      \n\n          printf(\"Process %d is performing iteration %d\\n\", id, i);\n      }\n\n    } else {\n      if (id == 0) {\n          printf(\"Please run with -np divisible by and less than or equal to %d\\n.\", REPS);\n      }\n    }\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    const int REPS = 8;                      \n\n    int id = -1, numProcesses = -1;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    \n\n    \n\n    \n\n    if ((REPS % numProcesses) == 0 && numProcesses <= REPS) {\n\n      int chunkSize = REPS / numProcesses;      \n\n      int start = id * chunkSize;               \n\n      int stop = start + chunkSize;             \n\n\n      for (int i = start; i < stop; i++) {      \n\n          printf(\"Process %d is performing iteration %d\\n\", id, i);\n      }\n\n    } else {\n      if (id == 0) {\n          printf(\"Please run with -np divisible by and less than or equal to %d\\n.\", REPS);\n      }\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "gnu3ra_1249", "code": "int main( int argc, char *argv[] )\n{\n    int rank, size;\n    int chunk = 128;\n    int i;\n    int *sb;\n    int *rb;\n    int status, gstatus;\n\n\n    for ( i=1 ; i < argc ; ++i ) {\n\tif ( argv[i][0] != '-' )\n\t    continue;\n\tswitch(argv[i][1]) {\n\tcase 'm':\n\t    chunk = atoi(argv[++i]);\n\t    break;\n\tdefault:\n\t    fprintf(stderr,\"Unrecognized argument %s\\n\",\n\t\t    argv[i]);\n\t}\n    }\n\n    sb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !sb ) {\n\tperror( \"can't allocate send buffer\" );\n    }\n    rb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !rb ) {\n\tperror( \"can't allocate recv buffer\");\n\tfree(sb);\n    }\n    for ( i=0 ; i < size*chunk ; ++i ) {\n\tsb[i] = rank + 1;\n\trb[i] = 0;\n    }\n\n    \n\n\n    \n\n    status =\n\n    \n\n\n    \n\n    if (rank == 0) {\n\tif (gstatus == 0) printf( \" No Errors\\n\" );\n\telse \n\t    printf(\"all_to_all returned %d\\n\",gstatus);\n    }\n\n    free(sb);\n    free(rb);\n\n\n    return(EXIT_SUCCESS);\n}", "label": "int main( int argc, char *argv[] )\n{\n    int rank, size;\n    int chunk = 128;\n    int i;\n    int *sb;\n    int *rb;\n    int status, gstatus;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    MPI_Comm_size(MPI_COMM_WORLD,&size);\n\n    for ( i=1 ; i < argc ; ++i ) {\n\tif ( argv[i][0] != '-' )\n\t    continue;\n\tswitch(argv[i][1]) {\n\tcase 'm':\n\t    chunk = atoi(argv[++i]);\n\t    break;\n\tdefault:\n\t    fprintf(stderr,\"Unrecognized argument %s\\n\",\n\t\t    argv[i]);\n\t    MPI_Abort(MPI_COMM_WORLD,EXIT_FAILURE);\n\t}\n    }\n\n    sb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !sb ) {\n\tperror( \"can't allocate send buffer\" );\n\tMPI_Abort(MPI_COMM_WORLD,EXIT_FAILURE);\n    }\n    rb = (int *)malloc(size*chunk*sizeof(int));\n    if ( !rb ) {\n\tperror( \"can't allocate recv buffer\");\n\tfree(sb);\n\tMPI_Abort(MPI_COMM_WORLD,EXIT_FAILURE);\n    }\n    for ( i=0 ; i < size*chunk ; ++i ) {\n\tsb[i] = rank + 1;\n\trb[i] = 0;\n    }\n\n    \n\n\n    \n\n    status = MPI_Alltoall(sb,chunk,MPI_INT,rb,chunk,MPI_INT,\n\t\t\t  MPI_COMM_WORLD);\n\n    \n\n    MPI_Allreduce( &status, &gstatus, 1, MPI_INT, MPI_SUM, \n\t\t   MPI_COMM_WORLD );\n\n    \n\n    if (rank == 0) {\n\tif (gstatus == 0) printf( \" No Errors\\n\" );\n\telse \n\t    printf(\"all_to_all returned %d\\n\",gstatus);\n    }\n\n    free(sb);\n    free(rb);\n\n    MPI_Finalize();\n\n    return(EXIT_SUCCESS);\n}"}
{"program": "bjoern-leder_1250", "code": "int main(int argc,char *argv[])\r\n{\r\n   int nc,iend;\r\n   double wt1,wt2,wtavg;\r\n\r\n\r\n   read_infile(argc,argv);\r\n   alloc_data();\r\n   check_files();\r\n   print_info();\r\n\r\n   geometry();\r\n\r\n   iend=0;\r\n   wtavg=0.0;\r\n\r\n   for (nc=first;(iend==0)&&(nc<=last);nc+=step)\r\n   {\r\n\r\n      if (my_rank==0)\r\n         printf(\"Configuration no %d\\n\",nc);\r\n\r\n      if (noexp)\r\n      {\r\n         sprintf(cnfg_file,\"%s/%sn%d_%d\",loc_dir,nbase,nc,my_rank);\r\n         read_cnfg(cnfg_file);\r\n      }\r\n      else\r\n      {\r\n         sprintf(cnfg_file,\"%s/%sn%d\",cnfg_dir,nbase,nc);\r\n         import_cnfg(cnfg_file);\r\n      }\r\n\r\n      set_data(nc);\r\n      save_data();\r\n      \n\r\n\r\n      wtavg+=(wt2-wt1);\r\n      error_chk();\r\n\r\n      if (my_rank==0)\r\n      {\r\n         printf(\"Configuration no %d fully processed in %.2e sec \",\r\n                nc,wt2-wt1);\r\n         printf(\"(average = %.2e sec)\\n\\n\",\r\n                wtavg/(double)((nc-first)/step+1));\r\n         fflush(flog);\r\n\r\n         copy_file(log_file,log_save);\r\n         copy_file(dat_file,dat_save);\r\n      }\r\n\r\n      check_endflag(&iend);\r\n   }\r\n\r\n   error_chk();\r\n\r\n   if (my_rank==0)\r\n   {\r\n      fflush(flog);\r\n      copy_file(log_file,log_save);\r\n      fclose(flog);\r\n   }\r\n\r\n   exit(0);\r\n}", "label": "int main(int argc,char *argv[])\r\n{\r\n   int nc,iend;\r\n   double wt1,wt2,wtavg;\r\n\r\n   MPI_Init(&argc,&argv);\r\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\r\n\r\n   read_infile(argc,argv);\r\n   alloc_data();\r\n   check_files();\r\n   print_info();\r\n\r\n   geometry();\r\n\r\n   iend=0;\r\n   wtavg=0.0;\r\n\r\n   for (nc=first;(iend==0)&&(nc<=last);nc+=step)\r\n   {\r\n      MPI_Barrier(MPI_COMM_WORLD);\r\n      wt1=MPI_Wtime();\r\n\r\n      if (my_rank==0)\r\n         printf(\"Configuration no %d\\n\",nc);\r\n\r\n      if (noexp)\r\n      {\r\n         sprintf(cnfg_file,\"%s/%sn%d_%d\",loc_dir,nbase,nc,my_rank);\r\n         read_cnfg(cnfg_file);\r\n      }\r\n      else\r\n      {\r\n         sprintf(cnfg_file,\"%s/%sn%d\",cnfg_dir,nbase,nc);\r\n         import_cnfg(cnfg_file);\r\n      }\r\n\r\n      set_data(nc);\r\n      save_data();\r\n      \n\r\n\r\n      MPI_Barrier(MPI_COMM_WORLD);\r\n      wt2=MPI_Wtime();\r\n      wtavg+=(wt2-wt1);\r\n      error_chk();\r\n\r\n      if (my_rank==0)\r\n      {\r\n         printf(\"Configuration no %d fully processed in %.2e sec \",\r\n                nc,wt2-wt1);\r\n         printf(\"(average = %.2e sec)\\n\\n\",\r\n                wtavg/(double)((nc-first)/step+1));\r\n         fflush(flog);\r\n\r\n         copy_file(log_file,log_save);\r\n         copy_file(dat_file,dat_save);\r\n      }\r\n\r\n      check_endflag(&iend);\r\n   }\r\n\r\n   error_chk();\r\n\r\n   if (my_rank==0)\r\n   {\r\n      fflush(flog);\r\n      copy_file(log_file,log_save);\r\n      fclose(flog);\r\n   }\r\n\r\n   MPI_Finalize();\r\n   exit(0);\r\n}"}
{"program": "amabdelrehim_1251", "code": "int main(int argc,char *argv[]) {\n\n  double plaquette_energy;\n  paramsXlfInfo *xlfInfo;\n  \n\n#ifdef MPI\n  \n#endif\n  g_rgi_C1 = 1.; \n  \n  \n\n  read_input(\"benchmark.input\");\n  \n  tmlqcd_mpi_init(argc, argv);\n  \n  \n#ifdef _GAUGE_COPY\n  init_gauge_field(VOLUMEPLUSRAND + g_dbw2rand, 1);\n#else\n  init_gauge_field(VOLUMEPLUSRAND + g_dbw2rand, 0);\n#endif\n  init_geometry_indices(VOLUMEPLUSRAND + g_dbw2rand);\n\n  if(g_proc_id == 0) {\n    fprintf(stdout,\"The number of processes is %d \\n\",g_nproc);\n    printf(\"# The lattice size is %d x %d x %d x %d\\n\",\n\t   (int)(T*g_nproc_t), (int)(LX*g_nproc_x), (int)(LY*g_nproc_y), (int)(g_nproc_z*LZ));\n    printf(\"# The local lattice size is %d x %d x %d x %d\\n\", \n\t   (int)(T), (int)(LX), (int)(LY),(int) LZ);\n    printf(\"# Testing IO routines for gauge-fields\\n\");\n    fflush(stdout);\n  }\n  \n  \n\n  geometry();\n  \n\n  boundary(g_kappa);\n\n  \n\n  start_ranlux(1, 123456);\n  random_gauge_field(reproduce_randomnumber_flag, g_gauge_field);\n\n#ifdef MPI\n  \n\n  xchange_gauge(g_gauge_field);\n#endif\n\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n\n  if(g_proc_id == 0) {\n    printf(\"# the first plaquette value is %e\\n\", plaquette_energy);\n    printf(\"# writing with lime first to conf.lime\\n\");\n  }\n\n  \n\n  xlfInfo = construct_paramsXlfInfo(plaquette_energy, 0);\n  write_lime_gauge_field( \"conf.lime\", 64, xlfInfo);\n\n#ifdef HAVE_LIBLEMON\n  if(g_proc_id == 0) {\n    printf(\"Now we do write with lemon to conf.lemon...\\n\");\n  }\n  write_lemon_gauge_field_parallel( \"conf.lemon\", 64, xlfInfo);\n\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lemon from conf.lime\\n\");\n  }\n  read_lemon_gauge_field_parallel(\"conf.lime\", NULL, NULL, NULL);\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lemon read of conf.lime is %e\\n\", plaquette_energy);\n  }\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lemon from conf.lemon\\n\");\n  }\n  read_lemon_gauge_field_parallel(\"conf.lemon\", NULL, NULL, NULL);\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lemon read of conf.lemon is %e\\n\", plaquette_energy);\n  }\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lime from conf.lemon\\n\");\n  }\n  read_lime_gauge_field(\"conf.lemon\");\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lime read of conf.lemon is %e\\n\", plaquette_energy);\n  }\n\n  free(xlfInfo);\n  if(g_proc_id==0) {\n    printf(\"done ...\\n\");\n  }\n#endif\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lime from conf.lime\\n\");\n  }\n  read_lime_gauge_field(\"conf.lime\", NULL, NULL, NULL);\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lime read of conf.lime is %e\\n\", plaquette_energy);\n  }\n\n\n#ifdef MPI\n#endif\n  free_gauge_field();\n  free_geometry_indices();\n  return(0);\n}", "label": "int main(int argc,char *argv[]) {\n\n  double plaquette_energy;\n  paramsXlfInfo *xlfInfo;\n  \n\n#ifdef MPI\n  \n  MPI_Init(&argc, &argv);\n#endif\n  g_rgi_C1 = 1.; \n  \n  \n\n  read_input(\"benchmark.input\");\n  \n  tmlqcd_mpi_init(argc, argv);\n  \n  \n#ifdef _GAUGE_COPY\n  init_gauge_field(VOLUMEPLUSRAND + g_dbw2rand, 1);\n#else\n  init_gauge_field(VOLUMEPLUSRAND + g_dbw2rand, 0);\n#endif\n  init_geometry_indices(VOLUMEPLUSRAND + g_dbw2rand);\n\n  if(g_proc_id == 0) {\n    fprintf(stdout,\"The number of processes is %d \\n\",g_nproc);\n    printf(\"# The lattice size is %d x %d x %d x %d\\n\",\n\t   (int)(T*g_nproc_t), (int)(LX*g_nproc_x), (int)(LY*g_nproc_y), (int)(g_nproc_z*LZ));\n    printf(\"# The local lattice size is %d x %d x %d x %d\\n\", \n\t   (int)(T), (int)(LX), (int)(LY),(int) LZ);\n    printf(\"# Testing IO routines for gauge-fields\\n\");\n    fflush(stdout);\n  }\n  \n  \n\n  geometry();\n  \n\n  boundary(g_kappa);\n\n  \n\n  start_ranlux(1, 123456);\n  random_gauge_field(reproduce_randomnumber_flag, g_gauge_field);\n\n#ifdef MPI\n  \n\n  xchange_gauge(g_gauge_field);\n#endif\n\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n\n  if(g_proc_id == 0) {\n    printf(\"# the first plaquette value is %e\\n\", plaquette_energy);\n    printf(\"# writing with lime first to conf.lime\\n\");\n  }\n\n  \n\n  xlfInfo = construct_paramsXlfInfo(plaquette_energy, 0);\n  write_lime_gauge_field( \"conf.lime\", 64, xlfInfo);\n\n#ifdef HAVE_LIBLEMON\n  if(g_proc_id == 0) {\n    printf(\"Now we do write with lemon to conf.lemon...\\n\");\n  }\n  write_lemon_gauge_field_parallel( \"conf.lemon\", 64, xlfInfo);\n\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lemon from conf.lime\\n\");\n  }\n  read_lemon_gauge_field_parallel(\"conf.lime\", NULL, NULL, NULL);\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lemon read of conf.lime is %e\\n\", plaquette_energy);\n  }\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lemon from conf.lemon\\n\");\n  }\n  read_lemon_gauge_field_parallel(\"conf.lemon\", NULL, NULL, NULL);\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lemon read of conf.lemon is %e\\n\", plaquette_energy);\n  }\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lime from conf.lemon\\n\");\n  }\n  read_lime_gauge_field(\"conf.lemon\");\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lime read of conf.lemon is %e\\n\", plaquette_energy);\n  }\n\n  free(xlfInfo);\n  if(g_proc_id==0) {\n    printf(\"done ...\\n\");\n  }\n#endif\n\n  if(g_proc_id == 0) {\n    printf(\"# now we read with lime from conf.lime\\n\");\n  }\n  read_lime_gauge_field(\"conf.lime\", NULL, NULL, NULL);\n  plaquette_energy = measure_gauge_action(g_gauge_field) / (6.*VOLUME*g_nproc);\n  if(g_proc_id == 0) {\n    printf(\"# the plaquette value after lime read of conf.lime is %e\\n\", plaquette_energy);\n  }\n\n\n#ifdef MPI\n  MPI_Finalize();\n#endif\n  free_gauge_field();\n  free_geometry_indices();\n  return(0);\n}"}
{"program": "bluemner_1252", "code": "int main(int argc, char **argv) {\n  \n  int my_rank; \n  int partner;\n  int size, i,t;\n  char greeting[100];\n  MPI_Status stat;\n  \n\n\n\n  \n  \n  sprintf(greeting, \"Hello world: processor %d of %d\\n\", my_rank, size);\n  \n  \n\n  \n  if (my_rank ==0) {\n    fputs(greeting, stdout);\n    for (partner = 1; partner < size; partner++){\n      \n      fputs (greeting, stdout);\n      \n    }\n  }\n  else {\n  }\n  \n  \n  \n  if (my_rank == 0) printf(\"That is all for now!\\n\");\n\n  \n}", "label": "int main(int argc, char **argv) {\n  \n  int my_rank; \n  int partner;\n  int size, i,t;\n  char greeting[100];\n  MPI_Status stat;\n  \n  MPI_Init(&argc, &argv); \n\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank); \n\n  MPI_Comm_size(MPI_COMM_WORLD, &size); \n\n  \n  \n  sprintf(greeting, \"Hello world: processor %d of %d\\n\", my_rank, size);\n  \n  \n\n  \n  if (my_rank ==0) {\n    fputs(greeting, stdout);\n    for (partner = 1; partner < size; partner++){\n      \n      MPI_Recv(greeting, sizeof(greeting), MPI_BYTE, partner, 1, MPI_COMM_WORLD, &stat);\n      fputs (greeting, stdout);\n      \n    }\n  }\n  else {\n    MPI_Send(greeting, strlen(greeting)+1, MPI_BYTE, 0,1,MPI_COMM_WORLD);\n  }\n  \n  \n  \n  if (my_rank == 0) printf(\"That is all for now!\\n\");\n  MPI_Finalize();  \n\n  \n}"}
{"program": "AleksanderGondek_1253", "code": "int main(int argc, char **argv)\n{\n\tdouble precision = _PRECISION_;\n\tint  my_rank, proc_count;\n\tdouble pi_final;\n\n\t\n\n\tint myN, myStopN, rangeLength;\n\tdouble myLocalSum, nThElement;\n\n\t\n\n\n\t\n\n\n\t\n\n\n\t\n\n\tif(precision < proc_count)\n\t{\n\t\tprintf(\"Precision smaller than the number of processes - try again\");\n\t\treturn -1;\n\t}\n\n\trangeLength = (int) floor(precision/proc_count);\n\t\n\n\tmyN = my_rank * rangeLength;\n\tmyStopN = (my_rank +1) * rangeLength;\n\n\t\n\n\tif(my_rank == proc_count -1)\n\t{\n\t\tmyStopN = (int) precision;\n\t}\n\n\tmyLocalSum = 0;\n\twhile(myN < myStopN)\n\t{\n\t\tnThElement = ( ( pow(-1.0, myN) ) / ( (2 * myN) + 1 ) );\n\t\tmyLocalSum = myLocalSum + nThElement;\n\t\tmyN = myN + proc_count;\n\t}\n\n\t\n\n\t\n\n\n\t\n\n\n\t\n\n\tif(!my_rank)\n\t{\n\t\tpi_final *= 4;\n\t\tprintf(\"pi = %f\", pi_final);\n\t}\n\n\t\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tdouble precision = _PRECISION_;\n\tint  my_rank, proc_count;\n\tdouble pi_final;\n\n\t\n\n\tint myN, myStopN, rangeLength;\n\tdouble myLocalSum, nThElement;\n\n\t\n\n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &proc_count);\n\n\t\n\n\tif(precision < proc_count)\n\t{\n\t\tprintf(\"Precision smaller than the number of processes - try again\");\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n\trangeLength = (int) floor(precision/proc_count);\n\t\n\n\tmyN = my_rank * rangeLength;\n\tmyStopN = (my_rank +1) * rangeLength;\n\n\t\n\n\tif(my_rank == proc_count -1)\n\t{\n\t\tmyStopN = (int) precision;\n\t}\n\n\tmyLocalSum = 0;\n\twhile(myN < myStopN)\n\t{\n\t\tnThElement = ( ( pow(-1.0, myN) ) / ( (2 * myN) + 1 ) );\n\t\tmyLocalSum = myLocalSum + nThElement;\n\t\tmyN = myN + proc_count;\n\t}\n\n\t\n\n\t\n\n\n\t\n\n\tMPI_Reduce(&myLocalSum, &pi_final,1,MPI_DOUBLE, MPI_SUM,0, MPI_COMM_WORLD);\n\n\t\n\n\tif(!my_rank)\n\t{\n\t\tpi_final *= 4;\n\t\tprintf(\"pi = %f\", pi_final);\n\t}\n\n\t\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "ghisvail_1254", "code": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(3, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_c2r(3, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[2];\n  ptrdiff_t n[3], ni[3], no[3];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_n[3], local_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = 16;\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  for(int t=0; t<3; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(3, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(3, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      3, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      3, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2c(3, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_c2r(3, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "sya-begleitmaterial_1256", "code": "int main(int argc, char **argv)\n{\n    \n\n    int myid, size, length, numprocs;\n    char name[BUFSIZ];\n\n    \n\n   \n\n    \n\n    \n\n    \n    \n\n\n    \n\n    printf(\"\\nAnzahl Prozesse: %d\\n\", numprocs);\n    printf(\"%s: Hallo von Prozess Nummer %d von %d\\n\", name, myid, size);\n\n    \n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    \n\n    int myid, size, length, numprocs;\n    char name[BUFSIZ];\n\n    \n\n    MPI_Init(&argc, &argv);\n   \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    \n    \n\n    MPI_Get_processor_name(name, &length);\n\n    \n\n    printf(\"\\nAnzahl Prozesse: %d\\n\", numprocs);\n    printf(\"%s: Hallo von Prozess Nummer %d von %d\\n\", name, myid, size);\n\n    \n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "JWCook_1258", "code": "int main(int argc, char *argv[]) {\n    int my_rank;\n    double mpi_start_time, mpi_end_time;\n    deme *subpop = (deme*) malloc(sizeof(deme));\n\n    init_population(subpop, argc, argv);\n    mpi_start_time =\n\n    while (!subpop->complete) {\n        migration(subpop);\n        reproduction(subpop);\n        crossover(subpop);\n        mutation(subpop);\n        fitness(subpop);\n        subpop->old_pop = subpop->new_pop;\n        subpop->cur_gen++;\n        check_complete(subpop);\n        sync_complete(subpop);\n        report_all(subpop);\n    }\n\n    mpi_end_time =\n    report_fittest(subpop);\n    printf(\"[%i] Elapsed time: %f\\n\", my_rank, mpi_end_time - mpi_start_time);\n    return 1;\n}", "label": "int main(int argc, char *argv[]) {\n    int my_rank;\n    double mpi_start_time, mpi_end_time;\n    deme *subpop = (deme*) malloc(sizeof(deme));\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    init_population(subpop, argc, argv);\n    mpi_start_time = MPI_Wtime();\n\n    while (!subpop->complete) {\n        migration(subpop);\n        reproduction(subpop);\n        crossover(subpop);\n        mutation(subpop);\n        fitness(subpop);\n        subpop->old_pop = subpop->new_pop;\n        subpop->cur_gen++;\n        check_complete(subpop);\n        sync_complete(subpop);\n        report_all(subpop);\n    }\n\n    mpi_end_time = MPI_Wtime();\n    report_fittest(subpop);\n    MPI_Finalize();\n    printf(\"[%i] Elapsed time: %f\\n\", my_rank, mpi_end_time - mpi_start_time);\n    return 1;\n}"}
{"program": "ghisvail_1259", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2c(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2; np[2] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_3d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_3d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_3d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2c(4, ni, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qsnake_1261", "code": "int\nmain(int argc, char **argv)\n{\n  int status;\n\n#ifdef GPAW_CRAYPAT\n  PAT_region_begin(1, \"C-Initializations\");\n#endif\n\n#ifndef GPAW_OMP\n#else\n  int granted;\n  if(granted != MPI_THREAD_MULTIPLE) exit(1);\n#endif \n\n\n#ifdef GPAW_PERFORMANCE_REPORT\n  gpaw_perf_init();\n#endif\n\n#ifdef IO_WRAPPERS\n  init_io_wrappers();\n#endif\n\n#ifdef GPAW_MPI_MAP\n  int tag = 99;\n  int myid, numprocs, i, procnamesize;\n  char procname[MPI_MAX_PROCESSOR_NAME];\n  if (myid > 0) {\n  }\n  else {\n      printf(\"MPI_COMM_SIZE is %d \\n\", numprocs);\n      printf(\"%s \\n\", procname);\n      \n      for (i = 1; i < numprocs; ++i) {\n\t  printf(\"%s \\n\", procname);\n      }\n  }\n#endif \n\n\n#ifdef GPAW_MPI_DEBUG\n  \n\n#endif\n\n#ifdef HDF5\n  init_h5py();\n#endif\n\n  Py_Initialize();\n\n#ifdef NO_SOCKET\n  initsocket();\n#endif\n\n  if (PyType_Ready(&MPIType) < 0)\n    return -1;\n\n  PyObject* m = Py_InitModule3(\"_gpaw\", functions,\n             \"C-extension for GPAW\\n\\n...\\n\");\n  if (m == NULL)\n    return -1;\n\n  Py_INCREF(&MPIType);\n  PyModule_AddObject(m, \"Communicator\", (PyObject *)&MPIType);\n  import_array1(-1);\n#ifdef GPAW_CRAYPAT\n  PAT_region_end(1);\n#endif\n  status = Py_Main(argc, argv);\n\n#ifdef GPAW_PERFORMANCE_REPORT\n  gpaw_perf_finalize();\n#endif\n\n  return status;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  int status;\n\n#ifdef GPAW_CRAYPAT\n  PAT_region_begin(1, \"C-Initializations\");\n#endif\n\n#ifndef GPAW_OMP\n  MPI_Init(&argc, &argv);\n#else\n  int granted;\n  MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &granted);\n  if(granted != MPI_THREAD_MULTIPLE) exit(1);\n#endif \n\n\n#ifdef GPAW_PERFORMANCE_REPORT\n  gpaw_perf_init();\n#endif\n\n#ifdef IO_WRAPPERS\n  init_io_wrappers();\n#endif\n\n#ifdef GPAW_MPI_MAP\n  int tag = 99;\n  int myid, numprocs, i, procnamesize;\n  char procname[MPI_MAX_PROCESSOR_NAME];\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs );\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid );\n  MPI_Get_processor_name(procname, &procnamesize);\n  if (myid > 0) {\n      MPI_Send(&procnamesize, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n      MPI_Send(procname, procnamesize, MPI_CHAR, 0, tag, MPI_COMM_WORLD);\n  }\n  else {\n      printf(\"MPI_COMM_SIZE is %d \\n\", numprocs);\n      printf(\"%s \\n\", procname);\n      \n      for (i = 1; i < numprocs; ++i) {\n\t  MPI_Recv(&procnamesize, 1, MPI_INT, i, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t  MPI_Recv(procname, procnamesize, MPI_CHAR, i, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t  printf(\"%s \\n\", procname);\n      }\n  }\n#endif \n\n\n#ifdef GPAW_MPI_DEBUG\n  \n\n  MPI_Errhandler_set(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n#endif\n\n#ifdef HDF5\n  init_h5py();\n#endif\n\n  Py_Initialize();\n\n#ifdef NO_SOCKET\n  initsocket();\n#endif\n\n  if (PyType_Ready(&MPIType) < 0)\n    return -1;\n\n  PyObject* m = Py_InitModule3(\"_gpaw\", functions,\n             \"C-extension for GPAW\\n\\n...\\n\");\n  if (m == NULL)\n    return -1;\n\n  Py_INCREF(&MPIType);\n  PyModule_AddObject(m, \"Communicator\", (PyObject *)&MPIType);\n  import_array1(-1);\n  MPI_Barrier(MPI_COMM_WORLD);\n#ifdef GPAW_CRAYPAT\n  PAT_region_end(1);\n#endif\n  status = Py_Main(argc, argv);\n\n#ifdef GPAW_PERFORMANCE_REPORT\n  gpaw_perf_finalize();\n#endif\n\n  MPI_Finalize();\n  return status;\n}"}
{"program": "ssmall41_1262", "code": "int main(int argc,char* argv[])\n{\n\t\n\n\n\t\n\n\tif(argc < 2)\n\t{\n\t\tif(my_rank == 0)\n\t\t{\n\t\t\tprintf(\"Command line parameter required:  A universal variable file (.gbl).\\n\");\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\treturn 1;\n\t}\n\n\t\n\n\ttime_t start,stop;\n\tdouble total_time;\n\tasynchsolver* asynch;\n\n\tif(my_rank == 0)\n\t\tprintf(\"\\nBeginning initialization...\\n*****************************\\n\");\n\tstart = time(NULL);\n\n\t\n\n\tasynch = Asynch_Init(MPI_COMM_WORLD,&argc,&argv);\n\tif(my_rank == 0)\tprintf(\"Reading global file...\\n\");\n\tAsynch_Parse_GBL(asynch,argv[1]);\n\tif(my_rank == 0)\tprintf(\"Loading network...\\n\");\n\tAsynch_Load_Network(asynch);\n\tif(my_rank == 0)\tprintf(\"Partitioning network...\\n\");\n\tAsynch_Partition_Network(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading parameters...\\n\");\n\tAsynch_Load_Network_Parameters(asynch,0);\n\tif(my_rank == 0)\tprintf(\"Reading dam and reservoir data...\\n\");\n\tAsynch_Load_Dams(asynch);\n\tif(my_rank == 0)\tprintf(\"Setting up numerical error data...\\n\");\n\tAsynch_Load_Numerical_Error_Data(asynch);\n\tif(my_rank == 0)\tprintf(\"Initializing model...\\n\");\n\tAsynch_Initialize_Model(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading initial conditions...\\n\");\n\tAsynch_Load_Initial_Conditions(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading forcings...\\n\");\n\tAsynch_Load_Forcings(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading output data information...\\n\");\n\tAsynch_Load_Save_Lists(asynch);\n\tif(my_rank == 0)\tprintf(\"Finalizing network...\\n\");\n\tAsynch_Finalize_Network(asynch);\n\tif(my_rank == 0)\tprintf(\"Calculating initial step sizes...\\n\");\n\tAsynch_Calculate_Step_Sizes(asynch);\n\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"\\nModel type is %u.\\nGlobal parameters are:\\n\",asynch->GlobalVars->type);\n\t\tPrint_Vector(asynch->GlobalVars->global_params);\n\t\tprintf(\"\\n\");\n\t}\n\n\t\n\n\tint id_setup = Asynch_Check_Output(asynch,\"LinkID\");\n\tif(id_setup != -1)\n\t{\n\t\tSet_Output_User_LinkID(asynch);\n\t\tAsynch_Set_Output(asynch,\"LinkID\",ASYNCH_INT,(void (*)(double,VEC*,VEC*,VEC*,int,void*)) &Output_Linkid,NULL,0);\n\t}\n\n\t\n\n\tAsynch_Prepare_Temp_Files(asynch);\n\tAsynch_Write_Current_Step(asynch);\t\t\n\n\tAsynch_Prepare_Peakflow_Output(asynch);\n\tAsynch_Prepare_Output(asynch);\n\n\t\n\n\tprintf(\"Process %i (%i total) is good to go with %i links.\\n\",my_rank,np,asynch->my_N);\n\tsleep(1);\n\n\tif(my_rank == 0)\n\t{\n\t\tstop = time(NULL);\n\t\ttotal_time = difftime(stop,start);\n\t\tprintf(\"Finished initialization. Total time: %f\\n\\n\\nComputing solution at each link...\\n************************************\\n\",total_time);\n\t}\n\tfflush(stdout);\n\n\t\n\n\ttime(&start);\n\tAsynch_Advance(asynch,1);\n\ttime(&stop);\n\n\t\n\n\ttotal_time += difftime(stop,start);\n\tif(my_rank == 0)\tprintf(\"\\nComputations complete. Total time for calculations: %f\\n\",difftime(stop,start));\n\tif(asynch->sys[asynch->my_sys[0]]->c == NULL)\n\t{\n\t\tprintf(\"[%i]: The solution at ID %i at time %.12f is\\n\",my_rank,asynch->sys[asynch->my_sys[0]]->ID,asynch->sys[asynch->my_sys[0]]->last_t);\n\t\tPrint_Vector(asynch->sys[asynch->my_sys[0]]->list->tail->y_approx);\n\t}\n\n\t\n\n\tAsynch_Take_System_Snapshot(asynch,NULL);\n\n\t\n\n\tAsynch_Create_Output(asynch,NULL);\n\tAsynch_Create_Peakflows_Output(asynch);\n\n\t\n\n\tAsynch_Delete_Temporary_Files(asynch);\n\tAsynch_Free(asynch);\n\treturn 0;\n}", "label": "int main(int argc,char* argv[])\n{\n\t\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD,&np);\n\n\t\n\n\tif(argc < 2)\n\t{\n\t\tif(my_rank == 0)\n\t\t{\n\t\t\tprintf(\"Command line parameter required:  A universal variable file (.gbl).\\n\");\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\t\n\n\ttime_t start,stop;\n\tdouble total_time;\n\tasynchsolver* asynch;\n\n\tif(my_rank == 0)\n\t\tprintf(\"\\nBeginning initialization...\\n*****************************\\n\");\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tstart = time(NULL);\n\n\t\n\n\tasynch = Asynch_Init(MPI_COMM_WORLD,&argc,&argv);\n\tif(my_rank == 0)\tprintf(\"Reading global file...\\n\");\n\tAsynch_Parse_GBL(asynch,argv[1]);\n\tif(my_rank == 0)\tprintf(\"Loading network...\\n\");\n\tAsynch_Load_Network(asynch);\n\tif(my_rank == 0)\tprintf(\"Partitioning network...\\n\");\n\tAsynch_Partition_Network(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading parameters...\\n\");\n\tAsynch_Load_Network_Parameters(asynch,0);\n\tif(my_rank == 0)\tprintf(\"Reading dam and reservoir data...\\n\");\n\tAsynch_Load_Dams(asynch);\n\tif(my_rank == 0)\tprintf(\"Setting up numerical error data...\\n\");\n\tAsynch_Load_Numerical_Error_Data(asynch);\n\tif(my_rank == 0)\tprintf(\"Initializing model...\\n\");\n\tAsynch_Initialize_Model(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading initial conditions...\\n\");\n\tAsynch_Load_Initial_Conditions(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading forcings...\\n\");\n\tAsynch_Load_Forcings(asynch);\n\tif(my_rank == 0)\tprintf(\"Loading output data information...\\n\");\n\tAsynch_Load_Save_Lists(asynch);\n\tif(my_rank == 0)\tprintf(\"Finalizing network...\\n\");\n\tAsynch_Finalize_Network(asynch);\n\tif(my_rank == 0)\tprintf(\"Calculating initial step sizes...\\n\");\n\tAsynch_Calculate_Step_Sizes(asynch);\n\n\tif(my_rank == 0)\n\t{\n\t\tprintf(\"\\nModel type is %u.\\nGlobal parameters are:\\n\",asynch->GlobalVars->type);\n\t\tPrint_Vector(asynch->GlobalVars->global_params);\n\t\tprintf(\"\\n\");\n\t}\n\n\t\n\n\tint id_setup = Asynch_Check_Output(asynch,\"LinkID\");\n\tif(id_setup != -1)\n\t{\n\t\tSet_Output_User_LinkID(asynch);\n\t\tAsynch_Set_Output(asynch,\"LinkID\",ASYNCH_INT,(void (*)(double,VEC*,VEC*,VEC*,int,void*)) &Output_Linkid,NULL,0);\n\t}\n\n\t\n\n\tAsynch_Prepare_Temp_Files(asynch);\n\tAsynch_Write_Current_Step(asynch);\t\t\n\n\tAsynch_Prepare_Peakflow_Output(asynch);\n\tAsynch_Prepare_Output(asynch);\n\n\t\n\n\tprintf(\"Process %i (%i total) is good to go with %i links.\\n\",my_rank,np,asynch->my_N);\n\tsleep(1);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tif(my_rank == 0)\n\t{\n\t\tstop = time(NULL);\n\t\ttotal_time = difftime(stop,start);\n\t\tprintf(\"Finished initialization. Total time: %f\\n\\n\\nComputing solution at each link...\\n************************************\\n\",total_time);\n\t}\n\tfflush(stdout);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\t\n\n\ttime(&start);\n\tAsynch_Advance(asynch,1);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\ttime(&stop);\n\n\t\n\n\ttotal_time += difftime(stop,start);\n\tif(my_rank == 0)\tprintf(\"\\nComputations complete. Total time for calculations: %f\\n\",difftime(stop,start));\n\tif(asynch->sys[asynch->my_sys[0]]->c == NULL)\n\t{\n\t\tprintf(\"[%i]: The solution at ID %i at time %.12f is\\n\",my_rank,asynch->sys[asynch->my_sys[0]]->ID,asynch->sys[asynch->my_sys[0]]->last_t);\n\t\tPrint_Vector(asynch->sys[asynch->my_sys[0]]->list->tail->y_approx);\n\t}\n\n\t\n\n\tAsynch_Take_System_Snapshot(asynch,NULL);\n\n\t\n\n\tAsynch_Create_Output(asynch,NULL);\n\tAsynch_Create_Peakflows_Output(asynch);\n\n\t\n\n\tAsynch_Delete_Temporary_Files(asynch);\n\tAsynch_Free(asynch);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "rahlk_1265", "code": "main (int ac, char **av)\n{\n  int i, j;\n  MPI_Request r1, r2;\n  MPI_Status s1, s2;\n  int cnt = MSG_CNT;\n  int tag = 111;\n  int comm = MPI_COMM_WORLD;\n  int nprocs, rank;\n  float sendX[MSG_CNT] = { 1, 2, 3, 4 };\n  float recvX[MSG_CNT];\n  int flag;\n\n  for (i = 0; i < 10; i++)\n    {\n      int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n      int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n      for (flag = 0; !flag;)\n\t{\n\t}\n      for (flag = 0; !flag;)\n\t{\n\t}\n    }\n}", "label": "main (int ac, char **av)\n{\n  int i, j;\n  MPI_Request r1, r2;\n  MPI_Status s1, s2;\n  int cnt = MSG_CNT;\n  int tag = 111;\n  int comm = MPI_COMM_WORLD;\n  int nprocs, rank;\n  float sendX[MSG_CNT] = { 1, 2, 3, 4 };\n  float recvX[MSG_CNT];\n  int flag;\n\n  MPI_Init (&ac, &av);\n  MPI_Comm_size (MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n  for (i = 0; i < 10; i++)\n    {\n      int src = (rank == 0) ? (nprocs - 1) : (rank - 1);\n      int dest = (rank == nprocs - 1) ? (0) : (rank + 1);\n      MPI_Irecv (recvX, cnt, MPI_FLOAT, src, tag, comm, &r1);\n      MPI_Isend (sendX, cnt, MPI_FLOAT, dest, tag, comm, &r2);\n      for (flag = 0; !flag;)\n\t{\n\t  MPI_Test (&r1, &flag, &s1);\n\t}\n      for (flag = 0; !flag;)\n\t{\n\t  MPI_Test (&r2, &flag, &s2);\n\t}\n    }\n  MPI_Finalize ();\n}"}
{"program": "bsc-performance-tools_1267", "code": "int main(int argc, char *argv[])\n{\n\tint v, vv;\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint v, vv;\n\tMPI_Init (&argc, &argv);\n\tMPI_Gather (&v, 1, MPI_INT, &vv, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "tcsiwula_1268", "code": "int main(int argc, char** argv) {\n    int         my_rank;   \n\n    int         p;         \n\n    double      a;         \n\n    double      b;         \n\n    int         n;         \n\n    double      h;         \n\n    double      local_a;   \n\n    double      local_b;   \n\n    int         local_n;   \n\n                           \n\n    double      my_area;   \n\n    double      total;     \n\n    int         source;    \n\n    int         dest = 0;  \n\n    int         tag = 0;\n    MPI_Status  status;\n\n    \n\n\n    \n\n\n    \n\n\n    Get_data(p, my_rank, &a, &b, &n);\n\n    h = (b-a)/n;    \n\n    local_n = n/p;  \n\n\n    \n\n    local_a = a + my_rank*local_n*h;\n    local_b = local_a + local_n*h;\n    my_area = Trap(local_a, local_b, local_n, h);\n\n    \n\n    if (my_rank == 0) {\n        total = my_area;\n        for (source = 1; source < p; source++) {\n            total = total + my_area;\n        }\n    } else {  \n    }\n\n    \n\n    if (my_rank == 0) {\n        printf(\"With n = %d trapezoids, our estimate\\n\",\n            n);\n        printf(\"of the area from %f to %f = %.15f\\n\",\n            a, b, total);\n    }\n\n    \n\n\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    int         my_rank;   \n\n    int         p;         \n\n    double      a;         \n\n    double      b;         \n\n    int         n;         \n\n    double      h;         \n\n    double      local_a;   \n\n    double      local_b;   \n\n    int         local_n;   \n\n                           \n\n    double      my_area;   \n\n    double      total;     \n\n    int         source;    \n\n    int         dest = 0;  \n\n    int         tag = 0;\n    MPI_Status  status;\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    \n\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    Get_data(p, my_rank, &a, &b, &n);\n\n    h = (b-a)/n;    \n\n    local_n = n/p;  \n\n\n    \n\n    local_a = a + my_rank*local_n*h;\n    local_b = local_a + local_n*h;\n    my_area = Trap(local_a, local_b, local_n, h);\n\n    \n\n    if (my_rank == 0) {\n        total = my_area;\n        for (source = 1; source < p; source++) {\n            MPI_Recv(&my_area, 1, MPI_DOUBLE, source, tag,\n                MPI_COMM_WORLD, &status);\n            total = total + my_area;\n        }\n    } else {  \n        MPI_Send(&my_area, 1, MPI_DOUBLE, dest,\n            tag, MPI_COMM_WORLD);\n    }\n\n    \n\n    if (my_rank == 0) {\n        printf(\"With n = %d trapezoids, our estimate\\n\",\n            n);\n        printf(\"of the area from %f to %f = %.15f\\n\",\n            a, b, total);\n    }\n\n    \n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "blakeMilner_1271", "code": "int main(int argc, char *argv[])\n{\n\n\n\n    int go_flag = GO;\n\n\n\n    int rank;\t\t\t\n\n    int number;\t\t\t\n\n\n\n\n\n\n\n\n\n\n\n#ifdef\tDEBUG\n\tfprintf(stderr, \"Initializing MPI [processors = %3d, rank = %3d].\\n\",number,rank);\n#endif\n\n    mpi_setmoddata(number, rank);\n\n\n\n    if (rank == 0) {\n\n\n\t\tmpi_mastering(argc, argv);\n\t\tmpi_shutdown();\n\n\n    } else {\n\t\twhile (go_flag != NOGO) {\n\t\t\tgo_flag = mpi_slaving();\n\t\t}\n    }\n\n\n\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\n\n\n    int go_flag = GO;\n\n\n\n    int rank;\t\t\t\n\n    int number;\t\t\t\n\n\n\n\n    MPI_Init(&argc, &argv);\n\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n    MPI_Comm_size(MPI_COMM_WORLD, &number);\n\n\n\n#ifdef\tDEBUG\n\tfprintf(stderr, \"Initializing MPI [processors = %3d, rank = %3d].\\n\",number,rank);\n#endif\n\n    mpi_setmoddata(number, rank);\n\n\n\n    if (rank == 0) {\n\n\n\t\tmpi_mastering(argc, argv);\n\t\tmpi_shutdown();\n\n\n    } else {\n\t\twhile (go_flag != NOGO) {\n\t\t\tgo_flag = mpi_slaving();\n\t\t}\n    }\n\n\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "gnu3ra_1272", "code": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    int rank, len, err, i;\n    int errs=0, toterrs;\n    char *filename;\n\n\n\n\n    if (!rank) {\n        i = 1;\n        while ((i < argc) && strcmp(\"-fname\", *argv)) {\n            i++;\n            argv++;\n        }\n        if (i >= argc) {\n            fprintf(stderr, \"\\n*#  Usage: excl -fname filename\\n\\n\");\n        }\n        argv++;\n        len = strlen(*argv);\n        filename = (char *) malloc(len+10);\n        strcpy(filename, *argv);\n    }\n    else {\n        filename = (char *) malloc(len+10);\n    }\n    \n\n\n    \n\n    err =\n    if (err != MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open failed when it should have succeeded\\n\", rank);\n    }\n\n\n    \n\n    err =\n    if (err == MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open succeeded when it should have failed\\n\", rank);\n    }\n\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(filename);\n    return 0; \n}", "label": "int main(int argc, char **argv)\n{\n    MPI_File fh;\n    int rank, len, err, i;\n    int errs=0, toterrs;\n    char *filename;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n\n    if (!rank) {\n        i = 1;\n        while ((i < argc) && strcmp(\"-fname\", *argv)) {\n            i++;\n            argv++;\n        }\n        if (i >= argc) {\n            fprintf(stderr, \"\\n*#  Usage: excl -fname filename\\n\\n\");\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n        argv++;\n        len = strlen(*argv);\n        filename = (char *) malloc(len+10);\n        strcpy(filename, *argv);\n        MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        filename = (char *) malloc(len+10);\n        MPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    \n\n    if (!rank) MPI_File_delete(filename, MPI_INFO_NULL);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    err = MPI_File_open(MPI_COMM_WORLD, filename, \n         MPI_MODE_CREATE | MPI_MODE_EXCL | MPI_MODE_RDWR, MPI_INFO_NULL , &fh);\n    if (err != MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open failed when it should have succeeded\\n\", rank);\n    }\n    else MPI_File_close(&fh);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    err = MPI_File_open(MPI_COMM_WORLD, filename, \n         MPI_MODE_CREATE | MPI_MODE_EXCL | MPI_MODE_RDWR, MPI_INFO_NULL , &fh);\n    if (err == MPI_SUCCESS) {\n\terrs++;\n\tfprintf(stderr, \"Process %d: open succeeded when it should have failed\\n\", rank);\n    }\n\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(filename);\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "aeslaughter_1273", "code": "int\nmain(int argc, char **argv)\n{\n\n#ifdef USE_PARALLEL\n#endif\n\n   printf(\"\\n*** Testing netcdf-4 large files.\\n\");\n   printf(\"**** testing with user-contributed test...\\n\");\n   {\n#define TIME_LEN 3000\n#define LAT_LEN 1000\n#define LON_LEN 2000\n#define NDIMS 3\n#define VAR_NAME \"the_big_enchilada\"\n#define NUM_FORMATS 2\n\n      int ncid, varid;\n      int dimids[NDIMS];\n      size_t start[NDIMS] = {0, 0, 0};\n      size_t count[NDIMS] = {1, LAT_LEN, LON_LEN};\n      char file_name[NC_MAX_NAME * 2 + 1];\n      float *data;\n      int this_format[NUM_FORMATS] = {NC_64BIT_OFFSET, NC_NETCDF4};\n      char format_name[NUM_FORMATS][NC_MAX_NAME + 1] =\n\t {\"64-bit offset\", \"netCDF-4\"};\n      int i, j, f;\n\n      printf(\"sizes: int - %d, size_t - %d, and int * - %d\\n\",\n\t     sizeof(int), sizeof(size_t), sizeof(int *));\n\n      \n\n      if (!(data = calloc(LAT_LEN * LON_LEN, sizeof(float)))) ERR;\n\n      \n\n      for (f = 0; f < NUM_FORMATS; f++)\n      {\n\t printf(\"\\t...testing with %s\\n\", format_name[f]);\n\t sprintf(file_name, \"%s/%s\", TEMP_LARGE, FILE_NAME);\n\t if (nc_create(file_name, this_format[f], &ncid)) ERR;\n\t if (nc_def_dim(ncid, \"lat\", LAT_LEN, &dimids[1])) ERR;\n\t if (nc_def_dim(ncid, \"lon\", LON_LEN, &dimids[2])) ERR;\n\t if (nc_def_dim(ncid, \"time\", TIME_LEN, &dimids[0])) ERR;\n\t if (nc_def_var(ncid, VAR_NAME, NC_FLOAT, 3, dimids, &varid)) ERR;\n\t if (nc_close(ncid)) ERR;\n\n\t \n\n\t if (nc_open(file_name, NC_WRITE, &ncid)) ERR;\n\t for (start[0] = 0; start[0] < TIME_LEN; start[0]++)\n\t {\n\t    \n\n\t    for (i = 0; i < LAT_LEN; i++)\n\t       for (j = 0; j < LON_LEN; j++)\n\t\t  data[j + LON_LEN * i] = (start[0] + i + j) % 19;\n\n\t    \n\n\t    if (nc_put_vara_float(ncid, varid, start, count, data)) ERR;\n\t }\n\t if (nc_close(ncid)) ERR;\n\n\t \n\n\t if (nc_open(file_name, NC_NOWRITE, &ncid)) ERR;\n\t if (nc_inq_varid(ncid, VAR_NAME, &varid)) ERR;\n\t for (start[0] = 0; start[0] < TIME_LEN; start[0]++)\n\t {\n\t    if (nc_get_vara_float(ncid, varid, start, count, data)) ERR;\n\t    for (i = 0; i < LAT_LEN; i++)\n\t       for (j = 0; j < LON_LEN; j++)\n\t       {\n\t\t  if (data[j + LON_LEN * i] != (start[0] + i + j) % 19)\n\t\t  {\n\t\t     printf(\"error on start[0]: %d i: %d j: %d expected %d got %g\\n\",\n\t\t\t    start[0], i, j, (start[0] + i + j), data[j + LON_LEN * i]);\n\t\t     ERR_RET;\n\t\t  }\n\t       }\n\t } \n\n      } \n\n\n      \n\n      free(data);\n   }\n   SUMMARIZE_ERR;\n\n#ifdef USE_PARALLEL\n#endif\n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char **argv)\n{\n\n#ifdef USE_PARALLEL\n   MPI_Init(&argc, &argv);\n#endif\n\n   printf(\"\\n*** Testing netcdf-4 large files.\\n\");\n   printf(\"**** testing with user-contributed test...\\n\");\n   {\n#define TIME_LEN 3000\n#define LAT_LEN 1000\n#define LON_LEN 2000\n#define NDIMS 3\n#define VAR_NAME \"the_big_enchilada\"\n#define NUM_FORMATS 2\n\n      int ncid, varid;\n      int dimids[NDIMS];\n      size_t start[NDIMS] = {0, 0, 0};\n      size_t count[NDIMS] = {1, LAT_LEN, LON_LEN};\n      char file_name[NC_MAX_NAME * 2 + 1];\n      float *data;\n      int this_format[NUM_FORMATS] = {NC_64BIT_OFFSET, NC_NETCDF4};\n      char format_name[NUM_FORMATS][NC_MAX_NAME + 1] =\n\t {\"64-bit offset\", \"netCDF-4\"};\n      int i, j, f;\n\n      printf(\"sizes: int - %d, size_t - %d, and int * - %d\\n\",\n\t     sizeof(int), sizeof(size_t), sizeof(int *));\n\n      \n\n      if (!(data = calloc(LAT_LEN * LON_LEN, sizeof(float)))) ERR;\n\n      \n\n      for (f = 0; f < NUM_FORMATS; f++)\n      {\n\t printf(\"\\t...testing with %s\\n\", format_name[f]);\n\t sprintf(file_name, \"%s/%s\", TEMP_LARGE, FILE_NAME);\n\t if (nc_create(file_name, this_format[f], &ncid)) ERR;\n\t if (nc_def_dim(ncid, \"lat\", LAT_LEN, &dimids[1])) ERR;\n\t if (nc_def_dim(ncid, \"lon\", LON_LEN, &dimids[2])) ERR;\n\t if (nc_def_dim(ncid, \"time\", TIME_LEN, &dimids[0])) ERR;\n\t if (nc_def_var(ncid, VAR_NAME, NC_FLOAT, 3, dimids, &varid)) ERR;\n\t if (nc_close(ncid)) ERR;\n\n\t \n\n\t if (nc_open(file_name, NC_WRITE, &ncid)) ERR;\n\t for (start[0] = 0; start[0] < TIME_LEN; start[0]++)\n\t {\n\t    \n\n\t    for (i = 0; i < LAT_LEN; i++)\n\t       for (j = 0; j < LON_LEN; j++)\n\t\t  data[j + LON_LEN * i] = (start[0] + i + j) % 19;\n\n\t    \n\n\t    if (nc_put_vara_float(ncid, varid, start, count, data)) ERR;\n\t }\n\t if (nc_close(ncid)) ERR;\n\n\t \n\n\t if (nc_open(file_name, NC_NOWRITE, &ncid)) ERR;\n\t if (nc_inq_varid(ncid, VAR_NAME, &varid)) ERR;\n\t for (start[0] = 0; start[0] < TIME_LEN; start[0]++)\n\t {\n\t    if (nc_get_vara_float(ncid, varid, start, count, data)) ERR;\n\t    for (i = 0; i < LAT_LEN; i++)\n\t       for (j = 0; j < LON_LEN; j++)\n\t       {\n\t\t  if (data[j + LON_LEN * i] != (start[0] + i + j) % 19)\n\t\t  {\n\t\t     printf(\"error on start[0]: %d i: %d j: %d expected %d got %g\\n\",\n\t\t\t    start[0], i, j, (start[0] + i + j), data[j + LON_LEN * i]);\n\t\t     ERR_RET;\n\t\t  }\n\t       }\n\t } \n\n      } \n\n\n      \n\n      free(data);\n   }\n   SUMMARIZE_ERR;\n\n#ifdef USE_PARALLEL\n   MPI_Finalize();\n#endif\n   FINAL_RESULTS;\n}"}
{"program": "xyuan_1274", "code": "int\nmain (int argc, char **argv)\n{\n  int                 mpiret;\n  int                 refine_level;\n  size_t              size2, size3;\n  long long           lsize[2], gsize[2];\n  mpi_context_t       mpi_context, *mpi = &mpi_context;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity2;\n  p8est_t            *p8est;\n  p8est_connectivity_t *connectivity3;\n\n  \n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpi->mpicomm = MPI_COMM_WORLD;        \n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  \n\n  sc_init (MPI_COMM_WORLD, 0, 0, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n  refine_level = 2;\n\n  \n\n  connectivity2 = p4est_connectivity_new_corner ();\n  size2 = p4est_connectivity_memory_used (connectivity2);\n  p4est = p4est_new_ext (mpi->mpicomm, connectivity2,\n                         0, refine_level, 1, 0, NULL, NULL);\n  size2 += p4est_memory_used (p4est);\n\n  \n\n  connectivity3 = p8est_connectivity_new_rotcubes ();\n  size3 = p8est_connectivity_memory_used (connectivity3);\n  p8est = p8est_new_ext (mpi->mpicomm, connectivity3,\n                         0, refine_level, 1, 0, NULL, NULL);\n  size3 += p8est_memory_used (p8est);\n\n  \n\n  lsize[0] = (long long) size2;\n  lsize[1] = (long long) size3;\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  P4EST_GLOBAL_INFOF (\"Total forest byte sizes: %lld (2D), %lld (3D)\\n\",\n                      gsize[0], gsize[1]);\n\n  \n\n  p4est_vtk_write_file (p4est, NULL, \"p4est_sieve\");\n  p8est_vtk_write_file (p8est, NULL, \"p8est_sieve\");\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity2);\n  p8est_destroy (p8est);\n  p8est_connectivity_destroy (connectivity3);\n\n  \n\n  sc_finalize ();\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int                 mpiret;\n  int                 refine_level;\n  size_t              size2, size3;\n  long long           lsize[2], gsize[2];\n  mpi_context_t       mpi_context, *mpi = &mpi_context;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity2;\n  p8est_t            *p8est;\n  p8est_connectivity_t *connectivity3;\n\n  \n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpi->mpicomm = MPI_COMM_WORLD;        \n\n  mpiret = MPI_Comm_size (mpi->mpicomm, &mpi->mpisize);\n  SC_CHECK_MPI (mpiret);\n  mpiret = MPI_Comm_rank (mpi->mpicomm, &mpi->mpirank);\n  SC_CHECK_MPI (mpiret);\n\n  \n\n  sc_init (MPI_COMM_WORLD, 0, 0, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n  refine_level = 2;\n\n  \n\n  connectivity2 = p4est_connectivity_new_corner ();\n  size2 = p4est_connectivity_memory_used (connectivity2);\n  p4est = p4est_new_ext (mpi->mpicomm, connectivity2,\n                         0, refine_level, 1, 0, NULL, NULL);\n  size2 += p4est_memory_used (p4est);\n\n  \n\n  connectivity3 = p8est_connectivity_new_rotcubes ();\n  size3 = p8est_connectivity_memory_used (connectivity3);\n  p8est = p8est_new_ext (mpi->mpicomm, connectivity3,\n                         0, refine_level, 1, 0, NULL, NULL);\n  size3 += p8est_memory_used (p8est);\n\n  \n\n  lsize[0] = (long long) size2;\n  lsize[1] = (long long) size3;\n  mpiret =\n    MPI_Reduce (lsize, gsize, 2, MPI_LONG_LONG_INT, MPI_SUM, 0, mpi->mpicomm);\n  SC_CHECK_MPI (mpiret);\n  P4EST_GLOBAL_INFOF (\"Total forest byte sizes: %lld (2D), %lld (3D)\\n\",\n                      gsize[0], gsize[1]);\n\n  \n\n  p4est_vtk_write_file (p4est, NULL, \"p4est_sieve\");\n  p8est_vtk_write_file (p8est, NULL, \"p8est_sieve\");\n\n  \n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity2);\n  p8est_destroy (p8est);\n  p8est_connectivity_destroy (connectivity3);\n\n  \n\n  sc_finalize ();\n\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "callmetaste_1275", "code": "int main( int argc, char *argv[] )\n{\n  int fail,myid,numprocs,\n    n\n, \n    dim\n, \n    nb_slices,\n    max_nb_slices, \n    deg\n;\n  double startwtime,trackwtime,wtime,*mytime;\n  MPI_Status status;\n\n  adainit();\n  srand(time(NULL));   \n\n  if(myid == 0) {\n    mytime = (double*) calloc(numprocs, sizeof(double));\n    startwtime = \n    \n    fail = read_witness_set(&n,&dim,&deg);\n    fail = define_output_file();\n\n    printf(\"Give the number of slices: \"); scanf(\"%d\",&nb_slices);\n    printf(\"Give the maximal number of slices: \"); scanf(\"%d\",&max_nb_slices);\n  } else {\n    trackwtime = \n    if (v>=3) printf(\"\\nLaunching slave %d\\n\", myid);\n  }\n  \n\n  dimension_broadcast(myid,&n);\n  if (v>=1) printf(\"Node %d: went through bcast of degree,dim,#slices.\\n\",myid);\n\n  fail = initialize_monodromy(max_nb_slices,deg,dim);\n  fail = build_trace_grid(myid,n,dim,deg,numprocs,&trackwtime);\n\n\n  trace_grid_broadcast(myid,deg,n);\n  all_slices_broadcast(myid,max_nb_slices,dim,n);\n  if (v>=1) printf(\"Node %d: construction and broadcast of trace grid and slices... done.\\n\",myid);\n\n    \n  fail = track_paths_to_new_slices(myid,nb_slices,deg,dim,n,numprocs,&trackwtime);\n\n\n\n  if (myid ==0) run_master(deg, nb_slices+1, max_nb_slices+1, numprocs);\n  else run_slave(deg);\n   \n  \n  if(myid == 0)\n    wtime = MPI_Wtime() - startwtime;\n  else\n    wtime = trackwtime;\n  if(myid == 0) {\n    print_time(mytime,numprocs);\n  }\n   \n  adafinal();\n  return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n  int fail,myid,numprocs,\n    n\n, \n    dim\n, \n    nb_slices,\n    max_nb_slices, \n    deg\n;\n  double startwtime,trackwtime,wtime,*mytime;\n  MPI_Status status;\n\n  adainit();\n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n  srand(time(NULL));   \n\n  if(myid == 0) {\n    mytime = (double*) calloc(numprocs, sizeof(double));\n    startwtime = MPI_Wtime(); \n    \n    fail = read_witness_set(&n,&dim,&deg);\n    fail = define_output_file();\n\n    printf(\"Give the number of slices: \"); scanf(\"%d\",&nb_slices);\n    printf(\"Give the maximal number of slices: \"); scanf(\"%d\",&max_nb_slices);\n  } else {\n    trackwtime = MPI_Wtime(); \n    if (v>=3) printf(\"\\nLaunching slave %d\\n\", myid);\n  }\n  \n  MPI_Barrier(MPI_COMM_WORLD);  \n\n  dimension_broadcast(myid,&n);\n  MPI_Bcast(&nb_slices,1,MPI_INT,0,MPI_COMM_WORLD);\n  MPI_Bcast(&max_nb_slices,1,MPI_INT,0,MPI_COMM_WORLD);\n  MPI_Bcast(&dim,1,MPI_INT,0,MPI_COMM_WORLD);\n  MPI_Bcast(&deg,1,MPI_INT,0,MPI_COMM_WORLD);\n  if (v>=1) printf(\"Node %d: went through bcast of degree,dim,#slices.\\n\",myid);\n\n  fail = initialize_monodromy(max_nb_slices,deg,dim);\n  fail = build_trace_grid(myid,n,dim,deg,numprocs,&trackwtime);\n\n  MPI_Barrier(MPI_COMM_WORLD);  \n\n  trace_grid_broadcast(myid,deg,n);\n  all_slices_broadcast(myid,max_nb_slices,dim,n);\n  if (v>=1) printf(\"Node %d: construction and broadcast of trace grid and slices... done.\\n\",myid);\n  MPI_Barrier(MPI_COMM_WORLD);  \n\n    \n  fail = track_paths_to_new_slices(myid,nb_slices,deg,dim,n,numprocs,&trackwtime);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n\n  if (myid ==0) run_master(deg, nb_slices+1, max_nb_slices+1, numprocs);\n  else run_slave(deg);\n   \n  MPI_Barrier(MPI_COMM_WORLD);\n  \n  if(myid == 0)\n    wtime = MPI_Wtime() - startwtime;\n  else\n    wtime = trackwtime;\n  MPI_Gather(&wtime,1,MPI_DOUBLE,mytime,1,MPI_DOUBLE,0,MPI_COMM_WORLD);\n  if(myid == 0) {\n    print_time(mytime,numprocs);\n  }\n   \n  MPI_Finalize();  \n  adafinal();\n  return 0;\n}"}
{"program": "germasch_1276", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  int testcase = 0;\n  mrc_params_get_option_int(\"testcase\", &testcase);\n\n  int N;\n  const struct entry **mat;\n\n  switch (testcase) {\n  case 0: N = N_0; mat = mat_0; break;\n  case 1: N = N_1; mat = mat_1; break;\n  default: assert(0);\n  }\n\n  MPI_Comm comm = MPI_COMM_WORLD;\n\n  struct mrc_domain *domain = mrc_domain_create(comm);\n  mrc_domain_set_type(domain, \"simple\");\n  mrc_domain_set_param_int3(domain, \"m\", (int [3]) { N, 1, 1});\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  \n  struct mrc_fld *x = mrc_domain_fld_create(domain, 0, \"x0\");\n  mrc_fld_set_type(x, FLD_TYPE);\n  mrc_fld_setup(x);\n  mrc_fld_view(x);\n  for (int p = 0; p < mrc_fld_nr_patches(x); p++) {\n    mrc_fld_foreach(x, i,j,k, 0, 0) {\n      M3(x, 0, i,0,0, p) = i;\n    } mrc_fld_foreach_end;\n  }\n\n  struct mrc_fld *y = mrc_domain_fld_create(domain, 0, \"y0\");\n  mrc_fld_set_type(y, FLD_TYPE);\n  mrc_fld_setup(y);\n\n  struct mrc_mat *A = mrc_mat_create(comm);\n  mrc_mat_set_type(A, \"csr_mpi\");\n  mrc_mat_set_param_int(A, \"m\", y->_len);\n  mrc_mat_set_param_int(A, \"n\", x->_len);\n  mrc_mat_set_from_options(A);\n  mrc_mat_setup(A);\n  mrc_mat_view(A);\n\n  \n\n  \n\n  \n\n  struct mrc_patch_info info;\n  mrc_domain_get_local_patch_info(domain, 0, &info);\n  int row_off = x->_len * info.global_patch;\n  \n  for (int i = 0; i < x->_len; i++) {\n    MRC_D1(x, i) = i + row_off;\n  }\n  \n  for (int i = 0; i < x->_len; i++) {\n    int row_idx = i + row_off;\n    \n    for (const struct entry *e = mat[row_idx]; e && e->col >= 0; e++) {\n      \n\n      mrc_mat_add_value(A, row_idx, e->col, e->val);\n    }\n  }\n  \n  \n\n  int rank;\n  int _r = 2;\n  \n\n  \n\n  if (testcase == 0 && rank * y->_len <= _r && _r < (rank + 1) * y->_len) {\n    \n\n    \n\n    mrc_mat_add_value(A, _r, 0, 10.0);\n    mrc_mat_add_value(A, _r, 1, 11.0);\n    mrc_mat_add_value(A, _r, 2, 12.0);\n    mrc_mat_add_value(A, _r, 3, 13.0);\n    mrc_mat_add_value(A, _r, 4, 14.0);\n    mrc_mat_add_value(A, _r, 5, 15.0);\n    mrc_mat_add_value(A, _r, 6, 16.0);\n    mrc_mat_add_value(A, _r, 7, 17.0);\n    \n\n    \n\n    \n\n    mrc_mat_add_value(A, _r, 1, -11.0);\n    mrc_mat_add_value(A, _r, 3, -13.0);\n    mrc_mat_add_value(A, _r, 5, -15.0);\n    mrc_mat_add_value(A, _r, 7, -17.0);\n    \n\n    mrc_mat_add_value(A, _r, 0, -10.0);\n    mrc_mat_add_value(A, _r, 2, -12.0);\n    mrc_mat_add_value(A, _r, 4, -14.0);\n    mrc_mat_add_value(A, _r, 6, -16.0);\n  }\n  \n\n  \n  mrc_mat_assemble(A);\n  mrc_mat_print(A);\n\n  mrc_fld_print(x, \"x\");\n\n  mrc_mat_apply(y->_vec, A, x->_vec);\n\n  mrc_fld_print(y, \"y\");\n\n  mrc_fld_destroy(x);\n  mrc_fld_destroy(y);\n  mrc_mat_destroy(A);\n\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  int testcase = 0;\n  mrc_params_get_option_int(\"testcase\", &testcase);\n\n  int N;\n  const struct entry **mat;\n\n  switch (testcase) {\n  case 0: N = N_0; mat = mat_0; break;\n  case 1: N = N_1; mat = mat_1; break;\n  default: assert(0);\n  }\n\n  MPI_Comm comm = MPI_COMM_WORLD;\n\n  struct mrc_domain *domain = mrc_domain_create(comm);\n  mrc_domain_set_type(domain, \"simple\");\n  mrc_domain_set_param_int3(domain, \"m\", (int [3]) { N, 1, 1});\n  mrc_domain_set_from_options(domain);\n  mrc_domain_setup(domain);\n  mrc_domain_view(domain);\n  \n  struct mrc_fld *x = mrc_domain_fld_create(domain, 0, \"x0\");\n  mrc_fld_set_type(x, FLD_TYPE);\n  mrc_fld_setup(x);\n  mrc_fld_view(x);\n  for (int p = 0; p < mrc_fld_nr_patches(x); p++) {\n    mrc_fld_foreach(x, i,j,k, 0, 0) {\n      M3(x, 0, i,0,0, p) = i;\n    } mrc_fld_foreach_end;\n  }\n\n  struct mrc_fld *y = mrc_domain_fld_create(domain, 0, \"y0\");\n  mrc_fld_set_type(y, FLD_TYPE);\n  mrc_fld_setup(y);\n\n  struct mrc_mat *A = mrc_mat_create(comm);\n  mrc_mat_set_type(A, \"csr_mpi\");\n  mrc_mat_set_param_int(A, \"m\", y->_len);\n  mrc_mat_set_param_int(A, \"n\", x->_len);\n  mrc_mat_set_from_options(A);\n  mrc_mat_setup(A);\n  mrc_mat_view(A);\n\n  \n\n  \n\n  \n\n  struct mrc_patch_info info;\n  mrc_domain_get_local_patch_info(domain, 0, &info);\n  int row_off = x->_len * info.global_patch;\n  \n  for (int i = 0; i < x->_len; i++) {\n    MRC_D1(x, i) = i + row_off;\n  }\n  \n  for (int i = 0; i < x->_len; i++) {\n    int row_idx = i + row_off;\n    \n    for (const struct entry *e = mat[row_idx]; e && e->col >= 0; e++) {\n      \n\n      mrc_mat_add_value(A, row_idx, e->col, e->val);\n    }\n  }\n  \n  \n\n  int rank;\n  MPI_Comm_rank(mrc_fld_comm(x), &rank);\n  int _r = 2;\n  \n\n  \n\n  if (testcase == 0 && rank * y->_len <= _r && _r < (rank + 1) * y->_len) {\n    \n\n    \n\n    mrc_mat_add_value(A, _r, 0, 10.0);\n    mrc_mat_add_value(A, _r, 1, 11.0);\n    mrc_mat_add_value(A, _r, 2, 12.0);\n    mrc_mat_add_value(A, _r, 3, 13.0);\n    mrc_mat_add_value(A, _r, 4, 14.0);\n    mrc_mat_add_value(A, _r, 5, 15.0);\n    mrc_mat_add_value(A, _r, 6, 16.0);\n    mrc_mat_add_value(A, _r, 7, 17.0);\n    \n\n    \n\n    \n\n    mrc_mat_add_value(A, _r, 1, -11.0);\n    mrc_mat_add_value(A, _r, 3, -13.0);\n    mrc_mat_add_value(A, _r, 5, -15.0);\n    mrc_mat_add_value(A, _r, 7, -17.0);\n    \n\n    mrc_mat_add_value(A, _r, 0, -10.0);\n    mrc_mat_add_value(A, _r, 2, -12.0);\n    mrc_mat_add_value(A, _r, 4, -14.0);\n    mrc_mat_add_value(A, _r, 6, -16.0);\n  }\n  \n\n  \n  mrc_mat_assemble(A);\n  mrc_mat_print(A);\n\n  mrc_fld_print(x, \"x\");\n\n  mrc_mat_apply(y->_vec, A, x->_vec);\n\n  MPI_Barrier(comm);\n  mrc_fld_print(y, \"y\");\n\n  mrc_fld_destroy(x);\n  mrc_fld_destroy(y);\n  mrc_mat_destroy(A);\n\n  MPI_Finalize();\n}"}
{"program": "JulianKunkel_1277", "code": "int main( int argc, char * argv[] ){\n\tint rank, size;\n\tMPI_File fh;\n\tMPI_Datatype etype;\n\tMPI_Status status;\n\tchar* buf;\n\tMPI_Info info;\n\n\n\t\n\n\tMPI_Comm oddeven;\n\n\n\tint oddevenRank;\n\tprintf(\"%d %d\\n\", rank, oddevenRank);\n\n\tMPI_Comm duplicate;\n\n\tMPI_Group group;\n\n\tMPI_Comm lastDuplicate;\n\n\n\tif (size > 2){\n\t\tint num = rank;\n\n\t\tif (rank == 0){\n\t\t}\n\t\tif (rank == 1){\t\t\t\n\t\t\tMPI_Status status;\n\t\t}\n\t}\n\n\n\n\treturn 0;\n}", "label": "int main( int argc, char * argv[] ){\n\tint rank, size;\n\tMPI_File fh;\n\tMPI_Datatype etype;\n\tMPI_Status status;\n\tchar* buf;\n\tMPI_Info info;\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\tMPI_Comm_size( MPI_COMM_WORLD, &size );\n\n\t\n\n\tMPI_Comm oddeven;\n\tMPI_Comm_split(MPI_COMM_WORLD, rank % 2, 0, & oddeven);\n\n\n\tint oddevenRank;\n\tMPI_Comm_rank( oddeven, & oddevenRank );\n\tprintf(\"%d %d\\n\", rank, oddevenRank);\n\n\tMPI_Comm duplicate;\n\tMPI_Comm_dup( oddeven, & duplicate );\n\n\tMPI_Group group;\n\tMPI_Comm_group( oddeven, & group );\n\n\tMPI_Comm lastDuplicate;\n\tMPI_Comm_create( MPI_COMM_WORLD, group,  &lastDuplicate );\n\n\n\tif (size > 2){\n\t\tint num = rank;\n\n\t\tif (rank == 0){\n\t\t\tMPI_Send( &num, 1, MPI_INT, 1, 4713, MPI_COMM_WORLD);\n\t\t\tMPI_Send( &num, 1, MPI_INT, 1, 4711, MPI_COMM_WORLD);\n\t\t}\n\t\tif (rank == 1){\t\t\t\n\t\t\tMPI_Status status;\n\t\t\tMPI_Recv( &num, 1, MPI_INT, 0, 4713, MPI_COMM_WORLD, & status);\n\t\t\tMPI_Recv( &num, 1, MPI_INT, 0, 4711, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\t\t\t\n\t\t}\n\t}\n\n\tMPI_Barrier( lastDuplicate );\n\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "bmi-forum_1279", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tXML_IO_Handler*\t\tio_handler = XML_IO_Handler_New();\n\t\tDictionary*\t\tdictionary = Dictionary_New();\n\t\tDictionary_Index \tindex;\n\t\tStream*\t\t\tstream = Journal_Register( InfoStream_Type, XML_IO_Handler_Type );\n\t\t\n\t\t\n\n\t\tprintf( \"\\ntest of raw data file:\\n\" );\n\t\tIO_Handler_ReadAllFromFile( io_handler, \"data/rawdata.xml\", dictionary ); \n\n\t\tprintf( \"\\ndictionary now contains:\\n\" );\n\t\tprintf( \"Dictionary:\\n\" );\n\t\tprintf( \"\\tsize: %u\\n\", dictionary->size );\n\t\tprintf( \"\\tdelta: %u\\n\", dictionary->delta );\n\t\tprintf( \"\\tcount: %u\\n\", dictionary->count );\n\t\tprintf( \"\\tentryPtr[0-%u]: {\\n\", dictionary->count );\n\t\tfor( index = 0; index < dictionary->count; index++ ) {\n\t\t\tprintf( \"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary->entryPtr[index], stream ); \n\t\t\tprintf( \"\\n\" );\n\t\t}\n\t\tprintf( \"\\t}\\n\" );\n\n\n\t\t\n\n\n\t\tIO_Handler_WriteAllToFile( io_handler, \"data/newrawdata.xml\", dictionary );\n\t\tStg_Class_Delete( io_handler );\n\t\tStg_Class_Delete( dictionary );\n\t}\n\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tXML_IO_Handler*\t\tio_handler = XML_IO_Handler_New();\n\t\tDictionary*\t\tdictionary = Dictionary_New();\n\t\tDictionary_Index \tindex;\n\t\tStream*\t\t\tstream = Journal_Register( InfoStream_Type, XML_IO_Handler_Type );\n\t\t\n\t\t\n\n\t\tprintf( \"\\ntest of raw data file:\\n\" );\n\t\tIO_Handler_ReadAllFromFile( io_handler, \"data/rawdata.xml\", dictionary ); \n\n\t\tprintf( \"\\ndictionary now contains:\\n\" );\n\t\tprintf( \"Dictionary:\\n\" );\n\t\tprintf( \"\\tsize: %u\\n\", dictionary->size );\n\t\tprintf( \"\\tdelta: %u\\n\", dictionary->delta );\n\t\tprintf( \"\\tcount: %u\\n\", dictionary->count );\n\t\tprintf( \"\\tentryPtr[0-%u]: {\\n\", dictionary->count );\n\t\tfor( index = 0; index < dictionary->count; index++ ) {\n\t\t\tprintf( \"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary->entryPtr[index], stream ); \n\t\t\tprintf( \"\\n\" );\n\t\t}\n\t\tprintf( \"\\t}\\n\" );\n\n\n\t\t\n\n\n\t\tIO_Handler_WriteAllToFile( io_handler, \"data/newrawdata.xml\", dictionary );\n\t\tStg_Class_Delete( io_handler );\n\t\tStg_Class_Delete( dictionary );\n\t}\n\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "TuftsBCB_1280", "code": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS *go = esl_getopts_CreateDefaultApp(options, 0, argc, argv, banner, usage);\n  int          my_rank;\n  int          nproc;\n\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\"))  pause();\n\n\n  utest_oprofileSendRecv(my_rank, nproc);\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS *go = esl_getopts_CreateDefaultApp(options, 0, argc, argv, banner, usage);\n  int          my_rank;\n  int          nproc;\n\n  \n\n  if (esl_opt_GetBoolean(go, \"--stall\"))  pause();\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  utest_oprofileSendRecv(my_rank, nproc);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "syftalent_1281", "code": "int main (int argc, char **argv)\n{\n    int i;\n    ADIO_File fd;\n    ADIO_Offset min_st_offset, max_end_offset;\n    int rank;\n    int nprocs_for_coll;\n    int lb;\n    MPI_Count size, extent;\n\n\n    if (argc != 4) {\n\tif (!rank)\n\t    printf (\"Usage: file_realms_test <number of aggregators> <lower bound> <upper bound>\\n\"\n\t\t    \"    simulates file_realm calculation\\n\");\n\treturn 1;\n    }\n\n    nprocs_for_coll = atoi (argv[1]);\n\n    min_st_offset = atoi (argv[2]);\n    max_end_offset = atoi (argv[3]);\n\n    if (max_end_offset < min_st_offset){\n\tif (!rank)\n\t    printf (\"end offset %lld is less then start offset %lld\\n\",\n\t\t    max_end_offset, min_st_offset);\n\treturn 1;\n    }\n\n    printf (\"min_st_offset = %lld\\nmax_end_offset = %lld\\n\",\n\t    min_st_offset, max_end_offset);\n\n    fd = (ADIO_File) ADIOI_Malloc (sizeof (struct ADIOI_FileD));\n    fd->hints = (ADIOI_Hints *)\n\tADIOI_Malloc (sizeof(struct ADIOI_Hints_struct));\n    fd->hints->cb_nodes = nprocs_for_coll;\n    ADIOI_Calc_file_realms (fd, min_st_offset, max_end_offset);\n\n    for (i=0; i < nprocs_for_coll; i++) {\n\tprintf (\"file_realm_st_offs[%d] = %lld\\n\", i, fd->file_realm_st_offs[i]);\n    }\n    for (i=0; i < nprocs_for_coll; i++) {\n\tprintf (\"file_realm [%d] size = %d\\n\", i, size);\n    }\n    for (i=0; i < nprocs_for_coll; i++) {\n\tprintf (\"file_realm [%d] extent = %d\\n\", i, extent);\n    }\n\n    for (i=0; i < nprocs_for_coll; i++)\n    ADIOI_Free (fd->file_realm_st_offs);\n    ADIOI_Free (fd->file_realm_types);\n    ADIOI_Free (fd->hints);\n    ADIOI_Free (fd);\n\n\n    return 0;\n}", "label": "int main (int argc, char **argv)\n{\n    int i;\n    ADIO_File fd;\n    ADIO_Offset min_st_offset, max_end_offset;\n    int rank;\n    int nprocs_for_coll;\n    int lb;\n    MPI_Count size, extent;\n\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n\n    if (argc != 4) {\n\tif (!rank)\n\t    printf (\"Usage: file_realms_test <number of aggregators> <lower bound> <upper bound>\\n\"\n\t\t    \"    simulates file_realm calculation\\n\");\n\tMPI_Finalize();\n\treturn 1;\n    }\n\n    nprocs_for_coll = atoi (argv[1]);\n\n    min_st_offset = atoi (argv[2]);\n    max_end_offset = atoi (argv[3]);\n\n    if (max_end_offset < min_st_offset){\n\tif (!rank)\n\t    printf (\"end offset %lld is less then start offset %lld\\n\",\n\t\t    max_end_offset, min_st_offset);\n\tMPI_Finalize();\n\treturn 1;\n    }\n\n    printf (\"min_st_offset = %lld\\nmax_end_offset = %lld\\n\",\n\t    min_st_offset, max_end_offset);\n\n    fd = (ADIO_File) ADIOI_Malloc (sizeof (struct ADIOI_FileD));\n    fd->hints = (ADIOI_Hints *)\n\tADIOI_Malloc (sizeof(struct ADIOI_Hints_struct));\n    fd->hints->cb_nodes = nprocs_for_coll;\n    ADIOI_Calc_file_realms (fd, min_st_offset, max_end_offset);\n\n    for (i=0; i < nprocs_for_coll; i++) {\n\tprintf (\"file_realm_st_offs[%d] = %lld\\n\", i, fd->file_realm_st_offs[i]);\n    }\n    for (i=0; i < nprocs_for_coll; i++) {\n\tMPI_Type_size_x (fd->file_realm_types[i], &size);\n\tprintf (\"file_realm [%d] size = %d\\n\", i, size);\n    }\n    for (i=0; i < nprocs_for_coll; i++) {\n\tMPI_Type_get_extent (fd->file_realm_types[i], &lb, &extent);\n\tprintf (\"file_realm [%d] extent = %d\\n\", i, extent);\n    }\n\n    for (i=0; i < nprocs_for_coll; i++)\n\tMPI_Type_free (&fd->file_realm_types[i]);\n    ADIOI_Free (fd->file_realm_st_offs);\n    ADIOI_Free (fd->file_realm_types);\n    ADIOI_Free (fd->hints);\n    ADIOI_Free (fd);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mnakao_1282", "code": "int main(int argc, char **argv){\n  int i, me, target;\n  unsigned int size;\n  double t, t_max;\n  MPI_Win win;\n\n\n  target = 1 - me;\n\n  init_buf(send_buf, me);\n\n  if(me==0) print_items();\n  \n  for(size=1;size<MAX_SIZE+1;size*=2){\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n\tt = wtime();\n\n      if(me == 0){\n        while(send_buf[0] == '0' || send_buf[size-1] == '0'){}  \n\n        send_buf[0] = '0'; send_buf[size-1] = '0';              \n\n      } \n      else {\n\twhile(send_buf[0] == '1' || send_buf[size-1] == '1'){}  \n\n\tsend_buf[0] = '1'; send_buf[size-1] = '1';              \n\n      } \n    } \n\n\n    t = wtime() - t;\n    if(me == 0)\n      print_results(size, t_max);\n  }\n\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int i, me, target;\n  unsigned int size;\n  double t, t_max;\n  MPI_Win win;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Win_create(&send_buf, sizeof(char)*MAX_SIZE, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n  MPI_Win_fence(0, win);\n  target = 1 - me;\n\n  init_buf(send_buf, me);\n\n  if(me==0) print_items();\n  \n  for(size=1;size<MAX_SIZE+1;size*=2){\n    MPI_Barrier(MPI_COMM_WORLD);\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n\tt = wtime();\n\n      if(me == 0){\n\tMPI_Put(send_buf, size, MPI_CHAR, target, 0, size, MPI_CHAR, win);\t\n\tMPI_Win_fence(0, win);\n\tMPI_Win_fence(0, win);\n        while(send_buf[0] == '0' || send_buf[size-1] == '0'){}  \n\n        send_buf[0] = '0'; send_buf[size-1] = '0';              \n\n      } \n      else {\n\tMPI_Win_fence(0, win);\n\twhile(send_buf[0] == '1' || send_buf[size-1] == '1'){}  \n\n\tsend_buf[0] = '1'; send_buf[size-1] = '1';              \n\n\tMPI_Put(send_buf, size, MPI_CHAR, target, 0, size, MPI_CHAR, win);\n\tMPI_Win_fence(0, win);\n      } \n    } \n\n\n    t = wtime() - t;\n    MPI_Reduce(&t, &t_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n    if(me == 0)\n      print_results(size, t_max);\n  }\n\n  MPI_Win_free(&win);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "UW-Hydro_1283", "code": "int\nmain(int    argc,\n     char **argv)\n{\n    int          status;\n    timer_struct global_timers[N_TIMERS];\n    char         state_filename[MAXSTRING];\n\n    \n\n    timer_start(&(global_timers[TIMER_VIC_ALL]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_INIT]));\n\n    \n\n    status =\n    if (status != MPI_SUCCESS) {\n        exit(EXIT_FAILURE);\n    }\n\n    \n\n    initialize_log();\n\n    \n\n    initialize_mpi();\n\n    \n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        cmd_proc(argc, argv, filenames.global);\n    }\n\n    \n\n    vic_image_start();\n\n    \n\n    vic_alloc();\n\n    \n\n    rout_alloc();   \n\n\n    \n\n    vic_image_init();\n\n    \n\n    rout_init();    \n\n\n    \n\n    vic_populate_model_state(&(dmy[0]));\n\n    \n\n    vic_init_output(&(dmy[0]));\n\n    \n\n    log_info(\n        \"Initialization is complete, print global param, parameters and options structures\");\n    print_global_param(&global_param);\n    print_option(&options);\n    print_parameters(&param);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_INIT]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_RUN]));\n\n    \n\n    for (current = 0; current < global_param.nrecs; current++) {\n        \n\n        timer_continue(&(global_timers[TIMER_VIC_FORCE]));\n        vic_force();\n        timer_stop(&(global_timers[TIMER_VIC_FORCE]));\n\n        \n\n        vic_image_run(&(dmy[current]));\n\n        \n\n        timer_continue(&(global_timers[TIMER_VIC_WRITE]));\n        vic_write_output(&(dmy[current]));\n        timer_stop(&(global_timers[TIMER_VIC_WRITE]));\n\n        \n\n        if (check_save_state_flag(current, &dmy_state)) {\n            debug(\"writing state file for timestep %zu\", current);\n            vic_store(&dmy_state, state_filename);\n            debug(\"finished storing state file: %s\", state_filename)\n        }\n    }\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_RUN]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    vic_image_finalize();\n\n    \n\n    rout_finalize();    \n\n\n    \n\n    status =\n    if (status != MPI_SUCCESS) {\n        log_err(\"MPI error: %d\", status);\n    }\n\n    log_info(\"Completed running VIC %s\", VIC_DRIVER);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_ALL]));\n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        \n\n        write_vic_timing_table(global_timers, VIC_DRIVER);\n    }\n\n    return EXIT_SUCCESS;\n}", "label": "int\nmain(int    argc,\n     char **argv)\n{\n    int          status;\n    timer_struct global_timers[N_TIMERS];\n    char         state_filename[MAXSTRING];\n\n    \n\n    timer_start(&(global_timers[TIMER_VIC_ALL]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_INIT]));\n\n    \n\n    status = MPI_Init(&argc, &argv);\n    if (status != MPI_SUCCESS) {\n        fprintf(stderr, \"MPI error in main(): %d\\n\", status);\n        exit(EXIT_FAILURE);\n    }\n\n    \n\n    initialize_log();\n\n    \n\n    initialize_mpi();\n\n    \n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        cmd_proc(argc, argv, filenames.global);\n    }\n\n    \n\n    vic_image_start();\n\n    \n\n    vic_alloc();\n\n    \n\n    rout_alloc();   \n\n\n    \n\n    vic_image_init();\n\n    \n\n    rout_init();    \n\n\n    \n\n    vic_populate_model_state(&(dmy[0]));\n\n    \n\n    vic_init_output(&(dmy[0]));\n\n    \n\n    log_info(\n        \"Initialization is complete, print global param, parameters and options structures\");\n    print_global_param(&global_param);\n    print_option(&options);\n    print_parameters(&param);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_INIT]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_RUN]));\n\n    \n\n    for (current = 0; current < global_param.nrecs; current++) {\n        \n\n        timer_continue(&(global_timers[TIMER_VIC_FORCE]));\n        vic_force();\n        timer_stop(&(global_timers[TIMER_VIC_FORCE]));\n\n        \n\n        vic_image_run(&(dmy[current]));\n\n        \n\n        timer_continue(&(global_timers[TIMER_VIC_WRITE]));\n        vic_write_output(&(dmy[current]));\n        timer_stop(&(global_timers[TIMER_VIC_WRITE]));\n\n        \n\n        if (check_save_state_flag(current, &dmy_state)) {\n            debug(\"writing state file for timestep %zu\", current);\n            vic_store(&dmy_state, state_filename);\n            debug(\"finished storing state file: %s\", state_filename)\n        }\n    }\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_RUN]));\n    \n\n    timer_start(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    vic_image_finalize();\n\n    \n\n    rout_finalize();    \n\n\n    \n\n    status = MPI_Finalize();\n    if (status != MPI_SUCCESS) {\n        log_err(\"MPI error: %d\", status);\n    }\n\n    log_info(\"Completed running VIC %s\", VIC_DRIVER);\n\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_FINAL]));\n    \n\n    timer_stop(&(global_timers[TIMER_VIC_ALL]));\n\n    if (mpi_rank == VIC_MPI_ROOT) {\n        \n\n        write_vic_timing_table(global_timers, VIC_DRIVER);\n    }\n\n    return EXIT_SUCCESS;\n}"}
{"program": "ClaudioNahmad_1284", "code": "int main(int argc, char ** argv)\n{\n    MPI_Info info = MPI_INFO_NULL;\n    MPI_File fh;\n    MPI_Offset off=0;\n    MPI_Status status;\n    int errcode;\n    int i, rank, errs=0, toterrs, buffer[BUFSIZE], buf2[BUFSIZE];\n\n\n\n\n    for (i=0; i<BUFSIZE; i++) {\n        buffer[i] = 10000+rank;\n    }\n    off = rank*sizeof(buffer);\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_open\");\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_write_at_all\");\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_close\");\n\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_open\");\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_read_at_all\");\n    errcode =\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_close\");\n\n    for (i=0; i<BUFSIZE; i++) {\n        if (buf2[i] != 10000+rank)\n\t    errs++;\n    }\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n    MPI_Info info = MPI_INFO_NULL;\n    MPI_File fh;\n    MPI_Offset off=0;\n    MPI_Status status;\n    int errcode;\n    int i, rank, errs=0, toterrs, buffer[BUFSIZE], buf2[BUFSIZE];\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Info_create(&info);\n    MPI_Info_set(info, \"romio_cb_write\", \"enable\");\n    MPI_Info_set(info, \"cb_nodes\", \"1\");\n\n    for (i=0; i<BUFSIZE; i++) {\n        buffer[i] = 10000+rank;\n    }\n    off = rank*sizeof(buffer);\n\n    errcode = MPI_File_open(MPI_COMM_WORLD, argv[1],\n\t\tMPI_MODE_WRONLY|MPI_MODE_CREATE, info, &fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_open\");\n    errcode = MPI_File_write_at_all(fh, off, buffer, BUFSIZE,\n\t\tMPI_INT,  &status);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_write_at_all\");\n    errcode = MPI_File_close(&fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_close\");\n\n    errcode = MPI_File_open(MPI_COMM_WORLD, argv[1],\n\t\tMPI_MODE_RDONLY, info, &fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_open\");\n    errcode = MPI_File_read_at_all(fh, off, buf2, BUFSIZE,\n\t\tMPI_INT,  &status);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_read_at_all\");\n    errcode = MPI_File_close(&fh);\n    if (errcode != MPI_SUCCESS) handle_error(errcode, \"MPI_File_close\");\n\n    for (i=0; i<BUFSIZE; i++) {\n        if (buf2[i] != 10000+rank)\n\t    errs++;\n    }\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n    MPI_Info_free(&info);\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mom-ocean_1287", "code": "void main(int argc, char **argv) {\n#ifdef _OPENMP\n#pragma omp parallel\n {\n   int thrnum = omp_get_thread_num();\n   printf( \"pe=%d thrnum=%d mld=%d\\n\", pe, thrnum, mld_id_() );\n }\n#endif\n  printf( \"pe=%d mld=%d\\n\", pe, mld_id_() );\n}", "label": "void main(int argc, char **argv) {\n  MPI_Init( &argc, &argv );\n  MPI_Comm_rank( MPI_COMM_WORLD, &pe );\n  MPI_Comm_size( MPI_COMM_WORLD, &npes );\n#ifdef _OPENMP\n#pragma omp parallel\n {\n   int thrnum = omp_get_thread_num();\n   printf( \"pe=%d thrnum=%d mld=%d\\n\", pe, thrnum, mld_id_() );\n }\n#endif\n  printf( \"pe=%d mld=%d\\n\", pe, mld_id_() );\n  MPI_Finalize();\n}"}
{"program": "linhbngo_1288", "code": "int main(int argc, char* argv[]){\n\n  int rawNum[N];\n  int sortNum[N];\n  int* local_bucket;\n  int rank,size;\n  int* proc_count;\n  int* disp;\n  MPI_Status status;\n  int i,j,counter;\n  int local_min,local_max;\n  int tmp;\n\n    \n  if (rank == 0){\n    \n\n    for (i = 0; i < N; i++){\n      rawNum[i] = rand() % N;\n    }\n  }\n\n  \n\n\n  \n\n  counter = 0;\n  local_min = rank * (N/size);\n  local_max = (rank + 1) * (N/size);  \n  for (i = 0; i < N; i++){\n    if ((rawNum[i] >= local_min) && (rawNum[i] < local_max)){\n      counter += 1;\n    }\n  }    \n    \n  printf(\"For rank %d, max is %d, min is %d, and there are %d elements in rawNum that falls within max and min \\n\",\n         rank,local_max,local_min,counter);\n\n\n  \n  \n  local_bucket = malloc(counter * sizeof(int));\n  counter = 0;\n  for (i = 0; i < N; i++){\n    if ((rawNum[i] >= local_min) && (rawNum[i] < local_max)){\n      local_bucket[counter] = rawNum[i];\n      counter += 1;\n    }\n  }\n\n  \n\n  for (i = 0; i < counter; i++){\n    for (j = i+1; j < counter; j++){\n      if (local_bucket[i] > local_bucket[j]){\n        tmp = local_bucket[i];\n        local_bucket[i] = local_bucket[j];\n        local_bucket[j] = tmp;\n      }\n    }\n  }\n\n\n  for (i = 0; i < counter; i++){\n    printf(\"%d %d \\n\",rank,local_bucket[i]);\n  }\n\n  \n\n  if (rank == 0){\n    proc_count = malloc(size * sizeof(int));\n    disp = malloc(size * sizeof(int));\n  }\n\n  \n\n\n  if (rank == 0){\n    disp[0] = 0;\n    for (i = 0; i < size-1; i++){\n      disp[i+1] = disp[i] + proc_count[i];\n    }\n  }\n\n  \n\n\n  if (rank == 0){\n    printf(\"Before sort: \\n\");\n    for (i = 0; i < N; i++) printf(\"%d \",rawNum[i]);\n    printf(\"\\nAfter sort: \\n\");\n    for (i = 0; i < N; i++) printf(\"%d \",sortNum[i]);\n  }\n\n  return 0;\n}", "label": "int main(int argc, char* argv[]){\n\n  int rawNum[N];\n  int sortNum[N];\n  int* local_bucket;\n  int rank,size;\n  int* proc_count;\n  int* disp;\n  MPI_Status status;\n  int i,j,counter;\n  int local_min,local_max;\n  int tmp;\n\n  MPI_Init(&argc,&argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n    \n  if (rank == 0){\n    \n\n    for (i = 0; i < N; i++){\n      rawNum[i] = rand() % N;\n    }\n  }\n\n  \n\n  MPI_Bcast(rawNum, N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  \n\n  counter = 0;\n  local_min = rank * (N/size);\n  local_max = (rank + 1) * (N/size);  \n  for (i = 0; i < N; i++){\n    if ((rawNum[i] >= local_min) && (rawNum[i] < local_max)){\n      counter += 1;\n    }\n  }    \n    \n  printf(\"For rank %d, max is %d, min is %d, and there are %d elements in rawNum that falls within max and min \\n\",\n         rank,local_max,local_min,counter);\n\n\n  \n  \n  local_bucket = malloc(counter * sizeof(int));\n  counter = 0;\n  for (i = 0; i < N; i++){\n    if ((rawNum[i] >= local_min) && (rawNum[i] < local_max)){\n      local_bucket[counter] = rawNum[i];\n      counter += 1;\n    }\n  }\n\n  \n\n  for (i = 0; i < counter; i++){\n    for (j = i+1; j < counter; j++){\n      if (local_bucket[i] > local_bucket[j]){\n        tmp = local_bucket[i];\n        local_bucket[i] = local_bucket[j];\n        local_bucket[j] = tmp;\n      }\n    }\n  }\n\n\n  for (i = 0; i < counter; i++){\n    printf(\"%d %d \\n\",rank,local_bucket[i]);\n  }\n\n  \n\n  if (rank == 0){\n    proc_count = malloc(size * sizeof(int));\n    disp = malloc(size * sizeof(int));\n  }\n\n  \n\n  MPI_Gather(&counter,1,MPI_INT,proc_count,1,MPI_INT,0,MPI_COMM_WORLD);\n\n  if (rank == 0){\n    disp[0] = 0;\n    for (i = 0; i < size-1; i++){\n      disp[i+1] = disp[i] + proc_count[i];\n    }\n  }\n\n  \n\n  MPI_Gatherv(local_bucket,counter,MPI_INT,sortNum,proc_count,disp,MPI_INT,0,MPI_COMM_WORLD);\n\n  if (rank == 0){\n    printf(\"Before sort: \\n\");\n    for (i = 0; i < N; i++) printf(\"%d \",rawNum[i]);\n    printf(\"\\nAfter sort: \\n\");\n    for (i = 0; i < N; i++) printf(\"%d \",sortNum[i]);\n  }\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "TuftsBCB_1289", "code": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS    *go      = esl_getopts_CreateDefaultApp(options, 1, argc, argv, banner, usage);\n  char           *hmmfile = esl_opt_GetArg(go, 1);\n  ESL_ALPHABET   *abc     = esl_alphabet_Create(eslAMINO);\n  P7_BG          *bg      = p7_bg_Create(abc);\n  int             my_rank;\n  int             nproc;\n  char           *buf    = NULL;\n  int             nbuf   = 0;\n  int             subtotalM = 0;\n  int             allM   = 0;\n  int             stalling = esl_opt_GetBoolean(go, \"--stall\");\n\n\n  while (stalling); \n\n  \n\n  if (my_rank == 0) \n    {\n      ESL_STOPWATCH  *w       = esl_stopwatch_Create();\n      P7_HMMFILE     *hfp     = NULL;\n      P7_OPROFILE    *om      = NULL;\n      P7_HMM         *hmm     = NULL;\n\n      \n\n      if (p7_hmmfile_Open(hmmfile, NULL, &hfp) != eslOK) p7_Fail(\"Failed to open HMM file %s\", hmmfile);\n\n      esl_stopwatch_Start(w);\n      while (p7_oprofile_ReadMSV(hfp, &abc, &om)  == eslOK &&\n\t     p7_oprofile_ReadRest(hfp, om)       == eslOK)\n\t{\n\t  if (!esl_opt_GetBoolean(go, \"-b\"))\n\t    p7_oprofile_MPISend(om, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n\n\t  p7_hmm_Destroy(hmm);\n\t  p7_oprofile_Destroy(om);\n\t}\n      p7_oprofile_MPISend(NULL, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n\n      printf(\"total: %d\\n\", allM);\n      esl_stopwatch_Stop(w);\n      esl_stopwatch_Display(stdout, w, \"CPU Time: \");\n      esl_stopwatch_Destroy(w);\n    }\n  \n\n  else \n    {\n      P7_OPROFILE     *om_recd = NULL;      \n\n      while (p7_oprofile_MPIRecv(0, 0, MPI_COMM_WORLD, &buf, &nbuf, &abc, &om_recd) == eslOK) \n\t{\n\t  subtotalM += om_recd->M;\n\t  p7_oprofile_Destroy(om_recd);  \n\t}\n    }\n\n  free(buf);\n  p7_bg_Destroy(bg);\n  esl_alphabet_Destroy(abc);\n  esl_getopts_Destroy(go);\n  exit(0);\n}", "label": "int\nmain(int argc, char **argv)\n{\n  ESL_GETOPTS    *go      = esl_getopts_CreateDefaultApp(options, 1, argc, argv, banner, usage);\n  char           *hmmfile = esl_opt_GetArg(go, 1);\n  ESL_ALPHABET   *abc     = esl_alphabet_Create(eslAMINO);\n  P7_BG          *bg      = p7_bg_Create(abc);\n  int             my_rank;\n  int             nproc;\n  char           *buf    = NULL;\n  int             nbuf   = 0;\n  int             subtotalM = 0;\n  int             allM   = 0;\n  int             stalling = esl_opt_GetBoolean(go, \"--stall\");\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  while (stalling); \n\n  \n\n  if (my_rank == 0) \n    {\n      ESL_STOPWATCH  *w       = esl_stopwatch_Create();\n      P7_HMMFILE     *hfp     = NULL;\n      P7_OPROFILE    *om      = NULL;\n      P7_HMM         *hmm     = NULL;\n\n      \n\n      if (p7_hmmfile_Open(hmmfile, NULL, &hfp) != eslOK) p7_Fail(\"Failed to open HMM file %s\", hmmfile);\n\n      esl_stopwatch_Start(w);\n      while (p7_oprofile_ReadMSV(hfp, &abc, &om)  == eslOK &&\n\t     p7_oprofile_ReadRest(hfp, om)       == eslOK)\n\t{\n\t  if (!esl_opt_GetBoolean(go, \"-b\"))\n\t    p7_oprofile_MPISend(om, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n\n\t  p7_hmm_Destroy(hmm);\n\t  p7_oprofile_Destroy(om);\n\t}\n      p7_oprofile_MPISend(NULL, 1, 0, MPI_COMM_WORLD, &buf, &nbuf); \n\n      MPI_Reduce(&subtotalM, &allM, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n      printf(\"total: %d\\n\", allM);\n      esl_stopwatch_Stop(w);\n      esl_stopwatch_Display(stdout, w, \"CPU Time: \");\n      esl_stopwatch_Destroy(w);\n    }\n  \n\n  else \n    {\n      P7_OPROFILE     *om_recd = NULL;      \n\n      while (p7_oprofile_MPIRecv(0, 0, MPI_COMM_WORLD, &buf, &nbuf, &abc, &om_recd) == eslOK) \n\t{\n\t  subtotalM += om_recd->M;\n\t  p7_oprofile_Destroy(om_recd);  \n\t}\n      MPI_Reduce(&subtotalM, &allM, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n  free(buf);\n  p7_bg_Destroy(bg);\n  esl_alphabet_Destroy(abc);\n  esl_getopts_Destroy(go);\n  MPI_Finalize();\n  exit(0);\n}"}
{"program": "joeladams_1291", "code": "int main(int argc, char** argv) {\n    const int MAX = 8;\n    int* arrSend = NULL;\n    int* arrRcv = NULL;\n    int numProcs = -1, myRank = -1, numSent = -1;\n\n\n\n    if (myRank == 0) {                                 \n\n        arrSend = (int*) malloc( MAX * sizeof(int) );  \n\n        for (int i = 0; i < MAX; i++) {                \n\n            arrSend[i] = (i+1) * 11;\n        }\n        print(myRank, \"arrSend\", arrSend, MAX);        \n\n    }\n     \n    numSent = MAX / numProcs;                          \n\n    arrRcv = (int*) malloc( numSent * sizeof(int) );   \n\n\n\n\n    print(myRank, \"arrRcv\", arrRcv, numSent);          \n\n\n    free(arrSend);                                     \n\n    free(arrRcv);\n    return 0;\n}", "label": "int main(int argc, char** argv) {\n    const int MAX = 8;\n    int* arrSend = NULL;\n    int* arrRcv = NULL;\n    int numProcs = -1, myRank = -1, numSent = -1;\n\n    MPI_Init(&argc, &argv);                            \n\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    if (myRank == 0) {                                 \n\n        arrSend = (int*) malloc( MAX * sizeof(int) );  \n\n        for (int i = 0; i < MAX; i++) {                \n\n            arrSend[i] = (i+1) * 11;\n        }\n        print(myRank, \"arrSend\", arrSend, MAX);        \n\n    }\n     \n    numSent = MAX / numProcs;                          \n\n    arrRcv = (int*) malloc( numSent * sizeof(int) );   \n\n\n    MPI_Scatter(arrSend, numSent, MPI_INT, arrRcv,     \n\n                 numSent, MPI_INT, 0, MPI_COMM_WORLD); \n\n\n    print(myRank, \"arrRcv\", arrRcv, numSent);          \n\n\n    free(arrSend);                                     \n\n    free(arrRcv);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "syftalent_1292", "code": "int main( int argc, char **argv )\n{\n    int              rank, size, i;\n    int              data;\n    int              errors=0;\n    int              result = -100;\n    int              correct_result;\n    MPI_Op           op_assoc, op_addem;\n    MPI_Comm comm=MPI_COMM_WORLD;\n    \n\n    \n\n\n\n    data = rank;\n\t\n    correct_result = 0;\n    for (i=0;i<=rank;i++)\n        correct_result += i;\n    \n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error suming ints with scan\\n\", rank );\n        errors++;\n    }\n\n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error summing ints with scan (2)\\n\", rank );\n        errors++;\n    }\n    \n    data = rank;\n    result = -100;\n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error summing ints with scan (userop)\\n\", \n                 rank );\n        errors++;\n    }\n    \n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error summing ints with scan (userop2)\\n\", \n                 rank );\n        errors++;\n    }\n    result = -100;\n    data = rank;\n    if (result == BAD_ANSWER) {\n        fprintf( stderr, \"[%d] Error scanning with non-commutative op\\n\",\n                 rank );\n        errors++;\n    }\n\n#if MTEST_HAVE_MIN_MPI_VERSION(2,2)\n    if (MPI_SUCCESS == MPI_Scan( &data, &data, 1, MPI_INT, op_assoc, comm))         errors++;\n#endif\n\n    \n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n\n    return errors;\n}", "label": "int main( int argc, char **argv )\n{\n    int              rank, size, i;\n    int              data;\n    int              errors=0;\n    int              result = -100;\n    int              correct_result;\n    MPI_Op           op_assoc, op_addem;\n    MPI_Comm comm=MPI_COMM_WORLD;\n    \n    MPI_Init( &argc, &argv );\n    MPI_Op_create( (MPI_User_function *)assoc, 0, &op_assoc );\n    MPI_Op_create( (MPI_User_function *)addem, 1, &op_addem );\n\n    \n\n\n    MPI_Comm_rank( comm, &rank );\n    MPI_Comm_size( comm, &size );\n\n    data = rank;\n\t\n    correct_result = 0;\n    for (i=0;i<=rank;i++)\n        correct_result += i;\n    \n    MPI_Scan ( &data, &result, 1, MPI_INT, MPI_SUM, comm );\n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error suming ints with scan\\n\", rank );\n        errors++;\n    }\n\n    MPI_Scan ( &data, &result, 1, MPI_INT, MPI_SUM, comm );\n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error summing ints with scan (2)\\n\", rank );\n        errors++;\n    }\n    \n    data = rank;\n    result = -100;\n    MPI_Scan ( &data, &result, 1, MPI_INT, op_addem, comm );\n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error summing ints with scan (userop)\\n\", \n                 rank );\n        errors++;\n    }\n    \n    MPI_Scan ( &data, &result, 1, MPI_INT, op_addem, comm );\n    if (result != correct_result) {\n        fprintf( stderr, \"[%d] Error summing ints with scan (userop2)\\n\", \n                 rank );\n        errors++;\n    }\n    result = -100;\n    data = rank;\n    MPI_Scan ( &data, &result, 1, MPI_INT, op_assoc, comm );\n    if (result == BAD_ANSWER) {\n        fprintf( stderr, \"[%d] Error scanning with non-commutative op\\n\",\n                 rank );\n        errors++;\n    }\n\n#if MTEST_HAVE_MIN_MPI_VERSION(2,2)\n    MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n    if (MPI_SUCCESS == MPI_Scan( &data, &data, 1, MPI_INT, op_assoc, comm))\n        errors++;\n#endif\n\n    MPI_Op_free( &op_assoc );\n    MPI_Op_free( &op_addem );\n    \n    MPI_Finalize();\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n\n    return errors;\n}"}
{"program": "joao-lima_1293", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 1);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\n\tstarpu_vector_data_register(&token_handle, STARPU_MAIN_RAM, (uintptr_t)&token, 1, sizeof(token));\n\n\tint nloops = NITER;\n\tint loop;\n\n\tint last_loop = nloops - 1;\n\tint last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (loop == 0 && rank == 0)\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tFPRINTF(stdout, \"Start with token value %u\\n\", token);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstarpu_mpi_irecv_detached(token_handle, (rank+size-1)%size, tag, MPI_COMM_WORLD, NULL, NULL);\n\t\t}\n\n\t\tincrement_token();\n\n\t\tif (loop == last_loop && rank == last_rank)\n\t\t{\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tFPRINTF(stdout, \"Finished : token value %u\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstarpu_mpi_isend_detached(token_handle, (rank+1)%size, tag+1, MPI_COMM_WORLD, NULL, NULL);\n\t\t}\n\t}\n\n\tstarpu_task_wait_for_all();\n\n\tstarpu_data_unregister(token_handle);\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tif (rank == last_rank)\n\t{\n\t\tFPRINTF(stderr, \"[%d] token = %u == %u * %d ?\\n\", rank, token, nloops, size);\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 1);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need at least 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\n\tstarpu_vector_data_register(&token_handle, STARPU_MAIN_RAM, (uintptr_t)&token, 1, sizeof(token));\n\n\tint nloops = NITER;\n\tint loop;\n\n\tint last_loop = nloops - 1;\n\tint last_rank = size - 1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tint tag = loop*size + rank;\n\n\t\tif (loop == 0 && rank == 0)\n\t\t{\n\t\t\ttoken = 0;\n\t\t\tFPRINTF(stdout, \"Start with token value %u\\n\", token);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstarpu_mpi_irecv_detached(token_handle, (rank+size-1)%size, tag, MPI_COMM_WORLD, NULL, NULL);\n\t\t}\n\n\t\tincrement_token();\n\n\t\tif (loop == last_loop && rank == last_rank)\n\t\t{\n\t\t\tstarpu_data_acquire(token_handle, STARPU_R);\n\t\t\tFPRINTF(stdout, \"Finished : token value %u\\n\", token);\n\t\t\tstarpu_data_release(token_handle);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tstarpu_mpi_isend_detached(token_handle, (rank+1)%size, tag+1, MPI_COMM_WORLD, NULL, NULL);\n\t\t}\n\t}\n\n\tstarpu_task_wait_for_all();\n\n\tstarpu_data_unregister(token_handle);\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tif (rank == last_rank)\n\t{\n\t\tFPRINTF(stderr, \"[%d] token = %u == %u * %d ?\\n\", rank, token, nloops, size);\n\t\tSTARPU_ASSERT(token == nloops*size);\n\t}\n\n\treturn 0;\n}\n"}
{"program": "90jrong_1294", "code": "int\nmain(int argc, char** argv)\n{\n   int ncid,varid;\n   int retval;\n\n   printf(\"\\n*** Testing illegal mode combinations\\n\");\n\n\n   printf(\"*** Testing create + MPIO + fletcher32\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_fletcher32(ncid,varid,NC_FLETCHER32);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n   printf(\"*** Testing create + MPIO + deflation\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_deflate(ncid,varid, NC_NOSHUFFLE, 1, 1);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n\n   SUMMARIZE_ERR;\n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char** argv)\n{\n   int ncid,varid;\n   int retval;\n\n   printf(\"\\n*** Testing illegal mode combinations\\n\");\n\n   MPI_Init(&argc,&argv);\n\n   printf(\"*** Testing create + MPIO + fletcher32\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_fletcher32(ncid,varid,NC_FLETCHER32);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n   printf(\"*** Testing create + MPIO + deflation\\n\");\n   if ((retval = nc_create_par(FILE_NAME, NC_CLOBBER|NC_NETCDF4, MPI_COMM_WORLD, MPI_INFO_NULL, &ncid))) ERR;\n   if ((retval = nc_def_var(ncid,\"whatever\",NC_INT,0,NULL,&varid))) ERR;\n   retval = nc_def_var_deflate(ncid,varid, NC_NOSHUFFLE, 1, 1);\n   if(retval != NC_EINVAL) ERR;\n   if ((retval = nc_abort(ncid))) ERR;\n\n   MPI_Finalize();\n\n   SUMMARIZE_ERR;\n   FINAL_RESULTS;\n}"}
{"program": "mpip_1299", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(4, ni, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_real(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 15; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft_r2c(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft_c2r(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft_r2c(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft_c2r(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(4, ni, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bmi-forum_1300", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tStream* stream;\n\t\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\n\tstream = Journal_Register( Info_Type, Dictionary_Type );\n\tstJournal->firewallProducesAssert = False;\n\tStream_RedirectFile(Journal_Register( Error_Type, \"DictionaryCheck\"), \"DictionaryCheck.txt\");\n\tStream_RedirectFile(stream, \"DictionaryCheck.txt\");\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tDictionary*\t\t\tdictionary = Dictionary_New();\n\t\tDictionary*\t\t\tdictionary2 = Dictionary_New();\n\t\t\n\t\tDictionary_Index\tindex;\n\t\tchar*\t\t\terrMessage = \"Component dictionary must have unique names\\n\";\n\t\t\n\t\t\n\n\t\t\n\n\t\tDictionary_Add( dictionary, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\tDictionary_Add( dictionary, \"test_dict_double\",\n\t\t\tDictionary_Entry_Value_FromDouble( 45.567 ) );\n\t\tDictionary_Add( dictionary, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"goodbye\" ) );\t\n\t\tDictionary_Add( dictionary, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\tDictionary_Add( dictionary, \"test_dict_string2\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\t\n\t\t\n\n\t\tDictionary_Add( dictionary2, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\tDictionary_Add( dictionary2, \"test_dict_double\",\n\t\t\tDictionary_Entry_Value_FromDouble( 45.567 ) );\n\t\tDictionary_Add( dictionary2, \"test_dict_stuff\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\") );\n\n\t\t\n\n\t\tJournal_Printf(stream, \"Dictionary:\\n\" );\n\t\tJournal_Printf(stream, \"\\tsize: %u\\n\", dictionary->size );\n\t\tJournal_Printf(stream, \"\\tdelta: %u\\n\", dictionary->delta );\n\t\tJournal_Printf(stream, \"\\tcount: %u\\n\", dictionary->count );\n\t\tJournal_Printf(stream, \"\\tentryPtr[0-%u]: {\\n\", dictionary->count );\n\t\tfor( index = 0; index < dictionary->count; index++ ) {\n\t\t\t\n\t\t\tJournal_Printf(stream,\"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary->entryPtr[index], stream ); \n\t\t\tJournal_Printf(stream, \"\\n\" );\n\t\t}\n\t\tJournal_Printf(stream, \"\\t}\\n\" );\n\t\t\n\t\tJournal_Printf(stream, \"Dictionary 2:\\n\" );\n\t\tJournal_Printf(stream, \"\\tsize: %u\\n\", dictionary2->size );\n\t\tJournal_Printf(stream, \"\\tdelta: %u\\n\", dictionary2->delta );\n\t\tJournal_Printf(stream, \"\\tcount: %u\\n\", dictionary2->count );\n\t\tJournal_Printf(stream, \"\\tentryPtr[0-%u]: {\\n\", dictionary2->count );\n\t\tfor( index = 0; index < dictionary2->count; index++ ) {\n\t\t\t\n\t\t\tJournal_Printf(stream, \"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary2->entryPtr[index], stream ); \n\t\t\tJournal_Printf(stream, \"\\n\" );\n\t\t}\n\t\tJournal_Printf(stream, \"\\t}\\n\" );\n\n\n\t\t\n\n\t\tCheckDictionaryKeys(dictionary2, errMessage);\n\t\tCheckDictionaryKeys(dictionary,  errMessage);\n\t\t\n\t\tStg_Class_Delete( dictionary );\n\t\tStg_Class_Delete( dictionary2 );\n\t}\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tStream* stream;\n\t\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\t\n\n\tstream = Journal_Register( Info_Type, Dictionary_Type );\n\tstJournal->firewallProducesAssert = False;\n\tStream_RedirectFile(Journal_Register( Error_Type, \"DictionaryCheck\"), \"DictionaryCheck.txt\");\n\tStream_RedirectFile(stream, \"DictionaryCheck.txt\");\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\tDictionary*\t\t\tdictionary = Dictionary_New();\n\t\tDictionary*\t\t\tdictionary2 = Dictionary_New();\n\t\t\n\t\tDictionary_Index\tindex;\n\t\tchar*\t\t\terrMessage = \"Component dictionary must have unique names\\n\";\n\t\t\n\t\t\n\n\t\t\n\n\t\tDictionary_Add( dictionary, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\tDictionary_Add( dictionary, \"test_dict_double\",\n\t\t\tDictionary_Entry_Value_FromDouble( 45.567 ) );\n\t\tDictionary_Add( dictionary, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"goodbye\" ) );\t\n\t\tDictionary_Add( dictionary, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\tDictionary_Add( dictionary, \"test_dict_string2\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\t\n\t\t\n\n\t\tDictionary_Add( dictionary2, \"test_dict_string\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\" ) );\n\t\tDictionary_Add( dictionary2, \"test_dict_double\",\n\t\t\tDictionary_Entry_Value_FromDouble( 45.567 ) );\n\t\tDictionary_Add( dictionary2, \"test_dict_stuff\",\n\t\t\tDictionary_Entry_Value_FromString( \"hello\") );\n\n\t\t\n\n\t\tJournal_Printf(stream, \"Dictionary:\\n\" );\n\t\tJournal_Printf(stream, \"\\tsize: %u\\n\", dictionary->size );\n\t\tJournal_Printf(stream, \"\\tdelta: %u\\n\", dictionary->delta );\n\t\tJournal_Printf(stream, \"\\tcount: %u\\n\", dictionary->count );\n\t\tJournal_Printf(stream, \"\\tentryPtr[0-%u]: {\\n\", dictionary->count );\n\t\tfor( index = 0; index < dictionary->count; index++ ) {\n\t\t\t\n\t\t\tJournal_Printf(stream,\"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary->entryPtr[index], stream ); \n\t\t\tJournal_Printf(stream, \"\\n\" );\n\t\t}\n\t\tJournal_Printf(stream, \"\\t}\\n\" );\n\t\t\n\t\tJournal_Printf(stream, \"Dictionary 2:\\n\" );\n\t\tJournal_Printf(stream, \"\\tsize: %u\\n\", dictionary2->size );\n\t\tJournal_Printf(stream, \"\\tdelta: %u\\n\", dictionary2->delta );\n\t\tJournal_Printf(stream, \"\\tcount: %u\\n\", dictionary2->count );\n\t\tJournal_Printf(stream, \"\\tentryPtr[0-%u]: {\\n\", dictionary2->count );\n\t\tfor( index = 0; index < dictionary2->count; index++ ) {\n\t\t\t\n\t\t\tJournal_Printf(stream, \"\\t\\t\" );\n\t\t\tDictionary_Entry_Print( dictionary2->entryPtr[index], stream ); \n\t\t\tJournal_Printf(stream, \"\\n\" );\n\t\t}\n\t\tJournal_Printf(stream, \"\\t}\\n\" );\n\n\n\t\t\n\n\t\tCheckDictionaryKeys(dictionary2, errMessage);\n\t\tCheckDictionaryKeys(dictionary,  errMessage);\n\t\t\n\t\tStg_Class_Delete( dictionary );\n\t\tStg_Class_Delete( dictionary2 );\n\t}\n\t\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "lesina_1302", "code": "int main(int argc, char* argv[])\n{\n\tint N = 100;\n\tdouble x[N], TotalSum, ProcSum = 0.0;\n\tint ProcRank, ProcNum, k, i1,i2,i;\n\tMPI_Status status;\n\n\t\n\tif (ProcRank == 0 )\n\t{\n\t\tfor(i1=0;i1<N;++i1)\n\t\t{\n\t\t\tx[i1]=rand();\n\t\t}\n\t}\n\tk = N / ProcNum;\n        i1 = k * ProcRank;\n        i2 = k * ( ProcRank + 1 );\n        if ( ProcRank == ProcNum-1 ) i2 = N;\n\t\n\t\n\n        return 0;\n\n}", "label": "int main(int argc, char* argv[])\n{\n\tint N = 100;\n\tdouble x[N], TotalSum, ProcSum = 0.0;\n\tint ProcRank, ProcNum, k, i1,i2,i;\n\tMPI_Status status;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &ProcNum);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &ProcRank);\n\t\n\tif (ProcRank == 0 )\n\t{\n\t\tfor(i1=0;i1<N;++i1)\n\t\t{\n\t\t\tx[i1]=rand();\n\t\t}\n\t}\n\tMPI_Bcast(x, N, MPI_DOUBLE, 0, MPI_COMM_WORLD );\n\tk = N / ProcNum;\n        i1 = k * ProcRank;\n        i2 = k * ( ProcRank + 1 );\n        if ( ProcRank == ProcNum-1 ) i2 = N;\n\t\n\t\n\n\tMPI_Finalize();\n        return 0;\n\n}"}
{"program": "joao-lima_1303", "code": "int main(int argc, char **argv)\n{\n\tint dst, rank, size;\n\tint result=0;\n\tsize_t *comm_amount_with_cache;\n\tsize_t *comm_amount_without_cache;\n\n\n\tsetenv(\"STARPU_COMM_STATS\", \"1\", 1);\n\n\tcomm_amount_with_cache = malloc(size * sizeof(size_t));\n\tcomm_amount_without_cache = malloc(size * sizeof(size_t));\n\n\ttest_cache(rank, \"0\", comm_amount_with_cache);\n\ttest_cache(rank, \"1\", comm_amount_without_cache);\n\n\tif (rank == 0 || rank == 1)\n\t{\n\t\tdst = (rank == 0) ? 1 : 0;\n\t\tresult = (comm_amount_with_cache[dst] == comm_amount_without_cache[dst] * 5);\n\t\tFPRINTF_MPI(stderr, \"Communication cache mechanism is %sworking\\n\", result?\"\":\"NOT \");\n\t}\n\telse\n\t{\n\t\tresult = 1;\n\t}\n\n\tfree(comm_amount_without_cache);\n\tfree(comm_amount_with_cache);\n\n\treturn !result;\n}", "label": "int main(int argc, char **argv)\n{\n\tint dst, rank, size;\n\tint result=0;\n\tsize_t *comm_amount_with_cache;\n\tsize_t *comm_amount_without_cache;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tsetenv(\"STARPU_COMM_STATS\", \"1\", 1);\n\n\tcomm_amount_with_cache = malloc(size * sizeof(size_t));\n\tcomm_amount_without_cache = malloc(size * sizeof(size_t));\n\n\ttest_cache(rank, \"0\", comm_amount_with_cache);\n\ttest_cache(rank, \"1\", comm_amount_without_cache);\n\n\tif (rank == 0 || rank == 1)\n\t{\n\t\tdst = (rank == 0) ? 1 : 0;\n\t\tresult = (comm_amount_with_cache[dst] == comm_amount_without_cache[dst] * 5);\n\t\tFPRINTF_MPI(stderr, \"Communication cache mechanism is %sworking\\n\", result?\"\":\"NOT \");\n\t}\n\telse\n\t{\n\t\tresult = 1;\n\t}\n\n\tfree(comm_amount_without_cache);\n\tfree(comm_amount_with_cache);\n\n\tMPI_Finalize();\n\treturn !result;\n}"}
{"program": "jbigot_1304", "code": "int main(int argc, char *argv[])\n{\n    int rank, nbProcs, nbLines, i, M, arg;\n    double wtime, *h, *g, memSize, localerror, globalerror = 1;\n\n    FTI_Init(argv[2], MPI_COMM_WORLD);\n\n    arg = atoi(argv[1]);\n    M = (int)sqrt((double)(arg * 1024.0 * 512.0 * nbProcs)/sizeof(double));\n    nbLines = (M / nbProcs)+3;\n    h = (double *) malloc(sizeof(double *) * M * nbLines);\n    g = (double *) malloc(sizeof(double *) * M * nbLines);\n    initData(nbLines, M, rank, g);\n    memSize = M * nbLines * 2 * sizeof(double) / (1024 * 1024);\n    if (rank == 0) printf(\"Local data size is %d x %d = %f MB (%d).\\n\", M, nbLines, memSize, arg);\n    if (rank == 0) printf(\"Target precision : %f \\n\", PRECISION);\n    if (rank == 0) printf(\"Maximum number of iterations : %d \\n\", ITER_TIMES);\n\n    FTI_Protect(0, &i, 1, FTI_INTG);\n    FTI_Protect(1, h, M*nbLines, FTI_DBLE);\n    FTI_Protect(2, g, M*nbLines, FTI_DBLE);\n\n    wtime =\n    for(i = 0; i < ITER_TIMES; i++)\n    {\n        int checkpointed = FTI_Snapshot();\n        if (checkpointed == 1) printf(\"A checkpoint was performed. \\n\");\n        FTI_BitFlip(2);\n        localerror = doWork(nbProcs, rank, M, nbLines, g, h);\n        if (((i%ITER_OUT) == 0) && (rank == 0)) printf(\"Step : %d, error = %f\\n\", i, globalerror);\n        if ((i%REDUCE) == 0)\n        if(globalerror < PRECISION) break;\n    }\n    if (rank == 0) printf(\"Execution finished in %lf seconds.\\n\",\n\n    free(h);\n    free(g);\n    FTI_Finalize();\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, nbProcs, nbLines, i, M, arg;\n    double wtime, *h, *g, memSize, localerror, globalerror = 1;\n\n    MPI_Init(&argc, &argv);\n    FTI_Init(argv[2], MPI_COMM_WORLD);\n    MPI_Comm_size(FTI_COMM_WORLD, &nbProcs);\n    MPI_Comm_rank(FTI_COMM_WORLD, &rank);\n\n    arg = atoi(argv[1]);\n    M = (int)sqrt((double)(arg * 1024.0 * 512.0 * nbProcs)/sizeof(double));\n    nbLines = (M / nbProcs)+3;\n    h = (double *) malloc(sizeof(double *) * M * nbLines);\n    g = (double *) malloc(sizeof(double *) * M * nbLines);\n    initData(nbLines, M, rank, g);\n    memSize = M * nbLines * 2 * sizeof(double) / (1024 * 1024);\n    if (rank == 0) printf(\"Local data size is %d x %d = %f MB (%d).\\n\", M, nbLines, memSize, arg);\n    if (rank == 0) printf(\"Target precision : %f \\n\", PRECISION);\n    if (rank == 0) printf(\"Maximum number of iterations : %d \\n\", ITER_TIMES);\n\n    FTI_Protect(0, &i, 1, FTI_INTG);\n    FTI_Protect(1, h, M*nbLines, FTI_DBLE);\n    FTI_Protect(2, g, M*nbLines, FTI_DBLE);\n\n    wtime = MPI_Wtime();\n    for(i = 0; i < ITER_TIMES; i++)\n    {\n        int checkpointed = FTI_Snapshot();\n        if (checkpointed == 1) printf(\"A checkpoint was performed. \\n\");\n        FTI_BitFlip(2);\n        localerror = doWork(nbProcs, rank, M, nbLines, g, h);\n        if (((i%ITER_OUT) == 0) && (rank == 0)) printf(\"Step : %d, error = %f\\n\", i, globalerror);\n        if ((i%REDUCE) == 0) MPI_Allreduce(&localerror, &globalerror, 1, MPI_DOUBLE, MPI_MAX, FTI_COMM_WORLD);\n        if(globalerror < PRECISION) break;\n    }\n    if (rank == 0) printf(\"Execution finished in %lf seconds.\\n\", MPI_Wtime() - wtime);\n\n    free(h);\n    free(g);\n    FTI_Finalize();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "marcel-sl_1305", "code": "int main(int argc, char * argv[]) {\n\n\n    libfastpm_init();\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n\n    fastpm_set_msg_handler(fastpm_void_msg_handler, comm, NULL);\n\n    FastPM * solver = & (FastPM) {\n        .nc = 128,\n        .boxsize = 32.,\n        .alloc_factor = 2.0,\n        .omega_m = 0.292,\n        .vpminit = (VPMInit[]) {\n            {.a_start = 0, .pm_nc_factor = 2},\n            {.a_start = -1, .pm_nc_factor = 0},\n        },\n        .FORCE_TYPE = FASTPM_FORCE_FASTPM,\n        .USE_NONSTDDA = 0,\n        .USE_MODEL = 0,\n        .nLPT = 2.5,\n        .K_LINEAR = 0.04,\n    };\n\n    fastpm_init(solver, 0, 0, comm);\n\n    FastPMFloat * rho_init_ktruth = pm_alloc(solver->pm_2lpt);\n    FastPMFloat * rho_final_ktruth = pm_alloc(solver->pm_2lpt);\n    FastPMFloat * rho_final_xtruth = pm_alloc(solver->pm_2lpt);\n\n    \n\n    struct fastpm_powerspec_eh_params eh = {\n        .Norm = 10000.0, \n\n        .hubble_param = 0.7,\n        .omegam = 0.260,\n        .omegab = 0.044,\n    };\n    fastpm_ic_fill_gaussiank(solver->pm_2lpt, rho_init_ktruth, 2004, FASTPM_DELTAK_GADGET);\n    fastpm_ic_induce_correlation(solver->pm_2lpt, rho_init_ktruth, (fastpm_fkfunc)fastpm_utils_powerspec_eh, &eh);\n\n    double time_step[] = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, .9, 1.0};\n    fastpm_setup_ic(solver, rho_init_ktruth);\n\n    fastpm_evolve(solver, time_step, sizeof(time_step) / sizeof(time_step[0]));\n\n    fastpm_utils_paint(solver->pm_2lpt, solver->base.p, rho_final_xtruth, rho_final_ktruth, NULL, 0);\n    fastpm_utils_dump(solver->pm_2lpt, \"fastpm_rho_final_xtruth.raw\", rho_final_xtruth);\n\n    pm_free(solver->pm_2lpt, rho_final_xtruth);\n    pm_free(solver->pm_2lpt, rho_final_ktruth);\n    pm_free(solver->pm_2lpt, rho_init_ktruth);\n\n    libfastpm_cleanup();\n    return 0;\n}", "label": "int main(int argc, char * argv[]) {\n\n    MPI_Init(&argc, &argv);\n\n    libfastpm_init();\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n\n    fastpm_set_msg_handler(fastpm_void_msg_handler, comm, NULL);\n\n    FastPM * solver = & (FastPM) {\n        .nc = 128,\n        .boxsize = 32.,\n        .alloc_factor = 2.0,\n        .omega_m = 0.292,\n        .vpminit = (VPMInit[]) {\n            {.a_start = 0, .pm_nc_factor = 2},\n            {.a_start = -1, .pm_nc_factor = 0},\n        },\n        .FORCE_TYPE = FASTPM_FORCE_FASTPM,\n        .USE_NONSTDDA = 0,\n        .USE_MODEL = 0,\n        .nLPT = 2.5,\n        .K_LINEAR = 0.04,\n    };\n\n    fastpm_init(solver, 0, 0, comm);\n\n    FastPMFloat * rho_init_ktruth = pm_alloc(solver->pm_2lpt);\n    FastPMFloat * rho_final_ktruth = pm_alloc(solver->pm_2lpt);\n    FastPMFloat * rho_final_xtruth = pm_alloc(solver->pm_2lpt);\n\n    \n\n    struct fastpm_powerspec_eh_params eh = {\n        .Norm = 10000.0, \n\n        .hubble_param = 0.7,\n        .omegam = 0.260,\n        .omegab = 0.044,\n    };\n    fastpm_ic_fill_gaussiank(solver->pm_2lpt, rho_init_ktruth, 2004, FASTPM_DELTAK_GADGET);\n    fastpm_ic_induce_correlation(solver->pm_2lpt, rho_init_ktruth, (fastpm_fkfunc)fastpm_utils_powerspec_eh, &eh);\n\n    double time_step[] = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, .9, 1.0};\n    fastpm_setup_ic(solver, rho_init_ktruth);\n\n    fastpm_evolve(solver, time_step, sizeof(time_step) / sizeof(time_step[0]));\n\n    fastpm_utils_paint(solver->pm_2lpt, solver->base.p, rho_final_xtruth, rho_final_ktruth, NULL, 0);\n    fastpm_utils_dump(solver->pm_2lpt, \"fastpm_rho_final_xtruth.raw\", rho_final_xtruth);\n\n    pm_free(solver->pm_2lpt, rho_final_xtruth);\n    pm_free(solver->pm_2lpt, rho_final_ktruth);\n    pm_free(solver->pm_2lpt, rho_init_ktruth);\n\n    libfastpm_cleanup();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "abanaiyan_1307", "code": "int main(int argc, char **argv)\n{\n  int i, j, p, me, nprocs, num_threads, num_slabs, spp;\n  int *my_slabs, *count;\n  double x, sum;\n#ifdef _OPENMP\n  int np;\n#endif \n\n#ifdef USE_MPI\n  int namelen;\n  char processor_name[MPI_MAX_PROCESSOR_NAME];\n#endif \n\n\n#ifdef USE_MPI\n#else \n\n  nprocs = 1;\n  me = 0;\n#endif \n\n\n  SimRoiStart();\n\n#ifdef _OPENMP\n  num_threads = omp_get_max_threads();\n#else \n\n  num_threads = 1;\n#endif \n\n  printf(\"Process %d of %d\", me, nprocs);\n#ifdef USE_MPI\n  printf(\" running on %s\", processor_name);\n#endif \n\n#ifdef _OPENMP\n  printf(\" using OpenMP with %d threads\",\n    num_threads);\n#endif \n\n  printf(\"\\n\");\n  \n\n  if (!me) num_slabs = read_slab_info();\n#ifdef USE_MPI\n  if (MPI_Bcast(&num_slabs, 1, MPI_INT, 0,\n      MPI_COMM_WORLD) != MPI_SUCCESS)\n#endif \n\n\n  if (num_slabs < nprocs)\n    exit_on_error(\"Number of slabs may not exceed number of processes\");\n  \n\n  spp = (int)ceil((double)num_slabs /\n  (double)nprocs);\n  if (!me) printf(\"No more than %d slabs will assigned to each process\\n\", spp);\n\n  \n\n  if (!(my_slabs = (int *)malloc(nprocs*spp*\n  sizeof(int)))) {\n  perror(\"my_slabs\");\n  exit(2);\n  }\n  if (!(count = (int *)malloc(nprocs*sizeof(int)))) {\n    perror(\"count\");\n    exit(2);\n  }\n  \n\n  for (p = 0; p < nprocs; p++) count[p] = 0;\n  \n\n  for (i = j = p = 0; i < num_slabs; i++) {\n    my_slabs[p*spp+j] = i;\n    count[p]++;\n    if (p == nprocs -1)\n      p = 0, j++;\n    else\n      p++;\n  }\n\n  \n\n#pragma omp parallel for reduction(+: x)\n  for (i = 0; i < count[me]; i++) {\n    printf(\"%d: slab %d being processed\", me,\n      my_slabs[me*spp+i]);\n#ifdef _OPENMP\n    printf(\" by thread %d\", omp_get_thread_num());\n#endif \n\n    printf(\"\\n\");\n    x += process_slab(my_slabs[me*spp+i]);\n  }\n\n#ifdef USE_MPI\n  if (MPI_Reduce(&x, &sum, 1, MPI_DOUBLE, MPI_SUM, 0,\n      MPI_COMM_WORLD) != MPI_SUCCESS)\n#else \n\n  sum = x;\n#endif \n\n\n  if (!me) printf(\"Sum is %lg\\n\", sum);\n\n  SimRoiEnd();\n\n#ifdef USE_MPI\n#endif \n\n  exit(0);\n}", "label": "int main(int argc, char **argv)\n{\n  int i, j, p, me, nprocs, num_threads, num_slabs, spp;\n  int *my_slabs, *count;\n  double x, sum;\n#ifdef _OPENMP\n  int np;\n#endif \n\n#ifdef USE_MPI\n  int namelen;\n  char processor_name[MPI_MAX_PROCESSOR_NAME];\n#endif \n\n\n#ifdef USE_MPI\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Get_processor_name(processor_name, &namelen);\n#else \n\n  nprocs = 1;\n  me = 0;\n#endif \n\n\n  SimRoiStart();\n\n#ifdef _OPENMP\n  num_threads = omp_get_max_threads();\n#else \n\n  num_threads = 1;\n#endif \n\n  printf(\"Process %d of %d\", me, nprocs);\n#ifdef USE_MPI\n  printf(\" running on %s\", processor_name);\n#endif \n\n#ifdef _OPENMP\n  printf(\" using OpenMP with %d threads\",\n    num_threads);\n#endif \n\n  printf(\"\\n\");\n  \n\n  if (!me) num_slabs = read_slab_info();\n#ifdef USE_MPI\n  if (MPI_Bcast(&num_slabs, 1, MPI_INT, 0,\n      MPI_COMM_WORLD) != MPI_SUCCESS)\n    exit_on_error(\"Error in MPI_Bcast()\");\n#endif \n\n\n  if (num_slabs < nprocs)\n    exit_on_error(\"Number of slabs may not exceed number of processes\");\n  \n\n  spp = (int)ceil((double)num_slabs /\n  (double)nprocs);\n  if (!me) printf(\"No more than %d slabs will assigned to each process\\n\", spp);\n\n  \n\n  if (!(my_slabs = (int *)malloc(nprocs*spp*\n  sizeof(int)))) {\n  perror(\"my_slabs\");\n  exit(2);\n  }\n  if (!(count = (int *)malloc(nprocs*sizeof(int)))) {\n    perror(\"count\");\n    exit(2);\n  }\n  \n\n  for (p = 0; p < nprocs; p++) count[p] = 0;\n  \n\n  for (i = j = p = 0; i < num_slabs; i++) {\n    my_slabs[p*spp+j] = i;\n    count[p]++;\n    if (p == nprocs -1)\n      p = 0, j++;\n    else\n      p++;\n  }\n\n  \n\n#pragma omp parallel for reduction(+: x)\n  for (i = 0; i < count[me]; i++) {\n    printf(\"%d: slab %d being processed\", me,\n      my_slabs[me*spp+i]);\n#ifdef _OPENMP\n    printf(\" by thread %d\", omp_get_thread_num());\n#endif \n\n    printf(\"\\n\");\n    x += process_slab(my_slabs[me*spp+i]);\n  }\n\n#ifdef USE_MPI\n  if (MPI_Reduce(&x, &sum, 1, MPI_DOUBLE, MPI_SUM, 0,\n      MPI_COMM_WORLD) != MPI_SUCCESS)\n    exit_on_error(\"Error in MPI_Reduce()\");\n#else \n\n  sum = x;\n#endif \n\n\n  if (!me) printf(\"Sum is %lg\\n\", sum);\n\n  SimRoiEnd();\n\n#ifdef USE_MPI\n  printf(\"%d: Calling MPI_Finalize()\\n\", me);\n  MPI_Finalize();\n#endif \n\n  exit(0);\n}"}
{"program": "fintler_1310", "code": "int main(int argc, char *argv[])\n{\n\tint myid, numprocs;\n\tint io_rank=0;\n\n\n\n\tif (numprocs!=2) {\n\t\tif (myid==io_rank) {\n\t\t\tprintf(\"ping pong test requires exactly 2 nodes.\\n\");\n\t\t}\n\t\treturn -1;\n\t}\n\n\tif (myid==0) { \n\t\tmaster(); \n\t} else { \n\t\tworker(); \n\t}\n\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint myid, numprocs;\n\tint io_rank=0;\n\n\tMPI_Init(&argc,&argv);\n\tMPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tif (numprocs!=2) {\n\t\tif (myid==io_rank) {\n\t\t\tprintf(\"ping pong test requires exactly 2 nodes.\\n\");\n\t\t}\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n\tif (myid==0) { \n\t\tmaster(); \n\t} else { \n\t\tworker(); \n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tMPI_Finalize();\n\n\treturn 0;\n}"}
{"program": "daniel-koehn_1312", "code": "int main(int argc, char **argv){\n\n\n\nint j, i, ntr=0, nshots=0, ** recpos=NULL;\nfloat ** srcpos=NULL;\nfloat  **S, ** TT, ** Vp;\ndouble time1, time8;\nchar ext[10], *fileinp, *fileinp1;\t\n\n\n\nfloat * Tmod, * Tobs, * Tres;\n\n\n\nMPI_Init(&argc,&argv);\nMPI_Comm_size(MPI_COMM_WORLD,&NP);\nMPI_Comm_rank(MPI_COMM_WORLD,&MYID);\n\nsetvbuf(stdout, NULL, _IONBF, 0);\n\nif (MYID == 0){\n   clock();\n}\n\t\t\n\n\nif (MYID == 0) info(stdout);\n\n\n\nfileinp=argv[1];\nfileinp1=argv[2];\nFP=fopen(fileinp,\"r\");\nif(FP==NULL) {\n\tif (MYID == 0){\n\t\tprintf(\"\\n==================================================================\\n\");\n\t\tprintf(\" Cannot open Rajzel input file %s \\n\",fileinp);\n\t\tprintf(\"\\n==================================================================\\n\\n\");\n\t\terr(\" --- \");\n\t}\n}\n\n\n\n\nread_par(FP);\n\nif (MYID == 0) note(stdout);\n\n\n\n\n\nsprintf(ext,\".%i\",MYID);  \nstrcat(LOG_FILE,ext);\n\nif ((MYID==0) && (LOG==1)) FP=stdout;\nelse FP=fopen(LOG_FILE,\"w\");\nfprintf(FP,\" This is the log-file generated by PE %d \\n\\n\",MYID);\n\n\n\nif (MYID==0) write_par(FP);\n\t\n\n\nNXG=NX;\nNYG=NY;\n\nrecpos=receiver(FP, &ntr, 1);\n\nif(N_STREAMER>0){\n  free_imatrix(recpos,1,3,1,ntr);\n}\n\n\n\nTmod = vector(1,ntr);\n\nif(INVMAT){\n   Tobs = vector(1,ntr);\n   Tres = vector(1,ntr);\n}\t\n\n\n\nTT =  matrix(1,NY,1,NX);\nVp =  matrix(1,NY,1,NX);\nS = matrix(1,NY,1,NX);\n\n\n\nINFO=0;\n\n\n\nif(INVMAT==0){\n   INFO=1;\n}\n\n\n\n\n\n \t\nsrcpos=sources(&nshots);\n\n\n\ninit_MPIshot(nshots);\n\n\n\nif (READMOD){\n    readmod(Vp); \n}else{\n    model(Vp);\n}\n\n\n\ncalc_S(Vp,S); \n\n\n\n\n\nif(INVMAT==0){\n   forward(S,TT,Tmod,srcpos,nshots,recpos,ntr);\n}\n\n\n\n\n\nif(INVMAT==1){\n   fatt(Vp,S,TT,Tmod,Tobs,Tres,srcpos,nshots,recpos,ntr,fileinp1);\n}\n          \n\n\n\n\nif(INVMAT==2){\n   grid_search(Vp,S,TT,Tmod,Tobs,Tres,srcpos,nshots,recpos,ntr);\n}\n\n\n\nfree_matrix(Vp,1,NY,1,NX);\nfree_matrix(TT,1,NY,1,NX);\nfree_vector(Tmod,1,ntr);\nfree_matrix(S,1,NY,1,NX);\n\nif(INVMAT){\n\n   free_vector(Tobs,1,ntr);\n   free_vector(Tres,1,ntr);\n\n}\n\n\n\nfree_matrix(srcpos,1,8,1,nshots);\n \nMPI_Barrier(MPI_COMM_WORLD);\n\nif (MYID==0){\n\tfprintf(FP,\"\\n **Info from main (written by PE %d): \\n\",MYID);\n\tfprintf(FP,\" CPU time of program per PE: %li seconds.\\n\",clock()/CLOCKS_PER_SEC);\n\tfprintf(FP,\" Total real time of program: %4.2f seconds.\\n\",time8-time1);\t\t\n}\n\nfclose(FP);\n\nMPI_Finalize();\nreturn 0;\t\n\n}", "label": "int main(int argc, char **argv){\n\n\n\nint j, i, ntr=0, nshots=0, ** recpos=NULL;\nfloat ** srcpos=NULL;\nfloat  **S, ** TT, ** Vp;\ndouble time1, time8;\nchar ext[10], *fileinp, *fileinp1;\t\n\n\n\nfloat * Tmod, * Tobs, * Tres;\n\n\n\nMPI_Init(&argc,&argv);\nMPI_Comm_size(MPI_COMM_WORLD,&NP);\nMPI_Comm_rank(MPI_COMM_WORLD,&MYID);\n\nsetvbuf(stdout, NULL, _IONBF, 0);\n\nif (MYID == 0){\n   time1=MPI_Wtime(); \n   clock();\n}\n\t\t\n\n\nif (MYID == 0) info(stdout);\n\n\n\nfileinp=argv[1];\nfileinp1=argv[2];\nFP=fopen(fileinp,\"r\");\nif(FP==NULL) {\n\tif (MYID == 0){\n\t\tprintf(\"\\n==================================================================\\n\");\n\t\tprintf(\" Cannot open Rajzel input file %s \\n\",fileinp);\n\t\tprintf(\"\\n==================================================================\\n\\n\");\n\t\terr(\" --- \");\n\t}\n}\n\n\n\n\nread_par(FP);\n\nif (MYID == 0) note(stdout);\n\n\n\n\n\nsprintf(ext,\".%i\",MYID);  \nstrcat(LOG_FILE,ext);\n\nif ((MYID==0) && (LOG==1)) FP=stdout;\nelse FP=fopen(LOG_FILE,\"w\");\nfprintf(FP,\" This is the log-file generated by PE %d \\n\\n\",MYID);\n\n\n\nif (MYID==0) write_par(FP);\n\t\n\n\nNXG=NX;\nNYG=NY;\n\nrecpos=receiver(FP, &ntr, 1);\n\nif(N_STREAMER>0){\n  free_imatrix(recpos,1,3,1,ntr);\n}\n\n\n\nTmod = vector(1,ntr);\n\nif(INVMAT){\n   Tobs = vector(1,ntr);\n   Tres = vector(1,ntr);\n}\t\n\n\n\nTT =  matrix(1,NY,1,NX);\nVp =  matrix(1,NY,1,NX);\nS = matrix(1,NY,1,NX);\n\n\n\nINFO=0;\n\n\n\nif(INVMAT==0){\n   INFO=1;\n}\n\n\n\n\n\n \t\nsrcpos=sources(&nshots);\n\n\n\ninit_MPIshot(nshots);\n\n\n\nif (READMOD){\n    readmod(Vp); \n}else{\n    model(Vp);\n}\n\n\n\ncalc_S(Vp,S); \n\n\n\n\n\nif(INVMAT==0){\n   forward(S,TT,Tmod,srcpos,nshots,recpos,ntr);\n}\n\n\n\n\n\nif(INVMAT==1){\n   fatt(Vp,S,TT,Tmod,Tobs,Tres,srcpos,nshots,recpos,ntr,fileinp1);\n}\n          \n\n\n\n\nif(INVMAT==2){\n   grid_search(Vp,S,TT,Tmod,Tobs,Tres,srcpos,nshots,recpos,ntr);\n}\n\n\n\nfree_matrix(Vp,1,NY,1,NX);\nfree_matrix(TT,1,NY,1,NX);\nfree_vector(Tmod,1,ntr);\nfree_matrix(S,1,NY,1,NX);\n\nif(INVMAT){\n\n   free_vector(Tobs,1,ntr);\n   free_vector(Tres,1,ntr);\n\n}\n\n\n\nfree_matrix(srcpos,1,8,1,nshots);\n \nMPI_Barrier(MPI_COMM_WORLD);\n\nif (MYID==0){\n\tfprintf(FP,\"\\n **Info from main (written by PE %d): \\n\",MYID);\n\tfprintf(FP,\" CPU time of program per PE: %li seconds.\\n\",clock()/CLOCKS_PER_SEC);\n\ttime8=MPI_Wtime();\n\tfprintf(FP,\" Total real time of program: %4.2f seconds.\\n\",time8-time1);\t\t\n}\n\nfclose(FP);\n\nMPI_Finalize();\nreturn 0;\t\n\n}"}
{"program": "tcsiwula_1313", "code": "int main(void) {\n   char       greeting[MAX_STRING];\n   int        my_rank, p, q;\n\n   \n\n\n   \n\n\n   \n\n\n   if (my_rank == 0) {\n      printf(\"Greetings from process %d of %d\\n\", my_rank, p);\n      for (q = 1; q < p; q++) {\n         printf(\"%s\\n\", greeting);\n      } \n   } else {\n      sprintf(greeting, \"Greetings from process %d of %d\", \n            my_rank, p);\n   }\n\n   \n\n   return 0;\n}", "label": "int main(void) {\n   char       greeting[MAX_STRING];\n   int        my_rank, p, q;\n\n   \n\n   MPI_Init(NULL, NULL);\n\n   \n\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   if (my_rank == 0) {\n      printf(\"Greetings from process %d of %d\\n\", my_rank, p);\n      for (q = 1; q < p; q++) {\n         MPI_Recv(greeting, MAX_STRING, MPI_CHAR, q, \n            0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         printf(\"%s\\n\", greeting);\n      } \n   } else {\n      sprintf(greeting, \"Greetings from process %d of %d\", \n            my_rank, p);\n      MPI_Send(greeting, strlen(greeting)+1, MPI_CHAR, 0, 0,\n            MPI_COMM_WORLD);\n   }\n\n   \n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "ClaudioNahmad_1314", "code": "int main(int argc, char **argv)\n{\n    int i, mynod, nprocs, len, errs=0, sum_errs=0, verbose=0;\n    char *filename;\n    char * cb_config_string;\n    int cb_config_len;\n    ADIO_cb_name_array array;\n\n\n\n\n    \n\n    if (!mynod) {\n\ti = 1;\n\t\n\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: noncontig_coll -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+1);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+1);\n    }\n\n    \n\n    cb_gather_name_array(MPI_COMM_WORLD,  &array);\n\n    \n\n    if (!mynod) {\n\t    if (array->namect < 2 ) {\n\t\t    fprintf(stderr, \"Run this test on two or more hosts\\n\");\n\t    }\n    }\n    \n\n    if (!mynod) {\n\t    cb_config_len = 0;\n\t    for (i=0; i < array->namect; i++) {\n\t\t    \n\n\t\t    cb_config_len += strlen(array->names[i]) + 1;\n\t    }\n\t    ++cb_config_len;\n    }\n    if ( (cb_config_string = malloc(cb_config_len)) == NULL ) {\n\t    perror(\"malloc\");\n    }\n\n    \n\n    errs += test_file(filename, mynod, nprocs, NULL, \"collective w/o hinting\", verbose);\n\n    \n\n    default_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: default order\", verbose);\n\n    \n\n    reverse_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: reverse order\", verbose);\n\n    \n\n    reverse_alternating_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string,\"collective w/ hinting: permutation1\", verbose);\n\n    \n\n    simple_shuffle_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: permutation2\", verbose);\n\n\n    if (!mynod) {\n\t    if (sum_errs) fprintf(stderr, \"Found %d error cases\\n\", sum_errs);\n\t    else printf(\" No Errors\\n\");\n    }\n    free(filename);\n    free(cb_config_string);\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int i, mynod, nprocs, len, errs=0, sum_errs=0, verbose=0;\n    char *filename;\n    char * cb_config_string;\n    int cb_config_len;\n    ADIO_cb_name_array array;\n\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n\n\n    \n\n    if (!mynod) {\n\ti = 1;\n\t\n\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: noncontig_coll -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+1);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+1, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+1);\n\tMPI_Bcast(filename, len+1, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n    \n\n    cb_gather_name_array(MPI_COMM_WORLD,  &array);\n\n    \n\n    if (!mynod) {\n\t    if (array->namect < 2 ) {\n\t\t    fprintf(stderr, \"Run this test on two or more hosts\\n\");\n\t\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t    }\n    }\n    \n\n    if (!mynod) {\n\t    cb_config_len = 0;\n\t    for (i=0; i < array->namect; i++) {\n\t\t    \n\n\t\t    cb_config_len += strlen(array->names[i]) + 1;\n\t    }\n\t    ++cb_config_len;\n    }\n    MPI_Bcast(&cb_config_len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if ( (cb_config_string = malloc(cb_config_len)) == NULL ) {\n\t    perror(\"malloc\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    \n\n    errs += test_file(filename, mynod, nprocs, NULL, \"collective w/o hinting\", verbose);\n\n    \n\n    default_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: default order\", verbose);\n\n    \n\n    reverse_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: reverse order\", verbose);\n\n    \n\n    reverse_alternating_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string,\"collective w/ hinting: permutation1\", verbose);\n\n    \n\n    simple_shuffle_str(mynod, cb_config_len, array, cb_config_string);\n    errs += test_file(filename, mynod, nprocs, cb_config_string, \"collective w/ hinting: permutation2\", verbose);\n\n    MPI_Allreduce(&errs, &sum_errs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    if (!mynod) {\n\t    if (sum_errs) fprintf(stderr, \"Found %d error cases\\n\", sum_errs);\n\t    else printf(\" No Errors\\n\");\n    }\n    free(filename);\n    free(cb_config_string);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "jhorey_1316", "code": "int main (int argc, char *argv[])\n{\n  char name[50];\n  char* ips[15];\n  char* labels[15];\n  int num;\n  int rank,nprocs,nid;\n  MPI_Status status;\n  cargs_t *arg;\n\n  \n\n  cargs_t* *cargs = parse_args(argc, argv);\n\n  \n\n\n  \n\n  if( (arg = contains_arg(cargs, \"help\") ) != NULL ) {\n    print_help();\n  }\n\n  \n\n  if( (arg = contains_arg(cargs, \"local\") ) != NULL ) {\n    \n\n    get_local_name(name, sizeof(name));\n    export_host(name);\n    num = get_local_addresses(rank, ips, labels);\n\n    \n\n    export_local_addresses(ips, labels, num);\n\n    \n\n    char *dir = \"/tmp/network\";\n    if(arg->value != NULL) {\n      dir = strtok(arg->value, \" \");\n    }\n\n    output_local_hosts(name, ips, labels, num, dir);\n  }\n\n  arg = NULL;\n  if( (arg = contains_arg(cargs, \"global\") ) != NULL ) {\n    \n\n    char* hosts = collect_global_hosts(nprocs);\n\n    \n\n    char *dir = \"/tmp/hosts.txt\";\n    if(arg->value != NULL) {      \n      dir = strtok(arg->value, \" \");\n    }\n\n    output_global_hosts(rank, nprocs,\n\t\t\thosts, dir);\n  }\n\n  \n\n  \n\n  arg = NULL;\n  if( (arg = contains_arg(cargs, \"export\") ) != NULL ) {\n    char *label;\n    char *to_rank;\n\n    \n\n    cargs_t *dir_arg = NULL;\n    char *export_dir = NULL;\n    if( (dir_arg = contains_arg(cargs, \"dir\") ) != NULL ) {\n      export_dir = strtok(dir_arg->value, \" \");\n    }\n    else {\n      export_dir = \"/tmp/environment\";\n    }\n\n    \n\n    \n\n    label = strtok(arg->value, \" \");\n    to_rank = strtok(NULL, \" \");\n\n    \n\n    output_env(label, to_rank, export_dir);\n  }\n\n  \n\n  arg = NULL;\n  if( (arg = contains_arg(cargs, \"cmd\") ) != NULL ) {\n    launch(arg->value);\n  }\n\n  \n\n\n  return 0;\n}", "label": "int main (int argc, char *argv[])\n{\n  char name[50];\n  char* ips[15];\n  char* labels[15];\n  int num;\n  int rank,nprocs,nid;\n  MPI_Status status;\n  cargs_t *arg;\n\n  \n\n  cargs_t* *cargs = parse_args(argc, argv);\n\n  \n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  \n\n  if( (arg = contains_arg(cargs, \"help\") ) != NULL ) {\n    print_help();\n  }\n\n  \n\n  if( (arg = contains_arg(cargs, \"local\") ) != NULL ) {\n    \n\n    get_local_name(name, sizeof(name));\n    export_host(name);\n    num = get_local_addresses(rank, ips, labels);\n\n    \n\n    export_local_addresses(ips, labels, num);\n\n    \n\n    char *dir = \"/tmp/network\";\n    if(arg->value != NULL) {\n      dir = strtok(arg->value, \" \");\n    }\n\n    output_local_hosts(name, ips, labels, num, dir);\n  }\n\n  arg = NULL;\n  if( (arg = contains_arg(cargs, \"global\") ) != NULL ) {\n    \n\n    char* hosts = collect_global_hosts(nprocs);\n\n    \n\n    char *dir = \"/tmp/hosts.txt\";\n    if(arg->value != NULL) {      \n      dir = strtok(arg->value, \" \");\n    }\n\n    output_global_hosts(rank, nprocs,\n\t\t\thosts, dir);\n  }\n\n  \n\n  \n\n  arg = NULL;\n  if( (arg = contains_arg(cargs, \"export\") ) != NULL ) {\n    char *label;\n    char *to_rank;\n\n    \n\n    cargs_t *dir_arg = NULL;\n    char *export_dir = NULL;\n    if( (dir_arg = contains_arg(cargs, \"dir\") ) != NULL ) {\n      export_dir = strtok(dir_arg->value, \" \");\n    }\n    else {\n      export_dir = \"/tmp/environment\";\n    }\n\n    \n\n    \n\n    label = strtok(arg->value, \" \");\n    to_rank = strtok(NULL, \" \");\n\n    \n\n    output_env(label, to_rank, export_dir);\n  }\n\n  \n\n  arg = NULL;\n  if( (arg = contains_arg(cargs, \"cmd\") ) != NULL ) {\n    launch(arg->value);\n  }\n\n  \n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "gnu3ra_1317", "code": "int main(int argc, char *argv[])\n{\n    \n\n    int a[100][100], b[100][100];\n    MPI_Datatype row, xpose;\n    MPI_Aint sizeofint;\n\t\n    int err, errs = 0;\n    int bufsize, position = 0;\n    void *buffer;\n  \n    int i, j;\n  \n    \n\n    for(i = 0; i < 100; i++) {\n\tfor(j = 0; j < 100; j++) {\n\t    a[i][j] = i*1000+j;\n\t    b[i][j] = -1;\n\t}\n    }\n  \n    \n\n    parse_args(argc, argv);\n\n\t\n    \n\n\t\n    \n\n    buffer = (char *) malloc((unsigned) bufsize);\n\n    \n\n\n    err =\n\t\n    \n\n    position = 0;\n    err =\n\n    for (i = 0; i < 100; i++) {\n\tfor (j = 0; j < 100; j++) {\n\t    if(b[i][j] != a[j][i]) {\n\t\terrs++;\n\t\tif (verbose) fprintf(stderr, \"b[%d][%d] = %d, should be %d\\n\",\n\t\t\t\t     i, j, b[i][j], a[j][i]);\n\t    }\n\t}\n    }\n\n    \n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    \n\n    int a[100][100], b[100][100];\n    MPI_Datatype row, xpose;\n    MPI_Aint sizeofint;\n\t\n    int err, errs = 0;\n    int bufsize, position = 0;\n    void *buffer;\n  \n    int i, j;\n  \n    \n\n    for(i = 0; i < 100; i++) {\n\tfor(j = 0; j < 100; j++) {\n\t    a[i][j] = i*1000+j;\n\t    b[i][j] = -1;\n\t}\n    }\n  \n    \n\n    MPI_Init(&argc, &argv);\n    parse_args(argc, argv);\n\n    MPI_Type_extent(MPI_INT, &sizeofint);\n\t\n    \n\n    MPI_Type_vector(100, 1, 100, MPI_INT, &row);\n    MPI_Type_hvector(100, 1, sizeofint, row, &xpose);\n    MPI_Type_commit(&xpose);\n\t\n    \n\n    MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);\n    buffer = (char *) malloc((unsigned) bufsize);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    err = MPI_Pack(a,\n\t\t   1,\n\t\t   xpose,\n\t\t   buffer,\n\t\t   bufsize,\n\t\t   &position,\n\t\t   MPI_COMM_WORLD);\n\t\n    \n\n    position = 0;\n    err = MPI_Unpack(buffer,\n\t\t     bufsize,\n\t\t     &position,\n\t\t     b,\n\t\t     100*100,\n\t\t     MPI_INT,\n\t\t     MPI_COMM_WORLD);\n\n    for (i = 0; i < 100; i++) {\n\tfor (j = 0; j < 100; j++) {\n\t    if(b[i][j] != a[j][i]) {\n\t\terrs++;\n\t\tif (verbose) fprintf(stderr, \"b[%d][%d] = %d, should be %d\\n\",\n\t\t\t\t     i, j, b[i][j], a[j][i]);\n\t    }\n\t}\n    }\n\n    MPI_Type_free(&xpose);\n    MPI_Type_free(&row);\n    \n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "pf-aics-riken_1318", "code": "int\nmain(int argc, char **argv)\n{\n    kmr_init();\n    KMR *mr = kmr_create_context(MPI_COMM_WORLD, MPI_INFO_NULL, 0);\n    int rank;\n\n    if (rank == 0) {\n\tfprintf(stderr, \"Start\\n\");\n    }\n\n    KMR_KVS *kvs_commands = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    int ret = kmr_map_once(kvs_commands, 0, kmr_noopt, 1, gen_cmdkvs);\n    if (ret != MPI_SUCCESS) {\n    }\n    kmr_dump_kvs(kvs_commands, 1);\n\n    if (rank == 0) {\n\tfprintf(stderr, \"MAP_ONCE DONE\\n\");\n    }\n\n    KMR_KVS *kvs_runcmds = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    ret = kmr_shuffle(kvs_commands, kvs_runcmds, kmr_noopt);\n    if (ret != MPI_SUCCESS) {\n    }\n    kmr_dump_kvs(kvs_runcmds, 1);\n\n    if (rank == 0) {\n\tfprintf(stderr, \"SHUFFLE DONE\\n\");\n    }\n\n    KMR_KVS *kvs_results = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_INTEGER);\n    struct kmr_spawn_option sopt_sepsp = { .separator_space = 1,\n\t\t\t\t\t   .take_ckpt = 1 };\n    ret = kmr_map_serial_processes(kvs_runcmds, kvs_results, 0, MPI_INFO_NULL,\n\t\t\t\t   sopt_sepsp, output_result);\n    kmr_dump_kvs(kvs_results, 1);\n    kmr_free_kvs(kvs_results);\n\n    if (rank == 0) {\n\tfprintf(stderr, \"MAP_SPAWN DONE\\n\");\n    }\n\n    if (rank == 0) {\n\tfprintf(stderr, \"Finish\\n\");\n    }\n\n    kmr_free_context(mr);\n    kmr_fin();\n}", "label": "int\nmain(int argc, char **argv)\n{\n    MPI_Init(&argc, &argv);\n    kmr_init();\n    KMR *mr = kmr_create_context(MPI_COMM_WORLD, MPI_INFO_NULL, 0);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n\tfprintf(stderr, \"Start\\n\");\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    KMR_KVS *kvs_commands = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    int ret = kmr_map_once(kvs_commands, 0, kmr_noopt, 1, gen_cmdkvs);\n    if (ret != MPI_SUCCESS) {\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    kmr_dump_kvs(kvs_commands, 1);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n\tfprintf(stderr, \"MAP_ONCE DONE\\n\");\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    KMR_KVS *kvs_runcmds = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_OPAQUE);\n    ret = kmr_shuffle(kvs_commands, kvs_runcmds, kmr_noopt);\n    if (ret != MPI_SUCCESS) {\n\tMPI_Abort(MPI_COMM_WORLD, 1);\n    }\n    kmr_dump_kvs(kvs_runcmds, 1);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n\tfprintf(stderr, \"SHUFFLE DONE\\n\");\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    KMR_KVS *kvs_results = kmr_create_kvs(mr, KMR_KV_INTEGER, KMR_KV_INTEGER);\n    struct kmr_spawn_option sopt_sepsp = { .separator_space = 1,\n\t\t\t\t\t   .take_ckpt = 1 };\n    ret = kmr_map_serial_processes(kvs_runcmds, kvs_results, 0, MPI_INFO_NULL,\n\t\t\t\t   sopt_sepsp, output_result);\n    kmr_dump_kvs(kvs_results, 1);\n    kmr_free_kvs(kvs_results);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank == 0) {\n\tfprintf(stderr, \"MAP_SPAWN DONE\\n\");\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n\tfprintf(stderr, \"Finish\\n\");\n    }\n\n    kmr_free_context(mr);\n    kmr_fin();\n    MPI_Finalize();\n}"}
{"program": "cot_1319", "code": "int main(int argc, char *argv[])\n{\n    int rank, size;\n    int i, index;\n    int buffer[400];\n    MPI_Request request[4];\n    MPI_Status status[4];\n\n    if (size != 4)\n    {\n        printf(\"Please run with 4 processes.\\n\");fflush(stdout);\n        return 1;\n    }\n\n    if (rank == 0)\n    {\n        for (i=0; i<size * 100; i++)\n            buffer[i] = i/100;\n        for (i=0; i<size-1; i++)\n        {\n        }\n\n\twhile(1) {\n\t  \n\n\t  int flag;\n\t  MPI_Status status;\n\t  if(flag){\n\t    break;\n\t  }\n\t}\n    }\n    else\n    {\n        printf(\"%d: buffer[0] = %d\\n\", rank, buffer[0]);fflush(stdout);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, size;\n    int i, index;\n    int buffer[400];\n    MPI_Request request[4];\n    MPI_Status status[4];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size != 4)\n    {\n        printf(\"Please run with 4 processes.\\n\");fflush(stdout);\n        MPI_Finalize();\n        return 1;\n    }\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0)\n    {\n        for (i=0; i<size * 100; i++)\n            buffer[i] = i/100;\n        for (i=0; i<size-1; i++)\n        {\n            MPI_Isend(&buffer[i*100], 100, MPI_INT, i+1, 123, MPI_COMM_WORLD, &request[i]);\n        }\n\n\twhile(1) {\n\t  \n\n\t  int flag;\n\t  MPI_Status status;\n\t  MPI_Test(&request[0], &flag, &status);\n\t  if(flag){\n\t    break;\n\t  }\n\t}\n\tMPI_Waitall(size-1, request, status);\n\tMPI_Waitall(size-1, request, status);\n    }\n    else\n    {\n        MPI_Recv(buffer, 100, MPI_INT, 0, 123, MPI_COMM_WORLD, &status[0]);\n        printf(\"%d: buffer[0] = %d\\n\", rank, buffer[0]);fflush(stdout);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "thiagovsk_1320", "code": "nt main(int argc, char* argv[]) {\n   int p_id, p, i;\n\n   if (p_id==0) \n\n      mestre();\n   else  \n\n      escravo();\n   return(0);\n}\n", "label": "nt main(int argc, char* argv[]) {\n   int p_id, p, i;\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n   MPI_Comm_rank(MPI_COMM_WORLD, &p_id);\n   if (p_id==0) \n\n      mestre();\n   else  \n\n      escravo();\n   MPI_Finalize();\n   return(0);\n}\n"}
{"program": "FreeON_1322", "code": "int\nmain (int argc, char **argv)\n{\n  int rank;\n  int size;\n  int i, j;\n  double temp;\n  int workload;\n  char buffer;\n  MPI_Comm spawn;\n  FILE *random_dev;\n  int random_seed;\n\n\n\n  \n\n  random_dev = fopen(\"/dev/urandom\", \"r\");\n  fgets((char*) &random_seed, sizeof(int), random_dev);\n  srand(random_seed);\n  fclose(random_dev);\n\n  workload = 20000+(rand()/(double) RAND_MAX-0.5)*3000;\n  printf(\"[slave %i] working with workload %i\\n\", rank, workload);\n\n  for (i = 0; i < workload; ++i) {\n    for (j = 0; j < 100000; ++j)\n    {\n      temp = rand();\n    }\n  }\n\n  printf(\"[slave %i] sending message to master\\n\", rank);\n\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int rank;\n  int size;\n  int i, j;\n  double temp;\n  int workload;\n  char buffer;\n  MPI_Comm spawn;\n  FILE *random_dev;\n  int random_seed;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  \n\n  random_dev = fopen(\"/dev/urandom\", \"r\");\n  fgets((char*) &random_seed, sizeof(int), random_dev);\n  srand(random_seed);\n  fclose(random_dev);\n\n  workload = 20000+(rand()/(double) RAND_MAX-0.5)*3000;\n  printf(\"[slave %i] working with workload %i\\n\", rank, workload);\n\n  for (i = 0; i < workload; ++i) {\n    for (j = 0; j < 100000; ++j)\n    {\n      temp = rand();\n    }\n  }\n\n  printf(\"[slave %i] sending message to master\\n\", rank);\n  MPI_Comm_get_parent(&spawn);\n  MPI_Send(&buffer, 1, MPI_CHAR, 0, 0, spawn);\n\n  MPI_Finalize();\n}"}
{"program": "gnu3ra_1325", "code": "int main ( int argc, char ** argv )\n{\n    MPI_Comm     intercomm, intracomm;\n    MPI_Request  request;\n    MPI_Status   status;\n    int          ibuffer;\n    int          neighbor_rank, rank, size;\n    int         *usize, aflag;\n    \n\n\n#if 1\n    \n\n#endif\n\n    intercomm = MPI_COMM_NULL;\n\n#if defined( SERVER )\n    is_server = 1;\n    intercomm = server_init( MPI_COMM_WORLD );\n#elif defined( CLIENT )\n    is_client = 1;\n    intercomm = client_init( MPI_COMM_WORLD );\n#else\n    parse_args( argc, argv );\n    if ( is_server ) {\n        intercomm = server_init( MPI_COMM_WORLD );\n    }\n    else if ( is_client ) {\n        intercomm = client_init( MPI_COMM_WORLD );\n    }\n#endif\n\n    if ( intercomm == MPI_COMM_NULL ) {\n        if ( is_server ) {\n            fprintf( stderr, \"Server returns NULL intercommunicator!\" );\n        }\n        else if ( is_client ) {\n            fprintf( stderr, \"Client returns NULL intercommunicator!\" );\n        }\n        else {\n            fprintf( stderr, \"Unknown server/client: NULL intercommunicator!\" );\n        }\n        return -1;\n    }\n\n\n    if ( rank == 0 ) {\n    }\n\n\n\n    fprintf( stdout, \"[%d/%d] after Intercomm_merge()\\n\", rank, size );\n\n    if ( rank == size - 1 )\n        neighbor_rank = 0;\n    else\n        neighbor_rank = rank + 1;\n\n\n    return 0;\n}", "label": "int main ( int argc, char ** argv )\n{\n    MPI_Comm     intercomm, intracomm;\n    MPI_Request  request;\n    MPI_Status   status;\n    int          ibuffer;\n    int          neighbor_rank, rank, size;\n    int         *usize, aflag;\n    \n\n\n    MPI_Init(&argc, &argv);\n#if 1\n    \n\n    MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &usize, &aflag);\n#endif\n\n    intercomm = MPI_COMM_NULL;\n\n#if defined( SERVER )\n    is_server = 1;\n    intercomm = server_init( MPI_COMM_WORLD );\n#elif defined( CLIENT )\n    is_client = 1;\n    intercomm = client_init( MPI_COMM_WORLD );\n#else\n    parse_args( argc, argv );\n    if ( is_server ) {\n        intercomm = server_init( MPI_COMM_WORLD );\n    }\n    else if ( is_client ) {\n        intercomm = client_init( MPI_COMM_WORLD );\n    }\n#endif\n\n    if ( intercomm == MPI_COMM_NULL ) {\n        if ( is_server ) {\n            fprintf( stderr, \"Server returns NULL intercommunicator!\" );\n        }\n        else if ( is_client ) {\n            fprintf( stderr, \"Client returns NULL intercommunicator!\" );\n        }\n        else {\n            fprintf( stderr, \"Unknown server/client: NULL intercommunicator!\" );\n        }\n        return -1;\n    }\n\n    MPI_Comm_rank( intercomm, &rank );\n\n    if ( rank == 0 ) {\n        MPI_Irecv( &ibuffer, 1, MPI_INT, 0,\n                   9999, intercomm, &request );\n        MPI_Send( &rank, 1, MPI_INT, 0, 9999, intercomm );\n        MPI_Wait( &request, &status );\n    }\n\n\n    MPI_Intercomm_merge( intercomm, 0, &intracomm );\n    MPI_Comm_rank( intracomm, &rank );\n    MPI_Comm_size( intracomm, &size );\n\n    fprintf( stdout, \"[%d/%d] after Intercomm_merge()\\n\", rank, size );\n\n    if ( rank == size - 1 )\n        neighbor_rank = 0;\n    else\n        neighbor_rank = rank + 1;\n\n    MPI_Irecv( &ibuffer, 1, MPI_INT, MPI_ANY_SOURCE,\n               999, intracomm, &request );\n    MPI_Send( &rank, 1, MPI_INT, neighbor_rank, 999, intracomm );\n    MPI_Wait( &request, &status );\n\n    MPI_Comm_free( &intracomm );\n    MPI_Comm_disconnect( &intercomm );\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "browndeer_1326", "code": "int main( int argc, char** argv )\r\n{\r\n\r\n\tint procid, nproc;\r\n\r\n\r\n   cl_uint n = 64;\r\n\r\n\r\n\t\n\r\n   CLCONTEXT* cp = (stdgpu)? stdgpu : 0;\r\n\r\n\tif (!cp) { fprintf(stderr,\"error: no CL context\\n\"); exit(-1); }\r\n\r\n\r\n   unsigned int devnum = 0; \n\r\n\r\n#ifdef __FreeBSD__\r\n   void* clh = clopen(cp,\"outerprod.cl\",CLLD_NOW);\r\n   cl_kernel krn = clsym(cp,clh,\"outerprod_kern\",0);\r\n#else\r\n   cl_kernel krn = clsym(cp,0,\"outerprod_kern\",0);\r\n#endif\r\n\r\n\tif (!krn) { fprintf(stderr,\"error: no OpenCL kernel\\n\"); exit(-1); }\r\n\r\n   \n\r\n   cl_float* a = (float*)clmalloc(cp,n*sizeof(cl_float),0);\r\n   cl_float* b = (float*)clmalloc(cp,n*sizeof(cl_float),0);\r\n   cl_float* c = (float*)clmalloc(cp,n*sizeof(cl_float),0);\r\n\r\n   \n\r\n   int i; \r\n   for(i=0;i<n;i++) a[i] = 1.1f*(i+procid*n);\r\n   for(i=0;i<n;i++) b[i] = 2.2f*(i+procid*n);\r\n   for(i=0;i<n;i++) c[i] = 0.0f;\r\n\r\n   \n\r\n   clmsync(cp,devnum,a,CL_MEM_DEVICE|CL_EVENT_WAIT);\r\n   clmsync(cp,devnum,b,CL_MEM_DEVICE|CL_EVENT_WAIT);\r\n\r\n   \n\r\n   clndrange_t ndr = clndrange_init1d( 0, n, 64);\r\n\r\n   \n\r\n   clarg_set_global(cp,krn,0,a);\r\n   clarg_set_global(cp,krn,1,b);\r\n   clarg_set_global(cp,krn,2,c);\r\n\r\n   \n\r\n   clfork(cp,devnum,krn,&ndr,CL_EVENT_NOWAIT);\r\n\r\n   \n\r\n   clwait(cp,devnum,CL_ALL_EVENT);\r\n\r\n   \n\r\n   clmsync(cp,0,c,CL_MEM_HOST|CL_EVENT_NOWAIT);\r\n\r\n   \n\r\n   clwait(cp,devnum,CL_ALL_EVENT);\r\n\r\n\r\n\t\n\r\n\tfloat* d = (float*)malloc(nproc*n*sizeof(float));\r\n\r\n\r\n\tif (procid==0) \r\n   \tfor(i=0;i<nproc*n;i++) printf(\"%d %f\\n\",i,d[i]);\r\n\r\n\tfree(d);\r\n\r\n   clfree(a);\r\n   clfree(b);\r\n   clfree(c);\r\n\r\n#ifdef __FreeBSD__\r\n   clclose(cp,clh);\r\n#endif\r\n\r\n\r\n}", "label": "int main( int argc, char** argv )\r\n{\r\n\r\n\tint procid, nproc;\r\n\r\n\tMPI_Init( &argc, &argv );\r\n\tMPI_Comm_rank( MPI_COMM_WORLD, &procid );\r\n\tMPI_Comm_size( MPI_COMM_WORLD, &nproc );\r\n\r\n   cl_uint n = 64;\r\n\r\n\r\n\t\n\r\n   CLCONTEXT* cp = (stdgpu)? stdgpu : 0;\r\n\r\n\tif (!cp) { fprintf(stderr,\"error: no CL context\\n\"); exit(-1); }\r\n\r\n\r\n   unsigned int devnum = 0; \n\r\n\r\n#ifdef __FreeBSD__\r\n   void* clh = clopen(cp,\"outerprod.cl\",CLLD_NOW);\r\n   cl_kernel krn = clsym(cp,clh,\"outerprod_kern\",0);\r\n#else\r\n   cl_kernel krn = clsym(cp,0,\"outerprod_kern\",0);\r\n#endif\r\n\r\n\tif (!krn) { fprintf(stderr,\"error: no OpenCL kernel\\n\"); exit(-1); }\r\n\r\n   \n\r\n   cl_float* a = (float*)clmalloc(cp,n*sizeof(cl_float),0);\r\n   cl_float* b = (float*)clmalloc(cp,n*sizeof(cl_float),0);\r\n   cl_float* c = (float*)clmalloc(cp,n*sizeof(cl_float),0);\r\n\r\n   \n\r\n   int i; \r\n   for(i=0;i<n;i++) a[i] = 1.1f*(i+procid*n);\r\n   for(i=0;i<n;i++) b[i] = 2.2f*(i+procid*n);\r\n   for(i=0;i<n;i++) c[i] = 0.0f;\r\n\r\n   \n\r\n   clmsync(cp,devnum,a,CL_MEM_DEVICE|CL_EVENT_WAIT);\r\n   clmsync(cp,devnum,b,CL_MEM_DEVICE|CL_EVENT_WAIT);\r\n\r\n   \n\r\n   clndrange_t ndr = clndrange_init1d( 0, n, 64);\r\n\r\n   \n\r\n   clarg_set_global(cp,krn,0,a);\r\n   clarg_set_global(cp,krn,1,b);\r\n   clarg_set_global(cp,krn,2,c);\r\n\r\n   \n\r\n   clfork(cp,devnum,krn,&ndr,CL_EVENT_NOWAIT);\r\n\r\n   \n\r\n   clwait(cp,devnum,CL_ALL_EVENT);\r\n\r\n   \n\r\n   clmsync(cp,0,c,CL_MEM_HOST|CL_EVENT_NOWAIT);\r\n\r\n   \n\r\n   clwait(cp,devnum,CL_ALL_EVENT);\r\n\r\n\r\n\t\n\r\n\tfloat* d = (float*)malloc(nproc*n*sizeof(float));\r\n\r\n\tMPI_Allgather(c, n, MPI_FLOAT, d, n, MPI_FLOAT, MPI_COMM_WORLD);\r\n\r\n\tif (procid==0) \r\n   \tfor(i=0;i<nproc*n;i++) printf(\"%d %f\\n\",i,d[i]);\r\n\r\n\tfree(d);\r\n\r\n   clfree(a);\r\n   clfree(b);\r\n   clfree(c);\r\n\r\n#ifdef __FreeBSD__\r\n   clclose(cp,clh);\r\n#endif\r\n\r\n\tMPI_Finalize();\r\n\r\n}"}
{"program": "jeffhammond_1330", "code": "int main(int argc, char ** argv)\n{\n\n    int n = (argc>1) ? atoi(argv[1]) : 1000;\n\n    int * send = calloc(n,sizeof(int));\n    int * recv = calloc(n,sizeof(int));\n\n    for (int i=0; i<n; ++i) send[i] =  1;\n    for (int i=0; i<n; ++i) recv[i] = -1;\n\n    int np = 1;\n\n    int me = 0;\n\n    int root = 0;\n\n    int test = (me == root) ? np : -1;\n    for (int i=0; i<n; ++i) assert(recv[i] == test);\n\n\n    for (int i=0; i<n; ++i) recv[i] = -1;\n\n\n    for (int i=0; i<n; ++i) assert(recv[i] == np);\n\n    free(send);\n    free(recv);\n\n\n    return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n    MPI_Init(&argc, &argv);\n\n    int n = (argc>1) ? atoi(argv[1]) : 1000;\n\n    int * send = calloc(n,sizeof(int));\n    int * recv = calloc(n,sizeof(int));\n\n    for (int i=0; i<n; ++i) send[i] =  1;\n    for (int i=0; i<n; ++i) recv[i] = -1;\n\n    int np = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n    int me = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &me);\n\n    int root = 0;\n    MPI_Reduce(send, recv, n, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n\n    int test = (me == root) ? np : -1;\n    for (int i=0; i<n; ++i) assert(recv[i] == test);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    for (int i=0; i<n; ++i) recv[i] = -1;\n\n    MPI_Allreduce(send, recv, n, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i=0; i<n; ++i) assert(recv[i] == np);\n\n    free(send);\n    free(recv);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "arjona00_1331", "code": "int main(int argc, char** argv) {\n  if (argc != 3) {\n    fprintf(stderr, \"Usage: compare_bcast num_elements num_trials\\n\");\n    exit(1);\n  }\n\n  int num_elements = atoi(argv[1]);\n  int num_trials = atoi(argv[2]);\n\n\n  int world_rank;\n\n  double total_my_bcast_time = 0.0;\n  double total_mpi_bcast_time = 0.0;\n  int i;\n  int* data = (int*)malloc(sizeof(int) * num_elements);\n  assert(data != NULL);\n\n  for (i = 0; i < num_trials; i++) {\n    \n\n    \n\n    total_my_bcast_time -=\n    my_bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);\n    \n\n    total_my_bcast_time +=\n\n    \n\n    total_mpi_bcast_time -=\n    total_mpi_bcast_time +=\n  }\n\n  \n\n  if (world_rank == 0) {\n    printf(\"Data size = %d, Trials = %d\\n\", num_elements * (int)sizeof(int),\n           num_trials);\n    printf(\"Avg my_bcast time = %lf\\n\", total_my_bcast_time / num_trials);\n    printf(\"Avg MPI_Bcast time = %lf\\n\", total_mpi_bcast_time / num_trials);\n  }\n\n}", "label": "int main(int argc, char** argv) {\n  if (argc != 3) {\n    fprintf(stderr, \"Usage: compare_bcast num_elements num_trials\\n\");\n    exit(1);\n  }\n\n  int num_elements = atoi(argv[1]);\n  int num_trials = atoi(argv[2]);\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  double total_my_bcast_time = 0.0;\n  double total_mpi_bcast_time = 0.0;\n  int i;\n  int* data = (int*)malloc(sizeof(int) * num_elements);\n  assert(data != NULL);\n\n  for (i = 0; i < num_trials; i++) {\n    \n\n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n    total_my_bcast_time -= MPI_Wtime();\n    my_bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);\n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n    total_my_bcast_time += MPI_Wtime();\n\n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n    total_mpi_bcast_time -= MPI_Wtime();\n    MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n    total_mpi_bcast_time += MPI_Wtime();\n  }\n\n  \n\n  if (world_rank == 0) {\n    printf(\"Data size = %d, Trials = %d\\n\", num_elements * (int)sizeof(int),\n           num_trials);\n    printf(\"Avg my_bcast time = %lf\\n\", total_my_bcast_time / num_trials);\n    printf(\"Avg MPI_Bcast time = %lf\\n\", total_mpi_bcast_time / num_trials);\n  }\n\n  MPI_Finalize();\n}"}
{"program": "gagallo7_1332", "code": "int main ( )\n{\n    int rank, size;\n    int n, i;\n    double myNumber; \n\n    double local_a, local_b, h;\n    int local_n;\n    double sum; \n\n    double low, high;\n    double total_sum;\n    struct timeval start, end;\n    long unsigned duration;\n    \n\n    \n\n    int mod;\n\n\n\n\n    \n\n    if ( !rank )\n    {\n        \n\n        mod = 1;\n        scanf ( \" %lf %lf %d\", &low, &high, &n );\n        gettimeofday ( &start, NULL );\n        local_n = n/size;\n        h = (high-low)/n;\n        local_a = low + rank*local_n*h;\n        local_b = local_a + local_n*h;\n        sum = Trap ( local_a, local_b, h, local_n );\n        total_sum = 0;\n        \n\n        \n\n        for ( i = 1; i < size; i++ )\n        {\n        }\n    }\n    else\n    {\n        sum = 0;\n\n        \n\n        mod = 1;\n        local_n = n/size;\n        h = (high-low)/n;\n        local_a = low + rank*local_n*h;\n        local_b = local_a + local_n*h;\n\n        sum = Trap ( local_a, local_b, h, local_n );\n        \n\n        \n\n        \n\n    }\n\n    \n\n\n    \n\n    if ( !rank )\n    {\n        gettimeofday ( &end, NULL );\n        printf ( \"%.5lf\\n\", total_sum );\n        \n\n        duration = ( ( end.tv_sec * 1000000 + end.tv_usec ) - ( start.tv_sec * 1000000 + start.tv_usec ) );\n        printf ( \"%lu\\n\", duration );\n    }\n    return 0;\n}\n", "label": "int main ( )\n{\n    int rank, size;\n    int n, i;\n    double myNumber; \n\n    double local_a, local_b, h;\n    int local_n;\n    double sum; \n\n    double low, high;\n    double total_sum;\n    struct timeval start, end;\n    long unsigned duration;\n    \n\n    \n\n    int mod;\n\n    MPI_Init ( NULL, NULL );\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);    \n\n    MPI_Comm_size (MPI_COMM_WORLD, &size);    \n\n\n    \n\n    if ( !rank )\n    {\n        \n\n        mod = 1;\n        scanf ( \" %lf %lf %d\", &low, &high, &n );\n        gettimeofday ( &start, NULL );\n        local_n = n/size;\n        h = (high-low)/n;\n        local_a = low + rank*local_n*h;\n        local_b = local_a + local_n*h;\n        sum = Trap ( local_a, local_b, h, local_n );\n        total_sum = 0;\n        \n\n        \n\n        for ( i = 1; i < size; i++ )\n        {\n            MPI_Send ( &n, 1, MPI_INT, i, 0, MPI_COMM_WORLD );\n            MPI_Send ( &low, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD );\n            MPI_Send ( &high, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD );\n        }\n    }\n    else\n    {\n        sum = 0;\n        MPI_Recv ( &n, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE );\n        MPI_Recv ( &low, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE );\n        MPI_Recv ( &high, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE );\n\n        \n\n        mod = 1;\n        local_n = n/size;\n        h = (high-low)/n;\n        local_a = low + rank*local_n*h;\n        local_b = local_a + local_n*h;\n\n        sum = Trap ( local_a, local_b, h, local_n );\n        \n\n        \n\n        \n\n    }\n\n    MPI_Reduce ( &sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD );\n    \n\n\n    \n\n    if ( !rank )\n    {\n        gettimeofday ( &end, NULL );\n        printf ( \"%.5lf\\n\", total_sum );\n        \n\n        duration = ( ( end.tv_sec * 1000000 + end.tv_usec ) - ( start.tv_sec * 1000000 + start.tv_usec ) );\n        printf ( \"%lu\\n\", duration );\n    }\n    MPI_Finalize();\n    return 0;\n}\n"}
{"program": "UnProgrammatore_1333", "code": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tlong long the_number;\n\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t}\n\telse\n\t\tthe_number = atoll(argv[1]);\n\n\tif(my_rank == 0) {\n\t\tdouble t1 = get_time();\t\n\n\t\tmaster_procedure(comm_size);\n\t\t\n\t\tdouble t2 = get_time();\t\n\n\t\tstruct result r;\n\t\tr.num = the_number;\n\t\tr.w_time = t2 - t1;\n\t\t\n\t\tprint_results(r);\n\t}\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tlong long the_number;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t\tMPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\telse\n\t\tthe_number = atoll(argv[1]);\n\n\tif(my_rank == 0) {\n\t\tdouble t1 = get_time();\t\n\n\t\tmaster_procedure(comm_size);\n\t\t\n\t\tdouble t2 = get_time();\t\n\n\t\tstruct result r;\n\t\tr.num = the_number;\n\t\tr.w_time = t2 - t1;\n\t\t\n\t\tprint_results(r);\n\t}\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "MengbinZhu_1334", "code": "int\nmain(int argc, char* argv[])\n{\n    hid_t file1, file2, fapl;\n    MPI_File\t*mpifh_p = NULL;\n    char\tname[1024];\n    const char  *envval = NULL;\n    int mpi_size, mpi_rank;\n    MPI_Comm comm  = MPI_COMM_WORLD;\n    MPI_Info info  = MPI_INFO_NULL;\n\n\n    fapl = H5Pcreate(H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, comm, info);\n\n    if(mpi_rank == 0)\n\tTESTING(\"H5Fflush (part1)\");\n    envval = HDgetenv(\"HDF5_DRIVER\");\n    if(envval == NULL)\n        envval = \"nomatch\";\n    if(HDstrcmp(envval, \"split\")) {\n\t\n\n\th5_fixname(FILENAME[0], fapl, name, sizeof name);\n\tfile1 = create_file(name, fapl);\n\t\n\n\tif(H5Fflush(file1, H5F_SCOPE_GLOBAL) < 0) goto error;\n\n\t\n\n\th5_fixname(FILENAME[1], fapl, name, sizeof name);\n\tfile2 = create_file(name, fapl);\n\n\n\tif(mpi_rank == 0)\n\t    PASSED();\n\tfflush(stdout);\n\tfflush(stderr);\n    } \n\n    else {\n        SKIPPED();\n        puts(\"    Test not compatible with current Virtual File Driver\");\n    } \n\n\n    \n\n\n    \n\n    if(H5Fget_vfd_handle(file1, fapl, (void **)&mpifh_p) < 0) {\n\tprintf(\"H5Fget_vfd_handle for file1 failed\\n\");\n\tgoto error;\n    } \n\n    if(MPI_File_close(mpifh_p) != MPI_SUCCESS) {\n\tprintf(\"MPI_File_close for file1 failed\\n\");\n\tgoto error;\n    } \n\n    \n\n    if(H5Fget_vfd_handle(file2, fapl, (void **)&mpifh_p) < 0) {\n\tprintf(\"H5Fget_vfd_handle for file2 failed\\n\");\n\tgoto error;\n    } \n\n    if(MPI_File_close(mpifh_p) != MPI_SUCCESS) {\n\tprintf(\"MPI_File_close for file2 failed\\n\");\n\tgoto error;\n    } \n\n\n    fflush(stdout);\n    fflush(stderr);\n    HD_exit(0);\n\nerror:\n    fflush(stdout);\n    fflush(stderr);\n    HD_exit(1);\n}", "label": "int\nmain(int argc, char* argv[])\n{\n    hid_t file1, file2, fapl;\n    MPI_File\t*mpifh_p = NULL;\n    char\tname[1024];\n    const char  *envval = NULL;\n    int mpi_size, mpi_rank;\n    MPI_Comm comm  = MPI_COMM_WORLD;\n    MPI_Info info  = MPI_INFO_NULL;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(comm, &mpi_size);\n    MPI_Comm_rank(comm, &mpi_rank);\n\n    fapl = H5Pcreate(H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, comm, info);\n\n    if(mpi_rank == 0)\n\tTESTING(\"H5Fflush (part1)\");\n    envval = HDgetenv(\"HDF5_DRIVER\");\n    if(envval == NULL)\n        envval = \"nomatch\";\n    if(HDstrcmp(envval, \"split\")) {\n\t\n\n\th5_fixname(FILENAME[0], fapl, name, sizeof name);\n\tfile1 = create_file(name, fapl);\n\t\n\n\tif(H5Fflush(file1, H5F_SCOPE_GLOBAL) < 0) goto error;\n\n\t\n\n\th5_fixname(FILENAME[1], fapl, name, sizeof name);\n\tfile2 = create_file(name, fapl);\n\n\n\tif(mpi_rank == 0)\n\t    PASSED();\n\tfflush(stdout);\n\tfflush(stderr);\n    } \n\n    else {\n        SKIPPED();\n        puts(\"    Test not compatible with current Virtual File Driver\");\n    } \n\n\n    \n\n\n    \n\n    if(H5Fget_vfd_handle(file1, fapl, (void **)&mpifh_p) < 0) {\n\tprintf(\"H5Fget_vfd_handle for file1 failed\\n\");\n\tgoto error;\n    } \n\n    if(MPI_File_close(mpifh_p) != MPI_SUCCESS) {\n\tprintf(\"MPI_File_close for file1 failed\\n\");\n\tgoto error;\n    } \n\n    \n\n    if(H5Fget_vfd_handle(file2, fapl, (void **)&mpifh_p) < 0) {\n\tprintf(\"H5Fget_vfd_handle for file2 failed\\n\");\n\tgoto error;\n    } \n\n    if(MPI_File_close(mpifh_p) != MPI_SUCCESS) {\n\tprintf(\"MPI_File_close for file2 failed\\n\");\n\tgoto error;\n    } \n\n\n    fflush(stdout);\n    fflush(stderr);\n    MPI_Finalize();\n    HD_exit(0);\n\nerror:\n    fflush(stdout);\n    fflush(stderr);\n    MPI_Finalize();\n    HD_exit(1);\n}"}
{"program": "pboueke_1335", "code": "int main(int argc, char** argv) {\n\n  int size, world_size, world_rank, Spartner_rank, Rpartner_rank, i;\n\n  size = 1000;\n\n  float A[size], B[size];\n\n\n\n\n  Spartner_rank = (world_rank + 1) % (world_size);\n  Rpartner_rank = (world_rank - 1); if (Rpartner_rank < 0){Rpartner_rank = world_size - 1;}\n  printf(\"WS: %d, MyRank: %d, SendP: %d, RecP: %d\\n\", world_size, world_rank, Spartner_rank, Rpartner_rank);\n\n  MPI_Request req[2];\n\tMPI_Status stat[2];\n\n  \n\n  \n\n\n  for (i = 0; i < size; i += 1)\n  {\n    A[i] = (float)world_rank;\n  }\n\n  printf(\"Proccess %d sent to proccess %d A[100]: %f  \\n\", world_rank, Spartner_rank, A[100]);\n\n\n\n  printf(\"Hello from rank %d, reading from %d value of B[100]: %f \\n\", world_rank, Rpartner_rank, B[100]);\n\n}", "label": "int main(int argc, char** argv) {\n\n  int size, world_size, world_rank, Spartner_rank, Rpartner_rank, i;\n\n  size = 1000;\n\n  float A[size], B[size];\n\n  MPI_Init(NULL, NULL);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  Spartner_rank = (world_rank + 1) % (world_size);\n  Rpartner_rank = (world_rank - 1); if (Rpartner_rank < 0){Rpartner_rank = world_size - 1;}\n  printf(\"WS: %d, MyRank: %d, SendP: %d, RecP: %d\\n\", world_size, world_rank, Spartner_rank, Rpartner_rank);\n\n  MPI_Request req[2];\n\tMPI_Status stat[2];\n\n  \n\n  \n\n\n  for (i = 0; i < size; i += 1)\n  {\n    A[i] = (float)world_rank;\n  }\n\n  MPI_Isend(A, size, MPI_INT, Spartner_rank, 0, MPI_COMM_WORLD, &req[0]);\n  printf(\"Proccess %d sent to proccess %d A[100]: %f  \\n\", world_rank, Spartner_rank, A[100]);\n\n  MPI_Irecv(&B, size, MPI_INT, Rpartner_rank, 0, MPI_COMM_WORLD, &req[1]);\n\n  MPI_Waitall(2, req, stat);\n\n  printf(\"Hello from rank %d, reading from %d value of B[100]: %f \\n\", world_rank, Rpartner_rank, B[100]);\n\n  MPI_Finalize();\n}"}
{"program": "haohaibo_1338", "code": "int main(int argc, char* argv[]){\n    int numtasks, rank, dest, source, rc, count, tag=1;\n    char inmsg, outmsg='x';\n    MPI_Status Stat;\n\n\n    if(rank == 0)\n    {\n        dest = 1;\n        source = 1;\n        rc =\n        rc =\n    }else if(rank == 1)\n    {\n        dest = 0;\n        source = 0;\n        rc =\n        rc =\n    }\n\n    rc =\n    printf(\"Task %d: Received %d char(s) from task %d with tag %d \\n\",\n            rank, count, Stat.MPI_SOURCE, Stat.MPI_TAG);\n    return 0;\n}", "label": "int main(int argc, char* argv[]){\n    int numtasks, rank, dest, source, rc, count, tag=1;\n    char inmsg, outmsg='x';\n    MPI_Status Stat;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0)\n    {\n        dest = 1;\n        source = 1;\n        rc = MPI_Send(&outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);\n        rc = MPI_Recv(&inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &Stat);\n    }else if(rank == 1)\n    {\n        dest = 0;\n        source = 0;\n        rc = MPI_Recv(&inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &Stat);\n        rc = MPI_Send(&outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);\n    }\n\n    rc = MPI_Get_count(&Stat, MPI_CHAR, &count);\n    printf(\"Task %d: Received %d char(s) from task %d with tag %d \\n\",\n            rank, count, Stat.MPI_SOURCE, Stat.MPI_TAG);\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "joao-lima_1340", "code": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\n\tif (size%2 != 0)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need a even number of processes.\\n\");\n\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, STARPU_MAIN_RAM, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tint nloops = NITER;\n\tint loop;\n\tint other_rank = rank%2 == 0 ? rank+1 : rank-1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif ((loop % 2) == (rank%2))\n\t\t{\n\t\t\tstarpu_mpi_send(tab_handle, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\n\tstarpu_data_unregister(tab_handle);\n\tfree(tab);\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tint ret, rank, size;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size%2 != 0)\n\t{\n\t\tif (rank == 0)\n\t\t\tFPRINTF(stderr, \"We need a even number of processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn STARPU_TEST_SKIPPED;\n\t}\n\n\tret = starpu_init(NULL);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_init\");\n\tret = starpu_mpi_init(NULL, NULL, 0);\n\tSTARPU_CHECK_RETURN_VALUE(ret, \"starpu_mpi_init\");\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, STARPU_MAIN_RAM, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tint nloops = NITER;\n\tint loop;\n\tint other_rank = rank%2 == 0 ? rank+1 : rank-1;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif ((loop % 2) == (rank%2))\n\t\t{\n\t\t\tstarpu_mpi_send(tab_handle, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tMPI_Status status;\n\t\t\tstarpu_mpi_req req;\n\t\t\tstarpu_mpi_irecv(tab_handle, &req, other_rank, loop, MPI_COMM_WORLD);\n\t\t\tstarpu_mpi_wait(&req, &status);\n\t\t}\n\t}\n\n\tstarpu_data_unregister(tab_handle);\n\tfree(tab);\n\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "darshanhegde_1341", "code": "int main (int argc, char *argv[])\n{\nint   numtasks, taskid, rc, dest, offset, i, j, tag1,\n      tag2, source, chunksize; \nfloat mysum, sum;\nfloat update(int myoffset, int chunk, int myid);\nMPI_Status status;\n\nMPI_Init(&argc, &argv);\n    \n\n\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nif (numtasks % 4 != 0) {\n   printf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n   exit(0);\n   }\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nprintf (\"MPI task %d has started...\\n\", taskid);\nchunksize = (ARRAYSIZE / numtasks);\ntag2 = 1;\ntag1 = 2;\n\n\n\nif (taskid == MASTER){\n\n  \n\n  sum = 0;\n  for(i=0; i<ARRAYSIZE; i++) {\n    data[i] =  i * 1.0;\n    sum = sum + data[i];\n    }\n  printf(\"Initialized array sum = %e\\n\",sum);\n\n  \n\n  offset = chunksize;\n  for (dest=1; dest<numtasks; dest++) {\n    printf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n    offset = offset + chunksize;\n    }\n\n  \n\n  offset = 0;\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  for (i=1; i<numtasks; i++) {\n    source = i;\n    }\n\n  \n  \n  printf(\"Sample results: \\n\");\n  offset = 0;\n  for (i=0; i<numtasks; i++) {\n    for (j=0; j<5; j++) \n      printf(\"  %e\",data[offset+j]);\n    printf(\"\\n\");\n    offset = offset + chunksize;\n    }\n  printf(\"*** Final sum= %e ***\\n\",sum);\n\n  }  \n\n\n\n\n\n\n\nif (taskid > MASTER) {\n\n  \n\n  source = MASTER;\n\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  dest = MASTER;\n\n\n  } \n\n\n\n\n}", "label": "int main (int argc, char *argv[])\n{\nint   numtasks, taskid, rc, dest, offset, i, j, tag1,\n      tag2, source, chunksize; \nfloat mysum, sum;\nfloat update(int myoffset, int chunk, int myid);\nMPI_Status status;\n\nMPI_Init(&argc, &argv);\n    \n\n\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nif (numtasks % 4 != 0) {\n   printf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n   MPI_Abort(MPI_COMM_WORLD, rc);\n   exit(0);\n   }\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nprintf (\"MPI task %d has started...\\n\", taskid);\nchunksize = (ARRAYSIZE / numtasks);\ntag2 = 1;\ntag1 = 2;\n\n\n\nif (taskid == MASTER){\n\n  \n\n  sum = 0;\n  for(i=0; i<ARRAYSIZE; i++) {\n    data[i] =  i * 1.0;\n    sum = sum + data[i];\n    }\n  printf(\"Initialized array sum = %e\\n\",sum);\n\n  \n\n  offset = chunksize;\n  for (dest=1; dest<numtasks; dest++) {\n    MPI_Send(&offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n    MPI_Send(&data[offset], chunksize, MPI_FLOAT, dest, tag2, MPI_COMM_WORLD);\n    printf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n    offset = offset + chunksize;\n    }\n\n  \n\n  offset = 0;\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  for (i=1; i<numtasks; i++) {\n    source = i;\n    MPI_Recv(&offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);\n    MPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n      MPI_COMM_WORLD, &status);\n    }\n\n  \n  \n  MPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n  printf(\"Sample results: \\n\");\n  offset = 0;\n  for (i=0; i<numtasks; i++) {\n    for (j=0; j<5; j++) \n      printf(\"  %e\",data[offset+j]);\n    printf(\"\\n\");\n    offset = offset + chunksize;\n    }\n  printf(\"*** Final sum= %e ***\\n\",sum);\n\n  }  \n\n\n\n\n\n\n\nif (taskid > MASTER) {\n\n  \n\n  source = MASTER;\n  MPI_Recv(&offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);\n  MPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2, \n    MPI_COMM_WORLD, &status);\n\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  dest = MASTER;\n  MPI_Send(&offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n  MPI_Send(&data[offset], chunksize, MPI_FLOAT, MASTER, tag2, MPI_COMM_WORLD);\n\n  MPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\n  } \n\n\n\n    MPI_Finalize();\n\n}"}
{"program": "callmetaste_1342", "code": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,len,n,choice;\n   MPI_Status status; \n\n   adainit();\n\n   if(myid == 0) choice = prompt_for_precision();\n   \n\n\n   if(choice == 0)\n   {\n      standard_dimension_broadcast(myid,&len,&n);\n      standard_solutions_broadcast(myid,len,n);\n      standard_print_broadcast(myid);\n   }\n   else if(choice == 1)\n   {\n      dobldobl_dimension_broadcast(myid,&len,&n);\n      dobldobl_solutions_broadcast(myid,len,n);\n      dobldobl_print_broadcast(myid);\n   }\n   else if(choice == 2)\n   {\n      quaddobl_dimension_broadcast(myid,&len,&n);\n      quaddobl_solutions_broadcast(myid,len,n);\n      quaddobl_print_broadcast(myid);\n   }\n   else\n      if(myid == 0) printf(\"Invalid selection.\\n\");\n\n\n   adafinal();\n\n   return 0;\n}", "label": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,len,n,choice;\n   MPI_Status status; \n\n   adainit();\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n\n   if(myid == 0) choice = prompt_for_precision();\n   \n\n   MPI_Bcast(&choice,1,MPI_INT,0,MPI_COMM_WORLD);  \n\n   if(choice == 0)\n   {\n      standard_dimension_broadcast(myid,&len,&n);\n      standard_solutions_broadcast(myid,len,n);\n      standard_print_broadcast(myid);\n   }\n   else if(choice == 1)\n   {\n      dobldobl_dimension_broadcast(myid,&len,&n);\n      dobldobl_solutions_broadcast(myid,len,n);\n      dobldobl_print_broadcast(myid);\n   }\n   else if(choice == 2)\n   {\n      quaddobl_dimension_broadcast(myid,&len,&n);\n      quaddobl_solutions_broadcast(myid,len,n);\n      quaddobl_print_broadcast(myid);\n   }\n   else\n      if(myid == 0) printf(\"Invalid selection.\\n\");\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   MPI_Finalize();\n   adafinal();\n\n   return 0;\n}"}
{"program": "lellisls_1345", "code": "int main( int argc, char *argv[] ) {\n  int myid, numprocs;\n  long int n, i;\n  double PI25DT = 3.141592653589793238462643;\n  double mypi, pi, h, sum, x;\n  MPI_Win nwin, piwin;\n\n  if( myid == 0 ) {\n  }\n  else {\n  }\n  while( 1 ) {\n    if( myid == 0 ) {\n      \n\n      n = 100000000;\n      pi = 0.0;\n    }\n    if( myid != 0 ) {\n    }\n    if( n == 0 ) {\n      break;\n    }\n    else {\n      h = 1.0 / ( double ) n;\n      sum = 0.0;\n      for( i = myid + 1; i <= n; i += numprocs ) {\n        x = h * ( ( double ) i - 0.5 );\n        sum += ( 4.0 / ( 1.0 + x * x ) );\n      }\n      mypi = h * sum;\n      if( myid == 0 ) {\n        printf( \"pi is approximately %.16f, Error is %.16f\\n\",\n                pi, fabs( pi - PI25DT ) );\n      }\n    }\n    break;\n  }\n  return( 0 );\n}", "label": "int main( int argc, char *argv[] ) {\n  int myid, numprocs;\n  long int n, i;\n  double PI25DT = 3.141592653589793238462643;\n  double mypi, pi, h, sum, x;\n  MPI_Win nwin, piwin;\n\n  MPI_Init( &argc, &argv );\n  MPI_Comm_size( MPI_COMM_WORLD, &numprocs );\n  MPI_Comm_rank( MPI_COMM_WORLD, &myid );\n  if( myid == 0 ) {\n    MPI_Win_create( &n, sizeof( int ), 1, MPI_INFO_NULL,\n                    MPI_COMM_WORLD, &nwin );\n    MPI_Win_create( &pi, sizeof( double ), 1, MPI_INFO_NULL,\n                    MPI_COMM_WORLD, &piwin );\n  }\n  else {\n    MPI_Win_create( MPI_BOTTOM, 0, 1, MPI_INFO_NULL,\n                    MPI_COMM_WORLD, &nwin );\n    MPI_Win_create( MPI_BOTTOM, 0, 1, MPI_INFO_NULL,\n                    MPI_COMM_WORLD, &piwin );\n  }\n  while( 1 ) {\n    if( myid == 0 ) {\n      \n\n      n = 100000000;\n      pi = 0.0;\n    }\n    MPI_Win_fence( 0, nwin );\n    if( myid != 0 ) {\n      MPI_Get( &n, 1, MPI_INT, 0, 0, 1, MPI_INT, nwin );\n    }\n    MPI_Win_fence( 0, nwin );\n    if( n == 0 ) {\n      break;\n    }\n    else {\n      h = 1.0 / ( double ) n;\n      sum = 0.0;\n      for( i = myid + 1; i <= n; i += numprocs ) {\n        x = h * ( ( double ) i - 0.5 );\n        sum += ( 4.0 / ( 1.0 + x * x ) );\n      }\n      mypi = h * sum;\n      MPI_Win_fence( 0, piwin );\n      MPI_Accumulate( &mypi, 1, MPI_DOUBLE, 0, 0, 1, MPI_DOUBLE,\n                      MPI_SUM, piwin );\n      MPI_Win_fence( 0, piwin );\n      if( myid == 0 ) {\n        printf( \"pi is approximately %.16f, Error is %.16f\\n\",\n                pi, fabs( pi - PI25DT ) );\n      }\n    }\n    break;\n  }\n  MPI_Win_free( &nwin );\n  MPI_Win_free( &piwin );\n  MPI_Finalize( );\n  return( 0 );\n}"}
{"program": "milfeld2_1351", "code": "int main(int argc, char **argv){\nint rank, nranks;         \n\nint nthrds, thrd, cpuid;  \n\n\nint nsec = 10;     \n\n\nint ierr;          \n\n\n   cmdln_get_nsec_or_help( &nsec, argc, argv); \n\n\n\n   hybrid_report_mask();                     \n\n\n   #pragma omp parallel private(thrd,ierr)\n   {\n      thrd   =   omp_get_thread_num();\n      nthrds =   omp_get_num_threads();\n\n      cpuid  =   thrd;                     \n\n  \n\n\n      hybrid_report_mask();        \n\n      load_cpu_nsec( nsec );       \n\n   }\n\n\n}", "label": "int main(int argc, char **argv){\nint rank, nranks;         \n\nint nthrds, thrd, cpuid;  \n\n\nint nsec = 10;     \n\n\nint ierr;          \n\n\n   cmdln_get_nsec_or_help( &nsec, argc, argv); \n\n\n   MPI_Init(&argc, &argv);\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   hybrid_report_mask();                     \n\n\n   #pragma omp parallel private(thrd,ierr)\n   {\n      thrd   =   omp_get_thread_num();\n      nthrds =   omp_get_num_threads();\n\n      cpuid  =   thrd;                     \n\n  \n\n\n      hybrid_report_mask();        \n\n      load_cpu_nsec( nsec );       \n\n   }\n\n   MPI_Finalize();\n\n}"}
{"program": "alucas_1352", "code": "int main(int argc, char **argv)\n{\n\n\tint rank, size;\n\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif (rank == 0)\n\t\t{\n\t\t\tstarpu_mpi_send(tab_handle, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tint received = 0;\n\t\t\tstarpu_mpi_irecv_detached(tab_handle, other_rank, loop, MPI_COMM_WORLD, callback, &received);\n\n\t\t\tpthread_mutex_lock(&mutex);\n\t\t\twhile (!received)\n\t\t\t\tpthread_cond_wait(&cond, &mutex);\n\t\t\tpthread_mutex_unlock(&mutex);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv)\n{\n\tMPI_Init(NULL, NULL);\n\n\tint rank, size;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size != 2)\n\t{\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"We need exactly 2 processes.\\n\");\n\n\t\tMPI_Finalize();\n\t\treturn 0;\n\t}\n\n\tstarpu_init(NULL);\n\tstarpu_mpi_initialize();\n\n\ttab = malloc(SIZE*sizeof(float));\n\n\tstarpu_vector_data_register(&tab_handle, 0, (uintptr_t)tab, SIZE, sizeof(float));\n\n\tunsigned nloops = NITER;\n\tunsigned loop;\n\n\tint other_rank = (rank + 1)%2;\n\n\tfor (loop = 0; loop < nloops; loop++)\n\t{\n\t\tif (rank == 0)\n\t\t{\n\t\t\tstarpu_mpi_send(tab_handle, other_rank, loop, MPI_COMM_WORLD);\n\t\t}\n\t\telse {\n\t\t\tMPI_Status status;\n\t\t\tint received = 0;\n\t\t\tstarpu_mpi_irecv_detached(tab_handle, other_rank, loop, MPI_COMM_WORLD, callback, &received);\n\n\t\t\tpthread_mutex_lock(&mutex);\n\t\t\twhile (!received)\n\t\t\t\tpthread_cond_wait(&cond, &mutex);\n\t\t\tpthread_mutex_unlock(&mutex);\n\t\t}\n\t}\n\t\n\tstarpu_mpi_shutdown();\n\tstarpu_shutdown();\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "raulperula_1355", "code": "int main (int argc,char*argv[])\n{\n\tint numtasks, taskid, rc, dest, offset, i, j, tag1, tag2, source, chunksize;\n\tfloat mysum, sum;\n\tfloat update(int myoffset,int chunk,int myid);\n\n\t\n\n\n\n\tMPI_Status status;\n\t\n\t\n\n\n\tif(numtasks%4 != 0){\n\t\tprintf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n\t\texit(0);\n\t}\n\t\n\tprintf (\"MPI task %d has started...\\n\", taskid);\n\tchunksize =(ARRAYSIZE / numtasks);\n\ttag2 = 1;\n\ttag1 = 2;\n\t\n\t\n\n\tif(taskid == MASTER){\n\t\t\n\n\t\tsum = 0;\n\t\tfor(i=0; i<ARRAYSIZE; i++){\n\t\t\tdata[i] = i *1.0;\n\t\t\tsum = sum+data[i];\n\t\t}\n\t\tprintf(\"Initialized array sum = %e\\n\",sum);\n\t\t\n\t\t\n\n\t\toffset = chunksize;\n\t\tfor(dest=1; dest<numtasks; dest++){\n\t\t\tprintf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\t\n\t\t\n\n\t\toffset = 0;\n\t\tmysum = update(offset, chunksize, taskid);\n\t\t\n\t\t\n\n\t\tfor(i=1; i<numtasks; i++){\n\t\t\tsource = i;\n\t\t}\n\t\t\n\t\t\n\n\t\tprintf(\"Sample results: \\n\");\n\t\toffset = 0;\n\t\t\n\t\tfor(i=0; i<numtasks; i++){\n\t\t\tfor(j=0; j<5; j++)\n\t\t\t\tprintf(\"  %e\",data[offset+j]);\n\t\t\tprintf(\"\\n\");\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\tprintf(\"*** Final sum= %e ***\\n\",sum);\n\t}\n\n\t\n\t\n\n\tif(taskid > MASTER){\n\t\t\n\n\t\tsource = MASTER;\n\t\tmysum = update(offset, chunksize, taskid);\n\t\t\n\t\t\n\n\t\tdest = MASTER;\n\t}\n\n\t\n\n\t\n}/* e", "label": "int main (int argc,char*argv[])\n{\n\tint numtasks, taskid, rc, dest, offset, i, j, tag1, tag2, source, chunksize;\n\tfloat mysum, sum;\n\tfloat update(int myoffset,int chunk,int myid);\n\n\t\n\n\tMPI_Init(&argc,&argv); \n\n\n\tMPI_Status status;\n\t\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n\n\tif(numtasks%4 != 0){\n\t\tprintf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n\t\tMPI_Abort(MPI_COMM_WORLD, rc);\n\t\texit(0);\n\t}\n\t\n\tMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n\tprintf (\"MPI task %d has started...\\n\", taskid);\n\tchunksize =(ARRAYSIZE / numtasks);\n\ttag2 = 1;\n\ttag1 = 2;\n\t\n\t\n\n\tif(taskid == MASTER){\n\t\t\n\n\t\tsum = 0;\n\t\tfor(i=0; i<ARRAYSIZE; i++){\n\t\t\tdata[i] = i *1.0;\n\t\t\tsum = sum+data[i];\n\t\t}\n\t\tprintf(\"Initialized array sum = %e\\n\",sum);\n\t\t\n\t\t\n\n\t\toffset = chunksize;\n\t\tfor(dest=1; dest<numtasks; dest++){\n\t\t\tMPI_Send(&offset,1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&data[offset],   chunksize,   MPI_FLOAT,   dest,   tag2,\n\t\t\tMPI_COMM_WORLD);\n\t\t\tprintf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\t\n\t\t\n\n\t\toffset = 0;\n\t\tmysum = update(offset, chunksize, taskid);\n\t\t\n\t\t\n\n\t\tfor(i=1; i<numtasks; i++){\n\t\t\tsource = i;\n\t\t\tMPI_Recv(&offset,1, MPI_INT, source, tag1, MPI_COMM_WORLD,&status);\n\t\t\tMPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n\t\t\tMPI_COMM_WORLD,&status);\n\t\t}\n\t\t\n\t\t\n\n\t\tMPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\t\tprintf(\"Sample results: \\n\");\n\t\toffset = 0;\n\t\t\n\t\tfor(i=0; i<numtasks; i++){\n\t\t\tfor(j=0; j<5; j++)\n\t\t\t\tprintf(\"  %e\",data[offset+j]);\n\t\t\tprintf(\"\\n\");\n\t\t\toffset = offset + chunksize;\n\t\t}\n\t\tprintf(\"*** Final sum= %e ***\\n\",sum);\n\t}\n\n\t\n\t\n\n\tif(taskid > MASTER){\n\t\t\n\n\t\tsource = MASTER;\n\t\tMPI_Recv(&offset,1, MPI_INT, source, tag1, MPI_COMM_WORLD,&status);\n\t\tMPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2, MPI_COMM_WORLD,&status);\n\t\tmysum = update(offset, chunksize, taskid);\n\t\t\n\t\t\n\n\t\tdest = MASTER;\n\t\tMPI_Send(&offset,1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n\t\tMPI_Send(&data[offset], chunksize, MPI_FLOAT, MASTER, tag2, MPI_COMM_WORLD);\n\t\tMPI_Reduce(&mysum,&sum,1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\t}\n\n\t\n\tMPI_Finalize(); \n\n\t\n}/* e"}
{"program": "tcew_1356", "code": "int main(int argc, char **argv){\n\n\n  \n\n  \n\n  undeadlockV2();\n  \n  \n  exit(0);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n\n  MPI_Init(&argc, &argv);\n\n  \n\n  \n\n  undeadlockV2();\n  \n  MPI_Finalize();\n  \n  exit(0);\n  return 0;\n}"}
{"program": "dash-project_1358", "code": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double t1, t2;\n  \n  int req, prov;\n  req = MPI_THREAD_SINGLE;\n\n\n  PMPI_Type_size(DATATYPE, &dsize);\n  \n  buf=(char*)malloc(SIZE*dsize);\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds with IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  \n\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      PMPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds without IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n\n  fopen(\"/dev/null\", \"r\");  \n}", "label": "int main( int argc, char* argv[] )\n{\n  int i;\n  int myrank, nprocs;\n  char *buf;\n  int dsize;\n  double t1, t2;\n  \n  int req, prov;\n  req = MPI_THREAD_SINGLE;\n\n  MPI_Init( &argc, &argv);\n\n  MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n  MPI_Comm_size( MPI_COMM_WORLD, &nprocs );\n  PMPI_Type_size(DATATYPE, &dsize);\n  \n  buf=(char*)malloc(SIZE*dsize);\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds with IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  \n\n\n  IPM_TIMESTAMP(t1);\n  for( i=0; i<REPEAT; i++ )\n    {\n      PMPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n    }\n  IPM_TIMESTAMP(t2);\n\n  fprintf(stderr, \"Processed %d events in %f seconds without IPM\\n\", \n\t  REPEAT, (t2-t1));\n\n  MPI_Finalize();\n\n  fopen(\"/dev/null\", \"r\");  \n}"}
{"program": "nschloe_1359", "code": "int main(int argc, char *argv[])\n{\nint have_my_cpu=1;                \n\nchar *recvbuf=NULL;\nchar type_name[MAX_NAME_LEN];    \n\nchar mask[MAX_NAME_LEN];\nhwloc_topology_t topology;\nconst struct hwloc_topology_support *support=NULL;\nhwloc_obj_t obj;\nhwloc_cpuset_t *cpuset=NULL;\nhwloc_cpuset_t binding;\nuint64_t local_memory;\nuint64_t total_memory;\nint size, rank, depth;\nint i, j, p;\nint rc, num;\n\n\n  \n\n\n  hwloc_topology_init(&topology);\n\n#if 0\n  \n\n  hwloc_topology_ignore_all_keep_structure(topology);\n#endif\n\n  \n\n\n  hwloc_topology_load(topology);\n\n  \n\n\n  support = hwloc_topology_get_support((struct hwloc_topology *)topology);\n\n  \n\n\n  depth = hwloc_topology_get_depth(topology);\n\n  \n\n\n  binding = hwloc_cpuset_alloc();\n  hwloc_cpuset_zero(binding);\n\n  if (support->cpubind->get_thisproc_cpubind != 1){\n    have_my_cpu = 0;\n  }\n  else{\n    rc = hwloc_get_cpubind(topology, binding, HWLOC_CPUBIND_STRICT);\n  \n    if ((rc < 0) || (hwloc_cpuset_weight(binding) > 1)){\n      have_my_cpu = 0;\n    }\n  }\n\n\n  if (!have_my_cpu && (rank == 0)){\n    printf(\"Warning: Unable to identify each MPI process with its CPU\\n\");\n  }\n\n  if (rank == 0){\n    recvbuf = (char *)calloc(size , MAX_NAME_LEN);\n  }\n  else{\n    recvbuf = NULL;\n  }\n\n  if (have_my_cpu){\n\n    hwloc_cpuset_snprintf(mask, MAX_NAME_LEN-1, binding);\n \n    if (rank == 0){\n      cpuset = (hwloc_cpuset_t*)malloc(sizeof(hwloc_cpuset_t) * size);\n\n      for (p=0; p < size; p++){\n        cpuset[p] = hwloc_cpuset_alloc();\n        hwloc_cpuset_from_string(cpuset[p], recvbuf + (p * MAX_NAME_LEN));\n      }\n    }\n\n  }\n\n  if (rank == 0){\n\n    \n\n\n    for (i=0; i < depth; i++){\n\n      num = hwloc_get_nbobjs_by_depth(topology, i);\n   \n      for (j = 0; j < num; j++){\n\n        obj = hwloc_get_obj_by_depth(topology, i, j);\n\n        if (j==0){\n          hwloc_obj_type_snprintf(type_name, MAX_NAME_LEN-1, obj, 1);\n          printf(\"\\n%d %s%s:\\n\",num,type_name, ((num> 1) ? \"s\" : \"\")); \n        }\n\n        hwloc_cpuset_snprintf(mask, MAX_NAME_LEN - 1, obj->cpuset); \n        local_memory = obj->memory.local_memory;\n        total_memory= obj->memory.total_memory;\n\n        printf(\"  cpu set mask %s,  total memory %10.0fKB, local memory %10.0fKB\",\n              mask, (float)total_memory/1024.0, (float)local_memory/1024.0);\n\n        if (have_my_cpu){\n          printf(\", ( MPI ranks \");\n          for (p=0; p < size; p++){\n            if (hwloc_cpuset_isincluded(cpuset[p], obj->cpuset)){\n              printf(\"%d \",p);\n            }\n          }\n        }\n        printf(\")\\n\");\n      }\n    }\n\n    if (cpuset){\n      for (p=0; p < size; p++){\n        hwloc_cpuset_free(cpuset[p]);\n      }\n      free(cpuset);\n    }\n    free(recvbuf);\n    printf(\"\\n%s\\n\",which_mpi());\n  }\n\n  hwloc_cpuset_free(binding);\n\n\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\nint have_my_cpu=1;                \n\nchar *recvbuf=NULL;\nchar type_name[MAX_NAME_LEN];    \n\nchar mask[MAX_NAME_LEN];\nhwloc_topology_t topology;\nconst struct hwloc_topology_support *support=NULL;\nhwloc_obj_t obj;\nhwloc_cpuset_t *cpuset=NULL;\nhwloc_cpuset_t binding;\nuint64_t local_memory;\nuint64_t total_memory;\nint size, rank, depth;\nint i, j, p;\nint rc, num;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  \n\n\n  hwloc_topology_init(&topology);\n\n#if 0\n  \n\n  hwloc_topology_ignore_all_keep_structure(topology);\n#endif\n\n  \n\n\n  hwloc_topology_load(topology);\n\n  \n\n\n  support = hwloc_topology_get_support((struct hwloc_topology *)topology);\n\n  \n\n\n  depth = hwloc_topology_get_depth(topology);\n\n  \n\n\n  binding = hwloc_cpuset_alloc();\n  hwloc_cpuset_zero(binding);\n\n  if (support->cpubind->get_thisproc_cpubind != 1){\n    have_my_cpu = 0;\n  }\n  else{\n    rc = hwloc_get_cpubind(topology, binding, HWLOC_CPUBIND_STRICT);\n  \n    if ((rc < 0) || (hwloc_cpuset_weight(binding) > 1)){\n      have_my_cpu = 0;\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (!have_my_cpu && (rank == 0)){\n    printf(\"Warning: Unable to identify each MPI process with its CPU\\n\");\n  }\n\n  if (rank == 0){\n    recvbuf = (char *)calloc(size , MAX_NAME_LEN);\n  }\n  else{\n    recvbuf = NULL;\n  }\n\n  if (have_my_cpu){\n\n    hwloc_cpuset_snprintf(mask, MAX_NAME_LEN-1, binding);\n    MPI_Gather(mask, MAX_NAME_LEN, MPI_CHAR, recvbuf, MAX_NAME_LEN, MPI_CHAR, 0, MPI_COMM_WORLD);\n \n    if (rank == 0){\n      cpuset = (hwloc_cpuset_t*)malloc(sizeof(hwloc_cpuset_t) * size);\n\n      for (p=0; p < size; p++){\n        cpuset[p] = hwloc_cpuset_alloc();\n        hwloc_cpuset_from_string(cpuset[p], recvbuf + (p * MAX_NAME_LEN));\n      }\n    }\n\n  }\n\n  if (rank == 0){\n\n    \n\n\n    for (i=0; i < depth; i++){\n\n      num = hwloc_get_nbobjs_by_depth(topology, i);\n   \n      for (j = 0; j < num; j++){\n\n        obj = hwloc_get_obj_by_depth(topology, i, j);\n\n        if (j==0){\n          hwloc_obj_type_snprintf(type_name, MAX_NAME_LEN-1, obj, 1);\n          printf(\"\\n%d %s%s:\\n\",num,type_name, ((num> 1) ? \"s\" : \"\")); \n        }\n\n        hwloc_cpuset_snprintf(mask, MAX_NAME_LEN - 1, obj->cpuset); \n        local_memory = obj->memory.local_memory;\n        total_memory= obj->memory.total_memory;\n\n        printf(\"  cpu set mask %s,  total memory %10.0fKB, local memory %10.0fKB\",\n              mask, (float)total_memory/1024.0, (float)local_memory/1024.0);\n\n        if (have_my_cpu){\n          printf(\", ( MPI ranks \");\n          for (p=0; p < size; p++){\n            if (hwloc_cpuset_isincluded(cpuset[p], obj->cpuset)){\n              printf(\"%d \",p);\n            }\n          }\n        }\n        printf(\")\\n\");\n      }\n    }\n\n    if (cpuset){\n      for (p=0; p < size; p++){\n        hwloc_cpuset_free(cpuset[p]);\n      }\n      free(cpuset);\n    }\n    free(recvbuf);\n    printf(\"\\n%s\\n\",which_mpi());\n  }\n\n  hwloc_cpuset_free(binding);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "dflowers7_1361", "code": "int main(int argc, char *argv[])\n{\n  realtype abstol, reltol, t, tout;\n  N_Vector u;\n  UserData data;\n  void *cvode_mem;\n  int iout, flag, my_pe, npes;\n  long int neq, local_N;\n  MPI_Comm comm;\n\n  u = NULL;\n  data = NULL;\n  cvode_mem = NULL;\n\n  \n\n  neq = NVARS*MX*MY;\n\n  \n\n  comm = MPI_COMM_WORLD;\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      fprintf(stderr, \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\\n\",\n\t      npes,NPEX*NPEY);\n    return(1);\n  }\n\n  \n\n  local_N = NVARS*MXSUB*MYSUB;\n\n  \n\n  data = (UserData) malloc(sizeof *data);\n  InitUserData(my_pe, comm, data);\n\n  \n \n  u = N_VNew_Parallel(comm, local_N, neq);\n  SetInitialProfiles(u, data);\n  abstol = ATOL; reltol = RTOL;\n\n  \n\n  cvode_mem = CVodeCreate(CV_BDF, CV_NEWTON);\n\n  \n\n  flag = CVodeSetUserData(cvode_mem, data);\n\n  \n\n  flag = CVodeInit(cvode_mem, f, T0, u);\n  if(check_flag(&flag, \"CVodeInit\", 1, my_pe)) return(1);\n\n  \n\n  flag = CVodeSStolerances(cvode_mem, reltol, abstol);\n  if (check_flag(&flag, \"CVodeSStolerances\", 1, my_pe)) return(1);\n\n  \n\n  flag = CVSpgmr(cvode_mem, PREC_LEFT, 0);\n\n  \n\n  flag = CVSpilsSetPreconditioner(cvode_mem, Precond, PSolve);\n\n  if (my_pe == 0)\n    printf(\"\\n2-species diurnal advection-diffusion problem\\n\\n\");\n\n  \n\n  for (iout=1, tout = TWOHR; iout <= NOUT; iout++, tout += TWOHR) {\n    flag = CVode(cvode_mem, tout, u, &t, CV_NORMAL);\n    if (check_flag(&flag, \"CVode\", 1, my_pe)) break;\n    PrintOutput(cvode_mem, my_pe, comm, u, t);\n  }\n\n  \n  \n  if (my_pe == 0) PrintFinalStats(cvode_mem);\n\n  \n\n  N_VDestroy_Parallel(u);\n  FreeUserData(data);\n  CVodeFree(&cvode_mem);\n\n\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  realtype abstol, reltol, t, tout;\n  N_Vector u;\n  UserData data;\n  void *cvode_mem;\n  int iout, flag, my_pe, npes;\n  long int neq, local_N;\n  MPI_Comm comm;\n\n  u = NULL;\n  data = NULL;\n  cvode_mem = NULL;\n\n  \n\n  neq = NVARS*MX*MY;\n\n  \n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &my_pe);\n\n  if (npes != NPEX*NPEY) {\n    if (my_pe == 0)\n      fprintf(stderr, \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\\n\",\n\t      npes,NPEX*NPEY);\n    MPI_Finalize();\n    return(1);\n  }\n\n  \n\n  local_N = NVARS*MXSUB*MYSUB;\n\n  \n\n  data = (UserData) malloc(sizeof *data);\n  if (check_flag((void *)data, \"malloc\", 2, my_pe)) MPI_Abort(comm, 1);\n  InitUserData(my_pe, comm, data);\n\n  \n \n  u = N_VNew_Parallel(comm, local_N, neq);\n  if (check_flag((void *)u, \"N_VNew\", 0, my_pe)) MPI_Abort(comm, 1);\n  SetInitialProfiles(u, data);\n  abstol = ATOL; reltol = RTOL;\n\n  \n\n  cvode_mem = CVodeCreate(CV_BDF, CV_NEWTON);\n  if (check_flag((void *)cvode_mem, \"CVodeCreate\", 0, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  flag = CVodeSetUserData(cvode_mem, data);\n  if (check_flag(&flag, \"CVodeSetUserData\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  flag = CVodeInit(cvode_mem, f, T0, u);\n  if(check_flag(&flag, \"CVodeInit\", 1, my_pe)) return(1);\n\n  \n\n  flag = CVodeSStolerances(cvode_mem, reltol, abstol);\n  if (check_flag(&flag, \"CVodeSStolerances\", 1, my_pe)) return(1);\n\n  \n\n  flag = CVSpgmr(cvode_mem, PREC_LEFT, 0);\n  if (check_flag(&flag, \"CVSpgmr\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  \n\n  flag = CVSpilsSetPreconditioner(cvode_mem, Precond, PSolve);\n  if (check_flag(&flag, \"CVSpilsSetPreconditioner\", 1, my_pe)) MPI_Abort(comm, 1);\n\n  if (my_pe == 0)\n    printf(\"\\n2-species diurnal advection-diffusion problem\\n\\n\");\n\n  \n\n  for (iout=1, tout = TWOHR; iout <= NOUT; iout++, tout += TWOHR) {\n    flag = CVode(cvode_mem, tout, u, &t, CV_NORMAL);\n    if (check_flag(&flag, \"CVode\", 1, my_pe)) break;\n    PrintOutput(cvode_mem, my_pe, comm, u, t);\n  }\n\n  \n  \n  if (my_pe == 0) PrintFinalStats(cvode_mem);\n\n  \n\n  N_VDestroy_Parallel(u);\n  FreeUserData(data);\n  CVodeFree(&cvode_mem);\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "qingu_1362", "code": "int main(int argc, char *argv[])\n{\n    int          rank, nproc;\n    int          errors = 0, all_errors = 0;\n    int          buf, my_buf;\n    MPI_Win      win;\n\n\n\n\n    \n\n    CHECK_ERR(MPI_Get(&my_buf, 1, MPI_INT, 0, 0, 1, MPI_INT, win));\n\n\n\n    if (rank == 0 && all_errors == 0) printf(\" No Errors\\n\");\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int          rank, nproc;\n    int          errors = 0, all_errors = 0;\n    int          buf, my_buf;\n    MPI_Win      win;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    MPI_Win_create(&buf, sizeof(int), sizeof(int),\n                    MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n    MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);\n\n    \n\n    CHECK_ERR(MPI_Get(&my_buf, 1, MPI_INT, 0, 0, 1, MPI_INT, win));\n\n    MPI_Win_free(&win);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0) printf(\" No Errors\\n\");\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mpip_1364", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      in);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      in);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in;\n  pfft_complex *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(2 * alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_dft_r2c_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_c2r_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real(3, n, local_ni, local_i_start,\n      in);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real(3, n, local_ni, local_i_start,\n      in);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_real(3, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "syftalent_1367", "code": "int main(int argc, char **argv)\n{\n    int rank, size, err;\n    char buf[10];\n    MPI_Request request;\n\n    if (size < 3) {\n        fprintf( stderr, \"Must run with at least 3 processes\\n\" );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =\n        err +=\n        if (err) {\n            fprintf(stderr, \"An error occurred during the send operation\\n\");\n        }\n    }\n\n    if (rank == 2) {\n        err =\n        err +=\n        if (err) {\n            fprintf(stderr, \"An error occurred during the recv operation\\n\");\n        } else {\n            printf(\" %s\\n\", buf);\n            fflush(stdout);\n        }\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, err;\n    char buf[10];\n    MPI_Request request;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size < 3) {\n        fprintf( stderr, \"Must run with at least 3 processes\\n\" );\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    if (rank == 0) {\n        err =  MPI_Isend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);\n        err += MPI_Wait(&request, MPI_STATUS_IGNORE);\n        if (err) {\n            fprintf(stderr, \"An error occurred during the send operation\\n\");\n        }\n    }\n\n    if (rank == 2) {\n        err =  MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);\n        err += MPI_Wait(&request, MPI_STATUS_IGNORE);\n        if (err) {\n            fprintf(stderr, \"An error occurred during the recv operation\\n\");\n        } else {\n            printf(\" %s\\n\", buf);\n            fflush(stdout);\n        }\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "brian-o_1368", "code": "int main(int argc, char ** argv)\n{\n    \n\n\n    \n\n    int world_size;\n\n    \n\n    int world_rank;\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n\n    \n\n    printf(\"Hello world from processor %s, rank %d\"\n           \" out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    \n\n}", "label": "int main(int argc, char ** argv)\n{\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    \n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    \n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    \n\n    printf(\"Hello world from processor %s, rank %d\"\n           \" out of %d processors\\n\",\n           processor_name, world_rank, world_size);\n\n    \n\n    MPI_Finalize();\n}"}
{"program": "gnu3ra_1371", "code": "int main( int argc, char *argv[] )\n{\n    int num_errors = 0, total_num_errors = 0;\n    int rank, size;\n    char port1[MPI_MAX_PORT_NAME];\n    char port2[MPI_MAX_PORT_NAME];\n    MPI_Status status;\n    MPI_Comm comm1, comm2;\n    int verbose = 0;\n    int data = 0;\n\n    if (getenv(\"MPITEST_VERBOSE\"))\n    {\n\tverbose = 1;\n    }\n\n\n    if (size < 3)\n    {\n\tprintf(\"Three processes needed to run this test.\\n\");\n\treturn 0;\n    }\n\n    if (rank == 0)\n    {\n\tIF_VERBOSE((\"0: opening ports.\\n\"));\n\n\tIF_VERBOSE((\"0: opened port1: <%s>\\n\", port1));\n\tIF_VERBOSE((\"0: opened port2: <%s>\\n\", port2));\n\tIF_VERBOSE((\"0: sending ports.\\n\"));\n\n\tIF_VERBOSE((\"0: accepting port2.\\n\"));\n\tIF_VERBOSE((\"0: accepting port1.\\n\"));\n\n\tIF_VERBOSE((\"0: closing ports.\\n\"));\n\n\tIF_VERBOSE((\"0: sending 1 to process 1.\\n\"));\n\tdata = 1;\n\n\tIF_VERBOSE((\"0: sending 2 to process 2.\\n\"));\n\tdata = 2;\n\n\tIF_VERBOSE((\"0: disconnecting.\\n\"));\n    }\n    else if (rank == 1)\n    {\n\tIF_VERBOSE((\"1: receiving port.\\n\"));\n\n\tIF_VERBOSE((\"1: received port1: <%s>\\n\", port1));\n\tIF_VERBOSE((\"1: connecting.\\n\"));\n\n\tif (data != 1)\n\t{\n\t    printf(\"Received %d from root when expecting 1\\n\", data);\n\t    fflush(stdout);\n\t    num_errors++;\n\t}\n\n\tIF_VERBOSE((\"1: disconnecting.\\n\"));\n    }\n    else if (rank == 2)\n    {\n\tIF_VERBOSE((\"2: receiving port.\\n\"));\n\n\tIF_VERBOSE((\"2: received port2: <%s>\\n\", port2));\n\t\n\n\tMTestSleep(3);\n\tIF_VERBOSE((\"2: connecting.\\n\"));\n\n\tif (data != 2)\n\t{\n\t    printf(\"Received %d from root when expecting 2\\n\", data);\n\t    fflush(stdout);\n\t    num_errors++;\n\t}\n\n\tIF_VERBOSE((\"2: disconnecting.\\n\"));\n    }\n\n\n    if (rank == 0)\n    {\n\tif (total_num_errors)\n\t{\n\t    printf(\" Found %d errors\\n\", total_num_errors);\n\t}\n\telse\n\t{\n\t    printf(\" No Errors\\n\");\n\t}\n\tfflush(stdout);\n    }\n    return total_num_errors;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int num_errors = 0, total_num_errors = 0;\n    int rank, size;\n    char port1[MPI_MAX_PORT_NAME];\n    char port2[MPI_MAX_PORT_NAME];\n    MPI_Status status;\n    MPI_Comm comm1, comm2;\n    int verbose = 0;\n    int data = 0;\n\n    if (getenv(\"MPITEST_VERBOSE\"))\n    {\n\tverbose = 1;\n    }\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (size < 3)\n    {\n\tprintf(\"Three processes needed to run this test.\\n\");\n\tMPI_Finalize();\n\treturn 0;\n    }\n\n    if (rank == 0)\n    {\n\tIF_VERBOSE((\"0: opening ports.\\n\"));\n\tMPI_Open_port(MPI_INFO_NULL, port1);\n\tMPI_Open_port(MPI_INFO_NULL, port2);\n\n\tIF_VERBOSE((\"0: opened port1: <%s>\\n\", port1));\n\tIF_VERBOSE((\"0: opened port2: <%s>\\n\", port2));\n\tIF_VERBOSE((\"0: sending ports.\\n\"));\n\tMPI_Send(port1, MPI_MAX_PORT_NAME, MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n\tMPI_Send(port2, MPI_MAX_PORT_NAME, MPI_CHAR, 2, 0, MPI_COMM_WORLD);\n\n\tIF_VERBOSE((\"0: accepting port2.\\n\"));\n\tMPI_Comm_accept(port2, MPI_INFO_NULL, 0, MPI_COMM_SELF, &comm2);\n\tIF_VERBOSE((\"0: accepting port1.\\n\"));\n\tMPI_Comm_accept(port1, MPI_INFO_NULL, 0, MPI_COMM_SELF, &comm1);\n\n\tIF_VERBOSE((\"0: closing ports.\\n\"));\n\tMPI_Close_port(port1);\n\tMPI_Close_port(port2);\n\n\tIF_VERBOSE((\"0: sending 1 to process 1.\\n\"));\n\tdata = 1;\n\tMPI_Send(&data, 1, MPI_INT, 0, 0, comm1);\n\n\tIF_VERBOSE((\"0: sending 2 to process 2.\\n\"));\n\tdata = 2;\n\tMPI_Send(&data, 1, MPI_INT, 0, 0, comm2);\n\n\tIF_VERBOSE((\"0: disconnecting.\\n\"));\n\tMPI_Comm_disconnect(&comm1);\n\tMPI_Comm_disconnect(&comm2);\n    }\n    else if (rank == 1)\n    {\n\tIF_VERBOSE((\"1: receiving port.\\n\"));\n\tMPI_Recv(port1, MPI_MAX_PORT_NAME, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n\n\tIF_VERBOSE((\"1: received port1: <%s>\\n\", port1));\n\tIF_VERBOSE((\"1: connecting.\\n\"));\n\tMPI_Comm_connect(port1, MPI_INFO_NULL, 0, MPI_COMM_SELF, &comm1);\n\n\tMPI_Recv(&data, 1, MPI_INT, 0, 0, comm1, &status);\n\tif (data != 1)\n\t{\n\t    printf(\"Received %d from root when expecting 1\\n\", data);\n\t    fflush(stdout);\n\t    num_errors++;\n\t}\n\n\tIF_VERBOSE((\"1: disconnecting.\\n\"));\n\tMPI_Comm_disconnect(&comm1);\n    }\n    else if (rank == 2)\n    {\n\tIF_VERBOSE((\"2: receiving port.\\n\"));\n\tMPI_Recv(port2, MPI_MAX_PORT_NAME, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n\n\tIF_VERBOSE((\"2: received port2: <%s>\\n\", port2));\n\t\n\n\tMTestSleep(3);\n\tIF_VERBOSE((\"2: connecting.\\n\"));\n\tMPI_Comm_connect(port2, MPI_INFO_NULL, 0, MPI_COMM_SELF, &comm2);\n\n\tMPI_Recv(&data, 1, MPI_INT, 0, 0, comm2, &status);\n\tif (data != 2)\n\t{\n\t    printf(\"Received %d from root when expecting 2\\n\", data);\n\t    fflush(stdout);\n\t    num_errors++;\n\t}\n\n\tIF_VERBOSE((\"2: disconnecting.\\n\"));\n\tMPI_Comm_disconnect(&comm2);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Reduce(&num_errors, &total_num_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0)\n    {\n\tif (total_num_errors)\n\t{\n\t    printf(\" Found %d errors\\n\", total_num_errors);\n\t}\n\telse\n\t{\n\t    printf(\" No Errors\\n\");\n\t}\n\tfflush(stdout);\n    }\n    MPI_Finalize();\n    return total_num_errors;\n}"}
{"program": "autarchprinceps_1372", "code": "int\nmain (int argc, char **argv)\n{\n  int me, nproc, n_local, res;\n\n  res =\n  assert (res == MPI_SUCCESS);\n  res =\n  assert (res == MPI_SUCCESS);\n  res =\n  assert (res == MPI_SUCCESS);\n\n  assert(argc > 1);\n  int n = atoi (argv[1]);\n  n_local = BLOCKSIZE(nproc,n);\n\n  \n\n  \n\n\n  \n\n  double *a = malloc(n_local*n*sizeof(*a));\n  assert(a != NULL);\n  \n\n  generate_matrix (n_local, n, (double (*)[n])a, me);\n\n  \n\n  res =\n  assert(res == MPI_SUCCESS);\n  double t0 = gettime ();\n\n  floyd (n_local, n, (double (*)[n])a);\n\n  res =\n  assert(res == MPI_SUCCESS);\n  t0 = gettime () - t0;\n  if(me == 0)\n    printf (\"time for Floyd on %d processors with n=%d: %9.6f\\n\", nproc, n, t0);\n\n  \n\n\n  res =\n  assert (res == MPI_SUCCESS);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int me, nproc, n_local, res;\n\n  res = MPI_Init (&argc, &argv);\n  assert (res == MPI_SUCCESS);\n  res = MPI_Comm_rank (MPI_COMM_WORLD, &me);\n  assert (res == MPI_SUCCESS);\n  res = MPI_Comm_size (MPI_COMM_WORLD, &nproc);\n  assert (res == MPI_SUCCESS);\n\n  assert(argc > 1);\n  int n = atoi (argv[1]);\n  n_local = BLOCKSIZE(nproc,n);\n\n  \n\n  \n\n\n  \n\n  double *a = malloc(n_local*n*sizeof(*a));\n  assert(a != NULL);\n  \n\n  generate_matrix (n_local, n, (double (*)[n])a, me);\n\n  \n\n  res = MPI_Barrier(MPI_COMM_WORLD);\n  assert(res == MPI_SUCCESS);\n  double t0 = gettime ();\n\n  floyd (n_local, n, (double (*)[n])a);\n\n  res = MPI_Barrier(MPI_COMM_WORLD);\n  assert(res == MPI_SUCCESS);\n  t0 = gettime () - t0;\n  if(me == 0)\n    printf (\"time for Floyd on %d processors with n=%d: %9.6f\\n\", nproc, n, t0);\n\n  \n\n\n  res = MPI_Finalize ();\n  assert (res == MPI_SUCCESS);\n\n  return 0;\n}"}
{"program": "gyaikhom_1374", "code": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    bc_init(BC_ERR | BC_PLIST_XALL);\n    FILE *f;\n    char fname[128];\n\n    sprintf(fname, \"gather_bc_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, gather_bc(101, 1L << i));\n    }\n    fclose(f);\n\n    sprintf(fname, \"gather_mpi_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, gather_mpi(101, 1L << i));\n    }\n    fclose(f);\n\n    bc_final();\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    MPI_Init(&argc, &argv);\n    bc_init(BC_ERR | BC_PLIST_XALL);\n    FILE *f;\n    char fname[128];\n\n    sprintf(fname, \"gather_bc_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, gather_bc(101, 1L << i));\n    }\n    fclose(f);\n\n    sprintf(fname, \"gather_mpi_%d_%d.dat\", bc_rank, bc_size);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, gather_mpi(101, 1L << i));\n    }\n    fclose(f);\n\n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "T-ProgAlg_1375", "code": "int main(int argc, char** argv) {\n    \n    \n\n    int n = 100000000; \n\n    int *original_array = malloc(n * sizeof(int));\n    unsigned long start_time_lt, initTime, compTime;\n    \n\n    \n    \n    \n    \n\n    int world_rank;\n    int world_size;\n    \n        \n    \n\n    int c;\n    srand(time(NULL));\n    \n    if(world_rank == 0) {\n        start_time_lt =  my_ftime(); \n        printf(\"Vector Size=%d\\tNumber of machines=%d\\t\", n, world_size);\n        printf(\"\\n\");\n        printf(\"Start initializing the unsorted array: \");\n        for(c = 0; c < n; c++) {\n            \n            original_array[c] = rand() % n;\n            \n\n            \n            }\n        printf(\"\\n\");\n        printf(\"\\n\");\n        initTime = my_ftime() - start_time_lt;  \n\n        printf(\"Sorting...\");\n    }\n\n\n    \n\n    int size = n/world_size;\n    \n    \n\n    int *sub_array = malloc(size * sizeof(int));\n    \n    \n\n    int *tmp_array = malloc(size * sizeof(int));\n    mergeSort(sub_array, tmp_array, 0, (size - 1));\n    \n    \n\n    int *sorted = NULL;\n    if(world_rank == 0) {\n        \n        sorted = malloc(n * sizeof(int));\n        \n        }\n    \n    \n    \n\n    if(world_rank == 0) {\n        \n        int *other_array = malloc(n * sizeof(int));\n        mergeSort(sorted, other_array, 0, (n - 1));\n        \n        compTime = my_ftime() - start_time_lt - initTime; \n\n        \n\n        \n        \n        printf(\"Vector Size=%d\\tinitTime=%g\\tcomputeTime=%g (%lu min, %lu sec)\\n\", n, initTime/1000.0, compTime/1000.0,\n         (compTime/1000)/60, (compTime/1000)%60);\n        printf(\"\\n\");\n        printf(\"\\n\");\n        printf(\"This is the sorted array: \");\n        \n\n            \n        \n            \n        \n\n        free(sorted);\n        free(other_array);\n            \n        }\n    \n    \n\n    free(original_array);\n    free(sub_array);\n    free(tmp_array);\n    \n    \n\n    \n    }", "label": "int main(int argc, char** argv) {\n    \n    \n\n    int n = 100000000; \n\n    int *original_array = malloc(n * sizeof(int));\n    unsigned long start_time_lt, initTime, compTime;\n    \n\n    \n    \n    \n    \n\n    int world_rank;\n    int world_size;\n    \n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n        \n    \n\n    int c;\n    srand(time(NULL));\n    \n    if(world_rank == 0) {\n        start_time_lt =  my_ftime(); \n        printf(\"Vector Size=%d\\tNumber of machines=%d\\t\", n, world_size);\n        printf(\"\\n\");\n        printf(\"Start initializing the unsorted array: \");\n        for(c = 0; c < n; c++) {\n            \n            original_array[c] = rand() % n;\n            \n\n            \n            }\n        printf(\"\\n\");\n        printf(\"\\n\");\n        initTime = my_ftime() - start_time_lt;  \n\n        printf(\"Sorting...\");\n    }\n\n\n    \n\n    int size = n/world_size;\n    \n    \n\n    int *sub_array = malloc(size * sizeof(int));\n    MPI_Scatter(original_array, size, MPI_INT, sub_array, size, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    \n\n    int *tmp_array = malloc(size * sizeof(int));\n    mergeSort(sub_array, tmp_array, 0, (size - 1));\n    \n    \n\n    int *sorted = NULL;\n    if(world_rank == 0) {\n        \n        sorted = malloc(n * sizeof(int));\n        \n        }\n    \n    MPI_Gather(sub_array, size, MPI_INT, sorted, size, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    \n\n    if(world_rank == 0) {\n        \n        int *other_array = malloc(n * sizeof(int));\n        mergeSort(sorted, other_array, 0, (n - 1));\n        \n        compTime = my_ftime() - start_time_lt - initTime; \n\n        \n\n        \n        \n        printf(\"Vector Size=%d\\tinitTime=%g\\tcomputeTime=%g (%lu min, %lu sec)\\n\", n, initTime/1000.0, compTime/1000.0,\n         (compTime/1000)/60, (compTime/1000)%60);\n        printf(\"\\n\");\n        printf(\"\\n\");\n        printf(\"This is the sorted array: \");\n        \n\n            \n        \n            \n        \n\n        free(sorted);\n        free(other_array);\n            \n        }\n    \n    \n\n    free(original_array);\n    free(sub_array);\n    free(tmp_array);\n    \n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Finalize();\n    \n    }"}
{"program": "autarchprinceps_1377", "code": "int main(int argc, char **argv) {\n\tint err, rank, n, i;\n\tint* vector;\n\tMPI_Status status;\n\n\terr =\n\terr =\n\terr =\n\t\n\tif(rank == 0) {\n\t\tvector = calloc(n, sizeof(int));\n\t} else {\n\t\tvector = malloc(n * sizeof(int));\n\t}\n\n\n\tfor(i = 0; i <= (n - 2); i++) {\n\t\tif(rank == i) {\n\t\t\t\n\n\t\t\tvector[i] = i + 1;\n\t\t\t\n\n\t\t\terr =\n\t\t}\n\t\tif(rank == i + 1) {\n\t\t\t\n\n\t\t\terr =\n\t\t}\n\t}\n\n\tif(rank == (n - 1)) {\n\t\t\n\n\t\tvector[n - 1] = n;\n\t\t\n\n\t\terr =\n\t}\n\tif(rank == 0) {\n\t\t\n\n\t\terr =\n\t\t\n\n\t\t\n\n\t\tint result = 1;\n\t\tfor(i = 0; i < n; i++) {\n\t\t\tif(vector[i] != i + 1) {\n\t\t\t\tresult = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif(result == 0) {\n\t\t\tfprintf(stderr, \"Result does not match expectation\\n\");\n\n\t\t} else {\n\t\t\tfor(i = 0; i < n; i++) {\n\t\t\t\tprintf(\"%i \", vector[i]);\n\t\t\t}\n\t\t\tprintf(\"Result matches expectation\\n\");\n\t\t}\n\t}\n\t\n\terr =\n\treturn 0;\n}", "label": "int main(int argc, char **argv) {\n\tint err, rank, n, i;\n\tint* vector;\n\tMPI_Status status;\n\n\terr = MPI_Init(&argc, &argv);\n\terr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\terr = MPI_Comm_size(MPI_COMM_WORLD, &n);\n\t\n\tif(rank == 0) {\n\t\tvector = calloc(n, sizeof(int));\n\t} else {\n\t\tvector = malloc(n * sizeof(int));\n\t}\n\n\n\tfor(i = 0; i <= (n - 2); i++) {\n\t\tif(rank == i) {\n\t\t\t\n\n\t\t\tvector[i] = i + 1;\n\t\t\t\n\n\t\t\terr = MPI_Send(vector, n, MPI_INT, i + 1, 4711, MPI_COMM_WORLD);\n\t\t}\n\t\tif(rank == i + 1) {\n\t\t\t\n\n\t\t\terr = MPI_Recv(vector, n, MPI_INT, i, 4711, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\n\tif(rank == (n - 1)) {\n\t\t\n\n\t\tvector[n - 1] = n;\n\t\t\n\n\t\terr = MPI_Send(vector, n, MPI_INT, 0, 4711, MPI_COMM_WORLD);\n\t}\n\tif(rank == 0) {\n\t\t\n\n\t\terr = MPI_Recv(vector, n, MPI_INT, n - 1, 4711, MPI_COMM_WORLD, &status);\n\t\t\n\n\t\t\n\n\t\tint result = 1;\n\t\tfor(i = 0; i < n; i++) {\n\t\t\tif(vector[i] != i + 1) {\n\t\t\t\tresult = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif(result == 0) {\n\t\t\tfprintf(stderr, \"Result does not match expectation\\n\");\n\n\t\t} else {\n\t\t\tfor(i = 0; i < n; i++) {\n\t\t\t\tprintf(\"%i \", vector[i]);\n\t\t\t}\n\t\t\tprintf(\"Result matches expectation\\n\");\n\t\t}\n\t}\n\t\n\terr = MPI_Finalize();\n\treturn 0;\n}"}
{"program": "ghisvail_1381", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2c(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], ni[4], no[4];\n  ptrdiff_t alloc_local_forw, alloc_local_back, alloc_local, howmany;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_n[4], local_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  ni[0] = ni[1] = ni[2] = ni[3] = 8;\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  for(int t=0; t<4; t++)\n    no[t] = ni[t];\n  np[0] = 2; np[1] = 2;\n  howmany = 1;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local_forw = pfft_local_size_many_dft(4, n, ni, n, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_n, local_start);\n\n  alloc_local_back = pfft_local_size_many_dft(4, n, n, no, howmany,\n      PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      comm_cart_2d, PFFT_TRANSPOSED_IN,\n      local_n, local_start, local_no, local_o_start);\n\n  \n\n  alloc_local = (alloc_local_forw > alloc_local_back) ?\n    alloc_local_forw : alloc_local_back;\n  in  = pfft_alloc_complex(alloc_local);\n  out = pfft_alloc_complex(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_many_dft(\n      4, n, ni, n, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  plan_back = pfft_plan_many_dft(\n      4, n, n, no, howmany, PFFT_DEFAULT_BLOCKS, PFFT_DEFAULT_BLOCKS,\n      out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_c2c(4, ni, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (n[0]*n[1]*n[2]*n[3]);\n  \n  \n\n  err = pfft_check_output_c2c(4, ni, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gustfrontar_1382", "code": "int main(int argc, char **argv){\n\n  int maxspawn = 100000;\n  int nprocs, myrank, new_nprocs[maxspawn], nspawn, i;\n  char path[maxspawn][SPAWNPATHLEN], wdir[maxspawn][SPAWNPATHLEN], cwd[SPAWNPATHLEN];\n  MPI_Comm intercomm;\n  MPI_Info info;\n  int *ary_ierr;\n  FILE *fp;\n\n  printf(\"nprocs=%d, myrank=%d\\n\", nprocs, myrank);\n\n  nspawn = 0;\n  if(myrank == 0){\n    fp = fopen(argv[1], \"r\");\n    while(feof(fp) == 0){\n      fscanf(fp, \"%d %\" SPAWNPATHLENS \"s %\" SPAWNPATHLENS \"s\", &(new_nprocs[nspawn]), path[nspawn], wdir[nspawn]);\n      printf(\"%d %s %s\\n\", new_nprocs[nspawn], path[nspawn], wdir[nspawn]);\n      if(wdir[nspawn][0] == 0) break;\n      nspawn += 1;\n    }\n    printf(\"nspawn = %d\\n\", nspawn);\n    fclose(fp);\n    \n\n    \n\n    \n\n    \n\n  }\n  \n  \n\n  for(i = 0; i < nspawn; i++){\n    printf(\"Spawn process %d\\n\", i);\n    ary_ierr = malloc(sizeof(int) * new_nprocs[i]);\n    getcwd(cwd, SPAWNPATHLEN);\n    if(chdir(wdir[i])){\n      printf(\"Error in chdir(%s)\\n\", wdir[i]);\n    }\n    chdir(wdir[i]);\n    chdir(cwd);\n    free(ary_ierr);\n  }\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n  return 0;\n}", "label": "int main(int argc, char **argv){\n\n  int maxspawn = 100000;\n  int nprocs, myrank, new_nprocs[maxspawn], nspawn, i;\n  char path[maxspawn][SPAWNPATHLEN], wdir[maxspawn][SPAWNPATHLEN], cwd[SPAWNPATHLEN];\n  MPI_Comm intercomm;\n  MPI_Info info;\n  int *ary_ierr;\n  FILE *fp;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  printf(\"nprocs=%d, myrank=%d\\n\", nprocs, myrank);\n\n  nspawn = 0;\n  if(myrank == 0){\n    fp = fopen(argv[1], \"r\");\n    while(feof(fp) == 0){\n      fscanf(fp, \"%d %\" SPAWNPATHLENS \"s %\" SPAWNPATHLENS \"s\", &(new_nprocs[nspawn]), path[nspawn], wdir[nspawn]);\n      printf(\"%d %s %s\\n\", new_nprocs[nspawn], path[nspawn], wdir[nspawn]);\n      if(wdir[nspawn][0] == 0) break;\n      nspawn += 1;\n    }\n    printf(\"nspawn = %d\\n\", nspawn);\n    fclose(fp);\n    \n\n    \n\n    \n\n    \n\n  }\n  MPI_Bcast(&nspawn, 1, MPI_INTEGER, 0, MPI_COMM_WORLD);\n  MPI_Bcast(new_nprocs, maxspawn, MPI_INTEGER, 0, MPI_COMM_WORLD);\n  MPI_Bcast(path, SPAWNPATHLEN * maxspawn, MPI_CHARACTER, 0, MPI_COMM_WORLD);\n  MPI_Bcast(wdir, SPAWNPATHLEN * maxspawn, MPI_CHARACTER, 0, MPI_COMM_WORLD);\n  \n  MPI_Info_create(&info);\n  \n\n  for(i = 0; i < nspawn; i++){\n    printf(\"Spawn process %d\\n\", i);\n    ary_ierr = malloc(sizeof(int) * new_nprocs[i]);\n    getcwd(cwd, SPAWNPATHLEN);\n    if(chdir(wdir[i])){\n      printf(\"Error in chdir(%s)\\n\", wdir[i]);\n      MPI_Abort(MPI_COMM_WORLD, 999);\n    }\n    chdir(wdir[i]);\n    MPI_Info_set(info, \"wdir\", wdir[i]);\n    MPI_Comm_spawn(path[i], MPI_ARGV_NULL, new_nprocs[i], info, 0, MPI_COMM_WORLD, &intercomm, ary_ierr);\n    chdir(cwd);\n    free(ary_ierr);\n  }\n  MPI_Info_free(&info);\n  MPI_Finalize();\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n  \n\n  \n\n  return 0;\n}"}
{"program": "ahmadia_1383", "code": "int main(int argc, char *argv[])\n{\n  int err;\n  collfs_open_fp _open_fp;\n  char path[MAXPATHLEN];\n\n  typedef void (*vfptr)(void); \n\n  \n  _dl_collfs_api.open = (vfptr) minimal_open;\n  _open_fp = (collfs_open_fp) _dl_collfs_api.open;  \n  int fid = _open_fp(\"test_open.txt\",O_RDONLY,0);\n  close(fid);\n  \n  if (!getcwd(path,sizeof path)) ERR(\"getcwd failed\");\n  strcat(path,\"/libminimal_thefunc.so\");\n  err = run_tests(path, path);CHK(err);\n  \n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n  int err;\n  collfs_open_fp _open_fp;\n  char path[MAXPATHLEN];\n\n  typedef void (*vfptr)(void); \n\n  MPI_Init(&argc,&argv);\n  \n  _dl_collfs_api.open = (vfptr) minimal_open;\n  _open_fp = (collfs_open_fp) _dl_collfs_api.open;  \n  int fid = _open_fp(\"test_open.txt\",O_RDONLY,0);\n  close(fid);\n  \n  if (!getcwd(path,sizeof path)) ERR(\"getcwd failed\");\n  strcat(path,\"/libminimal_thefunc.so\");\n  err = run_tests(path, path);CHK(err);\n  \n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_1384", "code": "int main(int argc, char **argv)\n{\n    int         rank, nproc;\n    int         out_val, i, counter = 0;\n    MPI_Win     win;\n\n\n\n\n    for (i = 0; i < NITER; i++) {\n\n        if (out_val != acc_val*i) {\n            errors++;\n            printf(\"Error: got %d, expected %d at iter %d\\n\", out_val, acc_val*i, i);\n            break;\n        }\n    }\n\n\n    if (errors == 0 && rank == 0)\n        printf(\" No errors\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int         rank, nproc;\n    int         out_val, i, counter = 0;\n    MPI_Win     win;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    MPI_Win_create(&counter, sizeof(int), sizeof(int), MPI_INFO_NULL,\n                   MPI_COMM_WORLD, &win);\n\n    for (i = 0; i < NITER; i++) {\n        MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);\n        MPI_Get_accumulate(&acc_val, 1, MPI_INT, &out_val, 1, MPI_INT,\n                            rank, 0, 1, MPI_INT, MPI_SUM, win);\n        MPI_Win_unlock(rank, win);\n\n        if (out_val != acc_val*i) {\n            errors++;\n            printf(\"Error: got %d, expected %d at iter %d\\n\", out_val, acc_val*i, i);\n            break;\n        }\n    }\n\n    MPI_Win_free(&win);\n\n    if (errors == 0 && rank == 0)\n        printf(\" No errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_1385", "code": "int main( int argc, char* argv[] ) {\n\tRMatrix\t\tmatrix;\n\tCoord\t\txz = { 1.0, 0.0, 1.0 };\n\tMPI_Comm\tCommWorld;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\t\n\t\n\n\t\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_ApplyRotationX( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tRMatrix_ApplyRotationY( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tRMatrix_ApplyRotationZ( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tRMatrix_ApplyRotationX( matrix, M_PI / 2.0 );\n\tRMatrix_ApplyRotationY( matrix, M_PI / 2.0 );\n\tRMatrix_ApplyRotationZ( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\t\n\treturn EXIT_SUCCESS;\n}", "label": "int main( int argc, char* argv[] ) {\n\tRMatrix\t\tmatrix;\n\tCoord\t\txz = { 1.0, 0.0, 1.0 };\n\tMPI_Comm\tCommWorld;\n\tint\t\trank;\n\tint\t\tnumProcessors;\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_ApplyRotationX( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tRMatrix_ApplyRotationY( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tRMatrix_ApplyRotationZ( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tRMatrix_LoadIdentity( matrix );\n\tRMatrix_ApplyRotationX( matrix, M_PI / 2.0 );\n\tRMatrix_ApplyRotationY( matrix, M_PI / 2.0 );\n\tRMatrix_ApplyRotationZ( matrix, M_PI / 2.0 );\n\tprintf( \"\\n\" ); RMatrix_Print( matrix );\n\tRMatrix_VectorMult( xz, matrix, xz );\n\tprintf( \"\\n\\t%g, %g, %g\\n\", xz[0], xz[1], xz[2] );\n\t\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn EXIT_SUCCESS;\n}"}
{"program": "bmi-forum_1387", "code": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* class0;\n\tEntryPoint* classVoidPtr;\n\n\t#define NUM_LISTENERS 3\n\tListener* listeners[NUM_LISTENERS];\n\tchar buf[100];\n\tint ii;\n\tint data = 5;\n\n\tStream* stream;\n\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\t\n\tRegressionTest_Init( \"Base/Extensibility/EntryPoint\" );\n\t\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, __FILE__ );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\n\t\n\n\tclass0 = EntryPoint_New( \"Class0\", EntryPoint_Class_0_CastType );\n\tclassVoidPtr = EntryPoint_New( \"Class_VoidPtr\", EntryPoint_Class_VoidPtr_CastType );\n\n\tRegressionTest_Check(\n\t\tclass0 != NULL &&\n\t\tclassVoidPtr != NULL,\n\t\tstream,\n\t\t\"EntryPoint creation with ClassHook types\",\n\t\t\"Can we create an EntryPoint with ClassHook run casts\" );\n\n\tfor ( ii = 0; ii < NUM_LISTENERS; ++ii ) {\n\t\tlisteners[ii] = Listener_New( ii );\n\t\tsprintf( buf, \"%d\", ii );\n\t\tEntryPoint_AppendClassHook( class0, buf, (void*)Listener_0_Func, __FILE__, listeners[ii] );\n\t\tEntryPoint_AppendClassHook( classVoidPtr, buf, (void*)Listener_VoidPtr_Func, __FILE__, listeners[ii] );\n\t}\n\n\tRegressionTest_Check(\n\t\tclass0->hooks->count == NUM_LISTENERS &&\n\t\tclassVoidPtr->hooks->count ==  NUM_LISTENERS,\n\t\tstream,\n\t\t\"Adding ClassHooks\",\n\t\t\"Can we add class hooks onto EntryPoints\" );\n\n\t\n\n\t((EntryPoint_Class_0_CallCast*) class0->run)( class0);\n\t\n\tRegressionTest_Check(\n\t\t1,\n\t\tstream,\n\t\t\"Run ClassHook with 0 arguments\",\n\t\t\"Can we run ClassHook with 0 arguments\" );\n\n\t((EntryPoint_Class_VoidPtr_CallCast*) classVoidPtr->run)( classVoidPtr, &data );\n\n\tRegressionTest_Check(\n\t\t1,\n\t\tstream,\n\t\t\"Run ClassHook with 1 argument\",\n\t\t\"Can we run ClassHook with 1 argument\" );\n\n\tfor ( ii = 0; ii < NUM_LISTENERS; ++ii ) {\n\t\tStg_Class_Delete( listeners[ii] );\n\t}\n\n\tRegressionTest_Check(\n\t\t1,\n\t\tstream,\n\t\t\"Destruction of ClassHooks\",\n\t\t\"Can we delete the ClassHooks\" );\n\n\t\n\n\tStg_Class_Delete( class0 );\n\tStg_Class_Delete( classVoidPtr );\n\n\tRegressionTest_Finalise();\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char* argv[] ) {\n\tMPI_Comm CommWorld;\n\tint rank;\n\tint numProcessors;\n\tint procToWatch;\n\tEntryPoint* class0;\n\tEntryPoint* classVoidPtr;\n\n\t#define NUM_LISTENERS 3\n\tListener* listeners[NUM_LISTENERS];\n\tchar buf[100];\n\tint ii;\n\tint data = 5;\n\n\tStream* stream;\n\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_dup( MPI_COMM_WORLD, &CommWorld );\n\tMPI_Comm_size( CommWorld, &numProcessors );\n\tMPI_Comm_rank( CommWorld, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\tBaseExtensibility_Init( &argc, &argv );\n\t\n\tRegressionTest_Init( \"Base/Extensibility/EntryPoint\" );\n\t\n\t\n\n\tstream =  Journal_Register( InfoStream_Type, __FILE__ );\n\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) Journal_Printf( (void*) stream, \"Watching rank: %i\\n\", rank );\n\t\n\t\n\n\n\t\n\n\tclass0 = EntryPoint_New( \"Class0\", EntryPoint_Class_0_CastType );\n\tclassVoidPtr = EntryPoint_New( \"Class_VoidPtr\", EntryPoint_Class_VoidPtr_CastType );\n\n\tRegressionTest_Check(\n\t\tclass0 != NULL &&\n\t\tclassVoidPtr != NULL,\n\t\tstream,\n\t\t\"EntryPoint creation with ClassHook types\",\n\t\t\"Can we create an EntryPoint with ClassHook run casts\" );\n\n\tfor ( ii = 0; ii < NUM_LISTENERS; ++ii ) {\n\t\tlisteners[ii] = Listener_New( ii );\n\t\tsprintf( buf, \"%d\", ii );\n\t\tEntryPoint_AppendClassHook( class0, buf, (void*)Listener_0_Func, __FILE__, listeners[ii] );\n\t\tEntryPoint_AppendClassHook( classVoidPtr, buf, (void*)Listener_VoidPtr_Func, __FILE__, listeners[ii] );\n\t}\n\n\tRegressionTest_Check(\n\t\tclass0->hooks->count == NUM_LISTENERS &&\n\t\tclassVoidPtr->hooks->count ==  NUM_LISTENERS,\n\t\tstream,\n\t\t\"Adding ClassHooks\",\n\t\t\"Can we add class hooks onto EntryPoints\" );\n\n\t\n\n\t((EntryPoint_Class_0_CallCast*) class0->run)( class0);\n\t\n\tRegressionTest_Check(\n\t\t1,\n\t\tstream,\n\t\t\"Run ClassHook with 0 arguments\",\n\t\t\"Can we run ClassHook with 0 arguments\" );\n\n\t((EntryPoint_Class_VoidPtr_CallCast*) classVoidPtr->run)( classVoidPtr, &data );\n\n\tRegressionTest_Check(\n\t\t1,\n\t\tstream,\n\t\t\"Run ClassHook with 1 argument\",\n\t\t\"Can we run ClassHook with 1 argument\" );\n\n\tfor ( ii = 0; ii < NUM_LISTENERS; ++ii ) {\n\t\tStg_Class_Delete( listeners[ii] );\n\t}\n\n\tRegressionTest_Check(\n\t\t1,\n\t\tstream,\n\t\t\"Destruction of ClassHooks\",\n\t\t\"Can we delete the ClassHooks\" );\n\n\t\n\n\tStg_Class_Delete( class0 );\n\tStg_Class_Delete( classVoidPtr );\n\n\tRegressionTest_Finalise();\n\n\tBaseExtensibility_Finalise();\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "tisma_1390", "code": "int main(int argc, char *argv[]) {\n\n\tint i, j = 1, n, rank, size, slice, msg = 0;\n\n\n\n\n\tif (size < 2 || size > N) {\n\t\texit(EXIT_FAILURE);\n\t}\n\n\t\n\n\tint blocklens[2];\t\t\t\n\n\tMPI_Aint offsets[2];\t\t\n\n\tMPI_Datatype oldtypes[2];\t\n\n\tMPI_Aint doubleExtent;\t\t\n\n\n\tblocklens[0] = 2;\n\toffsets[0] = 0;\n\toldtypes[0] = MPI_DOUBLE;\n\n\tblocklens[1] = 1;\n\toffsets[1] = 2 * doubleExtent;\n\toldtypes[1] = MPI_INT;\n\n\n\tif (rank == 0) {\n\t\t++msg;\n\t\tprintf(\"00:00%d:Unesite broj elemenata niza kompleksnih brojeva:\\n\", msg);\n\t\tscanf(\"%d\", &n);\n\t\tif (n % (size-1) != 0) {\n\t\t\tprintf(\"Morate uneti broj deljiv sa brojem procesa radnika\");\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t++msg;\n\t\t\tprintf(\"00:00%d:Unestite 2 realna broja i 1 ceo (0 ReIm ili 1 ModArg):\\n\", msg);\n\t\t\t++msg;\n\t\t\tprintf(\"00:00%d:Unesite %d. element niza:\\n\", msg, i);\n\t\t\tscanf(\"%lf %lf %d\", &complexArray[i].Re, &complexArray[i].Im, &complexArray[i].p);\n\t\t}\n\n\t\t\n\n\t\tslice = n / (size - 1);\n\n\n\t\tfor (i = 0; i < n; i += slice) {\n\t\t\tj++;\n\t\t}\n\n\t\tComplex locMaxs[slice];\n\t\tfor (i = 0; i < slice; i++) {\n\t\t}\n\n\t\tComplex max = locMaxs[0];\n\t\tfor (i = 1; i < slice; i++)\n\t\t\tif (absoluteValue(max) < absoluteValue(locMaxs[i]))\n\t\t\t\tmax = locMaxs[i];\n\n\t\t++msg;\n\t\tprintf(\"00:00%d:Kompleksni broj sa najvecom apsolutnom vrednoscu je:\\n\", msg);\n\t\t++msg;\n\t\tprintf(\"00:00%d: %.2lf %.2lf abs = %.2lf\\n\", msg, max.Re, max.Im, absoluteValue(max));\n\t}\n\telse {\n\t\tint slice;\n\n\t\tComplex array[slice];\n\t\tComplex locMax;\n\n\n\t\tlocMax = array[0];\n\t\tfor (i = 1; i < slice; i++) {\n\t\t\tif (absoluteValue(locMax) < absoluteValue(array[i]))\n\t\t\t\tlocMax = array[i];\n\t\t}\n\n\t}\n\n\n\n\treturn EXIT_SUCCESS;\n}", "label": "int main(int argc, char *argv[]) {\n\n\tint i, j = 1, n, rank, size, slice, msg = 0;\n\n\tMPI_Init(&argc, &argv);\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 2 || size > N) {\n\t\tMPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\t\n\n\tint blocklens[2];\t\t\t\n\n\tMPI_Aint offsets[2];\t\t\n\n\tMPI_Datatype oldtypes[2];\t\n\n\tMPI_Aint doubleExtent;\t\t\n\n\tMPI_Type_extent(MPI_DOUBLE, &doubleExtent);\n\n\tblocklens[0] = 2;\n\toffsets[0] = 0;\n\toldtypes[0] = MPI_DOUBLE;\n\n\tblocklens[1] = 1;\n\toffsets[1] = 2 * doubleExtent;\n\toldtypes[1] = MPI_INT;\n\n\tMPI_Type_struct(2, blocklens, offsets, oldtypes, &complex_type);\n\tMPI_Type_commit(&complex_type);\n\n\tif (rank == 0) {\n\t\t++msg;\n\t\tprintf(\"00:00%d:Unesite broj elemenata niza kompleksnih brojeva:\\n\", msg);\n\t\tscanf(\"%d\", &n);\n\t\tif (n % (size-1) != 0) {\n\t\t\tprintf(\"Morate uneti broj deljiv sa brojem procesa radnika\");\n\t\t\tMPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n\t\t\texit(EXIT_FAILURE);\n\t\t}\n\n\t\tfor (i = 0; i < n; i++) {\n\t\t\t++msg;\n\t\t\tprintf(\"00:00%d:Unestite 2 realna broja i 1 ceo (0 ReIm ili 1 ModArg):\\n\", msg);\n\t\t\t++msg;\n\t\t\tprintf(\"00:00%d:Unesite %d. element niza:\\n\", msg, i);\n\t\t\tscanf(\"%lf %lf %d\", &complexArray[i].Re, &complexArray[i].Im, &complexArray[i].p);\n\t\t}\n\n\t\t\n\n\t\tslice = n / (size - 1);\n\n\t\tMPI_Bcast(&slice, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t\tfor (i = 0; i < n; i += slice) {\n\t\t\tMPI_Send(&complexArray[i], slice, complex_type, j, 0, MPI_COMM_WORLD);\n\t\t\tj++;\n\t\t}\n\n\t\tComplex locMaxs[slice];\n\t\tfor (i = 0; i < slice; i++) {\n\t\t\tMPI_Recv(&locMaxs[i], 1, complex_type, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\n\t\tComplex max = locMaxs[0];\n\t\tfor (i = 1; i < slice; i++)\n\t\t\tif (absoluteValue(max) < absoluteValue(locMaxs[i]))\n\t\t\t\tmax = locMaxs[i];\n\n\t\t++msg;\n\t\tprintf(\"00:00%d:Kompleksni broj sa najvecom apsolutnom vrednoscu je:\\n\", msg);\n\t\t++msg;\n\t\tprintf(\"00:00%d: %.2lf %.2lf abs = %.2lf\\n\", msg, max.Re, max.Im, absoluteValue(max));\n\t}\n\telse {\n\t\tint slice;\n\n\t\tMPI_Bcast(&slice, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\t\tComplex array[slice];\n\t\tComplex locMax;\n\n\t\tMPI_Recv(&array, slice, complex_type, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n\t\tlocMax = array[0];\n\t\tfor (i = 1; i < slice; i++) {\n\t\t\tif (absoluteValue(locMax) < absoluteValue(array[i]))\n\t\t\t\tlocMax = array[i];\n\t\t}\n\n\t\tMPI_Send(&locMax, 1, complex_type, 0, 0, MPI_COMM_WORLD);\n\t}\n\n\tMPI_Type_free(&complex_type);\n\n\tMPI_Finalize();\n\n\treturn EXIT_SUCCESS;\n}"}
{"program": "syftalent_1394", "code": "int main(int argc, char *argv[])\n{\n    int rank, size, i, type_size;\n    int errs = 0;\n\n\n\n    if (rank == 0) {\n#ifdef HAVE_LONG_DOUBLE\n        if (MPI_LONG_DOUBLE != MPI_DATATYPE_NULL) {\n            if (type_size != sizeof(long double)) {\n                printf(\"type_size != sizeof(long double) : (%d != %zd)\\n\",\n                       type_size, sizeof(long double));\n                ++errs;\n            }\n        }\n#endif\n#if defined(HAVE_LONG_DOUBLE__COMPLEX) && defined(USE_LONG_DOUBLE_COMPLEX)\n        if (MPI_C_LONG_DOUBLE_COMPLEX != MPI_DATATYPE_NULL) {\n            if (type_size != sizeof(long double _Complex)) {\n                printf(\"type_size != sizeof(long double _Complex) : (%d != %zd)\\n\",\n                       type_size, sizeof(long double _Complex));\n                ++errs;\n            }\n        }\n#endif\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    int rank, size, i, type_size;\n    int errs = 0;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n#ifdef HAVE_LONG_DOUBLE\n        if (MPI_LONG_DOUBLE != MPI_DATATYPE_NULL) {\n            MPI_Type_size(MPI_LONG_DOUBLE, &type_size);\n            if (type_size != sizeof(long double)) {\n                printf(\"type_size != sizeof(long double) : (%d != %zd)\\n\",\n                       type_size, sizeof(long double));\n                ++errs;\n            }\n        }\n#endif\n#if defined(HAVE_LONG_DOUBLE__COMPLEX) && defined(USE_LONG_DOUBLE_COMPLEX)\n        if (MPI_C_LONG_DOUBLE_COMPLEX != MPI_DATATYPE_NULL) {\n            MPI_Type_size(MPI_C_LONG_DOUBLE_COMPLEX, &type_size);\n            if (type_size != sizeof(long double _Complex)) {\n                printf(\"type_size != sizeof(long double _Complex) : (%d != %zd)\\n\",\n                       type_size, sizeof(long double _Complex));\n                ++errs;\n            }\n        }\n#endif\n        if (errs) {\n            printf(\"found %d errors\\n\", errs);\n        }\n        else {\n            printf(\" No errors\\n\");\n        }\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ghisvail_1396", "code": "int main(int argc, char **argv){\n  int np[3], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M, local_N_total;\n  double f_hat_sum, x_max[3];\n  pnfft_complex *f_hat1, *f_hat2;\n  \n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=2; np[2]=2;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]*np[2]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2] = %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %d real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] x np[2] = %td x %td x %td processes (change with -pnfft_np * * *)\\n\", np[0], np[1], np[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  perform_pnfft_adj_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD,\n      &f_hat1, &f_hat_sum, &local_N_total);\n\n  \n\n  perform_pnfft_adj_guru(N, n, local_M, m+2, x_max, PNFFT_WINDOW_KAISER_BESSEL, np, MPI_COMM_WORLD,\n      &f_hat2, &f_hat_sum, &local_N_total);\n\n  \n\n  compare_f_hat(f_hat1, f_hat2, local_N_total, f_hat_sum, \"* Results in\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_free(f_hat1); pnfft_free(f_hat2);\n  pnfft_cleanup();\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[3], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M, local_N_total;\n  double f_hat_sum, x_max[3];\n  pnfft_complex *f_hat1, *f_hat2;\n  \n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=2; np[2]=2;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]*np[2]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2] = %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %d real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] x np[2] = %td x %td x %td processes (change with -pnfft_np * * *)\\n\", np[0], np[1], np[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  perform_pnfft_adj_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD,\n      &f_hat1, &f_hat_sum, &local_N_total);\n\n  \n\n  perform_pnfft_adj_guru(N, n, local_M, m+2, x_max, PNFFT_WINDOW_KAISER_BESSEL, np, MPI_COMM_WORLD,\n      &f_hat2, &f_hat_sum, &local_N_total);\n\n  \n\n  compare_f_hat(f_hat1, f_hat2, local_N_total, f_hat_sum, \"* Results in\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_free(f_hat1); pnfft_free(f_hat2);\n  pnfft_cleanup();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "syftalent_1397", "code": "int main (int argc, char *argv[]) {\n    int rank, size, i, j, k;\n    int errors = 0;\n    int origin_shm, origin_am, dest;\n    int *orig_buf = NULL, *result_buf = NULL, *compare_buf = NULL,\n        *target_buf = NULL, *check_buf = NULL;\n    MPI_Win win;\n    MPI_Status status;\n\n\n    if (size != 3) {\n        \n\n        goto exit_test;\n    }\n\n    \n\n    dest = 2;\n    origin_shm = 0;\n    origin_am = 1;\n\n    if (rank != dest) {\n    }\n\n\n    for (k = 0; k < LOOP_SIZE; k++)  {\n\n        \n\n        if (rank == origin_shm) {\n            orig_buf[0] = 1;\n            compare_buf[0] = 0;\n            result_buf[0] = 0;\n        }\n        else if (rank == origin_am) {\n            orig_buf[0] = 0;\n            compare_buf[0] = 1;\n            result_buf[0] = 0;\n        }\n        else {\n            target_buf[0] = 0;\n        }\n\n\n        \n\n        if (rank != dest) {\n        }\n\n\n        \n\n        if (rank != dest) {\n        }\n        else {\n\n            if (!(check_buf[dest] == 0 && check_buf[origin_shm] == 0 && check_buf[origin_am] == 1) &&\n                !(check_buf[dest] == 1 && check_buf[origin_shm] == 0 && check_buf[origin_am] == 0)) {\n\n                printf(\"Wrong results: target result = %d, origin_shm result = %d, origin_am result = %d\\n\",\n                       check_buf[dest], check_buf[origin_shm], check_buf[origin_am]);\n\n                printf(\"Expected results (1): target result = 1, origin_shm result = 0, origin_am result = 0\\n\");\n                printf(\"Expected results (2): target result = 0, origin_shm result = 0, origin_am result = 1\\n\");\n\n                errors++;\n            }\n\n        }\n    }\n\n\n    if (rank == origin_am || rank == origin_shm) {\n    }\n\n exit_test:\n    if (rank == dest && errors == 0)\n        printf(\" No Errors\\n\");\n\n    return 0;\n}", "label": "int main (int argc, char *argv[]) {\n    int rank, size, i, j, k;\n    int errors = 0;\n    int origin_shm, origin_am, dest;\n    int *orig_buf = NULL, *result_buf = NULL, *compare_buf = NULL,\n        *target_buf = NULL, *check_buf = NULL;\n    MPI_Win win;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size != 3) {\n        \n\n        goto exit_test;\n    }\n\n    \n\n    dest = 2;\n    origin_shm = 0;\n    origin_am = 1;\n\n    if (rank != dest) {\n        MPI_Alloc_mem(sizeof(int), MPI_INFO_NULL, &orig_buf);\n        MPI_Alloc_mem(sizeof(int), MPI_INFO_NULL, &result_buf);\n        MPI_Alloc_mem(sizeof(int), MPI_INFO_NULL, &compare_buf);\n    }\n\n    MPI_Win_allocate(sizeof(int), sizeof(int), MPI_INFO_NULL,\n                     MPI_COMM_WORLD, &target_buf, &win);\n\n    for (k = 0; k < LOOP_SIZE; k++)  {\n\n        \n\n        if (rank == origin_shm) {\n            orig_buf[0] = 1;\n            compare_buf[0] = 0;\n            result_buf[0] = 0;\n        }\n        else if (rank == origin_am) {\n            orig_buf[0] = 0;\n            compare_buf[0] = 1;\n            result_buf[0] = 0;\n        }\n        else {\n            MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);\n            target_buf[0] = 0;\n            MPI_Win_unlock(rank, win);\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        \n\n        MPI_Win_lock_all(0, win);\n        if (rank != dest) {\n            MPI_Compare_and_swap(orig_buf, compare_buf, result_buf, MPI_INT, dest, 0, win);\n            MPI_Win_flush(dest, win);\n        }\n        MPI_Win_unlock_all(win);\n\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        \n\n        if (rank != dest) {\n            MPI_Gather(result_buf, 1, MPI_INT, check_buf, 1, MPI_INT, dest, MPI_COMM_WORLD);\n        }\n        else {\n            MPI_Alloc_mem(sizeof(int) * 3, MPI_INFO_NULL, &check_buf);\n            MPI_Gather(target_buf, 1, MPI_INT, check_buf, 1, MPI_INT, dest, MPI_COMM_WORLD);\n\n            if (!(check_buf[dest] == 0 && check_buf[origin_shm] == 0 && check_buf[origin_am] == 1) &&\n                !(check_buf[dest] == 1 && check_buf[origin_shm] == 0 && check_buf[origin_am] == 0)) {\n\n                printf(\"Wrong results: target result = %d, origin_shm result = %d, origin_am result = %d\\n\",\n                       check_buf[dest], check_buf[origin_shm], check_buf[origin_am]);\n\n                printf(\"Expected results (1): target result = 1, origin_shm result = 0, origin_am result = 0\\n\");\n                printf(\"Expected results (2): target result = 0, origin_shm result = 0, origin_am result = 1\\n\");\n\n                errors++;\n            }\n\n            MPI_Free_mem(check_buf);\n        }\n    }\n\n    MPI_Win_free(&win);\n\n    if (rank == origin_am || rank == origin_shm) {\n        MPI_Free_mem(orig_buf);\n        MPI_Free_mem(result_buf);\n        MPI_Free_mem(compare_buf);\n    }\n\n exit_test:\n    if (rank == dest && errors == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "joao29a_1398", "code": "int main(int argc, char** argv){\n    if (argc > 4){\n        int tabu_size = atoi(argv[2]);\n        int candidates_size = atoi(argv[3]);\n        int iterations = atoi(argv[4]);\n        if (tabu_size <= 0 || candidates_size <= 0 || iterations <= 0){\n            printf(\"Candidates, tabu and iterations size must be higher than 0\\n\");\n            return 1;\n        }\n        Graph *graph;\n        graph = create_graph();\n        read_file_on_graph(graph, argv[1]);\n\n        int world_size, world_rank;\n\n        pthread_t* thread;\n        pthread_args_t args;\n\n        if (world_rank == 0){\n            thread = (pthread_t*) malloc(sizeof(pthread_t));\n            args.world_size = world_size;\n            args.colors = (int*) malloc(sizeof(int)*world_size);\n            pthread_create(thread, NULL, (void*(*)(void*)) &receive_message, &args);\n        }\n\n        iterations /= world_size;\n        candidates_size /= world_size;\n        int colors;\n        srand(time(NULL) / (world_rank + 1)); \n\n        tabu_search(graph, tabu_size, candidates_size, iterations, &colors);\n\n#ifdef PCOLORSD\n        printf(\"%d sending %d\\n\", world_rank, colors);\n#endif\n\n        if (world_rank == 0){\n            pthread_join(*thread, NULL);\n            free(thread);\n            args.colors[world_size - 1] = colors;\n            int least_color = INT_MAX;\n            for (int i = 0; i < world_size; i++){\n               if (args.colors[i] < least_color)\n                   least_color = args.colors[i];\n            }\n            free(args.colors);\n            printf(\"Colors used: %d\\n\", least_color);\n        }\n        else {\n        }\n\n        \n    }\n    else printf(\"Insert file name, tabu size, candidates size and iterations.\\n\");\n    return 0;\n}", "label": "int main(int argc, char** argv){\n    if (argc > 4){\n        int tabu_size = atoi(argv[2]);\n        int candidates_size = atoi(argv[3]);\n        int iterations = atoi(argv[4]);\n        if (tabu_size <= 0 || candidates_size <= 0 || iterations <= 0){\n            printf(\"Candidates, tabu and iterations size must be higher than 0\\n\");\n            return 1;\n        }\n        Graph *graph;\n        graph = create_graph();\n        read_file_on_graph(graph, argv[1]);\n\n        MPI_Init(NULL, NULL);\n        int world_size, world_rank;\n        MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n        MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n        pthread_t* thread;\n        pthread_args_t args;\n\n        if (world_rank == 0){\n            thread = (pthread_t*) malloc(sizeof(pthread_t));\n            args.world_size = world_size;\n            args.colors = (int*) malloc(sizeof(int)*world_size);\n            pthread_create(thread, NULL, (void*(*)(void*)) &receive_message, &args);\n        }\n\n        iterations /= world_size;\n        candidates_size /= world_size;\n        int colors;\n        srand(time(NULL) / (world_rank + 1)); \n\n        tabu_search(graph, tabu_size, candidates_size, iterations, &colors);\n\n#ifdef PCOLORSD\n        printf(\"%d sending %d\\n\", world_rank, colors);\n#endif\n\n        if (world_rank == 0){\n            pthread_join(*thread, NULL);\n            free(thread);\n            args.colors[world_size - 1] = colors;\n            int least_color = INT_MAX;\n            for (int i = 0; i < world_size; i++){\n               if (args.colors[i] < least_color)\n                   least_color = args.colors[i];\n            }\n            free(args.colors);\n            printf(\"Colors used: %d\\n\", least_color);\n        }\n        else {\n            MPI_Send(&colors, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n\n        MPI_Finalize();\n        \n    }\n    else printf(\"Insert file name, tabu size, candidates size and iterations.\\n\");\n    return 0;\n}"}
{"program": "VangelisTsiatouras_1399", "code": "int main(int argc, char *argv[]) {\n\n\tsrand(time(NULL));\n\t\n\tint i;\n\tint rank, num_of_proc;\n\tint dimension = -1, sub_grid_size = -1, loops = -1;\n\tint prints_enabled = 0;\n\tchar *input_file = NULL;\n\tint error = 0;\n\n\n\tdouble start_time =\n\t\n\t\n\n\tfor (i = 1; i < argc; i++) {\n\t\tif (!strcmp(argv[i], \"-d\")) {\n\t\t\ti++;\n\t\t\tif (argv[i] != NULL) {\n\t\t\t\tdimension = atoi(argv[i]);\n\t\t\t\tif (dimension < 16) {\n\t\t\t\t\terror = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terror = 2;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\telse if (!strcmp(argv[i], \"-f\")) {\n\t\t\ti++;\n\t\t\tif (argv[i] != NULL) {\n\t\t\t\tinput_file = argv[i];\n\t\t\t} else {\n\t\t\t\terror = 3;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else if (!strcmp(argv[i], \"-l\")) {\n\t\t\ti++;\n\t\t\tif (argv[i] != NULL) {\n\t\t\t\tloops = atoi(argv[i]);\n\t\t\t\tif (loops <= 0) {\n\t\t\t\t\terror = 4;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terror = 5;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else if (!strcmp(argv[i], \"-p\")) {\n\t\t\tprints_enabled = 1;\n\t\t\tcontinue;\n\t\t} else {\n\t\t\terror = 6;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t\n\n\tif (dimension == -1) {\n\t\tdimension = DEFAULT_DIMENSION_SIZE;\n\t}\n\tif (loops == -1) {\n\t\tloops = DEFAULT_NUMBER_OF_LOOPS;\n\t}\n\n\t\n\n\tif (rank == 0) {\n\t\tif (error != 0) {\n\t\t\tswitch (error) {\n\t\t\tcase 1:\n\t\t\t\tprintf(\"Error: wrong dimension size\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tprintf(\"Error: you did not specify dimension size\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tprintf(\"Error: you did not specify input file\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 4:\n\t\t\t\tprintf(\"Error: wrong number of loops\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 5:\n\t\t\t\tprintf(\"Error: you did not specify number of loops\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tprintf(\"Error: unknown argument: %s\\n\", argv[i]);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tprintf(\n\t\t\t\t\t\"\\nUSAGE: mpiexec -f <machine_file> -n <number_of_processes> ./game_of_life -d <dimension_of_grid> -f <input_file> -l <number_of_loops> -p\\n\");\n\t\t\tprintf(\"\\nFLAGS\\n\");\n\t\t\tprintf(\"-d <dimension_of_grid> : This flag sets up the size of the grid. If it is not set up, the program will use %d as deafault value.\\n\",DEFAULT_DIMENSION_SIZE);\n\t\t\tprintf(\"-f <input_file> : This flag forces the program to read an initial state of grid from file, and use it as the first generation.\\n\");\n\t\t\tprintf(\"-l <number_of_loops> : This flag determines the generations that will be completed.\\n\");\n\t\t\tprintf(\"-p : (OPTIONAL FLAG) This flag forces the program to print the generations to output files. ATTENTION! This flag causes major slowdown to the execution!\\n\");\n\t\t}\n\t}\n\tif (error != 0) {\n\t\texit(0);\n\t}\n\n\tsub_grid_size = calculateSubgridSize(dimension, num_of_proc);\n\n\tif (sub_grid_size == -1) {\n\t\tif (rank == 0) {\n\t\t\tprintf(\"Number of processes must be a perfect square. %d is not. \\n\", num_of_proc);\n\n\t\t}\n\t\texit(0);\n\t} else if (sub_grid_size == -2) {\n\t\tif (rank == 0) {\n\t\t\tprintf(\"the grid cannot be divided with %d processes\\n\", num_of_proc);\n\t\t}\n\t\texit(0);\n\t}\n\n\tif (prints_enabled == 1 && rank == 0) {\n\t\tprintf(\"dimension size: %d\\n\", dimension);\n\t\tprintf(\"number of loops: %d\\n\", loops);\n\t}\n\n\texecute(rank, num_of_proc, dimension, sub_grid_size, loops, input_file, prints_enabled);\n\n\tif (rank == 0) {\n\t\tdouble time = MPI_Wtime() - start_time;\n\t\tprintf(\"time elapsed: %lf seconds\\n\", time);\n\t\tprintf(\"Terminated successfully\\n\");\n\t}\n\n\texit(0);\n}", "label": "int main(int argc, char *argv[]) {\n\n\tsrand(time(NULL));\n\t\n\tint i;\n\tint rank, num_of_proc;\n\tint dimension = -1, sub_grid_size = -1, loops = -1;\n\tint prints_enabled = 0;\n\tchar *input_file = NULL;\n\tint error = 0;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_of_proc);\n\n\tdouble start_time = MPI_Wtime();\n\t\n\t\n\n\tfor (i = 1; i < argc; i++) {\n\t\tif (!strcmp(argv[i], \"-d\")) {\n\t\t\ti++;\n\t\t\tif (argv[i] != NULL) {\n\t\t\t\tdimension = atoi(argv[i]);\n\t\t\t\tif (dimension < 16) {\n\t\t\t\t\terror = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terror = 2;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\telse if (!strcmp(argv[i], \"-f\")) {\n\t\t\ti++;\n\t\t\tif (argv[i] != NULL) {\n\t\t\t\tinput_file = argv[i];\n\t\t\t} else {\n\t\t\t\terror = 3;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else if (!strcmp(argv[i], \"-l\")) {\n\t\t\ti++;\n\t\t\tif (argv[i] != NULL) {\n\t\t\t\tloops = atoi(argv[i]);\n\t\t\t\tif (loops <= 0) {\n\t\t\t\t\terror = 4;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terror = 5;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else if (!strcmp(argv[i], \"-p\")) {\n\t\t\tprints_enabled = 1;\n\t\t\tcontinue;\n\t\t} else {\n\t\t\terror = 6;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t\n\n\tif (dimension == -1) {\n\t\tdimension = DEFAULT_DIMENSION_SIZE;\n\t}\n\tif (loops == -1) {\n\t\tloops = DEFAULT_NUMBER_OF_LOOPS;\n\t}\n\n\t\n\n\tif (rank == 0) {\n\t\tif (error != 0) {\n\t\t\tswitch (error) {\n\t\t\tcase 1:\n\t\t\t\tprintf(\"Error: wrong dimension size\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tprintf(\"Error: you did not specify dimension size\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tprintf(\"Error: you did not specify input file\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 4:\n\t\t\t\tprintf(\"Error: wrong number of loops\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 5:\n\t\t\t\tprintf(\"Error: you did not specify number of loops\\n\");\n\t\t\t\tbreak;\n\t\t\tcase 6:\n\t\t\t\tprintf(\"Error: unknown argument: %s\\n\", argv[i]);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tprintf(\n\t\t\t\t\t\"\\nUSAGE: mpiexec -f <machine_file> -n <number_of_processes> ./game_of_life -d <dimension_of_grid> -f <input_file> -l <number_of_loops> -p\\n\");\n\t\t\tprintf(\"\\nFLAGS\\n\");\n\t\t\tprintf(\"-d <dimension_of_grid> : This flag sets up the size of the grid. If it is not set up, the program will use %d as deafault value.\\n\",DEFAULT_DIMENSION_SIZE);\n\t\t\tprintf(\"-f <input_file> : This flag forces the program to read an initial state of grid from file, and use it as the first generation.\\n\");\n\t\t\tprintf(\"-l <number_of_loops> : This flag determines the generations that will be completed.\\n\");\n\t\t\tprintf(\"-p : (OPTIONAL FLAG) This flag forces the program to print the generations to output files. ATTENTION! This flag causes major slowdown to the execution!\\n\");\n\t\t}\n\t}\n\tif (error != 0) {\n\t\tMPI_Finalize();\n\t\texit(0);\n\t}\n\n\tsub_grid_size = calculateSubgridSize(dimension, num_of_proc);\n\n\tif (sub_grid_size == -1) {\n\t\tif (rank == 0) {\n\t\t\tprintf(\"Number of processes must be a perfect square. %d is not. \\n\", num_of_proc);\n\n\t\t}\n\t\tMPI_Finalize();\n\t\texit(0);\n\t} else if (sub_grid_size == -2) {\n\t\tif (rank == 0) {\n\t\t\tprintf(\"the grid cannot be divided with %d processes\\n\", num_of_proc);\n\t\t}\n\t\tMPI_Finalize();\n\t\texit(0);\n\t}\n\n\tif (prints_enabled == 1 && rank == 0) {\n\t\tprintf(\"dimension size: %d\\n\", dimension);\n\t\tprintf(\"number of loops: %d\\n\", loops);\n\t}\n\n\texecute(rank, num_of_proc, dimension, sub_grid_size, loops, input_file, prints_enabled);\n\n\tif (rank == 0) {\n\t\tdouble time = MPI_Wtime() - start_time;\n\t\tprintf(\"time elapsed: %lf seconds\\n\", time);\n\t\tprintf(\"Terminated successfully\\n\");\n\t}\n\n\tMPI_Finalize();\n\texit(0);\n}"}
{"program": "PawseySupercomputing_1400", "code": "int main(int argc, char **argv) {\n  long i;\n  long Ncirc = 0;\n  double pi, xy[2];\n  double r = 1.0; \n\n  double r2 = r*r;\n\n  int rank, size, manager = 0;\n  MPI_Status status;\n  long my_trials, temp;\n  int j;\n\n\n  VSLStreamStatePtr stream;\n\n  my_trials = num_trials/size;\n  if (num_trials%(long)size > (long)rank) my_trials++;\n\n  vslNewStream(&stream, VSL_BRNG_MT2203+rank, 1);\n\n  for (i = 0; i < my_trials; i++) {\n    vdRngUniform(VSL_RNG_METHOD_UNIFORMBITS_STD, stream, 2, xy, 0.0, 1.0);\n    if ((xy[0]*xy[0] + xy[1]*xy[1]) <= r2)\n      Ncirc++;\n  }\n\n  if (rank == manager) {\n    for (j = 1; j < size; j++) {\n      Ncirc += temp;\n    }\n    pi = 4.0 * ((double)Ncirc)/((double)num_trials);\n    printf(\"\\n \\t Computing pi using MPI and threaded MKL for random number generator: \\n\");\n    printf(\"\\t For %ld trials, pi = %f\\n\", num_trials, pi);\n    printf(\"\\n\");\n  } else {\n  }\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  long i;\n  long Ncirc = 0;\n  double pi, xy[2];\n  double r = 1.0; \n\n  double r2 = r*r;\n\n  int rank, size, manager = 0;\n  MPI_Status status;\n  long my_trials, temp;\n  int j;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  VSLStreamStatePtr stream;\n\n  my_trials = num_trials/size;\n  if (num_trials%(long)size > (long)rank) my_trials++;\n\n  vslNewStream(&stream, VSL_BRNG_MT2203+rank, 1);\n\n  for (i = 0; i < my_trials; i++) {\n    vdRngUniform(VSL_RNG_METHOD_UNIFORMBITS_STD, stream, 2, xy, 0.0, 1.0);\n    if ((xy[0]*xy[0] + xy[1]*xy[1]) <= r2)\n      Ncirc++;\n  }\n\n  if (rank == manager) {\n    for (j = 1; j < size; j++) {\n      MPI_Recv(&temp, 1, MPI_LONG, j, j, MPI_COMM_WORLD, &status);\n      Ncirc += temp;\n    }\n    pi = 4.0 * ((double)Ncirc)/((double)num_trials);\n    printf(\"\\n \\t Computing pi using MPI and threaded MKL for random number generator: \\n\");\n    printf(\"\\t For %ld trials, pi = %f\\n\", num_trials, pi);\n    printf(\"\\n\");\n  } else {\n    MPI_Send(&Ncirc, 1, MPI_LONG, manager, rank, MPI_COMM_WORLD);\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bjpop_1401", "code": "int main(int argc, char *argv[])\n{\n    int i = 0, rank, size, mpi_errno = MPI_SUCCESS;\n    int far_proc = 0, skip, iterations;\n    double latency = 0, total = 0, tmp1 = 0, tmp2 = 0;\n    MPI_Status status;\n\n\n    if(numprocs < 2) {\n        if(rank == ROOT) {\n            fprintf(stderr, \"This test requires at least two processes\\n\");\n        }\n\n\n        return EXIT_FAILURE;\n    }\n\n    if(rank == ROOT) {\n        fprintf(stdout, \"# %s v%s\\n\", BENCHMARK, PACKAGE_VERSION);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH, \"Latency (us)\");\n        fflush(stdout);\n    }\n\n    for(i = 0; i < MAX_MSG_SIZE; i++) {\n        x[i] = 'a';\n    }\n\n    for(size=1; size <= MAX_MSG_SIZE; size *= 2) {\n\n        far_proc = get_far_proc(numprocs, rank, size);\n        get_ack_time(far_proc, rank);\n\n        if(size > large_message_size) {\n            skip = SKIP_LARGE;\n            iterations = ITERATIONS_LARGE;\n        }\n\n        else {\n            skip = SKIP;\n            iterations = ITERATIONS;\n        }\n\n\n        for(i=0; i < iterations + skip ; i++) {\n            if(i == skip && rank == ROOT) {\n                tmp1 = ret_us();\n            }\n\n\n            if(rank == ROOT) {\n                mpi_errno =\n\n                if(mpi_errno != MPI_SUCCESS) {\n                    fprintf(stderr, \"Receive failed\\n\");\n                }\n            }\n\n            if(rank == far_proc) {\n                mpi_errno =\n\n                if(mpi_errno != MPI_SUCCESS) {\n                    fprintf(stderr, \"Send failed\\n\");\n                }\n            }\n        }\n\n        if(rank == ROOT) {\n            tmp2 = ret_us();\n            total = tmp2 - tmp1;\n            latency = (double)total/iterations;\n\n            fprintf(stdout, \"%-*d%*.*f\\n\", 10, size, FIELD_WIDTH,\n                    FLOAT_PRECISION, latency - ack_time);\n            fflush(stdout);\n        }\n\n    }\n\n\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char *argv[])\n{\n    int i = 0, rank, size, mpi_errno = MPI_SUCCESS;\n    int far_proc = 0, skip, iterations;\n    double latency = 0, total = 0, tmp1 = 0, tmp2 = 0;\n    MPI_Status status;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    if(numprocs < 2) {\n        if(rank == ROOT) {\n            fprintf(stderr, \"This test requires at least two processes\\n\");\n        }\n\n        MPI_Finalize();\n\n        return EXIT_FAILURE;\n    }\n\n    if(rank == ROOT) {\n        fprintf(stdout, \"# %s v%s\\n\", BENCHMARK, PACKAGE_VERSION);\n        fprintf(stdout, \"%-*s%*s\\n\", 10, \"# Size\", FIELD_WIDTH, \"Latency (us)\");\n        fflush(stdout);\n    }\n\n    for(i = 0; i < MAX_MSG_SIZE; i++) {\n        x[i] = 'a';\n    }\n\n    for(size=1; size <= MAX_MSG_SIZE; size *= 2) {\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        far_proc = get_far_proc(numprocs, rank, size);\n        get_ack_time(far_proc, rank);\n\n        if(size > large_message_size) {\n            skip = SKIP_LARGE;\n            iterations = ITERATIONS_LARGE;\n        }\n\n        else {\n            skip = SKIP;\n            iterations = ITERATIONS;\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        for(i=0; i < iterations + skip ; i++) {\n            if(i == skip && rank == ROOT) {\n                tmp1 = ret_us();\n            }\n\n            MPI_Bcast(&x, size, MPI_CHAR, 0, MPI_COMM_WORLD);         \n\n            if(rank == ROOT) {\n                mpi_errno = MPI_Recv(&y, 0, MPI_CHAR, far_proc, 1,\n                        MPI_COMM_WORLD, &status);\n\n                if(mpi_errno != MPI_SUCCESS) {\n                    fprintf(stderr, \"Receive failed\\n\");\n                }\n            }\n\n            if(rank == far_proc) {\n                mpi_errno = MPI_Send(&y, 0, MPI_CHAR, ROOT, 1, MPI_COMM_WORLD);\n\n                if(mpi_errno != MPI_SUCCESS) {\n                    fprintf(stderr, \"Send failed\\n\");\n                }\n            }\n        }\n\n        if(rank == ROOT) {\n            tmp2 = ret_us();\n            total = tmp2 - tmp1;\n            latency = (double)total/iterations;\n\n            fprintf(stdout, \"%-*d%*.*f\\n\", 10, size, FIELD_WIDTH,\n                    FLOAT_PRECISION, latency - ack_time);\n            fflush(stdout);\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "CFDEMproject_1403", "code": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem;\n  UserData data;\n  int iout, thispe, ier, npes;\n  long int Neq, local_N;\n  realtype rtol, atol, t0, t1, tout, tret;\n  N_Vector uu, up, constraints, id, res;\n\n  mem = NULL;\n  data = NULL;\n  uu = up = constraints = id = res = NULL;\n\n  \n\n\n  comm = MPI_COMM_WORLD;\n  \n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\", \n              npes,NPEX*NPEY);\n    return(1);\n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB;\n  Neq     = MX * MY;\n  \n  \n\n\n  data = (UserData) malloc(sizeof *data);\n  data->pp = NULL;\n  if(check_flag((void *)data, \"malloc\", 2, thispe)) \n\n  uu = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)uu, \"N_VNew_Parallel\", 0, thispe)) \n\n  up = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)up, \"N_VNew_Parallel\", 0, thispe)) \n\n  res = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)res, \"N_VNew_Parallel\", 0, thispe)) \n\n  constraints = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)constraints, \"N_VNew_Parallel\", 0, thispe)) \n\n  id = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)id, \"N_VNew_Parallel\", 0, thispe)) \n\n  \n\n  data->pp = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)data->pp, \"N_VNew_Parallel\", 0, thispe)) \n\n  InitUserData(thispe, comm, data);\n  \n  \n\n\n  SetInitialProfile(uu, up, id, res, data);\n  \n  \n\n\n  N_VConst(ONE, constraints);\n  \n  t0 = ZERO; t1 = RCONST(0.01);\n  \n  \n\n\n  rtol = ZERO;\n  atol = RCONST(1.0e-3);\n\n  \n\n\n  mem = IDACreate();\n\n  ier = IDASetUserData(mem, data);\n\n  ier = IDASetSuppressAlg(mem, TRUE);\n\n  ier = IDASetId(mem, id);\n\n  ier = IDASetConstraints(mem, constraints);\n  N_VDestroy_Parallel(constraints);  \n\n  ier = IDAInit(mem, resHeat, t0, uu, up);\n  \n  ier = IDASStolerances(mem, rtol, atol);\n\n  \n\n\n  ier = IDASpgmr(mem, 0);\n\n  ier = IDASpilsSetPreconditioner(mem, PsetupHeat, PsolveHeat);\n\n  \n\n  \n  if (thispe == 0) PrintHeader(Neq, rtol, atol);\n  PrintOutput(thispe, mem, t0, uu); \n  \n  \n\n\n  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) {\n\n    ier = IDASolve(mem, tout, &tret, uu, up, IDA_NORMAL);\n\n    PrintOutput(thispe, mem, tret, uu);\n\n  }\n  \n  \n\n\n  if (thispe == 0) PrintFinalStats(mem);\n\n  \n\n\n  IDAFree(&mem);\n\n  N_VDestroy_Parallel(id);\n  N_VDestroy_Parallel(res);\n  N_VDestroy_Parallel(up);\n  N_VDestroy_Parallel(uu);\n\n  N_VDestroy_Parallel(data->pp);\n  free(data);\n\n\n  return(0);\n\n}", "label": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem;\n  UserData data;\n  int iout, thispe, ier, npes;\n  long int Neq, local_N;\n  realtype rtol, atol, t0, t1, tout, tret;\n  N_Vector uu, up, constraints, id, res;\n\n  mem = NULL;\n  data = NULL;\n  uu = up = constraints = id = res = NULL;\n\n  \n\n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &thispe);\n  \n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\", \n              npes,NPEX*NPEY);\n    MPI_Finalize();\n    return(1);\n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB;\n  Neq     = MX * MY;\n  \n  \n\n\n  data = (UserData) malloc(sizeof *data);\n  data->pp = NULL;\n  if(check_flag((void *)data, \"malloc\", 2, thispe)) \n    MPI_Abort(comm, 1);\n\n  uu = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)uu, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  up = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)up, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  res = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)res, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  constraints = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)constraints, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  id = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)id, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  \n\n  data->pp = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)data->pp, \"N_VNew_Parallel\", 0, thispe)) \n    MPI_Abort(comm, 1);\n\n  InitUserData(thispe, comm, data);\n  \n  \n\n\n  SetInitialProfile(uu, up, id, res, data);\n  \n  \n\n\n  N_VConst(ONE, constraints);\n  \n  t0 = ZERO; t1 = RCONST(0.01);\n  \n  \n\n\n  rtol = ZERO;\n  atol = RCONST(1.0e-3);\n\n  \n\n\n  mem = IDACreate();\n  if(check_flag((void *)mem, \"IDACreate\", 0, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetUserData(mem, data);\n  if(check_flag(&ier, \"IDASetUserData\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetSuppressAlg(mem, TRUE);\n  if(check_flag(&ier, \"IDASetSuppressAlg\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetId(mem, id);\n  if(check_flag(&ier, \"IDASetId\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASetConstraints(mem, constraints);\n  if(check_flag(&ier, \"IDASetConstraints\", 1, thispe)) MPI_Abort(comm, 1);\n  N_VDestroy_Parallel(constraints);  \n\n  ier = IDAInit(mem, resHeat, t0, uu, up);\n  if(check_flag(&ier, \"IDAInit\", 1, thispe)) MPI_Abort(comm, 1);\n  \n  ier = IDASStolerances(mem, rtol, atol);\n  if(check_flag(&ier, \"IDASStolerances\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n\n  ier = IDASpgmr(mem, 0);\n  if(check_flag(&ier, \"IDASpgmr\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = IDASpilsSetPreconditioner(mem, PsetupHeat, PsolveHeat);\n  if(check_flag(&ier, \"IDASpilsSetPreconditioner\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n  \n  if (thispe == 0) PrintHeader(Neq, rtol, atol);\n  PrintOutput(thispe, mem, t0, uu); \n  \n  \n\n\n  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) {\n\n    ier = IDASolve(mem, tout, &tret, uu, up, IDA_NORMAL);\n    if(check_flag(&ier, \"IDASolve\", 1, thispe)) MPI_Abort(comm, 1);\n\n    PrintOutput(thispe, mem, tret, uu);\n\n  }\n  \n  \n\n\n  if (thispe == 0) PrintFinalStats(mem);\n\n  \n\n\n  IDAFree(&mem);\n\n  N_VDestroy_Parallel(id);\n  N_VDestroy_Parallel(res);\n  N_VDestroy_Parallel(up);\n  N_VDestroy_Parallel(uu);\n\n  N_VDestroy_Parallel(data->pp);\n  free(data);\n\n  MPI_Finalize();\n\n  return(0);\n\n}"}
{"program": "ebaty_1405", "code": "int main(int argc, char **argv) {\n\n\tint rank;\n\n\t\n\n\tFILE *fp;\n\n\tif ( rank == 0 ) {\n\t\tfp = fopen(kFileName, \"w\");\n\t\tif ( fp == NULL ) {\n\t\t\tprintf(\"can't open %s.\\n\", kFileName);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tlong long i;\n\tfor(i = 1; i <= (1LL << 31); i <<= 1) {\n\t\tlong long size;\n\t\tdouble ave_time = 0;\n\t\tif ( i == 2147483648 ) i = 2147483647;\n\n\t\tprintf(\"%s, %d\\n\", __PRETTY_FUNCTION__, __FILE__, __LINE__);\n\t\tint j;\n\t\tfor(j = 0; j < 10; ++j) {\n\t\t\tmpi_result result = sendData(i);\n\t\t\tave_time += result.time;\n\t\t\tsize = result.size;\n\t\t}\n\t\tave_time /= 10.0f;\n\n\t\tif ( rank == 0 ) {\n\t\t\tfprintf(fp, \"%lld\\t%lf\\n\", size, ave_time);\n\t\t}\n\t}\n\n\tif ( rank == 0 ) fclose(fp);\n\n\n\treturn 0;\n}\n", "label": "int main(int argc, char **argv) {\n\tMPI_Init(&argc, &argv);\n\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t\n\n\tFILE *fp;\n\n\tif ( rank == 0 ) {\n\t\tfp = fopen(kFileName, \"w\");\n\t\tif ( fp == NULL ) {\n\t\t\tprintf(\"can't open %s.\\n\", kFileName);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tlong long i;\n\tfor(i = 1; i <= (1LL << 31); i <<= 1) {\n\t\tlong long size;\n\t\tdouble ave_time = 0;\n\t\tif ( i == 2147483648 ) i = 2147483647;\n\n\t\tprintf(\"%s, %d\\n\", __PRETTY_FUNCTION__, __FILE__, __LINE__);\n\t\tint j;\n\t\tfor(j = 0; j < 10; ++j) {\n\t\t\tmpi_result result = sendData(i);\n\t\t\tave_time += result.time;\n\t\t\tsize = result.size;\n\t\t}\n\t\tave_time /= 10.0f;\n\n\t\tif ( rank == 0 ) {\n\t\t\tfprintf(fp, \"%lld\\t%lf\\n\", size, ave_time);\n\t\t}\n\t}\n\n\tif ( rank == 0 ) fclose(fp);\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "benkirk_1406", "code": "int main(int argc, char* argv[])\n{\n    int rank, nprocs;\n    int message_size;\n    int pairs;\n    int po_ret = 0;\n    options.bench = PT2PT;\n    options.subtype = LAT;\n\n    set_header(HEADER);\n    set_benchmark_name(\"osu_multi_lat\");\n\n    po_ret = process_options(argc, argv);\n\n    if (PO_OKAY == po_ret && NONE != options.accel) {\n        if (init_accel()) {\n            fprintf(stderr, \"Error initializing device\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n\n\n    pairs = nprocs/2;\n\n    if (0 == rank) {\n        switch (po_ret) {\n            case PO_CUDA_NOT_AVAIL:\n                fprintf(stderr, \"CUDA support not enabled.  Please recompile \"\n                        \"benchmark with CUDA support.\\n\");\n                break;\n            case PO_OPENACC_NOT_AVAIL:\n                fprintf(stderr, \"OPENACC support not enabled.  Please \"\n                        \"recompile benchmark with OPENACC support.\\n\");\n                break;\n            case PO_BAD_USAGE:\n                print_bad_usage_message(rank);\n                break;\n            case PO_HELP_MESSAGE:\n                print_help_message(rank);\n                break;\n            case PO_VERSION_MESSAGE:\n                print_version_message(rank);\n                break;\n            case PO_OKAY:\n                break;\n        }\n    }\n\n    switch (po_ret) {\n        case PO_CUDA_NOT_AVAIL:\n        case PO_OPENACC_NOT_AVAIL:\n        case PO_BAD_USAGE:\n            exit(EXIT_FAILURE);\n        case PO_HELP_MESSAGE:\n        case PO_VERSION_MESSAGE:\n            exit(EXIT_SUCCESS);\n        case PO_OKAY:\n            break;\n    }\n\n    if (rank == 0) {\n        print_header(rank, LAT);\n        fflush(stdout);\n    }\n\n\n    message_size = multi_latency(rank, pairs);\n\n\n\n    if (NONE != options.accel) {\n        if (cleanup_accel()) {\n            fprintf(stderr, \"Error cleaning up device\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    if (0 != errors_reduced && options.validate && 0 == rank ) {\n        fprintf(stdout, \"DATA VALIDATION ERROR: %s exited with status %d on\"\n                \" message size %d.\\n\", argv[0], EXIT_FAILURE, message_size);\n        exit(EXIT_FAILURE);\n    }\n    return EXIT_SUCCESS;\n}", "label": "int main(int argc, char* argv[])\n{\n    int rank, nprocs;\n    int message_size;\n    int pairs;\n    int po_ret = 0;\n    options.bench = PT2PT;\n    options.subtype = LAT;\n\n    set_header(HEADER);\n    set_benchmark_name(\"osu_multi_lat\");\n\n    po_ret = process_options(argc, argv);\n\n    if (PO_OKAY == po_ret && NONE != options.accel) {\n        if (init_accel()) {\n            fprintf(stderr, \"Error initializing device\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n    MPI_CHECK(MPI_Init(&argc, &argv));\n\n    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank));\n    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &nprocs));\n\n    pairs = nprocs/2;\n\n    if (0 == rank) {\n        switch (po_ret) {\n            case PO_CUDA_NOT_AVAIL:\n                fprintf(stderr, \"CUDA support not enabled.  Please recompile \"\n                        \"benchmark with CUDA support.\\n\");\n                break;\n            case PO_OPENACC_NOT_AVAIL:\n                fprintf(stderr, \"OPENACC support not enabled.  Please \"\n                        \"recompile benchmark with OPENACC support.\\n\");\n                break;\n            case PO_BAD_USAGE:\n                print_bad_usage_message(rank);\n                break;\n            case PO_HELP_MESSAGE:\n                print_help_message(rank);\n                break;\n            case PO_VERSION_MESSAGE:\n                print_version_message(rank);\n                MPI_CHECK(MPI_Finalize());\n                break;\n            case PO_OKAY:\n                break;\n        }\n    }\n\n    switch (po_ret) {\n        case PO_CUDA_NOT_AVAIL:\n        case PO_OPENACC_NOT_AVAIL:\n        case PO_BAD_USAGE:\n            MPI_CHECK(MPI_Finalize());\n            exit(EXIT_FAILURE);\n        case PO_HELP_MESSAGE:\n        case PO_VERSION_MESSAGE:\n            MPI_CHECK(MPI_Finalize());\n            exit(EXIT_SUCCESS);\n        case PO_OKAY:\n            break;\n    }\n\n    if (rank == 0) {\n        print_header(rank, LAT);\n        fflush(stdout);\n    }\n\n    MPI_CHECK(MPI_Barrier(MPI_COMM_WORLD));\n\n    message_size = multi_latency(rank, pairs);\n\n    MPI_CHECK(MPI_Barrier(MPI_COMM_WORLD));\n\n    MPI_CHECK(MPI_Finalize());\n\n    if (NONE != options.accel) {\n        if (cleanup_accel()) {\n            fprintf(stderr, \"Error cleaning up device\\n\");\n            exit(EXIT_FAILURE);\n        }\n    }\n\n    if (0 != errors_reduced && options.validate && 0 == rank ) {\n        fprintf(stdout, \"DATA VALIDATION ERROR: %s exited with status %d on\"\n                \" message size %d.\\n\", argv[0], EXIT_FAILURE, message_size);\n        exit(EXIT_FAILURE);\n    }\n    return EXIT_SUCCESS;\n}"}
{"program": "qingu_1407", "code": "int main(int argc, char **argv) {\n    int i, j, rank, nranks, peer, bufsize, errors, total_errors;\n    double **buffer, *src_buf;\n    int count[2], src_stride, trg_stride, stride_level;\n    double scaling, time;\n\n    ARMCI_Init();\n\n\n    buffer = (double **) malloc(sizeof(double *) * nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    ARMCI_Malloc((void **) buffer, bufsize);\n    src_buf = ARMCI_Malloc_local(bufsize);\n\n    if (rank == 0)\n        printf(\"ARMCI Strided Accumulate Test:\\n\");\n\n    ARMCI_Access_begin(buffer[rank]);\n\n    for (i = 0; i < XDIM*YDIM; i++) {\n        *(buffer[rank] + i) = 1.0 + rank;\n        *(src_buf + i) = 1.0 + rank;\n    }\n\n    ARMCI_Access_end(buffer[rank]);\n\n    scaling = 2.0;\n\n    src_stride = XDIM * sizeof(double);\n    trg_stride = XDIM * sizeof(double);\n    stride_level = 1;\n\n    count[1] = YDIM;\n    count[0] = XDIM * sizeof(double);\n\n    ARMCI_Barrier();\n    time =\n\n    peer = (rank+1) % nranks;\n\n    for (i = 0; i < ITERATIONS; i++) {\n\n      ARMCI_AccS(ARMCI_ACC_DBL,\n          (void *) &scaling,\n          src_buf,\n          &src_stride,\n          (void *) buffer[peer],\n          &trg_stride,\n          count,\n          stride_level,\n          peer);\n    }\n\n    ARMCI_Barrier();\n    time = MPI_Wtime() - time;\n\n    if (rank == 0) printf(\"Time: %f sec\\n\", time);\n\n    ARMCI_Access_begin(buffer[rank]);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer[rank] + i + j*XDIM);\n        const double expected = (1.0 + rank) + scaling * (1.0 + ((rank+nranks-1)%nranks)) * (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    ARMCI_Access_end(buffer[rank]);\n\n\n    ARMCI_Free((void *) buffer[rank]);\n    ARMCI_Free_local(src_buf);\n    free(buffer);\n\n    ARMCI_Finalize();\n\n    if (total_errors == 0) {\n      if (rank == 0) printf(\"Success.\\n\");\n      return 0;\n    } else {\n      if (rank == 0) printf(\"Fail.\\n\");\n      return 1;\n    }\n}", "label": "int main(int argc, char **argv) {\n    int i, j, rank, nranks, peer, bufsize, errors, total_errors;\n    double **buffer, *src_buf;\n    int count[2], src_stride, trg_stride, stride_level;\n    double scaling, time;\n\n    MPI_Init(&argc, &argv);\n    ARMCI_Init();\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n    buffer = (double **) malloc(sizeof(double *) * nranks);\n\n    bufsize = XDIM * YDIM * sizeof(double);\n    ARMCI_Malloc((void **) buffer, bufsize);\n    src_buf = ARMCI_Malloc_local(bufsize);\n\n    if (rank == 0)\n        printf(\"ARMCI Strided Accumulate Test:\\n\");\n\n    ARMCI_Access_begin(buffer[rank]);\n\n    for (i = 0; i < XDIM*YDIM; i++) {\n        *(buffer[rank] + i) = 1.0 + rank;\n        *(src_buf + i) = 1.0 + rank;\n    }\n\n    ARMCI_Access_end(buffer[rank]);\n\n    scaling = 2.0;\n\n    src_stride = XDIM * sizeof(double);\n    trg_stride = XDIM * sizeof(double);\n    stride_level = 1;\n\n    count[1] = YDIM;\n    count[0] = XDIM * sizeof(double);\n\n    ARMCI_Barrier();\n    time = MPI_Wtime();\n\n    peer = (rank+1) % nranks;\n\n    for (i = 0; i < ITERATIONS; i++) {\n\n      ARMCI_AccS(ARMCI_ACC_DBL,\n          (void *) &scaling,\n          src_buf,\n          &src_stride,\n          (void *) buffer[peer],\n          &trg_stride,\n          count,\n          stride_level,\n          peer);\n    }\n\n    ARMCI_Barrier();\n    time = MPI_Wtime() - time;\n\n    if (rank == 0) printf(\"Time: %f sec\\n\", time);\n\n    ARMCI_Access_begin(buffer[rank]);\n    for (i = errors = 0; i < XDIM; i++) {\n      for (j = 0; j < YDIM; j++) {\n        const double actual   = *(buffer[rank] + i + j*XDIM);\n        const double expected = (1.0 + rank) + scaling * (1.0 + ((rank+nranks-1)%nranks)) * (ITERATIONS);\n        if (actual - expected > 1e-10) {\n          printf(\"%d: Data validation failed at [%d, %d] expected=%f actual=%f\\n\",\n              rank, j, i, expected, actual);\n          errors++;\n          fflush(stdout);\n        }\n      }\n    }\n    ARMCI_Access_end(buffer[rank]);\n\n    MPI_Allreduce(&errors, &total_errors, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    ARMCI_Free((void *) buffer[rank]);\n    ARMCI_Free_local(src_buf);\n    free(buffer);\n\n    ARMCI_Finalize();\n    MPI_Finalize();\n\n    if (total_errors == 0) {\n      if (rank == 0) printf(\"Success.\\n\");\n      return 0;\n    } else {\n      if (rank == 0) printf(\"Fail.\\n\");\n      return 1;\n    }\n}"}
{"program": "bsc-performance-tools_1408", "code": "int main(int argc, char *argv[])\n{\n\tint v;\n\tMPI_Request r;\n\tMPI_Status s;\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint v;\n\tMPI_Request r;\n\tMPI_Status s;\n\tMPI_Init (&argc, &argv);\n\tMPI_Ibcast (&v, 1, MPI_INT, 0, MPI_COMM_WORLD, &r);\n\tMPI_Wait (&r, &s);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "huudatHust_1409", "code": "int main (int argc, char *argv[])\n{\nint   numtasks, taskid, rc, dest, offset, i, j, tag1,\n      tag2, source, chunksize; \nfloat mysum, sum;\nfloat update(int myoffset, int chunk, int myid);\nMPI_Status status;\n\n\n\nMPI_Init(&argc, &argv);\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nif (numtasks % 4 != 0) {\n   printf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n   exit(0);\n   }\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nprintf (\"MPI task %d has started...\\n\", taskid);\nchunksize = (ARRAYSIZE / numtasks);\ntag2 = 1;\ntag1 = 2;\n\n\n\nif (taskid == MASTER){\n\n  \n\n  sum = 0;\n  for(i=0; i<ARRAYSIZE; i++) {\n    data[i] =  i * 1.0;\n    sum = sum + data[i];\n    }\n  printf(\"Initialized array sum = %e\\n\",sum);\n\n  \n\n  offset = chunksize;\n  for (dest=1; dest<numtasks; dest++) {\n    printf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n    offset = offset + chunksize;\n    }\n\n  \n\n  offset = 0;\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  for (i=1; i<numtasks; i++) {\n    source = i;\n    }\n\n  \n  \n  printf(\"Sample results: \\n\");\n  offset = 0;\n  for (i=0; i<numtasks; i++) {\n    for (j=0; j<5; j++) \n      printf(\"  %e\",data[offset+j]);\n    printf(\"\\n\");\n    offset = offset + chunksize;\n    }\n  printf(\"*** Final sum= %e ***\\n\",sum);\n\n  }  \n\n\n\n\n\n\n\nif (taskid > MASTER) {\n\n  \n\n  source = MASTER;\n\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  dest = MASTER;\n\n\n  } \n\n\n\nMPI_Finalize();\n\n}", "label": "int main (int argc, char *argv[])\n{\nint   numtasks, taskid, rc, dest, offset, i, j, tag1,\n      tag2, source, chunksize; \nfloat mysum, sum;\nfloat update(int myoffset, int chunk, int myid);\nMPI_Status status;\n\n\n\nMPI_Init(&argc, &argv);\nMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\nif (numtasks % 4 != 0) {\n   printf(\"Quitting. Number of MPI tasks must be divisible by 4.\\n\");\n   MPI_Abort(MPI_COMM_WORLD, rc);\n   exit(0);\n   }\nMPI_Comm_rank(MPI_COMM_WORLD,&taskid);\nprintf (\"MPI task %d has started...\\n\", taskid);\nchunksize = (ARRAYSIZE / numtasks);\ntag2 = 1;\ntag1 = 2;\n\n\n\nif (taskid == MASTER){\n\n  \n\n  sum = 0;\n  for(i=0; i<ARRAYSIZE; i++) {\n    data[i] =  i * 1.0;\n    sum = sum + data[i];\n    }\n  printf(\"Initialized array sum = %e\\n\",sum);\n\n  \n\n  offset = chunksize;\n  for (dest=1; dest<numtasks; dest++) {\n    MPI_Send(&offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n    MPI_Send(&data[offset], chunksize, MPI_FLOAT, dest, tag2, MPI_COMM_WORLD);\n    printf(\"Sent %d elements to task %d offset= %d\\n\",chunksize,dest,offset);\n    offset = offset + chunksize;\n    }\n\n  \n\n  offset = 0;\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  for (i=1; i<numtasks; i++) {\n    source = i;\n    MPI_Recv(&offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);\n    MPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2,\n      MPI_COMM_WORLD, &status);\n    }\n\n  \n  \n  MPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n  printf(\"Sample results: \\n\");\n  offset = 0;\n  for (i=0; i<numtasks; i++) {\n    for (j=0; j<5; j++) \n      printf(\"  %e\",data[offset+j]);\n    printf(\"\\n\");\n    offset = offset + chunksize;\n    }\n  printf(\"*** Final sum= %e ***\\n\",sum);\n\n  }  \n\n\n\n\n\n\n\nif (taskid > MASTER) {\n\n  \n\n  source = MASTER;\n  MPI_Recv(&offset, 1, MPI_INT, source, tag1, MPI_COMM_WORLD, &status);\n  MPI_Recv(&data[offset], chunksize, MPI_FLOAT, source, tag2, \n    MPI_COMM_WORLD, &status);\n\n  mysum = update(offset, chunksize, taskid);\n\n  \n\n  dest = MASTER;\n  MPI_Send(&offset, 1, MPI_INT, dest, tag1, MPI_COMM_WORLD);\n  MPI_Send(&data[offset], chunksize, MPI_FLOAT, MASTER, tag2, MPI_COMM_WORLD);\n\n  MPI_Reduce(&mysum, &sum, 1, MPI_FLOAT, MPI_SUM, MASTER, MPI_COMM_WORLD);\n\n  } \n\n\n\nMPI_Finalize();\n\n}"}
{"program": "byu-vv-lab_1410", "code": "int main(int argc, char * argv[]) \n{ \n    int rank;\n    int procs;\n    int* values;\n\n    \n    if (rank == 0 || rank == 2) {\n      values = (int*)malloc(sizeof(int)*procs);\n    }else{\n      values = (int*)malloc(sizeof(int));\n    }\n    \n    *values = procs + rank;\n\n#ifdef TYPE\n    if (rank != 2)\n    else\n#elif defined ROOT\n    if (rank != 2)\n    else\n#else\n    if (rank != 2)\n#endif\n    \n    free(values);\n    return 0; \n}", "label": "int main(int argc, char * argv[]) \n{ \n    int rank;\n    int procs;\n    int* values;\n\n    MPI_Init(&argc,&argv); \n    MPI_Comm_size(MPI_COMM_WORLD, &procs); \n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); \n    \n    if (rank == 0 || rank == 2) {\n      values = (int*)malloc(sizeof(int)*procs);\n    }else{\n      values = (int*)malloc(sizeof(int));\n    }\n    \n    *values = procs + rank;\n\n#ifdef TYPE\n    if (rank != 2)\n        MPI_Gather(values, 1, MPI_INT, values, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Gather(values, 1, MPI_FLOAT, values, 1, MPI_FLOAT, 0, MPI_COMM_WORLD);\n#elif defined ROOT\n    if (rank != 2)\n        MPI_Gather(values, 1, MPI_INT, values, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    else\n        MPI_Gather(values, 1, MPI_INT, values, 1, MPI_INT, 2, MPI_COMM_WORLD);\n#else\n    if (rank != 2)\n        MPI_Gather(values, 1, MPI_INT, values, 1, MPI_INT, 0, MPI_COMM_WORLD);\n#endif\n    \n    free(values);\n    MPI_Finalize(); \n    return 0; \n}"}
{"program": "Arnatar_1411", "code": "int main(int argc, char** argv) {\n  \n\n  \n\n  int world_rank;\n  int world_size;\n\n  int token;\n  \n\n  \n\n  if (world_rank != 0) {\n    printf(\"Process %d received token %d from process %d\\n\", world_rank, token,\n           world_rank - 1);\n  } else {\n    \n\n    token = -1;\n  }\n  \n\n  \n\n  \n\n  if (world_rank == 0) {\n    printf(\"Process %d received token %d from process %d\\n\", world_rank, token,\n           world_size - 1);\n  }\n}", "label": "int main(int argc, char** argv) {\n  \n\n  MPI_Init(NULL, NULL);\n  \n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int token;\n  \n\n  \n\n  if (world_rank != 0) {\n    MPI_Recv(&token, 1, MPI_INT, world_rank - 1, 0, MPI_COMM_WORLD,\n             MPI_STATUS_IGNORE);\n    printf(\"Process %d received token %d from process %d\\n\", world_rank, token,\n           world_rank - 1);\n  } else {\n    \n\n    token = -1;\n  }\n  MPI_Send(&token, 1, MPI_INT, (world_rank + 1) % world_size, 0,\n           MPI_COMM_WORLD);\n  \n\n  \n\n  \n\n  if (world_rank == 0) {\n    MPI_Recv(&token, 1, MPI_INT, world_size - 1, 0, MPI_COMM_WORLD,\n             MPI_STATUS_IGNORE);\n    printf(\"Process %d received token %d from process %d\\n\", world_rank, token,\n           world_size - 1);\n  }\n  MPI_Finalize();\n}"}
{"program": "frenchrd_1412", "code": "int main(int argc, char** argv) {\n\n\tProblemDescription p = specify_p_from_environment(argc,argv);\n\tInitialize_Walltimer(p.my_rank_id);\n\n\t\n\n\tdouble local_dot_product;\n\tint i;\n\tdouble h = (double)(2.0 * M_PI / (double)p.global_length);\n\tVector f1 = lb_allocate_vector(p.local_length);\n\tVector f2 = lb_allocate_vector(p.local_length);\n\tVector f1_dot_f2 = lb_allocate_vector(p.local_length);\n\tWalltimer_Label(\"Allocating giant arrays on host\");\n\n\t\n\n\tpopulate_vectors(f1.data,f2.data,p.local_length,h,p.lower_bound);\n\tWalltimer_Label(\"Populating vectors via device\");\n\n\t\n\n\tdouble dot_product_son;\n\tlbdp(f1,f2,&local_dot_product);\n\tWalltimer_Label(\"f1 dot f2\");\n\t\n\t\n\n\tdouble norm_f1;\n\tdouble local_norm;\n\tlbdp(f1,f1,&local_norm);\n\tif (p.my_rank_id == 0) norm_f1 = sqrt(norm_f1);\n\tWalltimer_Label(\"Norm of f1\");\n\t\n\t\n\n\tdouble norm_f2;\n\tlbdp(f2,f2,&local_norm);\n\tif (p.my_rank_id == 0) norm_f2 = sqrt(norm_f2);\n\tWalltimer_Label(\"Norm of f2\");\n\n\tif (p.my_rank_id == 0) {\n\t\tdouble theta = acos(dot_product_son / (norm_f1 * norm_f2));\n\t\tprintf(\"The dot product of f1 and f2 is %f\\n\",dot_product_son);\n\t\tprintf(\"The angle between f1 and f2 is %f\\n\",theta);\n\t}\t\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tMPI_Init(&argc, &argv);\n\n\tProblemDescription p = specify_p_from_environment(argc,argv);\n\tInitialize_Walltimer(p.my_rank_id);\n\n\t\n\n\tdouble local_dot_product;\n\tint i;\n\tdouble h = (double)(2.0 * M_PI / (double)p.global_length);\n\tVector f1 = lb_allocate_vector(p.local_length);\n\tVector f2 = lb_allocate_vector(p.local_length);\n\tVector f1_dot_f2 = lb_allocate_vector(p.local_length);\n\tWalltimer_Label(\"Allocating giant arrays on host\");\n\n\t\n\n\tpopulate_vectors(f1.data,f2.data,p.local_length,h,p.lower_bound);\n\tWalltimer_Label(\"Populating vectors via device\");\n\n\t\n\n\tdouble dot_product_son;\n\tlbdp(f1,f2,&local_dot_product);\n\tMPI_Reduce(&local_dot_product,&dot_product_son,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n\tWalltimer_Label(\"f1 dot f2\");\n\t\n\t\n\n\tdouble norm_f1;\n\tdouble local_norm;\n\tlbdp(f1,f1,&local_norm);\n\tMPI_Reduce(&local_norm,&norm_f1,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n\tif (p.my_rank_id == 0) norm_f1 = sqrt(norm_f1);\n\tWalltimer_Label(\"Norm of f1\");\n\t\n\t\n\n\tdouble norm_f2;\n\tlbdp(f2,f2,&local_norm);\n\tMPI_Reduce(&local_norm,&norm_f2,1,MPI_DOUBLE,MPI_SUM,0,MPI_COMM_WORLD);\n\tif (p.my_rank_id == 0) norm_f2 = sqrt(norm_f2);\n\tWalltimer_Label(\"Norm of f2\");\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tif (p.my_rank_id == 0) {\n\t\tdouble theta = acos(dot_product_son / (norm_f1 * norm_f2));\n\t\tprintf(\"The dot product of f1 and f2 is %f\\n\",dot_product_son);\n\t\tprintf(\"The angle between f1 and f2 is %f\\n\",theta);\n\t}\t\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "edhartnett_1413", "code": "int\nmain(int argc, char* argv[]) \n{\n   int n, my_rank;\n   int event_num[2][NUM_EVENTS];\n   int ret;\n\n   \n\n\n   \n\n\n   event_num[START][INIT] = MPE_Log_get_event_number();\n   event_num[END][INIT] = MPE_Log_get_event_number();\n   if (!my_rank)\n      MPE_Describe_state(event_num[START][INIT], event_num[END][INIT], \"init\", \"yellow\");\n   event_num[START][UPDATE] = MPE_Log_get_event_number();\n   event_num[END][UPDATE] = MPE_Log_get_event_number();\n   if (!my_rank)\n      MPE_Describe_state(event_num[START][UPDATE], event_num[END][UPDATE], \"update\", \"green\");\n\n   if ((ret = MPE_Log_event(event_num[START][INIT], 0, \"start init\")))\n   sleep(1);\n   if ((ret = MPE_Log_event(event_num[END][INIT], 0, \"end init\")))\n\n   if ((ret = MPE_Log_event(event_num[START][UPDATE], 0, \"start update\")))\n   sleep(1);\n   if ((ret = MPE_Log_event(event_num[END][UPDATE], 0, \"end update\")))\n\n   return 0;\n}", "label": "int\nmain(int argc, char* argv[]) \n{\n   int n, my_rank;\n   int event_num[2][NUM_EVENTS];\n   int ret;\n\n   \n\n   MPI_Init(&argc, &argv);\n   MPI_Errhandler_set(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &n);\n\n   event_num[START][INIT] = MPE_Log_get_event_number();\n   event_num[END][INIT] = MPE_Log_get_event_number();\n   if (!my_rank)\n      MPE_Describe_state(event_num[START][INIT], event_num[END][INIT], \"init\", \"yellow\");\n   event_num[START][UPDATE] = MPE_Log_get_event_number();\n   event_num[END][UPDATE] = MPE_Log_get_event_number();\n   if (!my_rank)\n      MPE_Describe_state(event_num[START][UPDATE], event_num[END][UPDATE], \"update\", \"green\");\n\n   if ((ret = MPE_Log_event(event_num[START][INIT], 0, \"start init\")))\n      MPIERR(ret);\n   sleep(1);\n   if ((ret = MPE_Log_event(event_num[END][INIT], 0, \"end init\")))\n      MPIERR(ret);\n\n   if ((ret = MPE_Log_event(event_num[START][UPDATE], 0, \"start update\")))\n      MPIERR(ret);\n   sleep(1);\n   if ((ret = MPE_Log_event(event_num[END][UPDATE], 0, \"end update\")))\n      MPIERR(ret);\n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "pigay_1414", "code": "gint main (gint argc, gchar ** argv)\n{\n  gint ret = 0;\n  VsgCommBuffer *cb;\n  gint mytid, numtasks;\n  gint i;\n\n  vsg_init_gdouble ();\n\n  if (argc > 1 && g_ascii_strncasecmp (argv[1], \"--version\", 9) == 0)\n    {\n      if (mytid == 0)\n        g_print (\"%s\\n\", PACKAGE_VERSION);\n      return 0;\n    }\n \n\n\n\n\n\n  cb = vsg_comm_buffer_new (MPI_COMM_WORLD);\n\n  for (i=0; i<numtasks; i++)\n    {\n      gchar *a=\"a\";\n      VsgVector3d v3 = VSG_V3D_J;\n\n      \n\n      if (i == 2) continue;\n\n      vsg_comm_buffer_send_append (cb, i, &i, 1, MPI_INT);\n      vsg_comm_buffer_send_append (cb, i, &v3, 1, VSG_MPI_TYPE_VECTOR3D);\n      vsg_comm_buffer_send_append (cb, i, a, 2, MPI_CHAR);\n    }\n\n  vsg_comm_buffer_share (cb);\n\n  if (mytid != 2)\n    for (i=0; i<numtasks; i++)\n      {\n        gchar ra[2] = \"\";\n        gint ri = 0;\n        VsgVector3d v3 = VSG_V3D_ZERO;\n\n        vsg_comm_buffer_recv_read (cb, i, &ri, 1, MPI_INT);\n        vsg_comm_buffer_recv_read (cb, i, &v3, 1, VSG_MPI_TYPE_VECTOR3D);\n        vsg_comm_buffer_recv_read (cb, i, ra, 2, MPI_CHAR);\n\n        if (ri != mytid)\n          {\n            g_printerr (\"%d: integer msg (from %d) error %d (should be %d).\\n\",\n                        mytid, i, ri, mytid);\n          }\n\n        if (vsg_vector3d_dist (&v3, &VSG_V3D_J) != 0.)\n          {\n            g_printerr (\"%d: VsgVector3d msg (from %d) error \",\n                        mytid, i);\n            vsg_vector3d_write (&v3, stderr);\n            g_printerr (\" (should be \");\n            vsg_vector3d_write (&VSG_V3D_J, stderr);\n            g_printerr (\").\\n\");\n          }\n\n        if (g_ascii_strncasecmp (ra, \"a\", 1) != 0)\n          {\n            g_printerr (\"%d: character msg (from %d) error %s (should be %s).\\n\",\n                        mytid, i, ra, \"a\");\n          }\n      }\n\n  vsg_comm_buffer_free (cb);\n\n\n  return ret;\n}", "label": "gint main (gint argc, gchar ** argv)\n{\n  gint ret = 0;\n  VsgCommBuffer *cb;\n  gint mytid, numtasks;\n  gint i;\n\n  vsg_init_gdouble ();\n\n  if (argc > 1 && g_ascii_strncasecmp (argv[1], \"--version\", 9) == 0)\n    {\n      if (mytid == 0)\n        g_print (\"%s\\n\", PACKAGE_VERSION);\n      return 0;\n    }\n \n  MPI_Init (&argc, &argv);\n\n  MPI_Comm_size (MPI_COMM_WORLD, &numtasks);\n  MPI_Comm_rank (MPI_COMM_WORLD, &mytid);\n\n\n\n\n  cb = vsg_comm_buffer_new (MPI_COMM_WORLD);\n\n  for (i=0; i<numtasks; i++)\n    {\n      gchar *a=\"a\";\n      VsgVector3d v3 = VSG_V3D_J;\n\n      \n\n      if (i == 2) continue;\n\n      vsg_comm_buffer_send_append (cb, i, &i, 1, MPI_INT);\n      vsg_comm_buffer_send_append (cb, i, &v3, 1, VSG_MPI_TYPE_VECTOR3D);\n      vsg_comm_buffer_send_append (cb, i, a, 2, MPI_CHAR);\n    }\n\n  vsg_comm_buffer_share (cb);\n\n  if (mytid != 2)\n    for (i=0; i<numtasks; i++)\n      {\n        gchar ra[2] = \"\";\n        gint ri = 0;\n        VsgVector3d v3 = VSG_V3D_ZERO;\n\n        vsg_comm_buffer_recv_read (cb, i, &ri, 1, MPI_INT);\n        vsg_comm_buffer_recv_read (cb, i, &v3, 1, VSG_MPI_TYPE_VECTOR3D);\n        vsg_comm_buffer_recv_read (cb, i, ra, 2, MPI_CHAR);\n\n        if (ri != mytid)\n          {\n            g_printerr (\"%d: integer msg (from %d) error %d (should be %d).\\n\",\n                        mytid, i, ri, mytid);\n          }\n\n        if (vsg_vector3d_dist (&v3, &VSG_V3D_J) != 0.)\n          {\n            g_printerr (\"%d: VsgVector3d msg (from %d) error \",\n                        mytid, i);\n            vsg_vector3d_write (&v3, stderr);\n            g_printerr (\" (should be \");\n            vsg_vector3d_write (&VSG_V3D_J, stderr);\n            g_printerr (\").\\n\");\n          }\n\n        if (g_ascii_strncasecmp (ra, \"a\", 1) != 0)\n          {\n            g_printerr (\"%d: character msg (from %d) error %s (should be %s).\\n\",\n                        mytid, i, ra, \"a\");\n          }\n      }\n\n  vsg_comm_buffer_free (cb);\n\n  MPI_Finalize ();\n\n  return ret;\n}"}
{"program": "raulperula_1415", "code": "int main(int argc, char** argv)\n{\n\tint myrank, size, i;\n\tchar cad1[N], cad2[N];\n\tMPI_Status estado;\n\t\n\n\t\n\n\n\t\n\tif(myrank == 0){\n\t\tprintf(\"Proceso master -> %d hijos\\n\", size);\n\t\t\n\t\tfor(i=1;i<size;i++){\n\t\t\tprintf(\"Cadena recibida: %s\\n\", cad2);\n\t\t}\n\t}\n\telse{\n\t\tsprintf(cad1, \"Hola mundo desde %d\\n\", myrank);\n\t}\n\t\t\n\n\t\n\treturn 0;\n}", "label": "int main(int argc, char** argv)\n{\n\tint myrank, size, i;\n\tchar cad1[N], cad2[N];\n\tMPI_Status estado;\n\t\n\tMPI_Init(&argc, &argv); \n\n\t\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n\n\tMPI_Comm_size(MPI_COMM_WORLD, &size); \n\n\t\n\tif(myrank == 0){\n\t\tprintf(\"Proceso master -> %d hijos\\n\", size);\n\t\t\n\t\tfor(i=1;i<size;i++){\n\t\t\tMPI_Recv(cad2, N, MPI_CHAR, i, 1, MPI_COMM_WORLD, &estado);\n\t\t\tprintf(\"Cadena recibida: %s\\n\", cad2);\n\t\t}\n\t}\n\telse{\n\t\tsprintf(cad1, \"Hola mundo desde %d\\n\", myrank);\n\t\tMPI_Send(cad1, N, MPI_CHAR, 0, 1, MPI_COMM_WORLD);\n\t}\n\t\t\n\tMPI_Finalize(); \n\n\t\n\treturn 0;\n}"}
{"program": "rahlk_1416", "code": "int main (int argc, char *argv[])\n{\nint reps,                   \n\n    tag,                    \n\n    numtasks,               \n\n    rank,                   \n\n    dest, source,           \n\n    avgT,                   \n\n    rc,                     \n\n    n;\ndouble T1, T2,              \n\n    sumT,                   \n\n    deltaT;                 \n\nchar msg;                   \n\nMPI_Status status;          \n\n\nMPI_Init(&argc,&argv);\nMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\nMPI_Comm_rank(MPI_COMM_WORLD,&rank);\nif (rank == 0 && numtasks != 2) {\n   printf(\"Number of tasks = %d\\n\",numtasks);\n   printf(\"Only need 2 tasks - extra will be ignored...\\n\");\n   }\n\n\n\nsumT = 0;\nmsg = 'x';\ntag = 1;\nreps = NUMBER_REPS;\n\nif (rank == 0) {\n   \n\n   printf(\"task %d has started...\\n\", rank);\n   printf(\"Beginning latency timing test. Number of reps = %d.\\n\", reps);\n   printf(\"***************************************************\\n\");\n   printf(\"Rep#       T1               T2            deltaT\\n\");\n   dest = 1;\n   source = 1;\n   for (n = 1; n <= reps; n++) {\n      T1 =     \n\n      \n\n      \n\n      rc =\n      if (rc != MPI_SUCCESS) {\n         printf(\"Send error in task 0!\\n\");\n         exit(1);\n         }\n      \n\n      \n\n      rc =\n      if (rc != MPI_SUCCESS) {\n         printf(\"Receive error in task 0!\\n\");\n         exit(1);\n         }\n      T2 =     \n\n\n      \n\n      deltaT = T2 - T1;\n      printf(\"%4d  %8.8f  %8.8f  %2.8f\\n\", n, T1, T2, deltaT);\n         sumT += deltaT;\n      }\n   avgT = (sumT*1000000)/reps;\n   printf(\"***************************************************\\n\");\n   printf(\"\\n*** Avg round trip time = %d microseconds\\n\", avgT);\n   printf(\"*** Avg one way latency = %d microseconds\\n\", avgT/2);\n   }\n\nelse if (rank == 1) {\n   printf(\"task %d has started...\\n\", rank);\n   dest = 0;\n   source = 0;\n   for (n = 1; n <= reps; n++) {\n      rc =\n      if (rc != MPI_SUCCESS) {\n         printf(\"Receive error in task 1!\\n\");\n         exit(1);\n         }\n      rc =\n      if (rc != MPI_SUCCESS) {\n         printf(\"Send error in task 1!\\n\");\n         exit(1);\n         }\n      }\n   }\n\nMPI_Finalize();\nexit(0);\n}", "label": "int main (int argc, char *argv[])\n{\nint reps,                   \n\n    tag,                    \n\n    numtasks,               \n\n    rank,                   \n\n    dest, source,           \n\n    avgT,                   \n\n    rc,                     \n\n    n;\ndouble T1, T2,              \n\n    sumT,                   \n\n    deltaT;                 \n\nchar msg;                   \n\nMPI_Status status;          \n\n\nMPI_Init(&argc,&argv);\nMPI_Comm_size(MPI_COMM_WORLD,&numtasks);\nMPI_Comm_rank(MPI_COMM_WORLD,&rank);\nif (rank == 0 && numtasks != 2) {\n   printf(\"Number of tasks = %d\\n\",numtasks);\n   printf(\"Only need 2 tasks - extra will be ignored...\\n\");\n   }\n\n\n\nsumT = 0;\nmsg = 'x';\ntag = 1;\nreps = NUMBER_REPS;\n\nif (rank == 0) {\n   \n\n   printf(\"task %d has started...\\n\", rank);\n   printf(\"Beginning latency timing test. Number of reps = %d.\\n\", reps);\n   printf(\"***************************************************\\n\");\n   printf(\"Rep#       T1               T2            deltaT\\n\");\n   dest = 1;\n   source = 1;\n   for (n = 1; n <= reps; n++) {\n      T1 = MPI_Wtime();     \n\n      \n\n      \n\n      rc = MPI_Send(&msg, 1, MPI_BYTE, dest, tag, MPI_COMM_WORLD);\n      if (rc != MPI_SUCCESS) {\n         printf(\"Send error in task 0!\\n\");\n         MPI_Abort(MPI_COMM_WORLD, rc);\n         exit(1);\n         }\n      \n\n      \n\n      rc = MPI_Recv(&msg, 1, MPI_BYTE, source, tag, MPI_COMM_WORLD,\n                    &status);\n      if (rc != MPI_SUCCESS) {\n         printf(\"Receive error in task 0!\\n\");\n         MPI_Abort(MPI_COMM_WORLD, rc);\n         exit(1);\n         }\n      T2 = MPI_Wtime();     \n\n\n      \n\n      deltaT = T2 - T1;\n      printf(\"%4d  %8.8f  %8.8f  %2.8f\\n\", n, T1, T2, deltaT);\n         sumT += deltaT;\n      }\n   avgT = (sumT*1000000)/reps;\n   printf(\"***************************************************\\n\");\n   printf(\"\\n*** Avg round trip time = %d microseconds\\n\", avgT);\n   printf(\"*** Avg one way latency = %d microseconds\\n\", avgT/2);\n   }\n\nelse if (rank == 1) {\n   printf(\"task %d has started...\\n\", rank);\n   dest = 0;\n   source = 0;\n   for (n = 1; n <= reps; n++) {\n      rc = MPI_Recv(&msg, 1, MPI_BYTE, source, tag, MPI_COMM_WORLD,\n                    &status);\n      if (rc != MPI_SUCCESS) {\n         printf(\"Receive error in task 1!\\n\");\n         MPI_Abort(MPI_COMM_WORLD, rc);\n         exit(1);\n         }\n      rc = MPI_Send(&msg, 1, MPI_BYTE, dest, tag, MPI_COMM_WORLD);\n      if (rc != MPI_SUCCESS) {\n         printf(\"Send error in task 1!\\n\");\n         MPI_Abort(MPI_COMM_WORLD, rc);\n         exit(1);\n         }\n      }\n   }\n\nMPI_Finalize();\nexit(0);\n}"}
{"program": "indiependente_1418", "code": "int main(int argc, char** argv)\n{\n\tint my_rank, p, tag = 0;\n\tint subrows, subcols;\n\tint *buffer, **submatrix, **matrix;\n\tint sqrtp;\n\tint i, j, k, count, my_sum, max_sum;\n\tMPI_Status status;\n\n\tsqrtp = (int)sqrt(p);\n\n\tif (N % (sqrtp) != 0)\n\t{\n\t\treturn 1;\n\t}\n\n\tsubrows = N / sqrtp;\n\tsubcols = N / sqrtp;\n\tbuffer = calloc(subrows*subcols, sizeof(int));\n\n\tif (my_rank == MASTER)\n\t{\n\t\tmatrix = allocate2Dint(N, N);\n\t\tfill_matrix(matrix, N, N, 1);\n\t\t\n\n\t\tprint_matrix(matrix, N, N, my_rank);\n\n\n\t\tfor (i = 0; i < sqrtp; ++i)\n\t\t{\n\t\t\tfor ( j = 0; j < sqrtp; ++j)\n\t\t\t{\n\t\t\t\tif (i!=0 || j!=0)\n\t\t\t\t{\n\t\t\t\t\tcount = 0;\n\t\t\t\t\tfor (k = 0; k < subrows; ++k)\n\t\t\t\t\t{\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tprint_matrix(matrix, subrows, subcols, my_rank);\n\t\tmy_sum = matrix_sum(matrix, subrows, subcols);\n\t}\n\telse\n\t{\n\t\tcount = 0;\n\t\tfor (i = 0; i < subrows; ++i)\n\t\t{\n\t\t}\n\t\tprint_matrix(submatrix, subrows, subcols, my_rank);\n\t\tmy_sum = matrix_sum(submatrix, subrows, subcols);\n\t}\n\n\n\tprintf(\"%d: local sum = %d\\n\", my_rank, my_sum);\n\n\tif (my_rank == MASTER)\n\t{\n\t\tprintf(\"Global max sum = %d\\n\", max_sum);\n\t}\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv)\n{\n\tint my_rank, p, tag = 0;\n\tint subrows, subcols;\n\tint *buffer, **submatrix, **matrix;\n\tint sqrtp;\n\tint i, j, k, count, my_sum, max_sum;\n\tMPI_Status status;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\tsqrtp = (int)sqrt(p);\n\n\tif (N % (sqrtp) != 0)\n\t{\n\t\tMPI_Finalize();\n\t\treturn 1;\n\t}\n\n\tsubrows = N / sqrtp;\n\tsubcols = N / sqrtp;\n\tbuffer = calloc(subrows*subcols, sizeof(int));\n\n\tif (my_rank == MASTER)\n\t{\n\t\tmatrix = allocate2Dint(N, N);\n\t\tfill_matrix(matrix, N, N, 1);\n\t\t\n\n\t\tprint_matrix(matrix, N, N, my_rank);\n\n\n\t\tfor (i = 0; i < sqrtp; ++i)\n\t\t{\n\t\t\tfor ( j = 0; j < sqrtp; ++j)\n\t\t\t{\n\t\t\t\tif (i!=0 || j!=0)\n\t\t\t\t{\n\t\t\t\t\tcount = 0;\n\t\t\t\t\tfor (k = 0; k < subrows; ++k)\n\t\t\t\t\t{\n\t\t\t\t\t\tMPI_Pack(&matrix[i * subrows + k][j * subcols], subcols, MPI_INT,\n\t\t\t\t\t\t\t\tbuffer, subrows * subcols * sizeof(int), &count, MPI_COMM_WORLD);\n\t\t\t\t\t}\n\t\t\t\t\tMPI_Send(buffer, count, MPI_PACKED, i*sqrtp + j, tag, MPI_COMM_WORLD);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tprint_matrix(matrix, subrows, subcols, my_rank);\n\t\tmy_sum = matrix_sum(matrix, subrows, subcols);\n\t}\n\telse\n\t{\n\t\tMPI_Recv(buffer, subrows * subcols * sizeof(int), MPI_PACKED, MASTER, tag, MPI_COMM_WORLD, &status);\n\t\tcount = 0;\n\t\tfor (i = 0; i < subrows; ++i)\n\t\t{\n\t\t\tMPI_Unpack(buffer, subrows * subcols * sizeof(int),\n\t\t\t\t&count, &submatrix[i][0], subcols, MPI_INT, MPI_COMM_WORLD);\n\t\t}\n\t\tprint_matrix(submatrix, subrows, subcols, my_rank);\n\t\tmy_sum = matrix_sum(submatrix, subrows, subcols);\n\t}\n\n\n\tprintf(\"%d: local sum = %d\\n\", my_rank, my_sum);\n\tMPI_Reduce(&my_sum, &max_sum, 1, MPI_INT, MPI_MAX, MASTER, MPI_COMM_WORLD);\n\n\tif (my_rank == MASTER)\n\t{\n\t\tprintf(\"Global max sum = %d\\n\", max_sum);\n\t}\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "gyaikhom_1419", "code": "int main(int argc, char *argv[])\n{\n\tint i;\n\tiskel_pipe_t *pipe; \t\n\n\n\t\n\n\tiskel_pipe_dmap_t dmap[]\n\t\t= {{bc_float, bc_float},\n\t\t   {bc_float, bc_float},\n\t\t   {bc_float, bc_float},\n\t\t   {bc_float, bc_float},\n\t\t   {bc_float, bc_float}};\n\n\t\n\n\tiskel_pipe_fptr_t func[] = {gaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage};\n\n\t\n\n\tbc_plist_t *plist;\n\n  \tbc_init(BC_ERR);\n\n\tif (bc_size != dim) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: %d processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c %d gauss_pipeline\\n\\n\",\n\t\t\t\t   dim, dim);\n\t\tbc_final();\n\t\treturn -1;\n\t}\n\n\t\n\n \tplist = bc_plist_create(NUM_STAGES, PROCESSES);\n\n\t\n\n\tpipe = iskel_pipe_create(plist, func, dmap, 1);\n\n\t\n\n \tiskel_pipe_exec(pipe, NULL, NULL);\n\n\t\n\n \tiskel_pipe_destroy(pipe);\n\n\t\n\n\tbc_plist_destroy(plist);\n\n \tfor (i = 0; i < dim; i++)\n \t\tprintf(\"U[%d, %d] = %8.5f\\n\", bc_rank, i, A[bc_rank][i]);\n\tprintf(\"y[%d] = %8.5f\\n\", bc_rank, A[bc_rank][y]);\n\n  \tbc_final();\n\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n\tint i;\n\tiskel_pipe_t *pipe; \t\n\n\n\t\n\n\tiskel_pipe_dmap_t dmap[]\n\t\t= {{bc_float, bc_float},\n\t\t   {bc_float, bc_float},\n\t\t   {bc_float, bc_float},\n\t\t   {bc_float, bc_float},\n\t\t   {bc_float, bc_float}};\n\n\t\n\n\tiskel_pipe_fptr_t func[] = {gaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage,\n\t\t\t\t\t\t\t\tgaussian_mcpy_stage};\n\n\t\n\n\tbc_plist_t *plist;\n\n  \tMPI_Init(&argc, &argv);\n  \tbc_init(BC_ERR);\n\n\tif (bc_size != dim) {\n\t\tif (bc_rank == 0)\n\t\t\tprintf(\"ERROR: %d processes are required.\\n\"\n\t\t\t\t   \"\\n\\tUSAGE: mpirun -c %d gauss_pipeline\\n\\n\",\n\t\t\t\t   dim, dim);\n\t\tbc_final();\n\t\tMPI_Finalize();\n\t\treturn -1;\n\t}\n\n\t\n\n \tplist = bc_plist_create(NUM_STAGES, PROCESSES);\n\n\t\n\n\tpipe = iskel_pipe_create(plist, func, dmap, 1);\n\n\t\n\n \tiskel_pipe_exec(pipe, NULL, NULL);\n\n\t\n\n \tiskel_pipe_destroy(pipe);\n\n\t\n\n\tbc_plist_destroy(plist);\n\n \tfor (i = 0; i < dim; i++)\n \t\tprintf(\"U[%d, %d] = %8.5f\\n\", bc_rank, i, A[bc_rank][i]);\n\tprintf(\"y[%d] = %8.5f\\n\", bc_rank, A[bc_rank][y]);\n\n  \tbc_final();\n  \tMPI_Finalize();\n\n    return 0;\n}"}
{"program": "viratupadhyay_1420", "code": "int main(int argc, char ** argv)\n{\n    fftw_complex *z;\n    fftw_plan plan;\n    gsl_rng *r;\n    const  gsl_rng_type *T;\n    struct parms parms;\n    double *phi, avg, ran, std;\n    double *philocal;\n    int Nx, Ny, Nz, size, rank, xnn;\n    int i, j, n1, n2, irank, i0, j0;\n    char filename[64];\n    FILE *file;\n    void   initrng(void);\n    double uniform(void);\n    MPI_Status status;\n\n    Nx = atoi(argv[1]);\n    Ny = atoi(argv[2]);\n    Nz = atoi(argv[3]);\n    avg = atof(argv[4]);\n    std = atof(argv[5]);\n    parms.Nx = Nx;  parms.Ny = Ny;  parms.Nz = Nz;\n    \n\n    if (rank == 0){\n        \n\n        gsl_rng_env_setup();\n        T = gsl_rng_default;\n        r = gsl_rng_alloc (T);\n        printf (\"r is a '%s' generator\\n\", gsl_rng_name (r));\n        for (i=0; i < 1000000; i++)\n            ran = avg + gsl_ran_gaussian(r,std); \n        xnn = Nx*Ny*Nz;\n        phi = (double *) calloc(Nx*Ny*Nz, sizeof(double));\n        for (i = 0; i < Nx*Ny*Nz; i++){\n            ran = avg + gsl_ran_gaussian(r,std);\n            phi[i] = (ran);\n        }\n    }\n\n\n    parms.Nx = Nx;  parms.Ny = Ny;    parms.Nz = Nz;\n    parms.rank = rank-1; parms.size = size-1;        \n\n    parms = mapping(parms);\n    philocal = (double *) calloc(parms.nx*parms.ny, sizeof(double));\n    \n    if (rank == 0){\n        for (irank = 0; irank < parms.size; irank++){\n            parms.rank = irank;\n            parms = mapping(parms);\n            for (i = 0; i < parms.nx; i++)\n            for (j = 0; j < parms.ny; j++){\n                i0 = parms.x0 + i;        \n\n                j0 = parms.y0 + j;\n                n1 = i*parms.ny + j;        \n\n                n2 = i0*parms.Ny + j0;        \n\n                philocal[n1] = phi[n2];\n            }\n            n1 = parms.nx*parms.ny;\n        }\n    }\n    else{\n        n1 = parms.nx*parms.ny;        \n\n\n        sprintf(filename, \"%s.%02d\", \"phi.dat\", parms.rank);\n        file = fopen(filename, \"w\");\n        fprintf(file, \"%5d %5d %5d %5d %5d    0    0.0\\n\", parms.Nx, parms.Ny, parms.nx, parms.ny, parms.Nz);\n        for (i = 0; i < parms.nx; i++)\n        for (j = 0; j < parms.ny; j++){\n            fprintf(file, \" % .3e\", philocal[i*parms.ny+j]);\n            if ((i*parms.ny+j+1)%10 == 0) fprintf(file, \"\\n\");\n        }\n    fclose(file);\n    }\n    return 0;\n}", "label": "int main(int argc, char ** argv)\n{\n    fftw_complex *z;\n    fftw_plan plan;\n    gsl_rng *r;\n    const  gsl_rng_type *T;\n    struct parms parms;\n    double *phi, avg, ran, std;\n    double *philocal;\n    int Nx, Ny, Nz, size, rank, xnn;\n    int i, j, n1, n2, irank, i0, j0;\n    char filename[64];\n    FILE *file;\n    void   initrng(void);\n    double uniform(void);\n    MPI_Status status;\n\n    Nx = atoi(argv[1]);\n    Ny = atoi(argv[2]);\n    Nz = atoi(argv[3]);\n    avg = atof(argv[4]);\n    std = atof(argv[5]);\n    parms.Nx = Nx;  parms.Ny = Ny;  parms.Nz = Nz;\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size (MPI_COMM_WORLD, &size);\n    if (rank == 0){\n        \n\n        gsl_rng_env_setup();\n        T = gsl_rng_default;\n        r = gsl_rng_alloc (T);\n        printf (\"r is a '%s' generator\\n\", gsl_rng_name (r));\n        for (i=0; i < 1000000; i++)\n            ran = avg + gsl_ran_gaussian(r,std); \n        xnn = Nx*Ny*Nz;\n        phi = (double *) calloc(Nx*Ny*Nz, sizeof(double));\n        for (i = 0; i < Nx*Ny*Nz; i++){\n            ran = avg + gsl_ran_gaussian(r,std);\n            phi[i] = (ran);\n        }\n    }\n\n\n    parms.Nx = Nx;  parms.Ny = Ny;    parms.Nz = Nz;\n    parms.rank = rank-1; parms.size = size-1;        \n\n    parms = mapping(parms);\n    philocal = (double *) calloc(parms.nx*parms.ny, sizeof(double));\n    \n    if (rank == 0){\n        for (irank = 0; irank < parms.size; irank++){\n            parms.rank = irank;\n            parms = mapping(parms);\n            for (i = 0; i < parms.nx; i++)\n            for (j = 0; j < parms.ny; j++){\n                i0 = parms.x0 + i;        \n\n                j0 = parms.y0 + j;\n                n1 = i*parms.ny + j;        \n\n                n2 = i0*parms.Ny + j0;        \n\n                philocal[n1] = phi[n2];\n            }\n            n1 = parms.nx*parms.ny;\n            MPI_Send(philocal, n1, MPI_DOUBLE, irank+1, 0, MPI_COMM_WORLD);\n        }\n    }\n    else{\n        n1 = parms.nx*parms.ny;        \n\n        MPI_Recv(philocal, n1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n\n        sprintf(filename, \"%s.%02d\", \"phi.dat\", parms.rank);\n        file = fopen(filename, \"w\");\n        fprintf(file, \"%5d %5d %5d %5d %5d    0    0.0\\n\", parms.Nx, parms.Ny, parms.nx, parms.ny, parms.Nz);\n        for (i = 0; i < parms.nx; i++)\n        for (j = 0; j < parms.ny; j++){\n            fprintf(file, \" % .3e\", philocal[i*parms.ny+j]);\n            if ((i*parms.ny+j+1)%10 == 0) fprintf(file, \"\\n\");\n        }\n    fclose(file);\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "enesates_1423", "code": "int main( int argc, char* argv[] )\n{\t\n\tint\t\t\tmy_rank;\t\t\n\n    int\t\t\tp;\t\t\t\t\n\n    int\t\t\tsum = 0;\t\t\n\n    int \t\ti;\t\t\t\t\n\n\tMPI_Status\tstatus;\t\t\t\n\n    int err;\n\n\t\n\n    err =\n\n\t\n\n\n\t\n\n\t\n\t\n\tint total[p]; \n\n\tint ARRAY_SIZE = p * k; \n\n\tint\tnumbers[ARRAY_SIZE]; \n\n\t\n\t\n\n\trandomNumbers(numbers, ARRAY_SIZE);\n\t\n\t\n\n    \n    \n\n    sum = subTotal(numbers);\n    \n    printf(\"I am %d. process, partial sum = %d\\n\", my_rank, sum);       \n\t\n\t\n\n\t\n\tif(my_rank == 0) {\n\t\t\n\n\t\t\n\t\t\n\n\t\tsum = 0;\n\t\tfor(i=0;i<p;i++)\n\t\t\tsum += total[i];\n\t\t\n\t\tprintf(\"Total : %d\\n\",sum);\t\n\t\t\n\t\tprintf(\"Numbers: [ \");\n\t\tfor(i=0; i<ARRAY_SIZE; i++)\t\n\t\t\tprintf(\"%d, \",numbers[i]);\n\t\tprintf(\"]\\n\");\n\t} \n\n\t\n\n    err =\n    return 0;\n}", "label": "int main( int argc, char* argv[] )\n{\t\n\tint\t\t\tmy_rank;\t\t\n\n    int\t\t\tp;\t\t\t\t\n\n    int\t\t\tsum = 0;\t\t\n\n    int \t\ti;\t\t\t\t\n\n\tMPI_Status\tstatus;\t\t\t\n\n    int err;\n\n\t\n\n    err = MPI_Init( &argc, &argv );\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &p);\n\t\n\t\n\tint total[p]; \n\n\tint ARRAY_SIZE = p * k; \n\n\tint\tnumbers[ARRAY_SIZE]; \n\n\t\n\t\n\n\trandomNumbers(numbers, ARRAY_SIZE);\n\t\n\t\n\n\tMPI_Scatter (numbers, k, MPI_INT, numbers,\n              ARRAY_SIZE, MPI_INT, 0, MPI_COMM_WORLD); \n    \n    \n\n    sum = subTotal(numbers);\n    \n    printf(\"I am %d. process, partial sum = %d\\n\", my_rank, sum);       \n\t\n\t\n\n\tMPI_Gather (&sum, 1, MPI_INT, total,\n\t  1, MPI_INT, 0, MPI_COMM_WORLD); \n\t\n\tif(my_rank == 0) {\n\t\t\n\n\t\t\n\t\t\n\n\t\tsum = 0;\n\t\tfor(i=0;i<p;i++)\n\t\t\tsum += total[i];\n\t\t\n\t\tprintf(\"Total : %d\\n\",sum);\t\n\t\t\n\t\tprintf(\"Numbers: [ \");\n\t\tfor(i=0; i<ARRAY_SIZE; i++)\t\n\t\t\tprintf(\"%d, \",numbers[i]);\n\t\tprintf(\"]\\n\");\n\t} \n\n\t\n\n    err = MPI_Finalize();\n    return 0;\n}"}
{"program": "mpip_1425", "code": "int main(int argc, char **argv){\n  int np[3], m,  compare_direct=0, debug;\n  unsigned pnfft_flags;\n  ptrdiff_t N[3], n[3], local_M, local_N_total;\n  double f_hat_sum, x_max[3];\n  unsigned compute_flags;\n  pnfft_complex *f_hat1=NULL, *f_hat2=NULL;\n  \n  pnfft_init();\n  \n  \n\n  pnfft_check_init_parameters(argc, argv, N, n, &local_M, &m, &pnfft_flags, &compute_flags,\n      x_max, np, &compare_direct, &debug);\n\n  \n\n  perform_pnfft_adj_guru(N, n, local_M, m,   x_max, pnfft_flags, compute_flags,\n      np, MPI_COMM_WORLD, \"PNFFT adj\",\n      &f_hat1, &f_hat_sum, &local_N_total);\n\n  \n\n  if(compare_direct) compute_flags |= PNFFT_COMPUTE_DIRECT;\n  else               m += 2;\n\n  perform_pnfft_adj_guru(N, n, local_M, m,   x_max, pnfft_flags, compute_flags,\n      np, MPI_COMM_WORLD, \"reference method\",\n      &f_hat2, &f_hat_sum, &local_N_total);\n\n  \n\n  compare_f_hat(f_hat1, f_hat2, local_N_total, f_hat_sum, \"* Results in f_hat\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_free(f_hat1); pnfft_free(f_hat2);\n  pnfft_cleanup();\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[3], m,  compare_direct=0, debug;\n  unsigned pnfft_flags;\n  ptrdiff_t N[3], n[3], local_M, local_N_total;\n  double f_hat_sum, x_max[3];\n  unsigned compute_flags;\n  pnfft_complex *f_hat1=NULL, *f_hat2=NULL;\n  \n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  pnfft_check_init_parameters(argc, argv, N, n, &local_M, &m, &pnfft_flags, &compute_flags,\n      x_max, np, &compare_direct, &debug);\n\n  \n\n  perform_pnfft_adj_guru(N, n, local_M, m,   x_max, pnfft_flags, compute_flags,\n      np, MPI_COMM_WORLD, \"PNFFT adj\",\n      &f_hat1, &f_hat_sum, &local_N_total);\n\n  \n\n  if(compare_direct) compute_flags |= PNFFT_COMPUTE_DIRECT;\n  else               m += 2;\n\n  perform_pnfft_adj_guru(N, n, local_M, m,   x_max, pnfft_flags, compute_flags,\n      np, MPI_COMM_WORLD, \"reference method\",\n      &f_hat2, &f_hat_sum, &local_N_total);\n\n  \n\n  compare_f_hat(f_hat1, f_hat2, local_N_total, f_hat_sum, \"* Results in f_hat\", MPI_COMM_WORLD);\n\n  \n\n  pnfft_free(f_hat1); pnfft_free(f_hat2);\n  pnfft_cleanup();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bjoern-leder_1427", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,ix,mu,n;\n   double d1,d1max,d2,d2max,dmax_all;\n   double *pd,*pds,*pdm,*pf;\n   su3_dble wd,vd,vds,*plnk;\n   FILE *flog=NULL;   \n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n\n      printf(\"\\n\");\n      printf(\"Staple routines provided by lnkstaple\\n\");\n      printf(\"----------------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n      fflush(flog);\n   }\n\n   start_ranlux(0,12345);\n   geometry();\n\n   plnk=alloc_lnk();\n   message(\"Fields allocated\\n\\n\");\n \n   random_ud();\n   assign_ud2lnk(plnk);\n   assign_ud2u();\n   \n   d1max=0.0;\n   d2max=0.0;\n\n   active_links();\n   for (n=0;n<nlks;n++)\n   {\n      ix=lks[n].ix;\n      mu=lks[n].mu;\n      \n      staples(ix,mu,&wd);\n      \n      my_staples(plnk,ix,mu,&vd);\n      \n      pf=(double*)&wd;\n      pd=(double*)&vd;\n      pdm=pd+18;\n      d1=0.0;\n      for (;pd<pdm;pd++)\n      {\n         d1+=fabs((*pf)-*pd);\n         pf+=1;\n      }\n            \n      if (d1>d1max)\n         d1max=d1;\n         \n      my_staples_save(plnk,ix,mu,&vds);\n      \n      pds=(double*)&vds;\n      pd=(double*)&vd;\n      d2=0.0;\n      for (;pd<pdm;pd++)\n      {\n         d2+=fabs(*pds-*pd);\n         pds+=1;\n      }\n            \n      if (d2>d2max)\n         d2max=d2;\n   }\n\n \n   if (my_rank==0)\n   {   \n      printf(\"Fast version (assuming all matrices are SU(3))\\n\");\n      printf(\"Maximal absolute deviation between staples and lnkstaple: %.1e\\n\",dmax_all);\n      printf(\"(Should be around 1.0e-6)\\n\\n\");\n   }   \n\n \n   if (my_rank==0)\n   {   \n      printf(\"Save version (no assumptions)\\n\");\n      printf(\"Maximal absolute deviation between lnkstaple and lnkstaple (save): %.1e\\n\",dmax_all);\n      printf(\"(Should be around 1.0e-14)\\n\\n\");\n      fclose(flog);\n   }   \n\n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,ix,mu,n;\n   double d1,d1max,d2,d2max,dmax_all;\n   double *pd,*pds,*pdm,*pf;\n   su3_dble wd,vd,vds,*plnk;\n   FILE *flog=NULL;   \n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check2.log\",\"w\",stdout);\n\n      printf(\"\\n\");\n      printf(\"Staple routines provided by lnkstaple\\n\");\n      printf(\"----------------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n      fflush(flog);\n   }\n\n   start_ranlux(0,12345);\n   geometry();\n\n   plnk=alloc_lnk();\n   message(\"Fields allocated\\n\\n\");\n \n   random_ud();\n   assign_ud2lnk(plnk);\n   assign_ud2u();\n   \n   d1max=0.0;\n   d2max=0.0;\n\n   active_links();\n   for (n=0;n<nlks;n++)\n   {\n      ix=lks[n].ix;\n      mu=lks[n].mu;\n      \n      staples(ix,mu,&wd);\n      \n      my_staples(plnk,ix,mu,&vd);\n      \n      pf=(double*)&wd;\n      pd=(double*)&vd;\n      pdm=pd+18;\n      d1=0.0;\n      for (;pd<pdm;pd++)\n      {\n         d1+=fabs((*pf)-*pd);\n         pf+=1;\n      }\n            \n      if (d1>d1max)\n         d1max=d1;\n         \n      my_staples_save(plnk,ix,mu,&vds);\n      \n      pds=(double*)&vds;\n      pd=(double*)&vd;\n      d2=0.0;\n      for (;pd<pdm;pd++)\n      {\n         d2+=fabs(*pds-*pd);\n         pds+=1;\n      }\n            \n      if (d2>d2max)\n         d2max=d2;\n   }\n\n   MPI_Reduce(&d1max,&dmax_all,1,MPI_DOUBLE,MPI_MAX,0,MPI_COMM_WORLD);\n \n   if (my_rank==0)\n   {   \n      printf(\"Fast version (assuming all matrices are SU(3))\\n\");\n      printf(\"Maximal absolute deviation between staples and lnkstaple: %.1e\\n\",dmax_all);\n      printf(\"(Should be around 1.0e-6)\\n\\n\");\n   }   \n\n   MPI_Reduce(&d2max,&dmax_all,1,MPI_DOUBLE,MPI_MAX,0,MPI_COMM_WORLD);\n \n   if (my_rank==0)\n   {   \n      printf(\"Save version (no assumptions)\\n\");\n      printf(\"Maximal absolute deviation between lnkstaple and lnkstaple (save): %.1e\\n\",dmax_all);\n      printf(\"(Should be around 1.0e-14)\\n\\n\");\n      fclose(flog);\n   }   \n\n   MPI_Finalize();    \n   exit(0);\n}"}
{"program": "qingu_1428", "code": "int main( int argc, char *argv[] )\n{\n    int myrank, size, src=0, dest=1;\n    int buf[20];\n\n\n    if (myrank == src || myrank == dest) {\n\tint partner = src;\n\tif (myrank == partner) partner = dest;\n    }\n    \n    if (myrank == src) {\n    }\n    else if (myrank == dest) {\n    }\n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int myrank, size, src=0, dest=1;\n    int buf[20];\n\n    MPI_Init( 0, 0 );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n    MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n\n    if (myrank == src || myrank == dest) {\n\tint partner = src;\n\tif (myrank == partner) partner = dest;\n        MPI_Sendrecv( MPI_BOTTOM, 0, MPI_INT, partner, 0, \n  \t    \t      MPI_BOTTOM, 0, MPI_INT, partner,  0, MPI_COMM_WORLD, \n\t\t      MPI_STATUS_IGNORE );\n    }\n    \n    if (myrank == src) {\n\tMPI_Send( buf, 20, MPI_INT, dest, 0, MPI_COMM_WORLD );\n    }\n    else if (myrank == dest) {\n\tMPI_Recv( buf, 20, MPI_INT, src, 0, MPI_COMM_WORLD, \n\t\t  MPI_STATUS_IGNORE );\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "ngcurrier_1429", "code": "int main(int argc, char **argv)\n{\n    int mpi_size, mpi_rank;                \n\n\n#ifndef H5_HAVE_WIN32_API\n    \n\n    HDsetbuf(stderr, NULL);\n    HDsetbuf(stdout, NULL);\n#endif\n\n\n    dim0 = ROW_FACTOR*mpi_size;\n    dim1 = COL_FACTOR*mpi_size;\n\n    if (MAINPROCESS){\n    printf(\"===================================\\n\");\n    printf(\"Shape Same Tests Start\\n\");\n        printf(\"    express_test = %d.\\n\", GetTestExpress());\n    printf(\"===================================\\n\");\n    }\n\n    \n\n    if (H5dont_atexit() < 0){\n    printf(\"%d: Failed to turn off atexit processing. Continue.\\n\", mpi_rank);\n    };\n    H5open();\n    h5_show_hostname();\n\n    \n\n    TestInit(argv[0], usage, parse_options);\n\n    \n\n    AddTest(\"sscontig1\", sscontig1, NULL,\n    \"Cntg hslab, ind IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sscontig2\", sscontig2, NULL,\n    \"Cntg hslab, col IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sscontig3\", sscontig3, NULL,\n    \"Cntg hslab, ind IO, chnk dsets\", PARATESTFILE);\n    AddTest(\"sscontig4\", sscontig4, NULL,\n    \"Cntg hslab, col IO, chnk dsets\", PARATESTFILE);\n\n    \n\n    AddTest(\"sschecker1\", sschecker1, NULL,\n    \"Check hslab, ind IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sschecker2\", sschecker2, NULL,\n    \"Check hslab, col IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sschecker3\", sschecker3, NULL,\n    \"Check hslab, ind IO, chnk dsets\", PARATESTFILE);\n    AddTest(\"sschecker4\", sschecker4, NULL,\n    \"Check hslab, col IO, chnk dsets\", PARATESTFILE);\n\n    \n\n    TestInfo(argv[0]);\n\n    \n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n\n    \n\n    TestParseCmdLine(argc, argv);\n\n    if (dxfer_coll_type == DXFER_INDEPENDENT_IO && MAINPROCESS){\n    printf(\"===================================\\n\"\n        \"   Using Independent I/O with file set view to replace collective I/O \\n\"\n        \"===================================\\n\");\n    }\n\n\n    \n\n    PerformTests();\n\n    \n\n\n    \n\n    if (MAINPROCESS && GetTestSummary())\n        TestSummary();\n\n    \n\n    h5_clean_files(FILENAME, fapl);\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    {\n        int temp;\n    nerrors=temp;\n    }\n\n    if (MAINPROCESS){        \n\n    printf(\"===================================\\n\");\n    if (nerrors)\n        printf(\"***Shape Same tests detected %d errors***\\n\", nerrors);\n    else\n        printf(\"Shape Same tests finished with no errors\\n\");\n    printf(\"===================================\\n\");\n    }\n\n    \n\n    H5close();\n\n    \n\n    TestShutdown();\n\n\n    \n\n    return(nerrors!=0);\n}", "label": "int main(int argc, char **argv)\n{\n    int mpi_size, mpi_rank;                \n\n\n#ifndef H5_HAVE_WIN32_API\n    \n\n    HDsetbuf(stderr, NULL);\n    HDsetbuf(stdout, NULL);\n#endif\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    dim0 = ROW_FACTOR*mpi_size;\n    dim1 = COL_FACTOR*mpi_size;\n\n    if (MAINPROCESS){\n    printf(\"===================================\\n\");\n    printf(\"Shape Same Tests Start\\n\");\n        printf(\"    express_test = %d.\\n\", GetTestExpress());\n    printf(\"===================================\\n\");\n    }\n\n    \n\n    if (H5dont_atexit() < 0){\n    printf(\"%d: Failed to turn off atexit processing. Continue.\\n\", mpi_rank);\n    };\n    H5open();\n    h5_show_hostname();\n\n    \n\n    TestInit(argv[0], usage, parse_options);\n\n    \n\n    AddTest(\"sscontig1\", sscontig1, NULL,\n    \"Cntg hslab, ind IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sscontig2\", sscontig2, NULL,\n    \"Cntg hslab, col IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sscontig3\", sscontig3, NULL,\n    \"Cntg hslab, ind IO, chnk dsets\", PARATESTFILE);\n    AddTest(\"sscontig4\", sscontig4, NULL,\n    \"Cntg hslab, col IO, chnk dsets\", PARATESTFILE);\n\n    \n\n    AddTest(\"sschecker1\", sschecker1, NULL,\n    \"Check hslab, ind IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sschecker2\", sschecker2, NULL,\n    \"Check hslab, col IO, cntg dsets\", PARATESTFILE);\n    AddTest(\"sschecker3\", sschecker3, NULL,\n    \"Check hslab, ind IO, chnk dsets\", PARATESTFILE);\n    AddTest(\"sschecker4\", sschecker4, NULL,\n    \"Check hslab, col IO, chnk dsets\", PARATESTFILE);\n\n    \n\n    TestInfo(argv[0]);\n\n    \n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n\n    \n\n    TestParseCmdLine(argc, argv);\n\n    if (dxfer_coll_type == DXFER_INDEPENDENT_IO && MAINPROCESS){\n    printf(\"===================================\\n\"\n        \"   Using Independent I/O with file set view to replace collective I/O \\n\"\n        \"===================================\\n\");\n    }\n\n\n    \n\n    PerformTests();\n\n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    if (MAINPROCESS && GetTestSummary())\n        TestSummary();\n\n    \n\n    h5_clean_files(FILENAME, fapl);\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    {\n        int temp;\n        MPI_Allreduce(&nerrors, &temp, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n    nerrors=temp;\n    }\n\n    if (MAINPROCESS){        \n\n    printf(\"===================================\\n\");\n    if (nerrors)\n        printf(\"***Shape Same tests detected %d errors***\\n\", nerrors);\n    else\n        printf(\"Shape Same tests finished with no errors\\n\");\n    printf(\"===================================\\n\");\n    }\n\n    \n\n    H5close();\n\n    \n\n    TestShutdown();\n\n    MPI_Finalize();\n\n    \n\n    return(nerrors!=0);\n}"}
{"program": "jeffhammond_1431", "code": "int main(int argc, char * argv[])\n{\n\n    int rank, size;\n\n    int *   shptr = NULL;\n    MPI_Win shwin;\n\n    \n\n    MPI_Aint rsize = 0;\n    int rdisp;\n    int * rptr = NULL;\n    int lint = -999;\n    if (rptr==NULL || rsize!=sizeof(int)) {\n        printf(\"rptr=%p rsize=%zu \\n\", rptr, (size_t)rsize);\n    }\n\n    \n\n\n    if (rank==0) {\n        *shptr = 42; \n\n    }\n    for (int j=1; j<size; j++) {\n        p2p_xsync(0, j, MPI_COMM_WORLD);\n    }\n    lint = *rptr;\n\n    \n\n\n    if (1==coll_check_equal(lint,MPI_COMM_WORLD)) {\n        if (rank==0) {\n            printf(\"SUCCESS!\\n\");\n        }\n    } else {\n        printf(\"rank %d: lint = %d \\n\", rank, lint);\n    }\n\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int *   shptr = NULL;\n    MPI_Win shwin;\n    MPI_Win_allocate_shared(rank==0 ? sizeof(int) : 0,sizeof(int),\n                            MPI_INFO_NULL, MPI_COMM_WORLD,\n                            &shptr, &shwin);\n\n    \n\n    MPI_Aint rsize = 0;\n    int rdisp;\n    int * rptr = NULL;\n    int lint = -999;\n    MPI_Win_shared_query(shwin, 0, &rsize, &rdisp, &rptr);\n    if (rptr==NULL || rsize!=sizeof(int)) {\n        printf(\"rptr=%p rsize=%zu \\n\", rptr, (size_t)rsize);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    \n\n\n    if (rank==0) {\n        MPI_Win_lock(MPI_LOCK_SHARED, 0, 0, shwin);\n        *shptr = 42; \n\n        MPI_Win_unlock(0, shwin);\n    }\n    for (int j=1; j<size; j++) {\n        p2p_xsync(0, j, MPI_COMM_WORLD);\n    }\n    MPI_Win_lock(MPI_LOCK_SHARED, 0, 0, shwin);\n    lint = *rptr;\n    MPI_Win_unlock(0, shwin);\n\n    \n\n\n    if (1==coll_check_equal(lint,MPI_COMM_WORLD)) {\n        if (rank==0) {\n            printf(\"SUCCESS!\\n\");\n        }\n    } else {\n        printf(\"rank %d: lint = %d \\n\", rank, lint);\n    }\n\n    MPI_Win_free(&shwin);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "aristotle-tek_1432", "code": "int main(int argc, char** argv) {\n   const int PING_PONG_LIMIT = 20;\n\n   \n\n   \n\n   int world_rank;\n   int world_size;\n\n   \n\n   if (world_size != 2) {\n      fprintf(stderr, \"World size must be two for %s\\n\", argv[0]);\n   }\n\n   int ping_pong_count = 0;\n   int partner_rank = (world_rank + 1) % 2;\n   while (ping_pong_count < PING_PONG_LIMIT) {\n      if (world_rank == ping_pong_count % 2) {\n         \n\n         ping_pong_count++;\n         printf(\"%d sent and incremented ping_pong_count %d to %d\\n\",\n                world_rank, ping_pong_count, partner_rank);\n      } else {\n         printf(\"%d received ping_pong_count %d from %d\\n\",\n                world_rank, ping_pong_count, partner_rank);\n      }\n   }\n}", "label": "int main(int argc, char** argv) {\n   const int PING_PONG_LIMIT = 20;\n\n   \n\n   MPI_Init(NULL, NULL);\n   \n\n   int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n   \n\n   if (world_size != 2) {\n      fprintf(stderr, \"World size must be two for %s\\n\", argv[0]);\n      MPI_Abort(MPI_COMM_WORLD, 1);\n   }\n\n   int ping_pong_count = 0;\n   int partner_rank = (world_rank + 1) % 2;\n   while (ping_pong_count < PING_PONG_LIMIT) {\n      if (world_rank == ping_pong_count % 2) {\n         \n\n         ping_pong_count++;\n         MPI_Send(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n         printf(\"%d sent and incremented ping_pong_count %d to %d\\n\",\n                world_rank, ping_pong_count, partner_rank);\n      } else {\n         MPI_Recv(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD,\n                  MPI_STATUS_IGNORE);\n         printf(\"%d received ping_pong_count %d from %d\\n\",\n                world_rank, ping_pong_count, partner_rank);\n      }\n   }\n   MPI_Finalize();\n}"}
{"program": "syftalent_1433", "code": "int \nmain( int argc, char *argv[] ) {\n\tint nproc = 1, rank = 0;\n    char *target = NULL;\n    int c;\n    MPI_Info info;\n    int mpi_ret;\n    int corrupt_blocks = 0;\n\n\n    if( (mpi_ret =\n    }\n\n    prog = strdup( argv[0] );\n\n    while( ( c = getopt( argc, argv, \"df:h\" ) ) != EOF ) {\n        switch( c ) {\n            case 'd':\n                debug = 1;\n                break;\n            case 'f':\n                target = strdup( optarg );\n                break;\n            case 'h':\n                set_hints( &info );\n                break;\n            default:\n                Usage( __LINE__ );\n        }\n    }\n    if ( ! target ) {\n        Usage( __LINE__ );\n    }\n\n    write_file( target, rank, &info );\n    read_file(  target, rank, &info, &corrupt_blocks );\n\n    corrupt_blocks = reduce_corruptions( corrupt_blocks );\n    if ( rank == 0 ) {\n\tif (corrupt_blocks == 0) {\n\t    fprintf(stdout, \" No Errors\\n\");\n\t} else {\n            fprintf(stdout, \"%d/%d blocks corrupt\\n\",\n                corrupt_blocks, nproc * NUM_OBJS );\n\t}\n    }\n\n    free(prog);\n    exit( 0 );\n}", "label": "int \nmain( int argc, char *argv[] ) {\n\tint nproc = 1, rank = 0;\n    char *target = NULL;\n    int c;\n    MPI_Info info;\n    int mpi_ret;\n    int corrupt_blocks = 0;\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if( (mpi_ret = MPI_Info_create(&info)) != MPI_SUCCESS) {\n        if(rank == 0) fatal_error( mpi_ret, NULL, \"MPI_info_create.\\n\");\n    }\n\n    prog = strdup( argv[0] );\n\n    while( ( c = getopt( argc, argv, \"df:h\" ) ) != EOF ) {\n        switch( c ) {\n            case 'd':\n                debug = 1;\n                break;\n            case 'f':\n                target = strdup( optarg );\n                break;\n            case 'h':\n                set_hints( &info );\n                break;\n            default:\n                Usage( __LINE__ );\n        }\n    }\n    if ( ! target ) {\n        Usage( __LINE__ );\n    }\n\n    write_file( target, rank, &info );\n    read_file(  target, rank, &info, &corrupt_blocks );\n\n    corrupt_blocks = reduce_corruptions( corrupt_blocks );\n    if ( rank == 0 ) {\n\tif (corrupt_blocks == 0) {\n\t    fprintf(stdout, \" No Errors\\n\");\n\t} else {\n            fprintf(stdout, \"%d/%d blocks corrupt\\n\",\n                corrupt_blocks, nproc * NUM_OBJS );\n\t}\n    }\n    MPI_Info_free(&info);\n\n    MPI_Finalize();\n    free(prog);\n    exit( 0 );\n}"}
{"program": "rpereira-dev_1436", "code": "int main(int argc, char **argv)\n{\n    int rank, nproc, tag, nvals_per_proc;\n\n\n\n    if (nproc == 1)\n    {\n        printf(\"Il faut au moins 2 processus MPI\\n\"); fflush(stdout);\n    }\n\n    tag = 1000;\n    nvals_per_proc = 100000;\n    if (rank == 0)\n    {\n        int isnd;\n        int dst_odd = 1;\n        int n_odd   = nproc / 2;\n        MPI_Request req;\n        int tab_vals[n_odd*nvals_per_proc];\n\n        for(isnd = 0 ; isnd < n_odd ; isnd++)\n        {\n            int *isnd_arr = tab_vals+isnd*nvals_per_proc;\n            fill_val_array(dst_odd, isnd_arr, nvals_per_proc);\n\n            \n\n            printf(\"P0 initiates send values [%d, %d] to process P%d\\n\", isnd_arr[0], isnd_arr[nvals_per_proc-1], dst_odd);\n            dst_odd += 2;\n        }\n    }\n    else if (rank % 2 == 1)\n    {\n        MPI_Request req;\n        int my_val_arr[nvals_per_proc];\n\n\n        check_val_array(rank, my_val_arr, nvals_per_proc);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, nproc, tag, nvals_per_proc;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    if (nproc == 1)\n    {\n        printf(\"Il faut au moins 2 processus MPI\\n\"); fflush(stdout);\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    tag = 1000;\n    nvals_per_proc = 100000;\n    if (rank == 0)\n    {\n        int isnd;\n        int dst_odd = 1;\n        int n_odd   = nproc / 2;\n        MPI_Request req;\n        int tab_vals[n_odd*nvals_per_proc];\n\n        for(isnd = 0 ; isnd < n_odd ; isnd++)\n        {\n            int *isnd_arr = tab_vals+isnd*nvals_per_proc;\n            fill_val_array(dst_odd, isnd_arr, nvals_per_proc);\n\n            \n\n            MPI_Isend(isnd_arr, nvals_per_proc, MPI_INT, dst_odd, tag, MPI_COMM_WORLD, &req);\n            printf(\"P0 initiates send values [%d, %d] to process P%d\\n\", isnd_arr[0], isnd_arr[nvals_per_proc-1], dst_odd);\n            dst_odd += 2;\n        }\n        MPI_Wait(&req, MPI_STATUS_IGNORE);\n    }\n    else if (rank % 2 == 1)\n    {\n        MPI_Request req;\n        int my_val_arr[nvals_per_proc];\n\n        MPI_Irecv(my_val_arr, nvals_per_proc, MPI_INT, 0, tag, MPI_COMM_WORLD, &req);\n        MPI_Wait(&req, MPI_STATUS_IGNORE);\n\n        check_val_array(rank, my_val_arr, nvals_per_proc);\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "krahser_1437", "code": "int main(int argc,char*argv[]){\n double *A,*B,*C,*W,*Buffer,*results;\n int i,j,k,N;\n int check=1;\n double temp,total;\n double totaltick, communicationtick;\n\n \n\n  if (argc < 3){\n   printf(\"\\n Faltan argumentos:: N dimension de la matriz y cantidad de threads \\n\"); \n   return 0;\n  }\n   N=atoi(argv[1]);\n   int numThreads = atoi(argv[2]);\n   omp_set_num_threads(numThreads);\t\n\n\n int worker_id, n_proc, chunk, size_chunk;\n chunk=N/n_proc;\n size_chunk=chunk*N;\n\n  B=(double*)malloc(sizeof(double)*N*N);\n  Buffer=(double*)malloc(sizeof(double)*size_chunk);\n  \n  for(i=0;i<size_chunk;i++) Buffer[i]=0;\n  \n  results=(double*)malloc(sizeof(double)*3);\n\n if(worker_id == MANAGER){\n  \n\n       A=(double*)malloc(sizeof(double)*N*N);\n       C=(double*)malloc(sizeof(double)*N*N);\n       W=(double*)malloc(sizeof(double)*N*N);\n       \n\n       for(i=0;i<N;i++){\n  \tfor(j=0;j<N;j++){\n  \t\tA[i*N+j]= 1.0;\n  \t\t\n\n  \t\tB[i+N*j]= 1.0;\n \t\tC[i*N+j]= 0.0;\n  \t\tW[i*N+j]= 1.0;\n      }\n    }\n }\n else {\n       A=(double*)malloc(sizeof(double)*size_chunk);\n       W=(double*)malloc(sizeof(double)*size_chunk);\n  }\n \n\n\n totaltick = dwalltime();\n\n \n results[0]=dwalltime() - totaltick;\n \n\n#pragma omp parallel default(none) firstprivate(chunk,N,totaltick) private(i,j) shared(A,W,temp,total)\n{\n  \n\n#pragma omp for reduction(+:temp,total) schedule(static,1) nowait \n  for(i=0;i<chunk;i++){\n\tfor(j=0;j<N;j++){\n\t   temp += W[i*N+j];\n\t   total += A[i*N+j] * W[i*N+j];\n\t}\n  }\n}\n\ncommunicationtick = dwalltime();\n\nMPI_Allreduce(&temp, &temp, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\nMPI_Allreduce(&total, &total, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\nMPI_Scatter (A, size_chunk, MPI_DOUBLE, A, size_chunk, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\nMPI_Bcast(B, N*N, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n\n results[1]=dwalltime() - communicationtick;\n \ntotal/=temp;\n\n\n#pragma omp parallel default(none) firstprivate(total,chunk,N) private(i,j,k) shared(A,B,Buffer)\n{\n  #pragma omp for private(i,j,k) schedule(static,1) nowait\n  for(i=0;i<chunk;i++){\n\tfor(j=0;j<N;j++){\n\t\tfor(k=0;k<N;k++){\n\t\t\tBuffer[i*N+j]+=sqrt((pow(A[i*N+k]-total,2))*(pow(B[j*N+k]-total,2)));\n\t\t}\n\t}\n  }\n}\n\ncommunicationtick = dwalltime();\n\nMPI_Gather(Buffer, size_chunk, MPI_DOUBLE, C, size_chunk, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n\n results[2]=dwalltime() - communicationtick;\n\n totaltick = dwalltime() - totaltick;\n\n\n for(i=0;i<3;i++)\nMPI_Allreduce(results+i, results+i, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n\n  \n\n  if(worker_id == MANAGER){\n  for(i=0;i<N*N;i++)\n\tcheck= check&&(C[i]==0.0);\n  if(check){\n   printf(\"Tiempo total en segundos: %f \\n\", totaltick);\n   for(i=0,communicationtick=0;i<3;i++) communicationtick+=results[i];\n   printf(\"Tiempo de comunicacion en segundos: %f \\n\", communicationtick);\n   printf(\"Resultado correcto\\n\");\n  }else{\n   printf(\"Resultado erroneo\\n\");\n  }\n free(C);\n }\n free(A);\n free(B);\n free(Buffer);\n free(results);\n free(W);\n return(0);\n}", "label": "int main(int argc,char*argv[]){\n double *A,*B,*C,*W,*Buffer,*results;\n int i,j,k,N;\n int check=1;\n double temp,total;\n double totaltick, communicationtick;\n\n \n\n  if (argc < 3){\n   printf(\"\\n Faltan argumentos:: N dimension de la matriz y cantidad de threads \\n\"); \n   return 0;\n  }\n   N=atoi(argv[1]);\n   int numThreads = atoi(argv[2]);\n   omp_set_num_threads(numThreads);\t\n\n\n int worker_id, n_proc, chunk, size_chunk;\n MPI_Init(&argc, &argv);\n MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n MPI_Comm_rank(MPI_COMM_WORLD, &worker_id);\n chunk=N/n_proc;\n size_chunk=chunk*N;\n\n  B=(double*)malloc(sizeof(double)*N*N);\n  Buffer=(double*)malloc(sizeof(double)*size_chunk);\n  \n  for(i=0;i<size_chunk;i++) Buffer[i]=0;\n  \n  results=(double*)malloc(sizeof(double)*3);\n\n if(worker_id == MANAGER){\n  \n\n       A=(double*)malloc(sizeof(double)*N*N);\n       C=(double*)malloc(sizeof(double)*N*N);\n       W=(double*)malloc(sizeof(double)*N*N);\n       \n\n       for(i=0;i<N;i++){\n  \tfor(j=0;j<N;j++){\n  \t\tA[i*N+j]= 1.0;\n  \t\t\n\n  \t\tB[i+N*j]= 1.0;\n \t\tC[i*N+j]= 0.0;\n  \t\tW[i*N+j]= 1.0;\n      }\n    }\n }\n else {\n       A=(double*)malloc(sizeof(double)*size_chunk);\n       W=(double*)malloc(sizeof(double)*size_chunk);\n  }\n \n\n\n totaltick = dwalltime();\n\n MPI_Scatter (A, size_chunk, MPI_DOUBLE, A, size_chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n MPI_Scatter (W, size_chunk, MPI_DOUBLE, W, size_chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n \n results[0]=dwalltime() - totaltick;\n \n\n#pragma omp parallel default(none) firstprivate(chunk,N,totaltick) private(i,j) shared(A,W,temp,total)\n{\n  \n\n#pragma omp for reduction(+:temp,total) schedule(static,1) nowait \n  for(i=0;i<chunk;i++){\n\tfor(j=0;j<N;j++){\n\t   temp += W[i*N+j];\n\t   total += A[i*N+j] * W[i*N+j];\n\t}\n  }\n}\n\ncommunicationtick = dwalltime();\n\nMPI_Allreduce(&temp, &temp, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\nMPI_Allreduce(&total, &total, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\nMPI_Scatter (A, size_chunk, MPI_DOUBLE, A, size_chunk, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\nMPI_Bcast(B, N*N, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n\n results[1]=dwalltime() - communicationtick;\n \ntotal/=temp;\n\n\n#pragma omp parallel default(none) firstprivate(total,chunk,N) private(i,j,k) shared(A,B,Buffer)\n{\n  #pragma omp for private(i,j,k) schedule(static,1) nowait\n  for(i=0;i<chunk;i++){\n\tfor(j=0;j<N;j++){\n\t\tfor(k=0;k<N;k++){\n\t\t\tBuffer[i*N+j]+=sqrt((pow(A[i*N+k]-total,2))*(pow(B[j*N+k]-total,2)));\n\t\t}\n\t}\n  }\n}\n\ncommunicationtick = dwalltime();\n\nMPI_Gather(Buffer, size_chunk, MPI_DOUBLE, C, size_chunk, MPI_DOUBLE, MANAGER, MPI_COMM_WORLD);\n\n results[2]=dwalltime() - communicationtick;\n\n totaltick = dwalltime() - totaltick;\n\n\n for(i=0;i<3;i++)\nMPI_Allreduce(results+i, results+i, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n\n  \n\n  if(worker_id == MANAGER){\n  for(i=0;i<N*N;i++)\n\tcheck= check&&(C[i]==0.0);\n  if(check){\n   printf(\"Tiempo total en segundos: %f \\n\", totaltick);\n   for(i=0,communicationtick=0;i<3;i++) communicationtick+=results[i];\n   printf(\"Tiempo de comunicacion en segundos: %f \\n\", communicationtick);\n   printf(\"Resultado correcto\\n\");\n  }else{\n   printf(\"Resultado erroneo\\n\");\n  }\n free(C);\n }\n free(A);\n free(B);\n free(Buffer);\n free(results);\n free(W);\n MPI_Finalize();\n return(0);\n}"}
{"program": "tancheng_1438", "code": "int main(int argc, char *argv[])\n{   \n\tint rank;\n\tint n_ranks = 16;\n\tint n = 10;\n\tint END = 50;\n\n\n\n\n#ifdef DUMP\n\tif(rank == 0)\n\t{       m5_dump_stats(0, 0);\n\t\tm5_reset_stats(0, 0);\n\t}\n#endif\n\n\tfor(n = 0; n < END; n++)\n\t{\n\t\tif(rank == 0)\n\t\t{\n\t\t\tu8 cipher_text[16] = {0xec,0xae,0xb0,0x2a,0xf2,0x51,0x45,0x25,0xb4,0x19,0x18,0x70,0x10,0x2,0x5,0x12};\n\t\t\trijndaelDecrypt_Master(decpt_rk, cipher_text, rank);\n\t\t}\n\t\telse if(rank == n_ranks - 1)\n\t\t{\n\t\t\tu8 decipher_text[16] = {0};\n\t\t\trijndaelDecrypt_Final(decpt_rk, decipher_text, rank);\n\t\t}\n\t\telse\n\t\t\trijndaelDecrypt_Middle(decpt_rk, rank);\n\t}\n\n\t\n\n\n\n\t\n\n\treturn 0;\n}", "label": "int main(int argc, char *argv[])\n{   \n\tint rank;\n\tint n_ranks = 16;\n\tint n = 10;\n\tint END = 50;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\n#ifdef DUMP\n\tif(rank == 0)\n\t{       m5_dump_stats(0, 0);\n\t\tm5_reset_stats(0, 0);\n\t}\n#endif\n\n\tfor(n = 0; n < END; n++)\n\t{\n\t\tif(rank == 0)\n\t\t{\n\t\t\tu8 cipher_text[16] = {0xec,0xae,0xb0,0x2a,0xf2,0x51,0x45,0x25,0xb4,0x19,0x18,0x70,0x10,0x2,0x5,0x12};\n\t\t\trijndaelDecrypt_Master(decpt_rk, cipher_text, rank);\n\t\t}\n\t\telse if(rank == n_ranks - 1)\n\t\t{\n\t\t\tu8 decipher_text[16] = {0};\n\t\t\trijndaelDecrypt_Final(decpt_rk, decipher_text, rank);\n\t\t}\n\t\telse\n\t\t\trijndaelDecrypt_Middle(decpt_rk, rank);\n\t}\n\n\t\n\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\t\n\n\tMPI_Finalize();   \n\treturn 0;\n}"}
{"program": "linhbngo_1439", "code": "int main(int argc, char** argv){\n  int rank, size;\n  MPI_Status status;\n  int local_sum, tmp;\n  int i, iter;\n  int distance;\n\n\n  local_sum = rank;\n  tmp = 0;\n\n  iter = log(size) / log(2);\n  printf(\"Process %d has prefix sum %d\\n\", rank, local_sum);\n  for (i = 0; i < iter; i++){\n    distance = pow(2,i);\n\n    if (rank == 0){\n      printf(\"iter %d and distance %d\\n\", i, distance);\n    }      \n    if (rank < (size - distance)){\n      printf(\"%d send to %d value %d\\n\",rank, rank+distance, local_sum);\n\n    }\n    if (rank >= distance){\n      printf(\"%d receive from %d \\n\", rank, rank - distance);\n      printf(\"%d receive from %d value %d\\n\",rank, rank-distance, tmp);\n      local_sum += tmp;\n    }\n    printf(\"Process %d has prefix sum %d\\n\", rank, local_sum);\n  }\n  return 0;\n}", "label": "int main(int argc, char** argv){\n  int rank, size;\n  MPI_Status status;\n  int local_sum, tmp;\n  int i, iter;\n  int distance;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n\n  local_sum = rank;\n  tmp = 0;\n\n  iter = log(size) / log(2);\n  printf(\"Process %d has prefix sum %d\\n\", rank, local_sum);\n  MPI_Barrier(MPI_COMM_WORLD);\n  for (i = 0; i < iter; i++){\n    distance = pow(2,i);\n\n    if (rank == 0){\n      printf(\"iter %d and distance %d\\n\", i, distance);\n    }      \n    if (rank < (size - distance)){\n      MPI_Send(&local_sum, 1, MPI_INT, rank + distance, 0, MPI_COMM_WORLD);\n      printf(\"%d send to %d value %d\\n\",rank, rank+distance, local_sum);\n\n    }\n    if (rank >= distance){\n      printf(\"%d receive from %d \\n\", rank, rank - distance);\n      MPI_Recv(&tmp, 1, MPI_INT, rank - distance, 0, MPI_COMM_WORLD, &status);\n      printf(\"%d receive from %d value %d\\n\",rank, rank-distance, tmp);\n      local_sum += tmp;\n    }\n    printf(\"Process %d has prefix sum %d\\n\", rank, local_sum);\n    MPI_Barrier(MPI_COMM_WORLD);\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "rahlk_1441", "code": "int main(int argc, char ** argv)\n{\n  MPI_Aint win_size = WIN_SIZE;\n  MPI_Win win;\n  MPI_Group group;\n  char* base;\n  int disp_unit = 1;\n  int rank, size, target_rank, target_disp = 1;\n  int r, flag;\n\n  \n\n  \n\n  \n\n\n  target_rank = (rank + 1) % size;\n  if ( NULL == base )\n  {\n    printf(\"failed to alloc %d\\n\", WIN_SIZE);\n    exit(16);\n  }\n\n\n  \n\n  \n\n  \n\n  \n\n  r = \n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_create\\n\", rank);\n\n  \n\n  \n\n  \n\n  \n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_get_group\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_post\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_start\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_lock\\n\", rank);\n\n  \n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Put\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_unlock\\n\", rank);\n\n  \n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Get\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) \n    printf(\"Rank %d failed MPI_Win_complete\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_test\\n\", rank);\n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_wait\\n\", rank);\n\n  \n\n  \n\n  \n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_fence\\n\", rank);\n\n  if ( rank == 0 )\n  {\n    \n\n    r =\n    if ( MPI_SUCCESS TEST_OP r ) \n      printf(\"Rank %d failed MPI_Accumulate\\n\", rank);\n  }\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_fence\\n\", rank);\n\n\n  \n\n  \n\n  \n\n  r =\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_free\\n\", rank);\n\n  free(base);\n\n}", "label": "int main(int argc, char ** argv)\n{\n  MPI_Aint win_size = WIN_SIZE;\n  MPI_Win win;\n  MPI_Group group;\n  char* base;\n  int disp_unit = 1;\n  int rank, size, target_rank, target_disp = 1;\n  int r, flag;\n\n  \n\n  \n\n  \n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  target_rank = (rank + 1) % size;\n  MPI_Alloc_mem(WIN_SIZE, MPI_INFO_NULL, &base);\n  if ( NULL == base )\n  {\n    printf(\"failed to alloc %d\\n\", WIN_SIZE);\n    exit(16);\n  }\n\n\n  \n\n  \n\n  \n\n  \n\n  r = MPI_Win_create(base, win_size, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win); \n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_create\\n\", rank);\n\n  \n\n  \n\n  \n\n  \n\n  r = MPI_Win_get_group(win, &group);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_get_group\\n\", rank);\n\n  r = MPI_Win_post(group, 0, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_post\\n\", rank);\n\n  r = MPI_Win_start(group, 0, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_start\\n\", rank);\n\n  r = MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_lock\\n\", rank);\n\n  \n\n  r = MPI_Put(base, WIN_SIZE, MPI_BYTE, target_rank, target_disp,\n     WIN_SIZE, MPI_BYTE, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Put\\n\", rank);\n\n  r = MPI_Win_unlock(target_rank, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_unlock\\n\", rank);\n\n  \n\n  r = MPI_Get(base, WIN_SIZE, MPI_BYTE, target_rank, target_disp,\n      WIN_SIZE, MPI_BYTE, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Get\\n\", rank);\n\n  r = MPI_Win_complete(win);\n  if ( MPI_SUCCESS TEST_OP r ) \n    printf(\"Rank %d failed MPI_Win_complete\\n\", rank);\n\n  r = MPI_Win_test(win, &flag);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_test\\n\", rank);\n\n  r = MPI_Win_wait(win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_wait\\n\", rank);\n\n  \n\n  \n\n  \n\n  r = MPI_Win_fence(0, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_fence\\n\", rank);\n\n  if ( rank == 0 )\n  {\n    \n\n    r = MPI_Accumulate(base, WIN_SIZE, MPI_BYTE, 0,\n        target_disp, WIN_SIZE, MPI_BYTE, MPI_SUM, win);\n    if ( MPI_SUCCESS TEST_OP r ) \n      printf(\"Rank %d failed MPI_Accumulate\\n\", rank);\n  }\n  r = MPI_Win_fence(0, win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_fence\\n\", rank);\n\n\n  \n\n  \n\n  \n\n  r = MPI_Win_free(&win);\n  if ( MPI_SUCCESS TEST_OP r ) printf(\"Rank %d failed MPI_Win_free\\n\", rank);\n\n  free(base);\n\n  MPI_Finalize();\n}"}
{"program": "qingu_1442", "code": "int main(int argc, char **argv) {\n  int        me, nproc, target;\n  int        msg_length, round, i;\n  double     t_start, t_stop;\n  uint8_t  *snd_buf;  \n\n  uint8_t  *rcv_buf;  \n\n  MPI_Win    window;\n\n\n\n  if (nproc < 2) {\n    if (me == 0) printf(\"This benchmark should be run on at least two processes\\n\");\n  }\n\n  if (me == 0)\n    printf(\"MPI-2 passive ping-pong latency test, performing %d rounds at each xfer size.\\n\", NUM_ROUNDS);\n\n\n\n  for (i = 0; i < MAX_SIZE; i++) {\n    snd_buf[i] = 1;\n  }\n\n  for (target = 1; target < nproc; target++) {\n    if (me == 0) printf(\"\\n========== Process pair: %d and %d ==========\\n\\n\", 0, target);\n\n    for (msg_length = 1; msg_length <= MAX_SIZE; msg_length *= 2) {\n      t_start =\n\n      if (me == 0 || me == target) {\n        \n\n        for (round = 0; round < NUM_ROUNDS*2; round++) {\n          int my_target = me == 0 ? target : 0;\n\n          \n\n          if ((round % 2 == 0 && me == 0) || (round % 2 != 0 && me != 0)) {\n            \n\n            rcv_buf[0] = 0;\n            rcv_buf[msg_length-1] = 0;\n\n          }\n\n          \n\n          else {\n            uint8_t val;\n\n            do {\n#ifdef PROBE_PROGRESS\n              \n\n#endif\n              val = ((volatile uint8_t*)rcv_buf)[0];\n            } while (val == 0);\n\n            do {\n#ifdef PROBE_PROGRESS\n#endif\n              val = ((volatile uint8_t*)rcv_buf)[msg_length-1];\n            } while (val == 0);\n          }\n        }\n      }\n\n      t_stop =\n\n      if (me == 0)\n        printf(\"%8d bytes \\t %12.8f us\\n\", msg_length, (t_stop-t_start)/NUM_ROUNDS*1.0e6);\n    }\n\n  }\n\n\n\n  return 0;\n}", "label": "int main(int argc, char **argv) {\n  int        me, nproc, target;\n  int        msg_length, round, i;\n  double     t_start, t_stop;\n  uint8_t  *snd_buf;  \n\n  uint8_t  *rcv_buf;  \n\n  MPI_Win    window;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  if (nproc < 2) {\n    if (me == 0) printf(\"This benchmark should be run on at least two processes\\n\");\n    MPI_Abort(MPI_COMM_WORLD, 1);\n  }\n\n  if (me == 0)\n    printf(\"MPI-2 passive ping-pong latency test, performing %d rounds at each xfer size.\\n\", NUM_ROUNDS);\n\n  MPI_Alloc_mem(MAX_SIZE, MPI_INFO_NULL, &rcv_buf);\n  MPI_Alloc_mem(MAX_SIZE, MPI_INFO_NULL, &snd_buf);\n\n  MPI_Win_create(rcv_buf, MAX_SIZE, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &window);\n\n  for (i = 0; i < MAX_SIZE; i++) {\n    snd_buf[i] = 1;\n  }\n\n  for (target = 1; target < nproc; target++) {\n    if (me == 0) printf(\"\\n========== Process pair: %d and %d ==========\\n\\n\", 0, target);\n\n    for (msg_length = 1; msg_length <= MAX_SIZE; msg_length *= 2) {\n      MPI_Barrier(MPI_COMM_WORLD);\n      t_start = MPI_Wtime();\n\n      if (me == 0 || me == target) {\n        \n\n        for (round = 0; round < NUM_ROUNDS*2; round++) {\n          int my_target = me == 0 ? target : 0;\n\n          \n\n          if ((round % 2 == 0 && me == 0) || (round % 2 != 0 && me != 0)) {\n            \n\n            MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, window);\n            rcv_buf[0] = 0;\n            rcv_buf[msg_length-1] = 0;\n            MPI_Win_unlock(me, window);\n\n            MPI_Win_lock(MPI_LOCK_EXCLUSIVE, my_target, 0, window);\n            MPI_Put(snd_buf, msg_length, MPI_BYTE, my_target, 0, msg_length, MPI_BYTE, window);\n            MPI_Win_unlock(my_target, window);\n          }\n\n          \n\n          else {\n            uint8_t val;\n\n            do {\n#ifdef PROBE_PROGRESS\n              \n\n              MPI_Iprobe(0, 0, MPI_COMM_WORLD, (void*) &val, MPI_STATUS_IGNORE);\n#endif\n              MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, window);\n              val = ((volatile uint8_t*)rcv_buf)[0];\n              MPI_Win_unlock(me, window);\n            } while (val == 0);\n\n            do {\n#ifdef PROBE_PROGRESS\n              MPI_Iprobe(0, 0, MPI_COMM_WORLD, (void*) &val, MPI_STATUS_IGNORE);\n#endif\n              MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, window);\n              val = ((volatile uint8_t*)rcv_buf)[msg_length-1];\n              MPI_Win_unlock(me, window);\n            } while (val == 0);\n          }\n        }\n      }\n\n      MPI_Barrier(MPI_COMM_WORLD);\n      t_stop = MPI_Wtime();\n\n      if (me == 0)\n        printf(\"%8d bytes \\t %12.8f us\\n\", msg_length, (t_stop-t_start)/NUM_ROUNDS*1.0e6);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n  }\n\n  MPI_Win_free(&window);\n  MPI_Free_mem(snd_buf);\n  MPI_Free_mem(rcv_buf);\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "astroza_1443", "code": "main(int argc, char *argv[])\n{\n    int rank, size;\n    int *matrix;\t\t\n\n    int n;\n\n    MPI_Datatype matrixType;\n\n\n\n    if (rank == 0) {\n\tn = atoi(argv[1]);\n\tmatrix = readMatrix(argv[2], n);\n\n\t\n\n\n\t\n\n\tmasterWork(matrix, n);\n\n    } else {\n\t\n\n\n\tmatrix = malloc(n * n * sizeof(int));\n\n\t\n\n\tslaveWork(matrix, n, rank);\n\n    }\n\n    return 0;\n}", "label": "main(int argc, char *argv[])\n{\n    int rank, size;\n    int *matrix;\t\t\n\n    int n;\n\n    MPI_Datatype matrixType;\n\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n\tn = atoi(argv[1]);\n\tmatrix = readMatrix(argv[2], n);\n\tMPI_Type_contiguous(n * n, MPI_INT, &matrixType);\n\tMPI_Type_commit(&matrixType);\n\n\t\n\n\tMPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t\n\n\tMPI_Bcast(matrix, 1, matrixType, 0, MPI_COMM_WORLD);\n\tmasterWork(matrix, n);\n\n    } else {\n\t\n\n\tMPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tMPI_Type_contiguous(n * n, MPI_INT, &matrixType);\n\tMPI_Type_commit(&matrixType);\n\tmatrix = malloc(n * n * sizeof(int));\n\n\t\n\n\tMPI_Bcast(matrix, 1, matrixType, 0, MPI_COMM_WORLD);\n\tslaveWork(matrix, n, rank);\n\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "joao-lima_1444", "code": "int main(int argc, char **argv)\n{\n\tint rank, size;\n\tint result=0;\n\tsize_t *comm_amount_with_cache;\n\tsize_t *comm_amount_without_cache;\n\n\n\tsetenv(\"STARPU_COMM_STATS\", \"1\", 1);\n\tsetenv(\"STARPU_MPI_CACHE_STATS\", \"1\", 1);\n\n\tcomm_amount_with_cache = malloc(size * sizeof(size_t));\n\tcomm_amount_without_cache = malloc(size * sizeof(size_t));\n\n\ttest_cache(rank, \"0\", comm_amount_with_cache);\n\ttest_cache(rank, \"1\", comm_amount_without_cache);\n\n\tif (rank == 1)\n\t{\n\t\tresult = (comm_amount_with_cache[0] == comm_amount_without_cache[0] * 2);\n\t\tFPRINTF_MPI(stderr, \"Communication cache mechanism is %sworking (with cache: %ld) (without cache: %ld)\\n\", result?\"\":\"NOT \", comm_amount_with_cache[0], comm_amount_without_cache[0]);\n\t}\n\telse\n\t{\n\t\tresult = 1;\n\t}\n\n\tfree(comm_amount_without_cache);\n\tfree(comm_amount_with_cache);\n\n\treturn !result;\n}", "label": "int main(int argc, char **argv)\n{\n\tint rank, size;\n\tint result=0;\n\tsize_t *comm_amount_with_cache;\n\tsize_t *comm_amount_without_cache;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tsetenv(\"STARPU_COMM_STATS\", \"1\", 1);\n\tsetenv(\"STARPU_MPI_CACHE_STATS\", \"1\", 1);\n\n\tcomm_amount_with_cache = malloc(size * sizeof(size_t));\n\tcomm_amount_without_cache = malloc(size * sizeof(size_t));\n\n\ttest_cache(rank, \"0\", comm_amount_with_cache);\n\ttest_cache(rank, \"1\", comm_amount_without_cache);\n\n\tif (rank == 1)\n\t{\n\t\tresult = (comm_amount_with_cache[0] == comm_amount_without_cache[0] * 2);\n\t\tFPRINTF_MPI(stderr, \"Communication cache mechanism is %sworking (with cache: %ld) (without cache: %ld)\\n\", result?\"\":\"NOT \", comm_amount_with_cache[0], comm_amount_without_cache[0]);\n\t}\n\telse\n\t{\n\t\tresult = 1;\n\t}\n\n\tfree(comm_amount_without_cache);\n\tfree(comm_amount_with_cache);\n\n\tMPI_Finalize();\n\treturn !result;\n}"}
{"program": "mpip_1445", "code": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, patience, verbose;\n  unsigned pfft_flags=0;\n  double err;\n  MPI_Comm comm_cart_3d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  pfft_init();\n  \n  \n\n  n[0] = n[1] = n[2] = 2; \n\n  np[0]=1; np[1]=1; np[2] = 3;\n  verbose = 1;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 2;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &patience, &verbose);\n\n  switch(patience){\n    case 0: pfft_flags = PFFT_ESTIMATE; break;\n    case 2: pfft_flags = PFFT_PATIENT; break;\n    case 3: pfft_flags = PFFT_EXHAUSTIVE; break;\n    default: pfft_flags = PFFT_MEASURE;\n  }\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_3d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_3d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_3d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_3d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= 4;\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, data, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  pfft_free(data);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, patience, verbose;\n  unsigned pfft_flags=0;\n  double err;\n  MPI_Comm comm_cart_3d;\n  pfft_complex *data;\n  pfft_gcplan ths;\n  \n  MPI_Init(&argc, &argv);\n  pfft_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &rnk_self);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  \n\n  n[0] = n[1] = n[2] = 2; \n\n  np[0]=1; np[1]=1; np[2] = 3;\n  verbose = 1;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 2;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &patience, &verbose);\n\n  switch(patience){\n    case 0: pfft_flags = PFFT_ESTIMATE; break;\n    case 2: pfft_flags = PFFT_PATIENT; break;\n    case 3: pfft_flags = PFFT_EXHAUSTIVE; break;\n    default: pfft_flags = PFFT_MEASURE;\n  }\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_ni, local_i_start, alloc_local, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  data = pfft_alloc_complex(alloc_local_gc);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      data, comm_cart_3d, PFFT_GC_NONTRANSPOSED);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      data);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ni, local_i_start, \"gcell input\", comm_cart_3d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_3d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(data, local_no, local_o_start, \"reduced gcells\", comm_cart_3d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    data[l] /= 4;\n\n  \n\n  MPI_Barrier(comm_cart_3d);\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, data, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(data);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "callmetaste_1446", "code": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,dim,deg,nbloops,fail;\n   double startwtime,trackwtime,wtime,*mytime;\n   MPI_Status status;\n\n   adainit();\n   srand(time(NULL));   \n\n   if(myid == 0)\n   {\n      mytime = (double*) calloc(numprocs, sizeof(double));\n      startwtime =\n      fail = read_witness_set(&n,&dim,&deg);\n      fail = define_output_file();\n      printf(\"Give the number of loops : \"); scanf(\"%d\",&nbloops);\n   }\n   else\n      trackwtime =\n\n\n   dimension_broadcast(myid,&n);\n\n   fail = monodromy_breakup(myid,n,dim,deg,nbloops,numprocs,&trackwtime);\n\n   if(myid == 0)\n     wtime = MPI_Wtime() - startwtime;\n   else\n     wtime = trackwtime;\n   if(myid == 0) print_time(mytime,numprocs);\n   adafinal();\n\n   return 0;\n}", "label": "int main ( int argc, char *argv[] )\n{\n   int myid,numprocs,n,dim,deg,nbloops,fail;\n   double startwtime,trackwtime,wtime,*mytime;\n   MPI_Status status;\n\n   adainit();\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n   srand(time(NULL));   \n\n   if(myid == 0)\n   {\n      mytime = (double*) calloc(numprocs, sizeof(double));\n      startwtime = MPI_Wtime();\n      fail = read_witness_set(&n,&dim,&deg);\n      fail = define_output_file();\n      printf(\"Give the number of loops : \"); scanf(\"%d\",&nbloops);\n   }\n   else\n      trackwtime = MPI_Wtime();\n\n   MPI_Barrier(MPI_COMM_WORLD);  \n\n   dimension_broadcast(myid,&n);\n   MPI_Bcast(&nbloops,1,MPI_INT,0,MPI_COMM_WORLD);\n\n   fail = monodromy_breakup(myid,n,dim,deg,nbloops,numprocs,&trackwtime);\n\n   MPI_Barrier(MPI_COMM_WORLD);\n   if(myid == 0)\n     wtime = MPI_Wtime() - startwtime;\n   else\n     wtime = trackwtime;\n   MPI_Gather(&wtime,1,MPI_DOUBLE,mytime,1,MPI_DOUBLE,0,MPI_COMM_WORLD);\n   if(myid == 0) print_time(mytime,numprocs);\n   MPI_Finalize();\n   adafinal();\n\n   return 0;\n}"}
{"program": "jag10_1447", "code": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n  const int SEED = 7;\n  srand(SEED);\n\n\n  int world_rank, world_size;\n\n  \n\n  float *rands = NULL;\n  if (world_rank == 0) {\n    rands = create_rands(ELEMENTS_PER_PROC * world_size, MAX_RANDOM_VAL);\n  }\n\n  \n\n  float *buff_rands = (float *)malloc(sizeof(float) * ELEMENTS_PER_PROC);\n\n  \n\n  dumpData ( world_rank , world_size , ELEMENTS_PER_PROC , buff_rands , \" Data \" , 1);\n\n  \n\n  float sub_avg = calc_avg(buff_rands, ELEMENTS_PER_PROC);\n\n  \n\n  float *avgs = NULL;\n  \n\n  avgs = (float *)malloc(sizeof(float) * world_size);\n  \n\n\n  float avg = calc_avg(avgs, world_size);\n  printf(\" Process %d: Avg of all elements is %f\\n\", world_rank, avg);\n\n  if (world_rank == 0) {\n    free(rands);\n  }\n  free(buff_rands);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n  const int ELEMENTS_PER_PROC = 3;\n  const int MAX_RANDOM_VAL = 10;\n  const int SEED = 7;\n  srand(SEED);\n\n  MPI_Init(NULL, NULL);\n\n  int world_rank, world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  \n\n  float *rands = NULL;\n  if (world_rank == 0) {\n    rands = create_rands(ELEMENTS_PER_PROC * world_size, MAX_RANDOM_VAL);\n  }\n\n  \n\n  float *buff_rands = (float *)malloc(sizeof(float) * ELEMENTS_PER_PROC);\n\n  \n\n  MPI_Scatter(rands, ELEMENTS_PER_PROC, MPI_FLOAT, buff_rands,\n              ELEMENTS_PER_PROC, MPI_FLOAT, 0, MPI_COMM_WORLD);\n  dumpData ( world_rank , world_size , ELEMENTS_PER_PROC , buff_rands , \" Data \" , 1);\n\n  \n\n  float sub_avg = calc_avg(buff_rands, ELEMENTS_PER_PROC);\n\n  \n\n  float *avgs = NULL;\n  \n\n  avgs = (float *)malloc(sizeof(float) * world_size);\n  \n\n  MPI_Allgather(&sub_avg, 1, MPI_FLOAT, avgs, 1, MPI_FLOAT, MPI_COMM_WORLD);\n\n  float avg = calc_avg(avgs, world_size);\n  printf(\" Process %d: Avg of all elements is %f\\n\", world_rank, avg);\n\n  if (world_rank == 0) {\n    free(rands);\n  }\n  free(buff_rands);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "karoraw1_1448", "code": "int main(int argc, char* argv[])\n{\n    int rank, nprocs, ncid, cmode, stat;\n\n\n    MPI_Comm comm=MPI_COMM_SELF;\n    MPI_Info info=MPI_INFO_NULL;\n\n    if (nprocs > 1 && rank == 0)\n        printf(\"This test program is intended to run on ONE process\\n\");\n    if (rank > 0) goto fn_exit;\n\n\n#ifdef DISABLE_PNETCDF_ALIGNMENT\n#endif\n\n    \n\n    printf(\"\\nWrite using PNETCDF; Read using CDF5\\n\");\n    \n    cmode = NC_PNETCDF | NC_CLOBBER;\n    if (stat=nc_create_par(FILENAME, cmode, comm, info, &ncid)) Error(stat);\n    if (stat=write(ncid,1)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n    \n\n    if (stat=nc_open_par(FILENAME, NC_WRITE|NC_PNETCDF, comm, info, &ncid)) Error(stat);\n    if(stat=extend(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    cmode = NC_CDF5 | NC_NOCLOBBER;\n    if (stat=nc_open(FILENAME, cmode, &ncid)) ERR_RET;\n    if (stat=read(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    unlink(FILENAME);\n\n    \n\n    printf(\"\\nWrite using CDF-5; Read using PNETCDF\\n\");\n    cmode = NC_CDF5 | NC_CLOBBER;\n    if (stat=nc_create(FILENAME, cmode, &ncid)) ERR_RET;\n    if (stat=write(ncid,0)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n    \n\n    if (stat=nc_open(FILENAME, NC_WRITE|NC_CDF5, &ncid)) ERR_RET;\n    if (stat=extend(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    cmode = NC_PNETCDF | NC_NOCLOBBER;\n    if (stat=nc_open_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n    if (stat=read(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    if (info != MPI_INFO_NULL)\n\nfn_exit:\n    SUMMARIZE_ERR;\n    FINAL_RESULTS;\n    return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n    int rank, nprocs, ncid, cmode, stat;\n\n\n    MPI_Comm comm=MPI_COMM_SELF;\n    MPI_Info info=MPI_INFO_NULL;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (nprocs > 1 && rank == 0)\n        printf(\"This test program is intended to run on ONE process\\n\");\n    if (rank > 0) goto fn_exit;\n\n\n#ifdef DISABLE_PNETCDF_ALIGNMENT\n    MPI_Info_create(&info);\n    MPI_Info_set(info, \"nc_header_align_size\", \"1\");\n    MPI_Info_set(info, \"nc_var_align_size\",    \"1\");\n#endif\n\n    \n\n    printf(\"\\nWrite using PNETCDF; Read using CDF5\\n\");\n    \n    cmode = NC_PNETCDF | NC_CLOBBER;\n    if (stat=nc_create_par(FILENAME, cmode, comm, info, &ncid)) Error(stat);\n    if (stat=write(ncid,1)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n    \n\n    if (stat=nc_open_par(FILENAME, NC_WRITE|NC_PNETCDF, comm, info, &ncid)) Error(stat);\n    if(stat=extend(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    cmode = NC_CDF5 | NC_NOCLOBBER;\n    if (stat=nc_open(FILENAME, cmode, &ncid)) ERR_RET;\n    if (stat=read(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    unlink(FILENAME);\n\n    \n\n    printf(\"\\nWrite using CDF-5; Read using PNETCDF\\n\");\n    cmode = NC_CDF5 | NC_CLOBBER;\n    if (stat=nc_create(FILENAME, cmode, &ncid)) ERR_RET;\n    if (stat=write(ncid,0)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n    \n\n    if (stat=nc_open(FILENAME, NC_WRITE|NC_CDF5, &ncid)) ERR_RET;\n    if (stat=extend(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    cmode = NC_PNETCDF | NC_NOCLOBBER;\n    if (stat=nc_open_par(FILENAME, cmode, comm, info, &ncid)) ERR_RET;\n    if (stat=read(ncid)) Error(stat);\n    if (stat=nc_close(ncid)) Error(stat);\n\n    if (info != MPI_INFO_NULL) MPI_Info_free(&info);\n\nfn_exit:\n    MPI_Finalize();\n    SUMMARIZE_ERR;\n    FINAL_RESULTS;\n    return 0;\n}"}
{"program": "lorenzgerber_1449", "code": "int main(void) {\n  int my_rank, comm_sz, n = 1024, local_n;   \n   double a = 0.0, b = 3.0, h, local_a, local_b;\n   double local_int, total_int;\n   int source; \n\n   \n\n\n   \n\n\n   \n\n\n   \n   h = (b-a)/n;          \n\n   local_n = n/comm_sz;  \n\n   \n\n   \n   \n\n   \n\n   \n\n   \n\n   \n\n   \n\n   \n\n\n   \n\n   \n\n\n   \n\n\n   local_a = calc_local_a(my_rank, a, b, n, comm_sz);\n   local_b = calc_local_b(my_rank, a, b, n, comm_sz);\n   \n     \n\n   local_int = Trap(local_a, local_b, local_n, h);\n\n   \n\n   if (my_rank != 0) { \n   } else {\n      total_int = local_int;\n      for (source = 1; source < comm_sz; source++) {\n         total_int += local_int;\n      }\n   } \n\n   \n\n   if (my_rank == 0) {\n      printf(\"With n = %d trapezoids, our estimate\\n\", n);\n      printf(\"of the integral from %f to %f = %.15e\\n\",\n          a, b, total_int);\n   }\n\n   \n\n\n   return 0;\n}", "label": "int main(void) {\n  int my_rank, comm_sz, n = 1024, local_n;   \n   double a = 0.0, b = 3.0, h, local_a, local_b;\n   double local_int, total_int;\n   int source; \n\n   \n\n   MPI_Init(NULL, NULL);\n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   \n\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n   \n   h = (b-a)/n;          \n\n   local_n = n/comm_sz;  \n\n   \n\n   \n   \n\n   \n\n   \n\n   \n\n   \n\n   \n\n   \n\n\n   \n\n   \n\n\n   \n\n\n   local_a = calc_local_a(my_rank, a, b, n, comm_sz);\n   local_b = calc_local_b(my_rank, a, b, n, comm_sz);\n   \n     \n\n   local_int = Trap(local_a, local_b, local_n, h);\n\n   \n\n   if (my_rank != 0) { \n      MPI_Send(&local_int, 1, MPI_DOUBLE, 0, 0, \n            MPI_COMM_WORLD); \n   } else {\n      total_int = local_int;\n      for (source = 1; source < comm_sz; source++) {\n         MPI_Recv(&local_int, 1, MPI_DOUBLE, source, 0,\n            MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         total_int += local_int;\n      }\n   } \n\n   \n\n   if (my_rank == 0) {\n      printf(\"With n = %d trapezoids, our estimate\\n\", n);\n      printf(\"of the integral from %f to %f = %.15e\\n\",\n          a, b, total_int);\n   }\n\n   \n\n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "gentryx_1450", "code": "int main(int argc, char * argv[])\n{\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n\n    int rank, size;\n\n    if (size<1) {\n        printf(\"Use 1 or more processes. \\n\");\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf_send = NULL;\n    char * buf_recv = NULL;\n\n    assert(buf_send!=NULL);\n    assert(buf_recv!=NULL);\n\n    for (MPI_Count j = 0; j < n; ++j) {\n        buf_send[j] = (unsigned char)rank;\n    }\n    memset(buf_recv, -1, (size_t)n);\n\n    \n\n\n    size_t errors = 0;\n    for (int i = 0; i < size; ++i) {\n        errors += verify_buffer(buf_recv + i * n, n, i);\n    }\n\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n\n    int rc = (errors > 0) ? 1 : 0;\n    return rc;\n}", "label": "int main(int argc, char * argv[])\n{\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size<1) {\n        printf(\"Use 1 or more processes. \\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    char * buf_send = NULL;\n    char * buf_recv = NULL;\n\n    MPI_Alloc_mem((MPI_Aint)n * 1,    MPI_INFO_NULL, &buf_send);\n    assert(buf_send!=NULL);\n    MPI_Alloc_mem((MPI_Aint)n * size, MPI_INFO_NULL, &buf_recv);\n    assert(buf_recv!=NULL);\n\n    for (MPI_Count j = 0; j < n; ++j) {\n        buf_send[j] = (unsigned char)rank;\n    }\n    memset(buf_recv, -1, (size_t)n);\n\n    \n\n    MPIX_Allgather_x(buf_send, n, MPI_CHAR,\n                     buf_recv, n, MPI_CHAR,\n                     MPI_COMM_WORLD);\n\n    size_t errors = 0;\n    for (int i = 0; i < size; ++i) {\n        errors += verify_buffer(buf_recv + i * n, n, i);\n    }\n\n    MPI_Free_mem(buf_send);\n    MPI_Free_mem(buf_recv);\n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n    MPI_Finalize();\n\n    int rc = (errors > 0) ? 1 : 0;\n    return rc;\n}"}
{"program": "chu11_1451", "code": "int main (int argc, char *argv[])\n{\n    int id, ntasks;\n    struct timespec t;\n    const char *label;\n\n    if (!(label = getenv (\"FLUX_JOB_CC\")))\n        if (!(label = getenv (\"FLUX_JOB_ID\")))\n            label = \"0\";\n\n    monotime (&t);\n    if (id == 0) {\n        printf (\"%s: completed MPI_Init in %0.3fs.  There are %d tasks\\n\",\n                label,\n                monotime_since (t) / 1000, ntasks);\n        fflush (stdout);\n    }\n\n    monotime (&t);\n    if (id == 0) {\n        printf (\"%s: completed first barrier in %0.3fs\\n\",\n                label,\n                monotime_since (t) / 1000);\n        fflush (stdout);\n    }\n\n    monotime (&t);\n    if (id == 0) {\n        printf (\"%s: completed MPI_Finalize in %0.3fs\\n\",\n                label,\n                monotime_since (t) / 1000);\n        fflush (stdout);\n    }\n    return 0;\n}", "label": "int main (int argc, char *argv[])\n{\n    int id, ntasks;\n    struct timespec t;\n    const char *label;\n\n    if (!(label = getenv (\"FLUX_JOB_CC\")))\n        if (!(label = getenv (\"FLUX_JOB_ID\")))\n            label = \"0\";\n\n    monotime (&t);\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank (MPI_COMM_WORLD, &id);\n    MPI_Comm_size (MPI_COMM_WORLD, &ntasks);\n    if (id == 0) {\n        printf (\"%s: completed MPI_Init in %0.3fs.  There are %d tasks\\n\",\n                label,\n                monotime_since (t) / 1000, ntasks);\n        fflush (stdout);\n    }\n\n    monotime (&t);\n    MPI_Barrier (MPI_COMM_WORLD);\n    if (id == 0) {\n        printf (\"%s: completed first barrier in %0.3fs\\n\",\n                label,\n                monotime_since (t) / 1000);\n        fflush (stdout);\n    }\n\n    monotime (&t);\n    MPI_Finalize ();\n    if (id == 0) {\n        printf (\"%s: completed MPI_Finalize in %0.3fs\\n\",\n                label,\n                monotime_since (t) / 1000);\n        fflush (stdout);\n    }\n    return 0;\n}"}
{"program": "darshanhegde_1452", "code": "int main (int argc, char *argv[])\n{\n    int numtasks, rank, buf, tag1=1, i, rc, dest, src, offset, nreqs;\n    double T1, T2;\n    MPI_Request reqs[REPS*2];\n    MPI_Status stats[REPS*2];\n    \n    \n    \n\n    if (rank == 0 ) {\n        if (numtasks != 4) {\n            printf(\"ERROR: Number of tasks must be 4. Quitting.\\n\");\n        }\n        printf(\"Starting isend/irecv send/irecv test...\\n\");\n    }\n    \n    \n\n    printf(\"Task %d starting...\\n\", rank);\n    \n    T1 =     \n\n    \n    \n\n    if (rank < 2) {\n        nreqs = REPS*2;\n        offset = 0;\n        if (rank == 0) {\n            src = 1;\n        }\n        if (rank == 1) {\n            src = 0;\n        }\n        dest = src;\n        \n        \n\n        for (i=0; i<REPS; i++) {\n            offset += 2;\n            if ((i+1)%DISP == 0)\n                printf(\"Task %d has done %d isends/irecvs\\n\", rank, i+1);\n        }\n    }\n    \n    \n\n    if (rank > 1) {\n        \n\n        if (rank == 2) {\n            dest = 3;\n            nreqs = 0;\n            for (i=0; i<REPS; i++) {\n                if ((i+1)%DISP == 0)\n                    printf(\"Task %d has done %d sends\\n\", rank, i+1);\n            }\n        }\n        \n        \n\n        if (rank == 3) {\n            src = 2;\n            offset = 0;\n            nreqs = REPS;\n            for (i=0; i<REPS; i++) {\n                offset += 1;\n                if ((i+1)%DISP == 0)\n                    printf(\"Task %d has done %d irecvs\\n\", rank, i+1);\n            }\n        }\n        \n    }\n    \n    \n\n    T2 =     \n\n    \n    printf(\"Task %d time(wall)= %lf sec\\n\", rank, T2-T1);\n    \n}", "label": "int main (int argc, char *argv[])\n{\n    int numtasks, rank, buf, tag1=1, i, rc, dest, src, offset, nreqs;\n    double T1, T2;\n    MPI_Request reqs[REPS*2];\n    MPI_Status stats[REPS*2];\n    \n    MPI_Init(&argc,&argv);\n    MPI_Comm_size(COMM, &numtasks);\n    MPI_Comm_rank(COMM, &rank);\n    \n    \n\n    if (rank == 0 ) {\n        if (numtasks != 4) {\n            printf(\"ERROR: Number of tasks must be 4. Quitting.\\n\");\n            MPI_Abort(COMM, rc);\n        }\n        printf(\"Starting isend/irecv send/irecv test...\\n\");\n    }\n    \n    \n\n    MPI_Barrier(COMM);\n    printf(\"Task %d starting...\\n\", rank);\n    MPI_Barrier(COMM);\n    \n    T1 = MPI_Wtime();     \n\n    \n    \n\n    if (rank < 2) {\n        nreqs = REPS*2;\n        offset = 0;\n        if (rank == 0) {\n            src = 1;\n        }\n        if (rank == 1) {\n            src = 0;\n        }\n        dest = src;\n        \n        \n\n        for (i=0; i<REPS; i++) {\n            MPI_Isend(&rank, 1, MPI_INT, dest, tag1, COMM, &reqs[offset]);\n            MPI_Irecv(&buf, 1, MPI_INT, src, tag1, COMM, &reqs[offset+1]);\n            offset += 2;\n            if ((i+1)%DISP == 0)\n                printf(\"Task %d has done %d isends/irecvs\\n\", rank, i+1);\n        }\n    }\n    \n    \n\n    if (rank > 1) {\n        \n\n        if (rank == 2) {\n            dest = 3;\n            nreqs = 0;\n            for (i=0; i<REPS; i++) {\n                MPI_Send(&rank, 1, MPI_INT, dest, tag1, COMM);\n                if ((i+1)%DISP == 0)\n                    printf(\"Task %d has done %d sends\\n\", rank, i+1);\n            }\n        }\n        \n        \n\n        if (rank == 3) {\n            src = 2;\n            offset = 0;\n            nreqs = REPS;\n            for (i=0; i<REPS; i++) {\n                MPI_Irecv(&buf, 1, MPI_INT, src, tag1, COMM, &reqs[offset]);\n                offset += 1;\n                if ((i+1)%DISP == 0)\n                    printf(\"Task %d has done %d irecvs\\n\", rank, i+1);\n            }\n        }\n        \n    }\n    \n    \n\n    MPI_Waitall(nreqs, reqs, stats);\n    T2 = MPI_Wtime();     \n\n    MPI_Barrier(COMM);\n    \n    printf(\"Task %d time(wall)= %lf sec\\n\", rank, T2-T1);\n    \n    MPI_Finalize();\n}"}
{"program": "majosm_1453", "code": "int main(int argc, char **argv) {\n\n\n  int WorldRank;\n\n  int Error;\n\n  bool Help;\n  int N;\n\n  Error = GetCommandLineArguments(argc, argv, &Help, &N);\n  if (Error) goto check_error;\n\n  if (!Help) {\n    Error = Interface(N);\n    if (Error) goto check_error;\n  }\n\n  check_error:\n    if (Error) {\n      if (WorldRank == 0) {\n        fprintf(stderr, \"Error occurred.\\n\"); fflush(stderr);\n      }\n    }\n\n\n  return 0;\n\n}", "label": "int main(int argc, char **argv) {\n\n  MPI_Init(&argc, &argv);\n\n  int WorldRank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &WorldRank);\n\n  int Error;\n\n  bool Help;\n  int N;\n\n  Error = GetCommandLineArguments(argc, argv, &Help, &N);\n  if (Error) goto check_error;\n\n  if (!Help) {\n    Error = Interface(N);\n    if (Error) goto check_error;\n  }\n\n  check_error:\n    if (Error) {\n      MPI_Barrier(MPI_COMM_WORLD);\n      if (WorldRank == 0) {\n        fprintf(stderr, \"Error occurred.\\n\"); fflush(stderr);\n      }\n    }\n\n  MPI_Finalize();\n\n  return 0;\n\n}"}
{"program": "auag92_1455", "code": "void main(int argc, char *argv[]) {\n  \n  \n  numworkers = numtasks-1;\n  \n  \n  if (taskid == MASTER) {\n    comp = (double *)malloc(MESHX*sizeof(double));\n    \n    initialize();\n    \n    averow  = MESHX/numworkers;\n    extra   = MESHX%numworkers;\n    \n    offset = 0;\n    \n     for (rank=1; rank <= (numworkers); rank++) {\n      rows = (rank <= extra) ? averow+1 : averow;\n      \n      left_node  = rank - 1;\n      right_node = rank + 1;\n      \n      if(rank == 1) {\n        left_node  = NONE;\n      }\n      \n      if(rank == (numworkers)) {\n        right_node = NONE; \n      }\n      \n      dest = rank;\n      \n      \n      offset = offset + rows;\n    }\n    \n    for (t=1; t < ntimesteps; t++) {\n      if (t%saveT == 0) {\n        receivefrmworker();\n        writetofile(t);\n      }\n    }\n    free(comp);\n  }\n  \n  if(taskid != MASTER) {\n    source  = MASTER;\n    msgtype = BEGIN;\n    \n    \n    start = 1;\n    if ((taskid == 1) || (taskid == numworkers)) {\n      comp = (double *)malloc((rows+1)*sizeof(double));\n      \n      if (taskid == 1) {\n      } else {\n      }\n      end   = rows-1;\n    } else {\n      comp = (double *)malloc((rows+2)*sizeof(double));\n      end = rows;\n    }\n    \n    long t;\n    \n    for (t=1; t < ntimesteps; t++) {\n      mpiexchange(taskid);\n      solverloop(comp);\n      apply_boundary_conditions(taskid);\n      if (t%saveT == 0) {\n        sendtomaster(taskid);\n      }\n    }\n    free(comp);\n  }\n\n\n}", "label": "void main(int argc, char *argv[]) {\n  \n  MPI_Init(&argc,&argv);\n  MPI_Comm_size(MPI_COMM_WORLD,&numtasks);\n  MPI_Comm_rank(MPI_COMM_WORLD,&taskid);\n  \n  numworkers = numtasks-1;\n  \n  \n  if (taskid == MASTER) {\n    comp = (double *)malloc(MESHX*sizeof(double));\n    \n    initialize();\n    \n    averow  = MESHX/numworkers;\n    extra   = MESHX%numworkers;\n    \n    offset = 0;\n    \n     for (rank=1; rank <= (numworkers); rank++) {\n      rows = (rank <= extra) ? averow+1 : averow;\n      \n      left_node  = rank - 1;\n      right_node = rank + 1;\n      \n      if(rank == 1) {\n        left_node  = NONE;\n      }\n      \n      if(rank == (numworkers)) {\n        right_node = NONE; \n      }\n      \n      dest = rank;\n      \n      MPI_Send(&offset,       1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&rows,         1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&left_node,    1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&right_node,   1,    MPI_INT,         dest, BEGIN, MPI_COMM_WORLD);\n      MPI_Send(&comp[offset], rows, MPI_DOUBLE,      dest, BEGIN, MPI_COMM_WORLD);\n      \n      offset = offset + rows;\n    }\n    \n    for (t=1; t < ntimesteps; t++) {\n      if (t%saveT == 0) {\n        receivefrmworker();\n        writetofile(t);\n      }\n    }\n    free(comp);\n  }\n  \n  if(taskid != MASTER) {\n    source  = MASTER;\n    msgtype = BEGIN;\n    MPI_Recv(&offset,     1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&rows,       1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&left_node,  1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    MPI_Recv(&right_node, 1, MPI_INT, source, msgtype, MPI_COMM_WORLD, &status);\n    \n    \n    start = 1;\n    if ((taskid == 1) || (taskid == numworkers)) {\n      comp = (double *)malloc((rows+1)*sizeof(double));\n      \n      if (taskid == 1) {\n        MPI_Recv(&comp[0], rows, MPI_DOUBLE, source, msgtype, MPI_COMM_WORLD, &status);\n      } else {\n        MPI_Recv(&comp[1], rows, MPI_DOUBLE, source, msgtype, MPI_COMM_WORLD, &status);\n      }\n      end   = rows-1;\n    } else {\n      comp = (double *)malloc((rows+2)*sizeof(double));\n      MPI_Recv(&comp[1], rows, MPI_DOUBLE, source, msgtype, MPI_COMM_WORLD, &status);\n      end = rows;\n    }\n    \n    long t;\n    \n    for (t=1; t < ntimesteps; t++) {\n      mpiexchange(taskid);\n      solverloop(comp);\n      apply_boundary_conditions(taskid);\n      if (t%saveT == 0) {\n        sendtomaster(taskid);\n      }\n    }\n    free(comp);\n  }\n\n\n  MPI_Finalize();\n}"}
{"program": "germasch_1456", "code": "int\nmain(int argc, char **argv)\n{\n  libmrc_params_init(argc, argv);\n\n  struct rlc *rlc = rlc_create(MPI_COMM_WORLD);\n  rlc_set_from_options(rlc);\n  rlc_setup(rlc);\n  rlc_view(rlc);\n\n  struct mrc_fld *x = rlc_get_fld(rlc, \"x\");\n\n  \n\n  Q(x) = 1.;\n  I(x) = 0.;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, rlc_diag, rlc);\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, rlc_calc_rhs, rlc);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  \n\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  rlc_destroy(rlc);\n\n  return 0;\n}", "label": "int\nmain(int argc, char **argv)\n{\n  MPI_Init(&argc, &argv);\n  libmrc_params_init(argc, argv);\n\n  struct rlc *rlc = rlc_create(MPI_COMM_WORLD);\n  rlc_set_from_options(rlc);\n  rlc_setup(rlc);\n  rlc_view(rlc);\n\n  struct mrc_fld *x = rlc_get_fld(rlc, \"x\");\n\n  \n\n  Q(x) = 1.;\n  I(x) = 0.;\n\n  \n\n  struct mrc_ts *ts = mrc_ts_create_std(MPI_COMM_WORLD, rlc_diag, rlc);\n  mrc_ts_set_solution(ts, mrc_fld_to_mrc_obj(x));\n  mrc_ts_set_rhs_function(ts, rlc_calc_rhs, rlc);\n  mrc_ts_set_from_options(ts);\n  mrc_ts_setup(ts);\n  mrc_ts_solve(ts);\n  \n\n  mrc_ts_destroy(ts);\n\n  mrc_fld_destroy(x);\n\n  rlc_destroy(rlc);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "UnProgrammatore_1458", "code": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tmpz_t the_number;\n\n\tmpz_init(the_number);\n\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t}\n\telse\n\t\tmpz_set_str(the_number, argv[1], 10); \n\n\n\tif(my_rank == 0)\n\t\tmaster_procedure(comm_size);\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tmpz_t the_number;\n\n\tmpz_init(the_number);\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t\tMPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\telse\n\t\tmpz_set_str(the_number, argv[1], 10); \n\n\n\tif(my_rank == 0)\n\t\tmaster_procedure(comm_size);\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "nschloe_1459", "code": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag, retval; \n  MPI_Request srequest,rrequest;\n  MPI_Status status;\n\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  retval =\n  printf(\"Isend = %d\\n\", retval);\n  retval =\n  printf(\"%s\\n\", message2);\n  printf(\"Irecv = %d\\n\", retval);\n\n  return 0;\n}", "label": "int main(int argc, char**argv) \n{\n  int my_rank;\n  int p;\n  char message1[50];\n  char message2[50];\n  int source, dest, tag, retval; \n  MPI_Request srequest,rrequest;\n  MPI_Status status;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  source = tag = dest = 0;\n  sprintf(message1, \"Hello there\");\n  retval = MPI_Isend(message1, strlen(message1)+1, MPI_CHAR, dest, tag, MPI_COMM_WORLD, &srequest);\n  printf(\"Isend = %d\\n\", retval);\n  retval = MPI_Irecv(message2, 50, MPI_CHAR, source, tag, MPI_COMM_WORLD, &rrequest);\n  MPI_Wait(&rrequest,&status);\n  printf(\"%s\\n\", message2);\n  printf(\"Irecv = %d\\n\", retval);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "kamagasako_1460", "code": "int main(int argc, char *argv[]) {\n\n  int rank;\n\n  e_init(NULL);\n  int primary = !(rank % 2);\n  int tmp = 0;\n  int tag = 0;\n  if (primary) {\n    e_reset_system();\n  } else {\n    MPI_Status status;\n  }\n\n  e_platform_t platform;\n  e_get_platform_info(&platform);\n  int rows = platform.rows / 2;\n  int cols = platform.cols;\n  e_epiphany_t dev;\n  e_open(&dev, (primary ? 0 : rows), 0, rows, cols);\n  e_reset_group(&dev);\n\n  e_load_group((char*) \"e-hello.srec\", &dev, 0, 0, rows, cols, E_TRUE);\n  usleep(10000);\n  int core_id;\n  for (int i = 0; i < rows; i++) {\n    for (int j = 0; j < cols; j++) {\n      e_read(&dev, i, j, 0x2000, &core_id, sizeof(int));\n      printf(\"hello, world from process %d, eCore: 0x%03X (%d, %d)\\n\", rank, core_id, i, j);\n    }\n  }\n\n  e_close(&dev);\n  tag++;\n  if (!primary) {\n  } else {\n    MPI_Status status;\n    e_finalize();\n  }\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  MPI_Init(&argc, &argv);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  e_init(NULL);\n  int primary = !(rank % 2);\n  int tmp = 0;\n  int tag = 0;\n  if (primary) {\n    e_reset_system();\n    MPI_Send(&tmp, 0, MPI_INT, rank + 1, tag, MPI_COMM_WORLD);\n  } else {\n    MPI_Status status;\n    MPI_Recv(&tmp, 0, MPI_INT, rank - 1, tag, MPI_COMM_WORLD, &status);\n  }\n\n  e_platform_t platform;\n  e_get_platform_info(&platform);\n  int rows = platform.rows / 2;\n  int cols = platform.cols;\n  e_epiphany_t dev;\n  e_open(&dev, (primary ? 0 : rows), 0, rows, cols);\n  e_reset_group(&dev);\n\n  e_load_group((char*) \"e-hello.srec\", &dev, 0, 0, rows, cols, E_TRUE);\n  usleep(10000);\n  int core_id;\n  for (int i = 0; i < rows; i++) {\n    for (int j = 0; j < cols; j++) {\n      e_read(&dev, i, j, 0x2000, &core_id, sizeof(int));\n      printf(\"hello, world from process %d, eCore: 0x%03X (%d, %d)\\n\", rank, core_id, i, j);\n    }\n  }\n\n  e_close(&dev);\n  tag++;\n  if (!primary) {\n    MPI_Send(&tmp, 0, MPI_INT, rank - 1, tag, MPI_COMM_WORLD);\n  } else {\n    MPI_Status status;\n    MPI_Recv(&tmp, 0, MPI_INT, rank + 1, tag, MPI_COMM_WORLD, &status);\n    e_finalize();\n  }\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "jcchurch_1461", "code": "int main(int argc, char **argv) {\n   int rank;\n   char host[150];\n   int namelen;\n\n   \n\n\n   \n\n\n   \n\n\n   printf(\"Hello world (Rank: %d / Host: %s)\\n\", rank, host);\n   fflush(stdout);\n\n   \n\n   return 0;\n}", "label": "int main(int argc, char **argv) {\n   int rank;\n   char host[150];\n   int namelen;\n\n   \n\n   MPI_Init(&argc, &argv);\n\n   \n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   \n\n   MPI_Get_processor_name(host,&namelen);\n\n   printf(\"Hello world (Rank: %d / Host: %s)\\n\", rank, host);\n   fflush(stdout);\n\n   \n\n   MPI_Finalize();\n   return 0;\n}"}
{"program": "INSA-Rennes_1462", "code": "int main(int argc, char * argv[])\n{\n  int i;\n  int j;\n  int k;\n  float A[N][N];\n  float B[N][N];\n  float C[N][N];\n\n  int rank, size;\n\n  MPI_Status stat;\n\n  if(size != 3) {\n    printf(\"Ce programme ne fonctionne qu\\'avec 3 processus\\n\");\n    exit(-1);\n  }\n\n  if(rank==0){\n    A[0][0] = 1;\n    A[0][1] = 2;\n    A[0][2] = 3;\n    A[1][0] = 4;\n    A[1][1] = 5;\n    A[1][2] = 6;\n    A[2][0] = 7;\n    A[2][1] = 8;\n    A[2][2] = 9;\n\n    B[0][0] = 10;\n    B[0][1] = 11;\n    B[0][2] = 12;\n    B[1][0] = 13;\n    B[1][1] = 14;\n    B[1][2] = 15;\n    B[2][0] = 16;\n    B[2][1] = 17;\n    B[2][2] = 18;\n\n    for(i=1 ; i<size ; i++){\n    }\n\n    \n\n    printf(\"Matrice A :\\n\");\n    for(i = 0 ; i < N ; i++) {\n      for(j = 0 ; j < N ; j++) {\n        printf(\"%f \", A[i][j]);\n      }\n      printf(\"\\n\");\n    }\n    printf(\"Matrice B :\\n\");\n    for(i = 0 ; i < N ; i++) {\n      for(j = 0 ; j < N ; j++) {\n        printf(\"%f \", B[i][j]);\n      }\n      printf(\"\\n\");\n    }\n  }\n\n  if(rank != 0){\n  }\n\n\n  for(i = 0 ; i < N/size ; i++) {\n    for(j = 0 ; j < N ; j++) {\n      float res_ij;\n      res_ij = 0;\n      for(k = 0 ; k < N ; k++) {\n        res_ij += A[i*(N/size)][k] * B[k][j];\n      }\n      C[rank*(N/size)+i][j] = res_ij;\n    }\n  }\n\n  if(rank != 0) {\n  }\n\n  if(rank == 0) {\n    for(i = 1; i < size ; i++) {\n    }\n\n    \n\n    printf(\"Matrice C :\\n\");\n    for(i = 0 ; i < N ; i++) {\n      for(j = 0 ; j < N ; j++) {\n        printf(\"%f \", C[i][j]);\n      }\n      printf(\"\\n\");\n    }\n  }\n\n\n  return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n  int i;\n  int j;\n  int k;\n  float A[N][N];\n  float B[N][N];\n  float C[N][N];\n\n  int rank, size;\n\n  MPI_Status stat;\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if(size != 3) {\n    printf(\"Ce programme ne fonctionne qu\\'avec 3 processus\\n\");\n    exit(-1);\n  }\n\n  if(rank==0){\n    A[0][0] = 1;\n    A[0][1] = 2;\n    A[0][2] = 3;\n    A[1][0] = 4;\n    A[1][1] = 5;\n    A[1][2] = 6;\n    A[2][0] = 7;\n    A[2][1] = 8;\n    A[2][2] = 9;\n\n    B[0][0] = 10;\n    B[0][1] = 11;\n    B[0][2] = 12;\n    B[1][0] = 13;\n    B[1][1] = 14;\n    B[1][2] = 15;\n    B[2][0] = 16;\n    B[2][1] = 17;\n    B[2][2] = 18;\n\n    for(i=1 ; i<size ; i++){\n      MPI_Send(B, N*N, MPI_FLOAT, i ,0 , MPI_COMM_WORLD);\n      MPI_Send(A+i*(N/size), N*(N/size), MPI_FLOAT, i, 0, MPI_COMM_WORLD);\n    }\n\n    \n\n    printf(\"Matrice A :\\n\");\n    for(i = 0 ; i < N ; i++) {\n      for(j = 0 ; j < N ; j++) {\n        printf(\"%f \", A[i][j]);\n      }\n      printf(\"\\n\");\n    }\n    printf(\"Matrice B :\\n\");\n    for(i = 0 ; i < N ; i++) {\n      for(j = 0 ; j < N ; j++) {\n        printf(\"%f \", B[i][j]);\n      }\n      printf(\"\\n\");\n    }\n  }\n\n  if(rank != 0){\n    MPI_Recv(B, N*N, MPI_FLOAT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &stat);\n    MPI_Recv(A+i*(N/size), N*(N/size), MPI_FLOAT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &stat);\n  }\n\n\n  for(i = 0 ; i < N/size ; i++) {\n    for(j = 0 ; j < N ; j++) {\n      float res_ij;\n      res_ij = 0;\n      for(k = 0 ; k < N ; k++) {\n        res_ij += A[i*(N/size)][k] * B[k][j];\n      }\n      C[rank*(N/size)+i][j] = res_ij;\n    }\n  }\n\n  if(rank != 0) {\n    MPI_Send(C + rank*(N/size), N*(N/size), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if(rank == 0) {\n    for(i = 1; i < size ; i++) {\n      MPI_Recv(C + i*(N/size), N*(N/size), MPI_FLOAT, i, MPI_ANY_TAG, MPI_COMM_WORLD, &stat);\n    }\n\n    \n\n    printf(\"Matrice C :\\n\");\n    for(i = 0 ; i < N ; i++) {\n      for(j = 0 ; j < N ; j++) {\n        printf(\"%f \", C[i][j]);\n      }\n      printf(\"\\n\");\n    }\n  }\n\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "qingu_1463", "code": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n\n    parse_args(argc, argv);\n\n    \n\n\n    \n\n    err = blockindexed_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in blockindexed test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int err, errs = 0;\n\n    MPI_Init(&argc, &argv); \n\n    parse_args(argc, argv);\n\n    \n\n    MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );\n\n    \n\n    err = blockindexed_test();\n    if (err && verbose) fprintf(stderr, \"%d errors in blockindexed test.\\n\",\n\t\t\t\terr);\n    errs += err;\n\n    \n\n    if (errs) {\n\tfprintf(stderr, \"Found %d errors\\n\", errs);\n    }\n    else {\n\tprintf(\" No Errors\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "m45t3r_1464", "code": "int main(int argc, char **argv) {\n\n  void mpifft(double *x, int n, int p, int s, int sign, double *w0, double *w,\n              double *tw, int *rho_np, int *rho_p);\n  void mpifft_init(int n, int p, int s, double *w0, double *w, double *tw,\n                   int *rho_np, int *rho_p);\n  int k1_init(int n, int p);\n\n  int p, s, n, np, k1, j, jglob, it, *rho_np, *rho_p;\n  double time0, time1, time2, ffttime, nflops, max_error, max_error_glob,\n      error_re, error_im, error, *x, *w0, *w, *tw;\n\n\n\n  if (s == 0) {\n    printf(\"Please enter length n: \\n\");\n    scanf(\"%d\", &n);\n    if (n < 2 * p)\n  }\n\n\n  if (s == 0) {\n    printf(\"FFT of vector of length %d using %d processors\\n\", n, p);\n    printf(\"performing %d forward and %d backward transforms\\n\", NITERS,\n           NITERS);\n  }\n\n  \n\n  np = n / p;\n  x = vecallocd(2 * np);\n  k1 = k1_init(n, p);\n  w0 = vecallocd(k1);\n  w = vecallocd(np);\n  tw = vecallocd(2 * np + p);\n  rho_np = vecalloci(np);\n  rho_p = vecalloci(p);\n\n  for (j = 0; j < np; j++) {\n    jglob = j * p + s;\n    x[2 * j] = (double)jglob;\n    x[2 * j + 1] = 1.0;\n  }\n  time0 =\n\n  \n\n  for (it = 0; it < NITERS; it++)\n    mpifft_init(n, p, s, w0, w, tw, rho_np, rho_p);\n  time1 =\n\n  \n\n  for (it = 0; it < NITERS; it++) {\n    mpifft(x, n, p, s, 1, w0, w, tw, rho_np, rho_p);\n    mpifft(x, n, p, s, -1, w0, w, tw, rho_np, rho_p);\n  }\n  time2 =\n\n  \n\n  max_error = 0.0;\n  for (j = 0; j < np; j++) {\n    jglob = j * p + s;\n    error_re = fabs(x[2 * j] - (double)jglob);\n    error_im = fabs(x[2 * j + 1] - 1.0);\n    error = sqrt(error_re * error_re + error_im * error_im);\n    if (error > max_error)\n      max_error = error;\n  }\n\n  for (j = 0; j < NPRINT && j < np; j++) {\n    jglob = j * p + s;\n    printf(\"proc=%d j=%d Re= %f Im= %f \\n\", s, jglob, x[2 * j], x[2 * j + 1]);\n  }\n  fflush(stdout);\n\n  if (s == 0) {\n    printf(\"Time per initialization = %lf sec \\n\", (time1 - time0) / NITERS);\n    ffttime = (time2 - time1) / (2.0 * NITERS);\n    printf(\"Time per FFT = %lf sec \\n\", ffttime);\n    nflops = 5 * n * log((double)n) / log(2.0) + 2 * n;\n    printf(\"Computing rate in FFT = %lf Mflop/s \\n\", nflops / (MEGA * ffttime));\n    printf(\"Absolute error= %e \\n\", max_error_glob);\n    printf(\"Relative error= %e \\n\\n\", max_error_glob / n);\n  }\n\n  vecfreei(rho_p);\n  vecfreei(rho_np);\n  vecfreed(tw);\n  vecfreed(w);\n  vecfreed(w0);\n  vecfreed(x);\n\n  exit(0);\n\n}", "label": "int main(int argc, char **argv) {\n\n  void mpifft(double *x, int n, int p, int s, int sign, double *w0, double *w,\n              double *tw, int *rho_np, int *rho_p);\n  void mpifft_init(int n, int p, int s, double *w0, double *w, double *tw,\n                   int *rho_np, int *rho_p);\n  int k1_init(int n, int p);\n\n  int p, s, n, np, k1, j, jglob, it, *rho_np, *rho_p;\n  double time0, time1, time2, ffttime, nflops, max_error, max_error_glob,\n      error_re, error_im, error, *x, *w0, *w, *tw;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n  MPI_Comm_rank(MPI_COMM_WORLD, &s);\n\n  if (s == 0) {\n    printf(\"Please enter length n: \\n\");\n    scanf(\"%d\", &n);\n    if (n < 2 * p)\n      MPI_Abort(MPI_COMM_WORLD, -7);\n  }\n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (s == 0) {\n    printf(\"FFT of vector of length %d using %d processors\\n\", n, p);\n    printf(\"performing %d forward and %d backward transforms\\n\", NITERS,\n           NITERS);\n  }\n\n  \n\n  np = n / p;\n  x = vecallocd(2 * np);\n  k1 = k1_init(n, p);\n  w0 = vecallocd(k1);\n  w = vecallocd(np);\n  tw = vecallocd(2 * np + p);\n  rho_np = vecalloci(np);\n  rho_p = vecalloci(p);\n\n  for (j = 0; j < np; j++) {\n    jglob = j * p + s;\n    x[2 * j] = (double)jglob;\n    x[2 * j + 1] = 1.0;\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  time0 = MPI_Wtime();\n\n  \n\n  for (it = 0; it < NITERS; it++)\n    mpifft_init(n, p, s, w0, w, tw, rho_np, rho_p);\n  MPI_Barrier(MPI_COMM_WORLD);\n  time1 = MPI_Wtime();\n\n  \n\n  for (it = 0; it < NITERS; it++) {\n    mpifft(x, n, p, s, 1, w0, w, tw, rho_np, rho_p);\n    mpifft(x, n, p, s, -1, w0, w, tw, rho_np, rho_p);\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  time2 = MPI_Wtime();\n\n  \n\n  max_error = 0.0;\n  for (j = 0; j < np; j++) {\n    jglob = j * p + s;\n    error_re = fabs(x[2 * j] - (double)jglob);\n    error_im = fabs(x[2 * j + 1] - 1.0);\n    error = sqrt(error_re * error_re + error_im * error_im);\n    if (error > max_error)\n      max_error = error;\n  }\n  MPI_Reduce(&max_error, &max_error_glob, 1, MPI_DOUBLE, MPI_MAX, 0,\n             MPI_COMM_WORLD);\n\n  for (j = 0; j < NPRINT && j < np; j++) {\n    jglob = j * p + s;\n    printf(\"proc=%d j=%d Re= %f Im= %f \\n\", s, jglob, x[2 * j], x[2 * j + 1]);\n  }\n  fflush(stdout);\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (s == 0) {\n    printf(\"Time per initialization = %lf sec \\n\", (time1 - time0) / NITERS);\n    ffttime = (time2 - time1) / (2.0 * NITERS);\n    printf(\"Time per FFT = %lf sec \\n\", ffttime);\n    nflops = 5 * n * log((double)n) / log(2.0) + 2 * n;\n    printf(\"Computing rate in FFT = %lf Mflop/s \\n\", nflops / (MEGA * ffttime));\n    printf(\"Absolute error= %e \\n\", max_error_glob);\n    printf(\"Relative error= %e \\n\\n\", max_error_glob / n);\n  }\n\n  vecfreei(rho_p);\n  vecfreei(rho_np);\n  vecfreed(tw);\n  vecfreed(w);\n  vecfreed(w0);\n  vecfreed(x);\n\n  MPI_Finalize();\n  exit(0);\n\n}"}
{"program": "edosedgar_1465", "code": "int main(int argc, char* argv[]) {\n        int rank, size;\n        int buf = -1;\n        MPI_Status status;\n\n        while (1) {\n                if (rank != 0 || buf != -1) {\n                        int adr = (rank - 1 == -1) ? (size - 1) : rank - 1;\n                        fprintf(stderr, \"Process %d received <msg> \"\n                                        \"from process %d\\n\", rank, adr);\n                        usleep(10000);\n                }\n                if (++buf == N * size && rank == 0)\n                        break;\n                fprintf(stderr, \"Process %d send <msg> to process %d\\n\", rank,\n                                (rank + 1)%(size));\n                usleep(10000);\n                if (rank != 0 && buf == (N - 1) * size + rank)\n                        break;\n        }\n        return 0;\n}", "label": "int main(int argc, char* argv[]) {\n        int rank, size;\n        int buf = -1;\n        MPI_Status status;\n\n        MPI_Init(&argc, &argv);\n        MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n        MPI_Comm_size( MPI_COMM_WORLD, &size );\n        while (1) {\n                if (rank != 0 || buf != -1) {\n                        int adr = (rank - 1 == -1) ? (size - 1) : rank - 1;\n                        MPI_Recv(&buf, 1, MPI_INT, adr, adr, MPI_COMM_WORLD,\n                                 &status);\n                        fprintf(stderr, \"Process %d received <msg> \"\n                                        \"from process %d\\n\", rank, adr);\n                        usleep(10000);\n                }\n                if (++buf == N * size && rank == 0)\n                        break;\n                fprintf(stderr, \"Process %d send <msg> to process %d\\n\", rank,\n                                (rank + 1)%(size));\n                usleep(10000);\n                MPI_Send(&buf, 1, MPI_INT, (rank + 1)%(size), rank,\n                         MPI_COMM_WORLD);\n                if (rank != 0 && buf == (N - 1) * size + rank)\n                        break;\n        }\n        MPI_Finalize();\n        return 0;\n}"}
{"program": "ankrt_1466", "code": "int main(int argc, char **argv)\n{\n        int rc, numprocs, myrank, namelen;\n        char name[MPI_MAX_PROCESSOR_NAME];\n\n        rc =\n        if (rc != MPI_SUCCESS) {\n                fprintf(stderr, \"Error starting MPI program\\n\");\n                exit(1);\n        }\n\n        int size, precision, arraylen;\n        double *full_array;\n        double *source_array;\n        double *result_array;\n        int *send_count = malloc(numprocs * sizeof(int));\n        int *recv_count = malloc(numprocs * sizeof(int));\n        int *send_displ = malloc(numprocs * sizeof(int));\n        int *recv_displ = malloc(numprocs * sizeof(int));\n        int local_complete = 0;\n        int global_complete = 0;\n        int iterations = 0;\n\n        handle_args(argc, argv, &size, &precision, &arraylen);\n        get_indexes(size, numprocs, send_count, send_displ,\n                        recv_count, recv_displ);\n        if (myrank == 0) {\n                full_array = malloc(arraylen * sizeof(double));\n                generate_array(arraylen, 1, full_array);\n                \n\n                \n\n        } else {\n                full_array = NULL;\n        }\n\n        source_array = malloc(send_count[myrank] * sizeof(double));\n        result_array = malloc(send_count[myrank] * sizeof(double));\n\n\n        \n\n        memcpy(result_array, source_array,\n                        (send_count[myrank] * sizeof(double)));\n\n        double tolerance = (double) 1 / precision;\n        do {\n                local_complete = relax(source_array, result_array,\n                                send_count[myrank], size, tolerance);\n                swap(&source_array, &result_array);\n                if (numprocs > 1) {\n                        send_receive(source_array, send_count, myrank, size,\n                                        numprocs);\n                }\n                iterations++;\n        } while (global_complete == 0);\n\n\n        \n\n        int offset;\n        if (myrank == 0) {\n                offset = 0;\n        } else {\n                offset = size;\n        }\n\n        \n\n        if (myrank == 0) printf(\"%d\\n\", iterations);\n\n        free(source_array);\n        free(result_array);\n        free(send_count);\n        free(recv_count);\n        free(send_displ);\n        free(recv_displ);\n        free(full_array);\n\n\n        return 0;\n}", "label": "int main(int argc, char **argv)\n{\n        int rc, numprocs, myrank, namelen;\n        char name[MPI_MAX_PROCESSOR_NAME];\n\n        rc = MPI_Init(&argc, &argv);\n        if (rc != MPI_SUCCESS) {\n                fprintf(stderr, \"Error starting MPI program\\n\");\n                MPI_Abort(MPI_COMM_WORLD, rc);\n                exit(1);\n        }\n        MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n        MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n        MPI_Get_processor_name(name, &namelen);\n\n        int size, precision, arraylen;\n        double *full_array;\n        double *source_array;\n        double *result_array;\n        int *send_count = malloc(numprocs * sizeof(int));\n        int *recv_count = malloc(numprocs * sizeof(int));\n        int *send_displ = malloc(numprocs * sizeof(int));\n        int *recv_displ = malloc(numprocs * sizeof(int));\n        int local_complete = 0;\n        int global_complete = 0;\n        int iterations = 0;\n\n        handle_args(argc, argv, &size, &precision, &arraylen);\n        get_indexes(size, numprocs, send_count, send_displ,\n                        recv_count, recv_displ);\n        if (myrank == 0) {\n                full_array = malloc(arraylen * sizeof(double));\n                generate_array(arraylen, 1, full_array);\n                \n\n                \n\n        } else {\n                full_array = NULL;\n        }\n\n        source_array = malloc(send_count[myrank] * sizeof(double));\n        result_array = malloc(send_count[myrank] * sizeof(double));\n\n        MPI_Scatterv(full_array, send_count, send_displ, MPI_DOUBLE,\n                        &source_array[0], arraylen, MPI_DOUBLE, 0,\n                        MPI_COMM_WORLD);\n\n        \n\n        memcpy(result_array, source_array,\n                        (send_count[myrank] * sizeof(double)));\n\n        double tolerance = (double) 1 / precision;\n        do {\n                local_complete = relax(source_array, result_array,\n                                send_count[myrank], size, tolerance);\n                swap(&source_array, &result_array);\n                if (numprocs > 1) {\n                        send_receive(source_array, send_count, myrank, size,\n                                        numprocs);\n                }\n                MPI_Allreduce(&local_complete, &global_complete, 1, MPI_INT,\n                                MPI_LAND, MPI_COMM_WORLD);\n                iterations++;\n        } while (global_complete == 0);\n\n\n        \n\n        int offset;\n        if (myrank == 0) {\n                offset = 0;\n        } else {\n                offset = size;\n        }\n        MPI_Gatherv(&source_array[offset], recv_count[myrank], MPI_DOUBLE,\n                        full_array, recv_count, recv_displ, MPI_DOUBLE, 0,\n                        MPI_COMM_WORLD);\n\n        \n\n        if (myrank == 0) printf(\"%d\\n\", iterations);\n\n        free(source_array);\n        free(result_array);\n        free(send_count);\n        free(recv_count);\n        free(send_displ);\n        free(recv_displ);\n        free(full_array);\n\n        MPI_Finalize();\n\n        return 0;\n}"}
{"program": "UnProgrammatore_1467", "code": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tlong long the_number;\n\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t}\n\telse\n\t\tthe_number = atoll(argv[1]);\n\n\tif(my_rank == 0)\n\t\tmaster_procedure(comm_size);\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\treturn 0;\n}", "label": "int main(int argc, char** argv) {\n\tint my_rank, comm_size;\n\tlong long the_number;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tif(argc <= 1) {\n\t\tfprintf(stderr, \"Missing number as argument\");\n\t\tMPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\telse\n\t\tthe_number = atoll(argv[1]);\n\n\tif(my_rank == 0)\n\t\tmaster_procedure(comm_size);\n\telse\n\t\tslave_procedure(my_rank, comm_size, the_number);\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "qingu_1469", "code": "int main( int argc, char *argv[] )\n{\n    int n, stride, err, errs = 0;\n    void *dest, *src;\n    double avgTimeUser, avgTimeMPI;\n\n    if (getenv(\"MPITEST_VERBOSE\")) verbose = 1;\n\n    n      = 30000;\n    stride = 4;\n    dest = (void *)malloc( n * sizeof(double) );\n    src  = (void *)malloc( n * ((1+stride)*sizeof(double)) );\n    \n\n    memset( src, 0, n * (1+stride)*sizeof(double) );\n    memset( dest, 0, n * sizeof(double) );\n\n    err = TestVecPackDouble( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t     dest, src );\n    errs += Report( \"VecPackDouble\", \"Pack\", avgTimeMPI, avgTimeUser );\n\n    err = TestVecUnPackDouble( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t       src, dest );\n    errs += Report( \"VecUnPackDouble\", \"Unpack\", avgTimeMPI, avgTimeUser );\n\n    err = TestIndexPackDouble( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t     dest, src );\n    errs += Report( \"VecIndexDouble\", \"Pack\", avgTimeMPI, avgTimeUser );\n\n    free(dest);\n    free(src);\n    \n    dest = (void *)malloc( 2*n * sizeof(double) );\n    src  = (void *)malloc( (1 + n) * ((1+stride)*sizeof(double)) );\n    memset( dest, 0, 2*n * sizeof(double) );\n    memset( src, 0, (1+n) * (1+stride)*sizeof(double) );\n    err = TestVecPack2Double( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t      dest, src );\n    errs += Report( \"VecPack2Double\", \"Pack\", avgTimeMPI, avgTimeUser );\n\n    free(dest);\n    free(src);\n    \n\n\n    if (errs == 0) {\n\tprintf( \" No Errors\\n\" );\n    }\n    else {\n\tprintf( \" Found %d performance problems\\n\", errs );\n    }\n\n    fflush(stdout);\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int n, stride, err, errs = 0;\n    void *dest, *src;\n    double avgTimeUser, avgTimeMPI;\n\n    MPI_Init( &argc, &argv );\n    if (getenv(\"MPITEST_VERBOSE\")) verbose = 1;\n\n    n      = 30000;\n    stride = 4;\n    dest = (void *)malloc( n * sizeof(double) );\n    src  = (void *)malloc( n * ((1+stride)*sizeof(double)) );\n    \n\n    memset( src, 0, n * (1+stride)*sizeof(double) );\n    memset( dest, 0, n * sizeof(double) );\n\n    err = TestVecPackDouble( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t     dest, src );\n    errs += Report( \"VecPackDouble\", \"Pack\", avgTimeMPI, avgTimeUser );\n\n    err = TestVecUnPackDouble( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t       src, dest );\n    errs += Report( \"VecUnPackDouble\", \"Unpack\", avgTimeMPI, avgTimeUser );\n\n    err = TestIndexPackDouble( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t     dest, src );\n    errs += Report( \"VecIndexDouble\", \"Pack\", avgTimeMPI, avgTimeUser );\n\n    free(dest);\n    free(src);\n    \n    dest = (void *)malloc( 2*n * sizeof(double) );\n    src  = (void *)malloc( (1 + n) * ((1+stride)*sizeof(double)) );\n    memset( dest, 0, 2*n * sizeof(double) );\n    memset( src, 0, (1+n) * (1+stride)*sizeof(double) );\n    err = TestVecPack2Double( n, stride, &avgTimeUser, &avgTimeMPI,\n\t\t\t      dest, src );\n    errs += Report( \"VecPack2Double\", \"Pack\", avgTimeMPI, avgTimeUser );\n\n    free(dest);\n    free(src);\n    \n\n\n    if (errs == 0) {\n\tprintf( \" No Errors\\n\" );\n    }\n    else {\n\tprintf( \" Found %d performance problems\\n\", errs );\n    }\n\n    fflush(stdout);\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "xyuan_1470", "code": "int\nmain (int argc, char **argv)\n{\n  int                 mpirank, mpisize;\n  int                 mpiret;\n  MPI_Comm            mpicomm;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n#ifdef P4_TO_P8\n  connectivity = p8est_connectivity_new_rotcubes ();\n#else\n  connectivity = p4est_connectivity_new_star ();\n#endif\n  p4est = p4est_new_ext (mpicomm, connectivity, 15, 0, 0, 0, NULL, NULL);\n  p4est_refine (p4est, 1, test_refine, NULL);\n  p4est_balance (p4est, P4EST_CONNECT_FULL, NULL);\n\n  coarsen_all = 1;\n  p4est_coarsen_both (p4est, 0, test_coarsen, NULL);\n  coarsen_all = 0;\n  p4est_coarsen_both (p4est, 1, test_coarsen, NULL);\n  p4est_balance (p4est, P4EST_CONNECT_FULL, NULL);\n  coarsen_all = 1;\n  p4est_coarsen_both (p4est, 1, test_coarsen, NULL);\n  p4est_vtk_write_file (p4est, NULL, P4EST_STRING \"_endcoarsen\");\n\n  if (mpisize == 1) {\n    SC_CHECK_ABORT (p4est->global_num_quadrants ==\n                    (p4est_gloidx_t) connectivity->num_trees, \"Coarsen\");\n  }\n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n  sc_finalize ();\n\n  mpiret =\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}", "label": "int\nmain (int argc, char **argv)\n{\n  int                 mpirank, mpisize;\n  int                 mpiret;\n  MPI_Comm            mpicomm;\n  p4est_t            *p4est;\n  p4est_connectivity_t *connectivity;\n\n  mpiret = MPI_Init (&argc, &argv);\n  SC_CHECK_MPI (mpiret);\n  mpicomm = MPI_COMM_WORLD;\n  mpiret = MPI_Comm_size (mpicomm, &mpisize);\n  SC_CHECK_MPI (mpiret);\n  mpiret = MPI_Comm_rank (mpicomm, &mpirank);\n  SC_CHECK_MPI (mpiret);\n\n  sc_init (mpicomm, 1, 1, NULL, SC_LP_DEFAULT);\n  p4est_init (NULL, SC_LP_DEFAULT);\n\n  \n\n#ifdef P4_TO_P8\n  connectivity = p8est_connectivity_new_rotcubes ();\n#else\n  connectivity = p4est_connectivity_new_star ();\n#endif\n  p4est = p4est_new_ext (mpicomm, connectivity, 15, 0, 0, 0, NULL, NULL);\n  p4est_refine (p4est, 1, test_refine, NULL);\n  p4est_balance (p4est, P4EST_CONNECT_FULL, NULL);\n\n  coarsen_all = 1;\n  p4est_coarsen_both (p4est, 0, test_coarsen, NULL);\n  coarsen_all = 0;\n  p4est_coarsen_both (p4est, 1, test_coarsen, NULL);\n  p4est_balance (p4est, P4EST_CONNECT_FULL, NULL);\n  coarsen_all = 1;\n  p4est_coarsen_both (p4est, 1, test_coarsen, NULL);\n  p4est_vtk_write_file (p4est, NULL, P4EST_STRING \"_endcoarsen\");\n\n  if (mpisize == 1) {\n    SC_CHECK_ABORT (p4est->global_num_quadrants ==\n                    (p4est_gloidx_t) connectivity->num_trees, \"Coarsen\");\n  }\n\n  p4est_destroy (p4est);\n  p4est_connectivity_destroy (connectivity);\n  sc_finalize ();\n\n  mpiret = MPI_Finalize ();\n  SC_CHECK_MPI (mpiret);\n\n  return 0;\n}"}
{"program": "callmetaste_1472", "code": "int main ( int argc, char *argv[] )\n{\n   int np,myid,dim,nspt,mysolnb,*mysol;\n   double startwtime,endwtime,wtime,*time;\n   MPI_Status status;\n\n   adainit();\n   \n   if(myid == 0)\n   {\n      time = (double*)calloc(np,sizeof(double));\n      mysol = (int*)calloc(np,sizeof(int));\n      startwtime =\n   }\n   else\n      startwtime =\n \n   retrieve_dimensions(myid,&nspt,&dim);\n   supports_broadcast(myid,nspt,dim);\n   system_broadcast(myid,dim-1);\n   distribute_cells(myid,np,nspt,dim,&mysolnb);\n   \n   endwtime =\n   wtime = endwtime-startwtime;\n   if(myid == 0)\n   {\n      printf(\"\\nTotal wall time = %lf seconds on %d processors\\n\",\n             time[0],np);\n      write_time_and_paths_to_defined_output_file(np,time,mysol);\n      free(time); free(mysol);\n   }\n    \n   adafinal();\n   return 0;\n}", "label": "int main ( int argc, char *argv[] )\n{\n   int np,myid,dim,nspt,mysolnb,*mysol;\n   double startwtime,endwtime,wtime,*time;\n   MPI_Status status;\n\n   adainit();\n   MPI_Init(&argc,&argv);\n   MPI_Comm_size(MPI_COMM_WORLD,&np);\n   MPI_Comm_rank(MPI_COMM_WORLD,&myid);\n   \n   if(myid == 0)\n   {\n      time = (double*)calloc(np,sizeof(double));\n      mysol = (int*)calloc(np,sizeof(int));\n      startwtime = MPI_Wtime();\n   }\n   else\n      startwtime = MPI_Wtime();\n \n   retrieve_dimensions(myid,&nspt,&dim);\n   supports_broadcast(myid,nspt,dim);\n   system_broadcast(myid,dim-1);\n   distribute_cells(myid,np,nspt,dim,&mysolnb);\n   \n   endwtime = MPI_Wtime();\n   wtime = endwtime-startwtime;\n   MPI_Gather(&wtime,1,MPI_DOUBLE,time,1,MPI_DOUBLE,0,MPI_COMM_WORLD);\n   MPI_Gather(&mysolnb,1,MPI_INT,mysol,1,MPI_INT,0,MPI_COMM_WORLD);\n   if(myid == 0)\n   {\n      printf(\"\\nTotal wall time = %lf seconds on %d processors\\n\",\n             time[0],np);\n      write_time_and_paths_to_defined_output_file(np,time,mysol);\n      free(time); free(mysol);\n   }\n    \n   MPI_Finalize();\n   adafinal();\n   return 0;\n}"}
{"program": "keremsahin1_1473", "code": "int main(int iArgCnt, char* sArrArgs[])\n{\n   int iIterationNo = 0;\n   int iArrayIndex = 0;\n   double dSubResult = 0, dTotalResult = 0;\n   double dTime0 = 0, dTime1 = 0, dTimeDiff = 0, dMinTimeDiff = 1000, dMaxTimeDiff = 0;\n\n   parseInputs(iArgCnt, sArrArgs);\n\n\n   initArrays();\n\n   for(iIterationNo = 0; iIterationNo < GiIterationCnt; iIterationNo++)\n   {\n      dSubResult = 0;\n\n      dTime0 =\n\n      for(iArrayIndex = 0; iArrayIndex < GiSubVectorLength; iArrayIndex++)\n      {\n         dSubResult += (GdArrSubX[iArrayIndex] * GdArrSubY[iArrayIndex]);\n      }\n\n\n      dTime1 =\n      dTimeDiff = (dTime1 - dTime0);\n\n      if(dTimeDiff > dMaxTimeDiff)\n         dMaxTimeDiff = dTimeDiff;\n      if(dTimeDiff < dMinTimeDiff)\n         dMinTimeDiff = dTimeDiff;\n   }\n\n   if(GiProcessRank == 0)\n      printf(\"Result=%f\\nMin Time=%f uSec\\nMax Time=%f uSec\\n\", dTotalResult, (1.e6 * dMinTimeDiff), (1.e6 * dMaxTimeDiff));\n   \n\n   return 0;\n}", "label": "int main(int iArgCnt, char* sArrArgs[])\n{\n   int iIterationNo = 0;\n   int iArrayIndex = 0;\n   double dSubResult = 0, dTotalResult = 0;\n   double dTime0 = 0, dTime1 = 0, dTimeDiff = 0, dMinTimeDiff = 1000, dMaxTimeDiff = 0;\n\n   parseInputs(iArgCnt, sArrArgs);\n\n   MPI_Init(&iArgCnt, &sArrArgs);\n   MPI_Comm_size(MPI_COMM_WORLD, &GiProcessCnt);\n   MPI_Comm_rank(MPI_COMM_WORLD, &GiProcessRank);\n\n   initArrays();\n\n   for(iIterationNo = 0; iIterationNo < GiIterationCnt; iIterationNo++)\n   {\n      dSubResult = 0;\n\n      MPI_Barrier(MPI_COMM_WORLD);\n      dTime0 = MPI_Wtime();\n\n      for(iArrayIndex = 0; iArrayIndex < GiSubVectorLength; iArrayIndex++)\n      {\n         dSubResult += (GdArrSubX[iArrayIndex] * GdArrSubY[iArrayIndex]);\n      }\n\n      MPI_Barrier(MPI_COMM_WORLD);\n      MPI_Reduce(&dSubResult, &dTotalResult, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n      dTime1 = MPI_Wtime();\n      dTimeDiff = (dTime1 - dTime0);\n\n      if(dTimeDiff > dMaxTimeDiff)\n         dMaxTimeDiff = dTimeDiff;\n      if(dTimeDiff < dMinTimeDiff)\n         dMinTimeDiff = dTimeDiff;\n   }\n\n   if(GiProcessRank == 0)\n      printf(\"Result=%f\\nMin Time=%f uSec\\nMax Time=%f uSec\\n\", dTotalResult, (1.e6 * dMinTimeDiff), (1.e6 * dMaxTimeDiff));\n   \n   MPI_Finalize();\n\n   return 0;\n}"}
{"program": "nschloe_1474", "code": "main(int argc, char *argv[])\n{\n  size_t beginning = get_heap_usage();\n\n  \n\n  \n\n  size_t firstiterbef = get_heap_usage();\n  test_function();\n  size_t firstiteraft = get_heap_usage();\n\n  size_t initheap = get_heap_usage();\n  for (int i = 0; i < NUM_ITER; i++) test_function();\n  size_t finalheap = get_heap_usage();\n\n  int myproc;\n\n  int localmax, globalmax;\n  localmax = total_leak;\n\n  size_t ending = get_heap_usage();\n\n  std::cout << \"KDDEND \" << myproc \n            << \" First MPI_Comm_dup leaked \" << firstiteraft - firstiterbef\n            << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Subsequent MPI_Comm_dups leaked (total) \"             << finalheap - initheap << \" = \" << total_leak << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Avg per Subsequent MPI_Comm_dup \" \n            << (finalheap - initheap) / NUM_ITER\n            << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Max per Subsequent MPI_Comm_dup \" \n            << globalmax\n            << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Total Leak \" \n            << (ending - beginning)\n            << std::endl;\n\n  return(0);  \n}", "label": "main(int argc, char *argv[])\n{\n  size_t beginning = get_heap_usage();\n  MPI_Init(&argc, &argv);\n\n  \n\n  \n\n  size_t firstiterbef = get_heap_usage();\n  test_function();\n  size_t firstiteraft = get_heap_usage();\n\n  size_t initheap = get_heap_usage();\n  for (int i = 0; i < NUM_ITER; i++) test_function();\n  size_t finalheap = get_heap_usage();\n\n  int myproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myproc);\n\n  int localmax, globalmax;\n  localmax = total_leak;\n  MPI_Allreduce(&localmax, &globalmax, 1, MPI_INTEGER, MPI_MAX, MPI_COMM_WORLD);\n\n  MPI_Finalize();\n  size_t ending = get_heap_usage();\n\n  std::cout << \"KDDEND \" << myproc \n            << \" First MPI_Comm_dup leaked \" << firstiteraft - firstiterbef\n            << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Subsequent MPI_Comm_dups leaked (total) \" \n            << finalheap - initheap << \" = \" << total_leak << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Avg per Subsequent MPI_Comm_dup \" \n            << (finalheap - initheap) / NUM_ITER\n            << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Max per Subsequent MPI_Comm_dup \" \n            << globalmax\n            << std::endl;\n  std::cout << \"KDDEND \" << myproc \n            << \" Total Leak \" \n            << (ending - beginning)\n            << std::endl;\n\n  return(0);  \n}"}
{"program": "Marcus-Rosti_1475", "code": "int main(int argc, char **argv) {\n    \n\n    \n\n    int rank, nprocs;\n    if (argc < 2)\n        die(\"ERROR: Must provide 2 arguments\\n\\tmpirun -n {num procs} EXEC\"\n                    \" {number of cities} {distance file}\\n\");\n    const int num_of_cities = atoi(argv[1]);\n    char *file_location = argv[2];\n    \n\n    \n\n\n    \n\n    \n\n    int **cityDistances = allocate_cells(num_of_cities, num_of_cities);\n    initialize_city_distances(file_location, cityDistances, num_of_cities);\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    time_t start_time = time(NULL);\n    if (rank == 0) \n\n        master(cityDistances, num_of_cities, nprocs, 10);\n    else \n\n        slave(cityDistances, num_of_cities, rank, nprocs, 10);\n    if (rank == 0) printf(\"Time to calc: %li\\n\", time(NULL) - start_time);\n    \n\n\n\n}", "label": "int main(int argc, char **argv) {\n    \n\n    \n\n    MPI_Init(&argc, &argv);\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    if (argc < 2)\n        die(\"ERROR: Must provide 2 arguments\\n\\tmpirun -n {num procs} EXEC\"\n                    \" {number of cities} {distance file}\\n\");\n    const int num_of_cities = atoi(argv[1]);\n    char *file_location = argv[2];\n    \n\n    \n\n\n    \n\n    \n\n    int **cityDistances = allocate_cells(num_of_cities, num_of_cities);\n    initialize_city_distances(file_location, cityDistances, num_of_cities);\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    time_t start_time = time(NULL);\n    if (rank == 0) \n\n        master(cityDistances, num_of_cities, nprocs, 10);\n    else \n\n        slave(cityDistances, num_of_cities, rank, nprocs, 10);\n    if (rank == 0) printf(\"Time to calc: %li\\n\", time(NULL) - start_time);\n    \n\n\n    MPI_Finalize(); \n\n}"}
{"program": "syftalent_1476", "code": "int main(int argc, char **argv)\n{\n    int rank, size, i, rc, errclass, toterrs, errs = 0;\n    char rbuf[100000];\n    char *sendbuf;\n    int deadprocs[1] = {1};\n    MPI_Group world, newgroup;\n    MPI_Comm newcomm;\n\n\n    if (size < 3) {\n        fprintf( stderr, \"Must run with at least 3 processes\\n\" );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    \n\n    sendbuf = (char *)malloc(10*size*sizeof(char));\n\n    if (rank == 0) {\n      for (i=0;i<size;i++) {\n          strcpy(sendbuf + i*10, \"No Errors\");\n      }\n    }\n\n    rc =\n\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n    if ((rc) && (errclass != MPIX_ERR_PROC_FAILED)) {\n        fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        errs++;\n    }\n#endif\n\n    \n\n    free(sendbuf);\n    memset(rbuf, 0, sizeof(rbuf));\n    sendbuf = (char *)malloc(100000*size*sizeof(char));\n\n    if (rank == 0) {\n      for (i=0;i<size;i++) {\n          strcpy(sendbuf + i*100000, \"No Errors\");\n      }\n    }\n\n    rc =\n\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n    if ((rc) && (errclass != MPIX_ERR_PROC_FAILED)) {\n        fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        errs++;\n    }\n#endif\n\n\n    rc =\n    if(rc)\n        fprintf(stderr, \"Failed to get errors from other processes\\n\");\n\n    if (rank == 0) {\n        if (toterrs) {\n            printf( \" Found %d errors\\n\", toterrs );\n        }\n        else {\n            printf( \" No Errors\\n\" );\n        }\n        fflush(stdout);\n    }\n\n    free(sendbuf);\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int rank, size, i, rc, errclass, toterrs, errs = 0;\n    char rbuf[100000];\n    char *sendbuf;\n    int deadprocs[1] = {1};\n    MPI_Group world, newgroup;\n    MPI_Comm newcomm;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);\n\n    if (size < 3) {\n        fprintf( stderr, \"Must run with at least 3 processes\\n\" );\n        MPI_Abort( MPI_COMM_WORLD, 1 );\n    }\n\n    if (rank == 1) {\n        exit(EXIT_FAILURE);\n    }\n\n    \n\n    sendbuf = (char *)malloc(10*size*sizeof(char));\n\n    if (rank == 0) {\n      for (i=0;i<size;i++) {\n          strcpy(sendbuf + i*10, \"No Errors\");\n      }\n    }\n\n    rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n    MPI_Error_class(rc, &errclass);\n    if ((rc) && (errclass != MPIX_ERR_PROC_FAILED)) {\n        fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        errs++;\n    }\n#endif\n\n    \n\n    free(sendbuf);\n    memset(rbuf, 0, sizeof(rbuf));\n    sendbuf = (char *)malloc(100000*size*sizeof(char));\n\n    if (rank == 0) {\n      for (i=0;i<size;i++) {\n          strcpy(sendbuf + i*100000, \"No Errors\");\n      }\n    }\n\n    rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n#if defined (MPICH) && (MPICH_NUMVERSION >= 30100102)\n    MPI_Error_class(rc, &errclass);\n    if ((rc) && (errclass != MPIX_ERR_PROC_FAILED)) {\n        fprintf(stderr, \"Wrong error code (%d) returned. Expected MPIX_ERR_PROC_FAILED\\n\", errclass);\n        errs++;\n    }\n#endif\n\n    MPI_Comm_group(MPI_COMM_WORLD, &world);\n    MPI_Group_excl(world, 1, deadprocs, &newgroup);\n    MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);\n\n    rc = MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, newcomm);\n    if(rc)\n        fprintf(stderr, \"Failed to get errors from other processes\\n\");\n\n    if (rank == 0) {\n        if (toterrs) {\n            printf( \" Found %d errors\\n\", toterrs );\n        }\n        else {\n            printf( \" No Errors\\n\" );\n        }\n        fflush(stdout);\n    }\n\n    free(sendbuf);\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "chasingegg_1479", "code": "int main(int argc, char* argv[]) {\r\n   int *loc_mat;\r\n   int n, loc_n, p, my_rank;\r\n   MPI_Comm comm;\r\n   MPI_Datatype blk_col_mpi_t;\r\n\r\n#  ifdef DEBUG\r\n   int i, j;\r\n#  endif\r\n\r\n   comm = MPI_COMM_WORLD;\r\n   \r\n   \n\n   \r\n   n = Read_n(my_rank, comm);\r\n   loc_n = n/p;\r\n   loc_mat = malloc(n*loc_n*sizeof(int));\r\n   int *loc_dist = malloc(loc_n * sizeof(int));\r\n   int *loc_pred = malloc(loc_n * sizeof(int));\r\n\r\n\r\n#  ifdef DEBUG\r\n   printf(\"Proc %d > p = %d, n = %d, loc_n = %d\\n\",\r\n         my_rank, p, n, loc_n);\r\n\r\n   \n\r\n   \n\r\n   for (i = 0; i < n; i++)\r\n      for (j = 0; j < loc_n; j++)\r\n         loc_mat[i*loc_n + j] = -1;\r\n#  endif   \r\n   \r\n   \n\r\n   blk_col_mpi_t = Build_blk_col_type(n, loc_n);\r\n\r\n   Read_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, comm);\r\n   \n\n   \n\n   \r\n   Dijkstra(loc_mat, loc_dist, n, loc_n, loc_pred, my_rank, comm);\r\n   Print_dists(loc_dist, n, loc_n, my_rank, comm);\r\n   Print_paths(loc_pred, n, loc_n, my_rank, comm);\r\n\r\n   free(loc_mat);\r\n   free(loc_dist);\r\n   free(loc_pred);\r\n   \n\r\n\r\n   return 0;\r\n}", "label": "int main(int argc, char* argv[]) {\r\n   int *loc_mat;\r\n   int n, loc_n, p, my_rank;\r\n   MPI_Comm comm;\r\n   MPI_Datatype blk_col_mpi_t;\r\n\r\n#  ifdef DEBUG\r\n   int i, j;\r\n#  endif\r\n\r\n   MPI_Init(&argc, &argv);\r\n   comm = MPI_COMM_WORLD;\r\n   MPI_Comm_size(comm, &p);\r\n   MPI_Comm_rank(comm, &my_rank);\r\n   \r\n   \n\n   \r\n   n = Read_n(my_rank, comm);\r\n   loc_n = n/p;\r\n   loc_mat = malloc(n*loc_n*sizeof(int));\r\n   int *loc_dist = malloc(loc_n * sizeof(int));\r\n   int *loc_pred = malloc(loc_n * sizeof(int));\r\n\r\n\r\n#  ifdef DEBUG\r\n   printf(\"Proc %d > p = %d, n = %d, loc_n = %d\\n\",\r\n         my_rank, p, n, loc_n);\r\n\r\n   \n\r\n   \n\r\n   for (i = 0; i < n; i++)\r\n      for (j = 0; j < loc_n; j++)\r\n         loc_mat[i*loc_n + j] = -1;\r\n#  endif   \r\n   \r\n   \n\r\n   blk_col_mpi_t = Build_blk_col_type(n, loc_n);\r\n\r\n   Read_matrix(loc_mat, n, loc_n, blk_col_mpi_t, my_rank, comm);\r\n   \n\n   \n\n   \r\n   Dijkstra(loc_mat, loc_dist, n, loc_n, loc_pred, my_rank, comm);\r\n   Print_dists(loc_dist, n, loc_n, my_rank, comm);\r\n   Print_paths(loc_pred, n, loc_n, my_rank, comm);\r\n\r\n   free(loc_mat);\r\n   free(loc_dist);\r\n   free(loc_pred);\r\n   \n\r\n   MPI_Type_free(&blk_col_mpi_t);\r\n\r\n   MPI_Finalize();\r\n   return 0;\r\n}"}
{"program": "gyaikhom_1480", "code": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    bc_init(BC_ERR | BC_PLIST_XALL);\n    FILE *f;\n    char fname[128];\n\n    sprintf(fname, \"scatter_bc_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, scatter_bc(11, 1L << i));\n    }\n    fclose(f);\n    bc_final();\n    return 0;\n}", "label": "int main(int argc, char *argv[]) {\n    int i, limit = 21;\n    MPI_Init(&argc, &argv);\n    bc_init(BC_ERR | BC_PLIST_XALL);\n    FILE *f;\n    char fname[128];\n\n    sprintf(fname, \"scatter_bc_%d.dat\", bc_rank);\n    f = fopen(fname, \"w\");\n    for (i = 0; i < limit; i++) {\n        fprintf(f, \"%d %ld %f\\n\", i, 1L << i, scatter_bc(11, 1L << i));\n    }\n    fclose(f);\n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "liuqx315_1481", "code": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem, *P_data;\n  UserData data;\n  int thispe, iout, ier, npes;\n  long int Neq, local_N, mudq, mldq, mukeep, mlkeep;\n  realtype rtol, atol, t0, t1, tout, tret;\n  N_Vector uu, up, constraints, id, res;\n\n  mem = P_data = NULL;\n  data = NULL;\n  uu = up = constraints = id = res = NULL;\n\n  \n\n\n  comm = MPI_COMM_WORLD;\n  \n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\", \n              npes,NPEX*NPEY);\n    return(1);\n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB;\n  Neq     = MX * MY;\n\n  \n\n\n  uu = N_VNew_Parallel(comm, local_N, Neq);\n\n  up = N_VNew_Parallel(comm, local_N, Neq);\n\n  res = N_VNew_Parallel(comm, local_N, Neq);\n\n  constraints = N_VNew_Parallel(comm, local_N, Neq);\n\n  id = N_VNew_Parallel(comm, local_N, Neq);\n\n  \n\n\n  data = (UserData) malloc(sizeof *data);\n\n  InitUserData(thispe, comm, data);\n\n  \n\n\n  SetInitialProfile(uu, up, id, res, data);\n  N_VConst(ONE, constraints);\n\n  t0 = ZERO; t1 = RCONST(0.01);\n\n  \n\n\n  rtol = ZERO;\n  atol = RCONST(1.0e-3);\n\n  \n\n\n  mem = CPodeCreate(CP_BDF, CP_NEWTON);\n\n  ier = CPodeSetUserData(mem, data);\n\n  ier = CPodeInitImpl(mem, heatres, t0, uu, up);\n\n  ier = CPodeSStolerances(mem, rtol, atol);\n\n  mudq = MXSUB;\n  mldq = MXSUB;\n  mukeep = 1;\n  mlkeep = 1;\n\n  \n\n\n  if (thispe == 0 ) PrintHeader(Neq, rtol, atol);\n\n  \n\n  ier = CPSpgmr(mem, PREC_LEFT, 0);\n\n  \n\n  ier = CPBBDPrecInit(mem, local_N, mudq, mldq, mukeep, mlkeep, \n                      ZERO, reslocal, NULL);\n\n  \n\n  if (thispe == 0) PrintCase(1, mudq, mukeep);\n\n  \n\n  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) { \n    \n    ier = CPode(mem, tout, &tret, uu, up, CP_NORMAL);\n\n    PrintOutput(thispe, mem, P_data, tret, uu);\n    \n  }\n\n  \n\n  if (thispe == 0) PrintFinalStats(mem);\n  \n  \n\n  CPodeFree(&mem);\n  free(data);\n  N_VDestroy_Parallel(id);\n  N_VDestroy_Parallel(res);\n  N_VDestroy_Parallel(up);\n  N_VDestroy_Parallel(uu);\n\n\n  return(0);\n\n}", "label": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem, *P_data;\n  UserData data;\n  int thispe, iout, ier, npes;\n  long int Neq, local_N, mudq, mldq, mukeep, mlkeep;\n  realtype rtol, atol, t0, t1, tout, tret;\n  N_Vector uu, up, constraints, id, res;\n\n  mem = P_data = NULL;\n  data = NULL;\n  uu = up = constraints = id = res = NULL;\n\n  \n\n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_size(comm, &npes);\n  MPI_Comm_rank(comm, &thispe);\n  \n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d is not equal to NPEX*NPEY = %d\\n\", \n              npes,NPEX*NPEY);\n    MPI_Finalize();\n    return(1);\n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB;\n  Neq     = MX * MY;\n\n  \n\n\n  uu = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)uu, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  up = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)up, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  res = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)res, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  constraints = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)constraints, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  id = N_VNew_Parallel(comm, local_N, Neq);\n  if(check_flag((void *)id, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  \n\n\n  data = (UserData) malloc(sizeof *data);\n  if(check_flag((void *)data, \"malloc\", 2, thispe)) MPI_Abort(comm, 1);\n\n  InitUserData(thispe, comm, data);\n\n  \n\n\n  SetInitialProfile(uu, up, id, res, data);\n  N_VConst(ONE, constraints);\n\n  t0 = ZERO; t1 = RCONST(0.01);\n\n  \n\n\n  rtol = ZERO;\n  atol = RCONST(1.0e-3);\n\n  \n\n\n  mem = CPodeCreate(CP_BDF, CP_NEWTON);\n  if(check_flag((void *)mem, \"CPodeCreate\", 0, thispe)) MPI_Abort(comm, 1);\n\n  ier = CPodeSetUserData(mem, data);\n  if(check_flag(&ier, \"CPodeSetUserData\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = CPodeInitImpl(mem, heatres, t0, uu, up);\n  if(check_flag(&ier, \"CPodeInitImpl\", 1, thispe)) MPI_Abort(comm, 1);\n\n  ier = CPodeSStolerances(mem, rtol, atol);\n  if(check_flag(&ier, \"CPodeSStolerances\", 1, thispe)) MPI_Abort(comm, 1);\n\n  mudq = MXSUB;\n  mldq = MXSUB;\n  mukeep = 1;\n  mlkeep = 1;\n\n  \n\n\n  if (thispe == 0 ) PrintHeader(Neq, rtol, atol);\n\n  \n\n  ier = CPSpgmr(mem, PREC_LEFT, 0);\n  if(check_flag(&ier, \"CPSpgmr\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n  ier = CPBBDPrecInit(mem, local_N, mudq, mldq, mukeep, mlkeep, \n                      ZERO, reslocal, NULL);\n  if(check_flag(&ier, \"CPBBDPrecAlloc\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n  if (thispe == 0) PrintCase(1, mudq, mukeep);\n\n  \n\n  for (tout = t1, iout = 1; iout <= NOUT; iout++, tout *= TWO) { \n    \n    ier = CPode(mem, tout, &tret, uu, up, CP_NORMAL);\n    if(check_flag(&ier, \"CPode\", 1, thispe)) MPI_Abort(comm, 1);\n\n    PrintOutput(thispe, mem, P_data, tret, uu);\n    \n  }\n\n  \n\n  if (thispe == 0) PrintFinalStats(mem);\n  \n  \n\n  CPodeFree(&mem);\n  free(data);\n  N_VDestroy_Parallel(id);\n  N_VDestroy_Parallel(res);\n  N_VDestroy_Parallel(up);\n  N_VDestroy_Parallel(uu);\n\n  MPI_Finalize();\n\n  return(0);\n\n}"}
{"program": "mF2C_1482", "code": "int main(int argc, char **argv) {\n    \n\n    int myid, numprocs;\n    int myresult = 0;\n    \n    \n\n    int DATA_SIZE = argc - 1;\n    int data[DATA_SIZE], result;\n    \n    \n\n    int i, low, high, size;\n    \n    \n\n    \n\n\n    \n\n    \n\n    if (myid == 0) {\n        printf(\"Processing data\\n\");\n        \n\n        for (i = 1; i < argc; ++i) {\n          data[i - 1] = atoi(argv[i]);\n          printf(\"  - DATA %d\\n\", data[i - 1]);\n        }\n    }\n\n    \n\n    \n    \n\n    \n\n    size = DATA_SIZE/numprocs;\n    low = myid * size;\n    high = low + size;\n    for (i = low; i < high; ++i) {\n        myresult += data[i];\n    }\n    printf(\"Process %d gets from %d to %d with result %d\\n\", myid, low, high, myresult);\n\n    \n\n    \n\n\n    \n\n    \n\n    if (myid == 0) {\n        printf(\"MULTIPLICATION_RESULT %d\\n\", result);\n    }\n\n    \n\n    \n\n}", "label": "int main(int argc, char **argv) {\n    \n\n    int myid, numprocs;\n    int myresult = 0;\n    \n    \n\n    int DATA_SIZE = argc - 1;\n    int data[DATA_SIZE], result;\n    \n    \n\n    int i, low, high, size;\n    \n    \n\n    \n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\n    \n\n    \n\n    if (myid == 0) {\n        printf(\"Processing data\\n\");\n        \n\n        for (i = 1; i < argc; ++i) {\n          data[i - 1] = atoi(argv[i]);\n          printf(\"  - DATA %d\\n\", data[i - 1]);\n        }\n    }\n\n    \n\n    MPI_Bcast(data, DATA_SIZE, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    \n\n    \n\n    size = DATA_SIZE/numprocs;\n    low = myid * size;\n    high = low + size;\n    for (i = low; i < high; ++i) {\n        myresult += data[i];\n    }\n    printf(\"Process %d gets from %d to %d with result %d\\n\", myid, low, high, myresult);\n\n    \n\n    \n\n    MPI_Reduce(&myresult, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    \n\n    \n\n    if (myid == 0) {\n        printf(\"MULTIPLICATION_RESULT %d\\n\", result);\n    }\n\n    \n\n    \n\n    MPI_Finalize();\n}"}
{"program": "qingu_1483", "code": "int main(int argc, char **argv) {\n    int      i, j, rank, nproc;\n    int      shm_rank, shm_nproc;\n    MPI_Info alloc_shared_info;\n    int      errors = 0, all_errors = 0;\n    int      disp_unit;\n    int     *my_base;\n    MPI_Win  shm_win;\n    MPI_Comm shm_comm;\n\n\n\n\n\n\n    \n\n\n\n    \n\n    for (i = 0; i < ELEM_PER_PROC; i++) {\n        my_base[i] = i;\n    }\n\n\n    \n\n    for (i = 0; i < shm_nproc; i++) {\n        int      *base;\n        MPI_Aint  size;\n\n        assert(size == ELEM_PER_PROC * sizeof(int));\n\n        for (j = 0; j < ELEM_PER_PROC; j++) {\n            if ( base[j] != j ) {\n                errors++;\n                printf(\"%d -- Got %d at rank %d index %d, expected %d\\n\", shm_rank, \n                       base[j], i, j, j);\n            }\n        }\n    }\n\n\n\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int      i, j, rank, nproc;\n    int      shm_rank, shm_nproc;\n    MPI_Info alloc_shared_info;\n    int      errors = 0, all_errors = 0;\n    int      disp_unit;\n    int     *my_base;\n    MPI_Win  shm_win;\n    MPI_Comm shm_comm;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    MPI_Info_create(&alloc_shared_info);\n    MPI_Info_set(alloc_shared_info, \"alloc_shared_noncontig\", \"true\");\n\n    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);\n\n    MPI_Comm_rank(shm_comm, &shm_rank);\n    MPI_Comm_size(shm_comm, &shm_nproc);\n\n    \n\n    MPI_Win_allocate_shared(sizeof(int)*ELEM_PER_PROC, sizeof(int), alloc_shared_info, \n                             shm_comm, &my_base, &shm_win);\n\n    MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);\n\n    \n\n    for (i = 0; i < ELEM_PER_PROC; i++) {\n        my_base[i] = i;\n    }\n\n    MPI_Win_sync(shm_win);\n    MPI_Barrier(shm_comm);\n    MPI_Win_sync(shm_win);\n\n    \n\n    for (i = 0; i < shm_nproc; i++) {\n        int      *base;\n        MPI_Aint  size;\n\n        MPI_Win_shared_query(shm_win, i, &size, &disp_unit, &base);\n        assert(size == ELEM_PER_PROC * sizeof(int));\n\n        for (j = 0; j < ELEM_PER_PROC; j++) {\n            if ( base[j] != j ) {\n                errors++;\n                printf(\"%d -- Got %d at rank %d index %d, expected %d\\n\", shm_rank, \n                       base[j], i, j, j);\n            }\n        }\n    }\n\n    MPI_Win_unlock_all(shm_win);\n    MPI_Win_free(&shm_win);\n    MPI_Comm_free(&shm_comm);\n\n    MPI_Info_free(&alloc_shared_info);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "huudatHust_1484", "code": "int main(int argc, char *argv[]){\n\t\tint numtasks, taskid, len, partner, message;\n\t\tchar hostname[MPI_MAX_PROCESSOR_NAME];\n\t\tMPI_Status status;\n\t\t\n\t\t\n\t\t\n\n\t\tif(numtasks % 2 != 0){\n\t\t\tif(taskid == MASTER)\n\t\t\t\tprintf(\"quitting. need an even number of tasks : numtasks = %d\\n\", numtasks);\t\n\t\t}\n\t\telse{\n\t\t\tif(taskid == MASTER)\n\t\t\t\tprintf(\"MASTER : number of MPI taks is : %d \\n\", numtasks);\n\t\t\tprintf(\"hello from task %d on %s\\n\", taskid, hostname);\n\t\t\t\n\t\t\t\n\n\t\t\tif(taskid < numtasks/2){\n\t\t\t\tpartner = numtasks/2 + taskid;\n\t\t\t}\n\t\t\telse{\n\t\t\t\tpartner = taskid - numtasks/2 ;\n\t\t\t}\n\t\t\tprintf(\"task %d is partner with %d \\n\", taskid, partner);\n\t\t}\n}", "label": "int main(int argc, char *argv[]){\n\t\tint numtasks, taskid, len, partner, message;\n\t\tchar hostname[MPI_MAX_PROCESSOR_NAME];\n\t\tMPI_Status status;\n\t\t\n\t\tMPI_Init(&argc, &argv);\n\t\tMPI_Comm_rank(MPI_COMM_WORLD, &taskid);\n\t\tMPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\t\t\n\t\t\n\n\t\tif(numtasks % 2 != 0){\n\t\t\tif(taskid == MASTER)\n\t\t\t\tprintf(\"quitting. need an even number of tasks : numtasks = %d\\n\", numtasks);\t\n\t\t}\n\t\telse{\n\t\t\tif(taskid == MASTER)\n\t\t\t\tprintf(\"MASTER : number of MPI taks is : %d \\n\", numtasks);\n\t\t\tMPI_Get_processor_name(hostname, &len);\n\t\t\tprintf(\"hello from task %d on %s\\n\", taskid, hostname);\n\t\t\t\n\t\t\t\n\n\t\t\tif(taskid < numtasks/2){\n\t\t\t\tpartner = numtasks/2 + taskid;\n\t\t\t\tMPI_Send(&taskid, 1, MPI_INT, partner, 1, MPI_COMM_WORLD);\n\t\t\t\tMPI_Recv(&message,1, MPI_INT, partner,1, MPI_COMM_WORLD, &status);\n\t\t\t}\n\t\t\telse{\n\t\t\t\tpartner = taskid - numtasks/2 ;\n\t\t\t\tMPI_Recv(&message, 1, MPI_INT, partner, 1, MPI_COMM_WORLD, &status);\n\t\t\t\tMPI_Send(&taskid, 1, MPI_INT, partner, 1, MPI_COMM_WORLD);\n\t\t\t}\n\t\t\tprintf(\"task %d is partner with %d \\n\", taskid, partner);\n\t\t}\n\t\tMPI_Finalize();\n}"}
{"program": "gnu3ra_1485", "code": "int main( int argc, char *argv[] )\n{\n    int  n, myid, numprocs, ii, jj;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int namelen; \n    int event1a, event1b, event2a, event2b,\n        event3a, event3b, event4a, event4b;\n    int event1, event2, event3;\n    char processor_name[ MPI_MAX_PROCESSOR_NAME ];\n\n        \n\n\n    fprintf( stderr, \"Process %d running on %s\\n\", myid, processor_name );\n\n    \n\n#if defined( NO_MPI_LOGGING )\n    MPE_Init_log();\n#endif\n\n    \n\n    MPE_Log_get_state_eventIDs( &event1a, &event1b );\n    MPE_Log_get_state_eventIDs( &event2a, &event2b );\n    MPE_Log_get_state_eventIDs( &event3a, &event3b );\n    MPE_Log_get_state_eventIDs( &event4a, &event4b );\n\n    if ( myid == 0 ) {\n        MPE_Describe_state( event1a, event1b, \"Broadcast\", \"red\" );\n        MPE_Describe_state( event2a, event2b, \"Sync\", \"orange\" );\n        MPE_Describe_state( event3a, event3b, \"Compute\", \"blue\" );\n        MPE_Describe_state( event4a, event4b, \"Reduce\", \"green\" );\n    }\n\n    \n\n    MPE_Log_get_solo_eventID( &event1 );\n    MPE_Log_get_solo_eventID( &event2 );\n    MPE_Log_get_solo_eventID( &event3 );\n\n    if ( myid == 0 ) {\n       MPE_Describe_event( event1, \"Broadcast Post\", \"white\" );\n       MPE_Describe_event( event2, \"Compute Start\", \"purple\" );\n       MPE_Describe_event( event3, \"Compute End\", \"navy\" );\n    }\n\n    if ( myid == 0 ) {\n        n = 1000000;\n        startwtime =\n    }\n\n    \n\n\n    for ( jj = 0; jj < 5; jj++ ) {\n        MPE_Log_event( event1a, 0, NULL );\n        MPE_Log_event( event1b, 0, NULL );\n\n        MPE_Log_event( event1, 0, NULL );\n    \n        MPE_Log_event( event2a, 0, NULL );\n        MPE_Log_event( event2b, 0, NULL );\n\n        MPE_Log_event( event2, 0, NULL );\n        MPE_Log_event( event3a, 0, NULL );\n        h   = 1.0 / (double) n;\n        sum = 0.0;\n        for ( ii = myid + 1; ii <= n; ii += numprocs ) {\n            x = h * ((double)ii - 0.5);\n            sum += f(x);\n        }\n        mypi = h * sum;\n        MPE_Log_event( event3b, 0, NULL );\n        MPE_Log_event( event3, 0, NULL );\n\n        pi = 0.0;\n        MPE_Log_event( event4a, 0, NULL );\n        MPE_Log_event( event4b, 0, NULL );\n\n        MPE_Log_sync_clocks();\n    }\n#if defined( NO_MPI_LOGGING )\n    if ( argv != NULL )\n        MPE_Finish_log( argv[0] );\n    else\n        MPE_Finish_log( \"cpilog\" );\n#endif\n\n    if ( myid == 0 ) {\n        endwtime =\n        printf( \"pi is approximately %.16f, Error is %.16f\\n\",\n                pi, fabs(pi - PI25DT) );\n        printf( \"wall clock time = %f\\n\", endwtime-startwtime );\n    }\n    return( 0 );\n}", "label": "int main( int argc, char *argv[] )\n{\n    int  n, myid, numprocs, ii, jj;\n    double PI25DT = 3.141592653589793238462643;\n    double mypi, pi, h, sum, x;\n    double startwtime = 0.0, endwtime;\n    int namelen; \n    int event1a, event1b, event2a, event2b,\n        event3a, event3b, event4a, event4b;\n    int event1, event2, event3;\n    char processor_name[ MPI_MAX_PROCESSOR_NAME ];\n\n    MPI_Init( &argc, &argv );\n        \n        MPI_Pcontrol( 0 );\n\n    MPI_Comm_size( MPI_COMM_WORLD, &numprocs );\n    MPI_Comm_rank( MPI_COMM_WORLD, &myid );\n\n    MPI_Get_processor_name( processor_name, &namelen );\n    fprintf( stderr, \"Process %d running on %s\\n\", myid, processor_name );\n\n    \n\n#if defined( NO_MPI_LOGGING )\n    MPE_Init_log();\n#endif\n\n    \n\n    MPE_Log_get_state_eventIDs( &event1a, &event1b );\n    MPE_Log_get_state_eventIDs( &event2a, &event2b );\n    MPE_Log_get_state_eventIDs( &event3a, &event3b );\n    MPE_Log_get_state_eventIDs( &event4a, &event4b );\n\n    if ( myid == 0 ) {\n        MPE_Describe_state( event1a, event1b, \"Broadcast\", \"red\" );\n        MPE_Describe_state( event2a, event2b, \"Sync\", \"orange\" );\n        MPE_Describe_state( event3a, event3b, \"Compute\", \"blue\" );\n        MPE_Describe_state( event4a, event4b, \"Reduce\", \"green\" );\n    }\n\n    \n\n    MPE_Log_get_solo_eventID( &event1 );\n    MPE_Log_get_solo_eventID( &event2 );\n    MPE_Log_get_solo_eventID( &event3 );\n\n    if ( myid == 0 ) {\n       MPE_Describe_event( event1, \"Broadcast Post\", \"white\" );\n       MPE_Describe_event( event2, \"Compute Start\", \"purple\" );\n       MPE_Describe_event( event3, \"Compute End\", \"navy\" );\n    }\n\n    if ( myid == 0 ) {\n        n = 1000000;\n        startwtime = MPI_Wtime();\n    }\n    MPI_Barrier( MPI_COMM_WORLD );\n\n    MPI_Pcontrol( 1 );\n    \n\n\n    for ( jj = 0; jj < 5; jj++ ) {\n        MPE_Log_event( event1a, 0, NULL );\n        MPI_Bcast( &n, 1, MPI_INT, 0, MPI_COMM_WORLD );\n        MPE_Log_event( event1b, 0, NULL );\n\n        MPE_Log_event( event1, 0, NULL );\n    \n        MPE_Log_event( event2a, 0, NULL );\n        MPI_Barrier( MPI_COMM_WORLD );\n        MPE_Log_event( event2b, 0, NULL );\n\n        MPE_Log_event( event2, 0, NULL );\n        MPE_Log_event( event3a, 0, NULL );\n        h   = 1.0 / (double) n;\n        sum = 0.0;\n        for ( ii = myid + 1; ii <= n; ii += numprocs ) {\n            x = h * ((double)ii - 0.5);\n            sum += f(x);\n        }\n        mypi = h * sum;\n        MPE_Log_event( event3b, 0, NULL );\n        MPE_Log_event( event3, 0, NULL );\n\n        pi = 0.0;\n        MPE_Log_event( event4a, 0, NULL );\n        MPI_Reduce( &mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD );\n        MPE_Log_event( event4b, 0, NULL );\n\n        MPE_Log_sync_clocks();\n    }\n#if defined( NO_MPI_LOGGING )\n    if ( argv != NULL )\n        MPE_Finish_log( argv[0] );\n    else\n        MPE_Finish_log( \"cpilog\" );\n#endif\n\n    if ( myid == 0 ) {\n        endwtime = MPI_Wtime();\n        printf( \"pi is approximately %.16f, Error is %.16f\\n\",\n                pi, fabs(pi - PI25DT) );\n        printf( \"wall clock time = %f\\n\", endwtime-startwtime );\n    }\n    MPI_Finalize();\n    return( 0 );\n}"}
{"program": "ginnungagapgroup_1486", "code": "int\nmain(int argc, char **argv)\n{\n\tbool hasFailed = false;\n\tint  rank      = 0;\n\tint  size      = 1;\n\n#ifdef WITH_MPI\n#endif\n\n\tif (rank == 0) {\n\t\tprintf(\"\\nTesting %s on %i %s\\n\",\n\t\t       NAME, size, size > 1 ? \"tasks\" : \"task\");\n\t}\n\n\tif (rank == 0) {\n\t\tprintf(\"\\nRunning tests for partBunch:\\n\");\n\t}\n\tRUNTEST(&partBunch_new_test, hasFailed);\n\tRUNTEST(&partBunch_del_test, hasFailed);\n\tRUNTEST(&partBunch_allocMem_test, hasFailed);\n\tRUNTEST(&partBunch_freeMem_test, hasFailed);\n\tRUNTEST(&partBunch_resize_test, hasFailed);\n\tRUNTEST(&partBunch_isAllocated_test, hasFailed);\n\tRUNTEST(&partBunch_getNumParticles_test, hasFailed);\n\tRUNTEST(&partBunch_at_test, hasFailed);\n#ifdef XMEM_TRACK_MEM\n\tif (rank == 0)\n\t\txmem_info(stdout);\n\tglobal_max_allocated_bytes = 0;\n#endif\n\n#ifdef WITH_MPI\n#endif\n\n\tif (hasFailed) {\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"\\nSome tests failed!\\n\\n\");\n\t\treturn EXIT_FAILURE;\n\t}\n\tif (rank == 0)\n\t\tprintf(\"\\nAll tests passed successfully!\\n\\n\");\n\n\treturn EXIT_SUCCESS;\n}", "label": "int\nmain(int argc, char **argv)\n{\n\tbool hasFailed = false;\n\tint  rank      = 0;\n\tint  size      = 1;\n\n#ifdef WITH_MPI\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n#endif\n\n\tif (rank == 0) {\n\t\tprintf(\"\\nTesting %s on %i %s\\n\",\n\t\t       NAME, size, size > 1 ? \"tasks\" : \"task\");\n\t}\n\n\tif (rank == 0) {\n\t\tprintf(\"\\nRunning tests for partBunch:\\n\");\n\t}\n\tRUNTEST(&partBunch_new_test, hasFailed);\n\tRUNTEST(&partBunch_del_test, hasFailed);\n\tRUNTEST(&partBunch_allocMem_test, hasFailed);\n\tRUNTEST(&partBunch_freeMem_test, hasFailed);\n\tRUNTEST(&partBunch_resize_test, hasFailed);\n\tRUNTEST(&partBunch_isAllocated_test, hasFailed);\n\tRUNTEST(&partBunch_getNumParticles_test, hasFailed);\n\tRUNTEST(&partBunch_at_test, hasFailed);\n#ifdef XMEM_TRACK_MEM\n\tif (rank == 0)\n\t\txmem_info(stdout);\n\tglobal_max_allocated_bytes = 0;\n#endif\n\n#ifdef WITH_MPI\n\tMPI_Finalize();\n#endif\n\n\tif (hasFailed) {\n\t\tif (rank == 0)\n\t\t\tfprintf(stderr, \"\\nSome tests failed!\\n\\n\");\n\t\treturn EXIT_FAILURE;\n\t}\n\tif (rank == 0)\n\t\tprintf(\"\\nAll tests passed successfully!\\n\\n\");\n\n\treturn EXIT_SUCCESS;\n}"}
{"program": "tomspur_1487", "code": "int main (int argc, char **argv) {\n    int rank, size;\n    int number = -1;\n    int i;\n    if (size > 0) {\n        if (rank == 0) {\n            for (i = 0; i < size; i++) {\n                number = i*i;\n                printf(\"Process %d sent number %d to process %d\\n\", rank, number, i);\n            }\n        } else {\n            printf(\"Process %d received number %d from process %d\\n\", rank, number, 0);\n            assert(number == rank*rank);\n        }\n    } else {\n        printf(\"Rank must be bigger than 0 for a ping-pong.\\n\");\n    }\n    return 0;\n}", "label": "int main (int argc, char **argv) {\n    int rank, size;\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank (MPI_COMM_WORLD, &rank);\n    MPI_Comm_size (MPI_COMM_WORLD, &size);\n    int number = -1;\n    int i;\n    if (size > 0) {\n        if (rank == 0) {\n            for (i = 0; i < size; i++) {\n                number = i*i;\n                MPI_Send(&number, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                printf(\"Process %d sent number %d to process %d\\n\", rank, number, i);\n            }\n        } else {\n            MPI_Recv(&number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            printf(\"Process %d received number %d from process %d\\n\", rank, number, 0);\n            assert(number == rank*rank);\n        }\n    } else {\n        printf(\"Rank must be bigger than 0 for a ping-pong.\\n\");\n    }\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "ghisvail_1488", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], N[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  pfft_r2r_kind kinds_forw[4], kinds_back[4];\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n  kinds_forw[3] = PFFT_RODFT10; kinds_back[3] = PFFT_RODFT01;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n  N[3] = 2*n[3];\n\n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r(4, n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r(\n      4, n, in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_r2r(\n      4, n, out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2r(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (N[0]*N[1]*N[2]*N[3]);\n  \n  \n\n  err = pfft_check_output_r2r(4, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[4], N[4];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[4], local_i_start[4];\n  ptrdiff_t local_no[4], local_o_start[4];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  pfft_r2r_kind kinds_forw[4], kinds_back[4];\n  \n  \n\n  n[0] = 13; n[1] = 14; n[2] = 19; n[3] = 17;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n  kinds_forw[3] = PFFT_RODFT10; kinds_back[3] = PFFT_RODFT01;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n  N[3] = 2*n[3];\n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r(4, n, comm_cart_2d, PFFT_TRANSPOSED_OUT,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r(\n      4, n, in, out, comm_cart_2d, kinds_forw, PFFT_TRANSPOSED_OUT| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_r2r(\n      4, n, out, in, comm_cart_2d, kinds_back, PFFT_TRANSPOSED_IN| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_r2r(4, n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2] * local_ni[3]; l++)\n    in[l] /= (N[0]*N[1]*N[2]*N[3]);\n  \n  \n\n  MPI_Barrier(MPI_COMM_WORLD);\n  err = pfft_check_output_r2r(4, n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td, %td):\\n\", n[0], n[1], n[2], n[3]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "aeslaughter_1490", "code": "int\nmain(int argc, char **argv)\n{\n\n#ifdef USE_PARALLEL\n#endif\n\n   printf(\"\\n*** Testing netcdf-4 large files.\\n\");\n   printf(\"**** testing simple fill value attribute creation...\");\n   {\n      int ncid, varid, dimids[NUMDIMS];\n      size_t index[NUMDIMS] = {0, 0};\n      signed char vals[DIM2];\n      signed char char_val_in;\n      size_t start[NUMDIMS] = {0, 0}, count[NUMDIMS] = {1, DIM2};\n      int j;\n\n      \n\n      for (j = 0; j < DIM2; j++)\n\t vals[j] = 9 * (j + 11); \n\n\n      \n\n      if (nc_create(FILE_NAME, NC_NETCDF4, &ncid)) ERR;\n      if (nc_set_fill(ncid, NC_NOFILL, NULL)) ERR;\n      if (nc_def_dim(ncid, \"dim1\", DIM1, &dimids[0])) ERR;\n      if (nc_def_dim(ncid, \"dim2\", DIM2, &dimids[1])) ERR;\n      if (nc_def_var(ncid, \"var\", NC_BYTE, NUMDIMS, dimids, &varid)) ERR;\n      if (nc_enddef(ncid)) ERR;\n\n      \n\n      if (nc_put_vara_schar(ncid, varid, start, count, vals)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n   }\n   SUMMARIZE_ERR;\n\n#ifdef USE_PARALLEL\n#endif\n   FINAL_RESULTS;\n}", "label": "int\nmain(int argc, char **argv)\n{\n\n#ifdef USE_PARALLEL\n   MPI_Init(&argc, &argv);\n#endif\n\n   printf(\"\\n*** Testing netcdf-4 large files.\\n\");\n   printf(\"**** testing simple fill value attribute creation...\");\n   {\n      int ncid, varid, dimids[NUMDIMS];\n      size_t index[NUMDIMS] = {0, 0};\n      signed char vals[DIM2];\n      signed char char_val_in;\n      size_t start[NUMDIMS] = {0, 0}, count[NUMDIMS] = {1, DIM2};\n      int j;\n\n      \n\n      for (j = 0; j < DIM2; j++)\n\t vals[j] = 9 * (j + 11); \n\n\n      \n\n      if (nc_create(FILE_NAME, NC_NETCDF4, &ncid)) ERR;\n      if (nc_set_fill(ncid, NC_NOFILL, NULL)) ERR;\n      if (nc_def_dim(ncid, \"dim1\", DIM1, &dimids[0])) ERR;\n      if (nc_def_dim(ncid, \"dim2\", DIM2, &dimids[1])) ERR;\n      if (nc_def_var(ncid, \"var\", NC_BYTE, NUMDIMS, dimids, &varid)) ERR;\n      if (nc_enddef(ncid)) ERR;\n\n      \n\n      if (nc_put_vara_schar(ncid, varid, start, count, vals)) ERR;\n      if (nc_close(ncid)) ERR;\n\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n   }\n   SUMMARIZE_ERR;\n\n#ifdef USE_PARALLEL\n   MPI_Finalize();\n#endif\n   FINAL_RESULTS;\n}"}
{"program": "vincent-noel_1492", "code": "int main (int argc, char * argv[])\n{\n\n#ifdef MPI\n\n\n\n\n\n#endif\n\n    int res;\n    initializeModels();\n\n    char * folder = NULL;\n    int model_id = -1;\n    int i_arg = 1;\n\n    \n\n    steady_states = 0;\n\n    while(i_arg < argc)\n    {\n        \n\n\n        if (strcmp(argv[i_arg],\"-o\") == 0)\n        {\n            folder = argv[i_arg+1];\n            i_arg += 2;\n        }\n\n        else if (strcmp(argv[i_arg], \"-m\") == 0)\n        {\n            model_id = atoi(argv[i_arg+1]);\n            if (model_id >= t_models->nb_models)\n            {\n                printf(\"! Model id #%d doesn't exist !\\n\", model_id);\n                return EXIT_FAILURE;\n            }\n            i_arg += 2;\n        }\n\n        \n\n        \n\n        \n\n        \n\n        \n\n\n        else if (strcmp(argv[i_arg], \"-s\") == 0)\n        {\n            steady_states = 1;\n            i_arg += 1;\n        }\n\n    }\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    if (model_id >= 0)\n        res = simulate_model(model_id, folder);\n    else\n        res = simulate_models(folder);\n    \n\n\n\n\n    finalize_models();\n\n#ifdef MPI\n#endif\n\n    if (res == -1) return EXIT_FAILURE;\n    else\n    {\n        \n\n        return EXIT_SUCCESS;\n    }\n}", "label": "int main (int argc, char * argv[])\n{\n\n#ifdef MPI\n\n    MPI_Init (&argc, &argv);\t\n\n    MPI_Comm_rank (MPI_COMM_WORLD, &my_proc);\t\n\n    MPI_Comm_size (MPI_COMM_WORLD, &nb_procs);\t\n\n\n#endif\n\n    int res;\n    initializeModels();\n\n    char * folder = NULL;\n    int model_id = -1;\n    int i_arg = 1;\n\n    \n\n    steady_states = 0;\n\n    while(i_arg < argc)\n    {\n        \n\n\n        if (strcmp(argv[i_arg],\"-o\") == 0)\n        {\n            folder = argv[i_arg+1];\n            i_arg += 2;\n        }\n\n        else if (strcmp(argv[i_arg], \"-m\") == 0)\n        {\n            model_id = atoi(argv[i_arg+1]);\n            if (model_id >= t_models->nb_models)\n            {\n                printf(\"! Model id #%d doesn't exist !\\n\", model_id);\n                return EXIT_FAILURE;\n            }\n            i_arg += 2;\n        }\n\n        \n\n        \n\n        \n\n        \n\n        \n\n\n        else if (strcmp(argv[i_arg], \"-s\") == 0)\n        {\n            steady_states = 1;\n            i_arg += 1;\n        }\n\n    }\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    if (model_id >= 0)\n        res = simulate_model(model_id, folder);\n    else\n        res = simulate_models(folder);\n    \n\n\n\n\n    finalize_models();\n\n#ifdef MPI\n    MPI_Finalize();\n#endif\n\n    if (res == -1) return EXIT_FAILURE;\n    else\n    {\n        \n\n        return EXIT_SUCCESS;\n    }\n}"}
{"program": "mpip_1493", "code": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  pfft_complex *cdata;\n  pfft_gcplan ths;\n  \n  pfft_init();\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 4;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n\n  \n\n  \n\n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_no, local_o_start, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  cdata = pfft_alloc_complex(alloc_local_gc > alloc_local ? alloc_local_gc : alloc_local);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      cdata, comm_cart_2d, PFFT_GC_TRANSPOSED_NONE | PFFT_GC_R2C);\n\n  \n\n  pfft_init_input_complex_3d(n, local_no, local_o_start,\n      cdata);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(cdata, local_no, local_o_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(cdata, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(cdata, local_no, local_o_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_no[0] * local_no[1] * local_no[2]; l++)\n    cdata[l] /= 2;\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_no, local_o_start, cdata, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one gcell exchange and reduce of logical size n=(%td, %td, %td),\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"physical size pn=(%td, %td, %td),\\n\", n[0], n[1], n[2]/2+1); \n  pfft_printf(comm_cart_2d, \"gc_below = (%td, %td, %td), gc_above = (%td, %td, %td):\\n\", gc_below[0], gc_below[1], gc_below[2], gc_above[0], gc_above[1], gc_above[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  pfft_free(cdata);\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  ptrdiff_t n[3], gc_below[3], gc_above[3];\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  ptrdiff_t local_ngc[3], local_gc_start[3];\n  ptrdiff_t alloc_local, alloc_local_gc;\n  int np[3], rnk_self, size, verbose;\n  double err;\n  MPI_Comm comm_cart_2d;\n  pfft_complex *cdata;\n  pfft_gcplan ths;\n  \n  MPI_Init(&argc, &argv);\n  pfft_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &rnk_self);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  \n\n  n[0] = n[1] = n[2] = 8; \n\n  np[0]=2; np[1]=2; np[2] = 1;\n\n  verbose = 0;\n  for(int t=0; t<3; t++){\n    gc_below[t] = 0;\n    gc_above[t] = 0;\n  }\n  gc_below[0] = 0;\n  gc_above[0] = 4;\n\n  \n\n  init_parameters(argc, argv, n, np, gc_below, gc_above, &verbose);\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n\n  \n\n  \n\n  \n\n  alloc_local = pfft_local_size_dft_r2c_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  alloc_local_gc = pfft_local_size_gc_3d(\n      local_no, local_o_start, gc_below, gc_above,\n      local_ngc, local_gc_start);\n\n  \n\n  cdata = pfft_alloc_complex(alloc_local_gc > alloc_local ? alloc_local_gc : alloc_local);\n\n  \n\n  ths = pfft_plan_cgc_3d(n, gc_below, gc_above,\n      cdata, comm_cart_2d, PFFT_GC_TRANSPOSED_NONE | PFFT_GC_R2C);\n\n  \n\n  pfft_init_input_complex_3d(n, local_no, local_o_start,\n      cdata);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(cdata, local_no, local_o_start, \"gcell input\", comm_cart_2d);\n\n  \n\n  pfft_exchange(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(cdata, local_ngc, local_gc_start, \"exchanged gcells\", comm_cart_2d);\n  \n  \n\n  pfft_reduce(ths);\n\n  \n\n  if(verbose)\n    pfft_apr_complex_3d(cdata, local_no, local_o_start, \"reduced gcells\", comm_cart_2d);\n\n  \n\n  for(ptrdiff_t l=0; l < local_no[0] * local_no[1] * local_no[2]; l++)\n    cdata[l] /= 2;\n\n  \n\n  MPI_Barrier(comm_cart_2d);\n  err = pfft_check_output_complex_3d(n, local_no, local_o_start, cdata, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one gcell exchange and reduce of logical size n=(%td, %td, %td),\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"physical size pn=(%td, %td, %td),\\n\", n[0], n[1], n[2]/2+1); \n  pfft_printf(comm_cart_2d, \"gc_below = (%td, %td, %td), gc_above = (%td, %td, %td):\\n\", gc_below[0], gc_below[1], gc_below[2], gc_above[0], gc_above[1], gc_above[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n\n\n  \n\n  pfft_destroy_gcplan(ths);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(cdata);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "JohnPJenkins_1495", "code": "int\nmain()\n{\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n\n  printf(\"HI\\n\");\n\n\n  \n\n  MPI_Comm comm;\n\n  \n\n  int argc = 3;\n  const char* argv[argc];\n  argv[0] = \"howdy\";\n  argv[1] = \"ok\";\n  argv[2] = \"bye\";\n\n  \n\n  for (int i = 0; i < 10; i++)\n  {\n    turbine_code rc =\n        turbine_run(comm, \"tests/strings.tcl\", argc, argv, NULL);\n    assert(rc == TURBINE_SUCCESS);\n  }\n\n  return 0;\n}", "label": "int\nmain()\n{\n  int mpi_argc = 0;\n  char** mpi_argv = NULL;\n\n  printf(\"HI\\n\");\n\n  MPI_Init(&mpi_argc, &mpi_argv);\n\n  \n\n  MPI_Comm comm;\n  MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n\n  \n\n  int argc = 3;\n  const char* argv[argc];\n  argv[0] = \"howdy\";\n  argv[1] = \"ok\";\n  argv[2] = \"bye\";\n\n  \n\n  for (int i = 0; i < 10; i++)\n  {\n    turbine_code rc =\n        turbine_run(comm, \"tests/strings.tcl\", argc, argv, NULL);\n    assert(rc == TURBINE_SUCCESS);\n  }\n\n  MPI_Comm_free(&comm);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "qingu_1496", "code": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nints, len;\n    char *filename, *tmp;\n    int errs=0, toterrs;\n    MPI_File fh;\n    MPI_Status status;\n    MPIO_Request request;\n    int errcode = 0;\n\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: async -fname filename\\n\\n\");\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n    }\n    else {\n\tfilename = (char *) malloc(len+10);\n    }\n\n\n    buf = (int *) malloc(SIZE);\n    nints = SIZE/sizeof(int);\n    for (i=0; i<nints; i++) buf[i] = rank*100000 + i;\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_iwrite\");\n    }\n#ifdef MPIO_USES_MPI_REQUEST\n#else    \n#endif\n\n    \n\n\n    for (i=0; i<nints; i++) buf[i] = 0;\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n\n    errcode =\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n#ifdef MPIO_USES_MPI_REQUEST\n#else\n#endif\n\n\n    \n\n    for (i=0; i<nints; i++) {\n\tif (buf[i] != (rank*100000 + i)) {\n\t    errs++;\n\t    fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n\t}\n    }\n\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(buf);\n    free(filename);\n    free(tmp);\n\n    return 0; \n}", "label": "int main(int argc, char **argv)\n{\n    int *buf, i, rank, nints, len;\n    char *filename, *tmp;\n    int errs=0, toterrs;\n    MPI_File fh;\n    MPI_Status status;\n    MPIO_Request request;\n    int errcode = 0;\n\n    MPI_Init(&argc,&argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n\n    if (!rank) {\n\ti = 1;\n\twhile ((i < argc) && strcmp(\"-fname\", *argv)) {\n\t    i++;\n\t    argv++;\n\t}\n\tif (i >= argc) {\n\t    fprintf(stderr, \"\\n*#  Usage: async -fname filename\\n\\n\");\n\t    MPI_Abort(MPI_COMM_WORLD, 1);\n\t}\n\targv++;\n\tlen = strlen(*argv);\n\tfilename = (char *) malloc(len+10);\n\tstrcpy(filename, *argv);\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n\tMPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\tfilename = (char *) malloc(len+10);\n\tMPI_Bcast(filename, len+10, MPI_CHAR, 0, MPI_COMM_WORLD);\n    }\n\n\n    buf = (int *) malloc(SIZE);\n    nints = SIZE/sizeof(int);\n    for (i=0; i<nints; i++) buf[i] = rank*100000 + i;\n\n    \n\n    tmp = (char *) malloc(len+10);\n    strcpy(tmp, filename);\n    sprintf(filename, \"%s.%d\", tmp, rank);\n\n    errcode = MPI_File_open(MPI_COMM_SELF, filename, \n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n    MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);\n    errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_iwrite\");\n    }\n#ifdef MPIO_USES_MPI_REQUEST\n    MPI_Wait( &request, &status );\n#else    \n    MPIO_Wait(&request, &status);\n#endif\n    MPI_File_close(&fh);\n\n    \n\n\n    for (i=0; i<nints; i++) buf[i] = 0;\n    errcode = MPI_File_open(MPI_COMM_SELF, filename, \n\t\t    MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n\n    MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);\n    errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);\n    if (errcode != MPI_SUCCESS) {\n\t    handle_error(errcode, \"MPI_File_open\");\n    }\n#ifdef MPIO_USES_MPI_REQUEST\n    MPI_Wait( &request, &status );\n#else\n    MPIO_Wait(&request, &status);\n#endif\n\n    MPI_File_close(&fh);\n\n    \n\n    for (i=0; i<nints; i++) {\n\tif (buf[i] != (rank*100000 + i)) {\n\t    errs++;\n\t    fprintf(stderr, \"Process %d: error, read %d, should be %d\\n\", rank, buf[i], rank*100000+i);\n\t}\n    }\n\n    MPI_Allreduce( &errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD );\n    if (rank == 0) {\n\tif( toterrs > 0) {\n\t    fprintf( stderr, \"Found %d errors\\n\", toterrs );\n\t}\n\telse {\n\t    fprintf( stdout, \" No Errors\\n\" );\n\t}\n    }\n\n    free(buf);\n    free(filename);\n    free(tmp);\n\n    MPI_Finalize();\n    return 0; \n}"}
{"program": "vincent-noel_1499", "code": "int \tmain (int argc, char ** argv)\n{\n\n#ifdef MPI\n\t\n\n\tint nnodes, myid;\n\n\tint rc = \t     \n\n\tif (rc != MPI_SUCCESS)\n\t\tprintf (\" > Error starting MPI program. \\n\");\n\n\n\n\n\n#endif\n\n\n\tk = 1e-6;\n\tn = 1e-6;\n\ttheta = 1e-6;\n\tbasal = 1e-6;\n\n\tsrand ( time(NULL) );\n\tseed = rand();\n\tseed *= rand();\n\n\t\n\n#ifdef MPI\n\tSAType * t_sa = InitPLSA(nnodes, myid);\n\n#else\n\tSAType * t_sa = InitPLSA();\n\n#endif\n\tt_sa->seed = seed;\n\tt_sa->scoreFunction = &score_function;\n\tt_sa->initial_temp = 100;\n\tt_sa->lambda = 0.00001;\n\tt_sa->initial_moves = 2000000;\n\tt_sa->tau = 10000;\n\tt_sa->interval = 1000;\n\tt_sa->criterion = 1e-4;\n\t\n\n\tPArrPtr * params = InitPLSAParameters(4);\n\tparams->array[0] = (ParamList) { &k, 1e-6, (Range) {0,1e+16}, 3, \"k\"};\n\tparams->array[1] = (ParamList) { &n, 1e-6, (Range) {0,1e+16}, 3, \"n\"};\n\tparams->array[2] = (ParamList) { &theta, 1e-6, (Range) {0,1e+16}, 3, \"theta\"};\n\tparams->array[3] = (ParamList) { &basal, 1e-6, (Range) {0,1e+16}, 3, \"basal\"};\n\n\t\n\n\tPLSARes * res = runPLSA();\n\n\tfree(params->array);\n\tfree(res->params);\n\tfree(res);\n\n\n#ifdef MPI\n\n\t\n\n#endif\n\n\treturn 0;\n}", "label": "int \tmain (int argc, char ** argv)\n{\n\n#ifdef MPI\n\t\n\n\tint nnodes, myid;\n\n\tint rc = MPI_Init(NULL, NULL); \t     \n\n\tif (rc != MPI_SUCCESS)\n\t\tprintf (\" > Error starting MPI program. \\n\");\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nnodes);        \n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);         \n\n\n\n#endif\n\n\n\tk = 1e-6;\n\tn = 1e-6;\n\ttheta = 1e-6;\n\tbasal = 1e-6;\n\n\tsrand ( time(NULL) );\n\tseed = rand();\n\tseed *= rand();\n\n\t\n\n#ifdef MPI\n\tSAType * t_sa = InitPLSA(nnodes, myid);\n\n#else\n\tSAType * t_sa = InitPLSA();\n\n#endif\n\tt_sa->seed = seed;\n\tt_sa->scoreFunction = &score_function;\n\tt_sa->initial_temp = 100;\n\tt_sa->lambda = 0.00001;\n\tt_sa->initial_moves = 2000000;\n\tt_sa->tau = 10000;\n\tt_sa->interval = 1000;\n\tt_sa->criterion = 1e-4;\n\t\n\n\tPArrPtr * params = InitPLSAParameters(4);\n\tparams->array[0] = (ParamList) { &k, 1e-6, (Range) {0,1e+16}, 3, \"k\"};\n\tparams->array[1] = (ParamList) { &n, 1e-6, (Range) {0,1e+16}, 3, \"n\"};\n\tparams->array[2] = (ParamList) { &theta, 1e-6, (Range) {0,1e+16}, 3, \"theta\"};\n\tparams->array[3] = (ParamList) { &basal, 1e-6, (Range) {0,1e+16}, 3, \"basal\"};\n\n\t\n\n\tPLSARes * res = runPLSA();\n\n\tfree(params->array);\n\tfree(res->params);\n\tfree(res);\n\n\n#ifdef MPI\n\n\t\n\n\tMPI_Finalize();\n#endif\n\n\treturn 0;\n}"}
{"program": "mkurnosov_1501", "code": "int main(int argc, char *argv[]) \n{\n    int commsize, rank;\n    double ttotal =\n        \n    int rows, cols, ticks;\n    \n    \n\n    if (rank == 0) {\n        rows = (argc > 1) ? atoi(argv[1]) : 100;\n        cols = (argc > 2) ? atoi(argv[2]) : 100;\n        ticks = (argc > 3) ? atoi(argv[3]) : 10;\n        \n        if (rows % commsize != 0) {\n            fprintf(stderr, \"commsize must be devisor of rows\\n\");\n        }\n        \n        if (rows < commsize) {\n            fprintf(stderr, \"Number of rows %d less then number of processes %d\\n\", rows, commsize);\n        }\n        \n        int args[3] = {rows, cols, ticks};\n    } else {\n        int args[3];\n        rows = args[0];\n        cols = args[1];\n        ticks = args[2];\n    }    \n   \n    \n\n    int local_rows = rows / commsize;\n    cell_t *local_grid = xcalloc((local_rows + 2) * cols, sizeof(*local_grid));\n    cell_t *local_newgrid = xcalloc((local_rows + 2) * cols, sizeof(*local_newgrid));\n\n    \n\n    srand(rank);\n    for (int i = 1; i <= local_rows; i++) {\n        for (int j = 0; j < cols; j++)\n            local_grid[IND(i, j)] = rand() % 10 > 0 ? 0 : 1;\n    }\n    \n    simulate_life(local_grid, local_newgrid, local_rows, cols, ticks);\n    \n    free(local_newgrid);\n    free(local_grid);\n\n    ttotal +=\n    printf(\"Process %2d total time: %.6f\\n\", rank, ttotal);\n\n    \n    return 0;\n}", "label": "int main(int argc, char *argv[]) \n{\n    int commsize, rank;\n    MPI_Init(&argc, &argv);\n    double ttotal = -MPI_Wtime();\n    MPI_Comm_size(MPI_COMM_WORLD, &commsize);   \n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        \n    int rows, cols, ticks;\n    \n    \n\n    if (rank == 0) {\n        rows = (argc > 1) ? atoi(argv[1]) : 100;\n        cols = (argc > 2) ? atoi(argv[2]) : 100;\n        ticks = (argc > 3) ? atoi(argv[3]) : 10;\n        \n        if (rows % commsize != 0) {\n            fprintf(stderr, \"commsize must be devisor of rows\\n\");\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n        \n        if (rows < commsize) {\n            fprintf(stderr, \"Number of rows %d less then number of processes %d\\n\", rows, commsize);\n            MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n        }\n        \n        int args[3] = {rows, cols, ticks};\n        MPI_Bcast(&args, NELEMS(args), MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        int args[3];\n        MPI_Bcast(&args, NELEMS(args), MPI_INT, 0, MPI_COMM_WORLD);\n        rows = args[0];\n        cols = args[1];\n        ticks = args[2];\n    }    \n   \n    \n\n    int local_rows = rows / commsize;\n    cell_t *local_grid = xcalloc((local_rows + 2) * cols, sizeof(*local_grid));\n    cell_t *local_newgrid = xcalloc((local_rows + 2) * cols, sizeof(*local_newgrid));\n\n    \n\n    srand(rank);\n    for (int i = 1; i <= local_rows; i++) {\n        for (int j = 0; j < cols; j++)\n            local_grid[IND(i, j)] = rand() % 10 > 0 ? 0 : 1;\n    }\n    \n    simulate_life(local_grid, local_newgrid, local_rows, cols, ticks);\n    \n    free(local_newgrid);\n    free(local_grid);\n\n    ttotal += MPI_Wtime();\n    printf(\"Process %2d total time: %.6f\\n\", rank, ttotal);\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
{"program": "chu11_1505", "code": "int main (int argc, char *argv[])\n{\n    int id, ntasks;\n    int abort_rank = -1;\n\n    if (argc == 2)\n        abort_rank = strtol (argv[1], NULL, 10);\n\n\n    printf (\"Hello World from rank %d\\n\", id);\n\n    if (id == abort_rank) {\n        fprintf (stderr, \"Rank %d is going to MPI_Abort now\\n\", id);\n    }\n\n\n    return 0;\n}", "label": "int main (int argc, char *argv[])\n{\n    int id, ntasks;\n    int abort_rank = -1;\n\n    if (argc == 2)\n        abort_rank = strtol (argv[1], NULL, 10);\n\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank (MPI_COMM_WORLD, &id);\n    MPI_Comm_size (MPI_COMM_WORLD, &ntasks);\n\n    printf (\"Hello World from rank %d\\n\", id);\n\n    if (id == abort_rank) {\n        fprintf (stderr, \"Rank %d is going to MPI_Abort now\\n\", id);\n        MPI_Abort (MPI_COMM_WORLD, 42);\n    }\n    MPI_Barrier (MPI_COMM_WORLD);\n\n    MPI_Finalize ();\n\n    return 0;\n}"}
{"program": "gnu3ra_1506", "code": "int main( int argc, char *argv[] )\n{\n    int myrank, size, src=0, dest=1, msgsize=1;\n    int buf[20];\n\n\n    if (myrank == src) {\n        buf[0] = 0xabcd0123;\n    }\n    else if (myrank == dest) {\n        buf[0] = 0xffffffff;\n\tif (buf[0] != 0xabcd0123) {\n\t    printf( \"Expected %x but got %x\\n\", 0xabcd0123, buf[0] );\n\t    fflush(stdout);\n\t}\n    }\n\n\n    return 0;\n}", "label": "int main( int argc, char *argv[] )\n{\n    int myrank, size, src=0, dest=1, msgsize=1;\n    int buf[20];\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n    MPI_Comm_rank( MPI_COMM_WORLD, &myrank );\n\n    if (myrank == src) {\n        buf[0] = 0xabcd0123;\n\tMPI_Send( buf, msgsize, MPI_INT, dest, 0, MPI_COMM_WORLD );\n    }\n    else if (myrank == dest) {\n        buf[0] = 0xffffffff;\n\tMPI_Recv( buf, msgsize, MPI_INT, src, 0, MPI_COMM_WORLD, \n\t\t  MPI_STATUS_IGNORE );\n\tif (buf[0] != 0xabcd0123) {\n\t    printf( \"Expected %x but got %x\\n\", 0xabcd0123, buf[0] );\n\t    fflush(stdout);\n\t}\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "bmi-forum_1507", "code": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tStream*\t\tstream;\n\tStream*\t\tdumpStream;\n\n\t\n\t\n\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\tstream = Journal_Register( Info_Type, \"testVariable\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\ttypedef double Triple[3];\n\n\t\tdouble* array;\n\t\tTriple* structArray;\n\t\tIndex length = 10;\n\t\t\n\t\tVariable* var;\n\t\tVariable* vec;\n\n\t\tVariable_Register* reg;\n\n\t\tint i;\n\n\t\tarray = Memory_Alloc_Array( double, length, \"test\" );\n\t\tstructArray = Memory_Alloc_Array( Triple, length, \"test\" );\n\n\t\treg = Variable_Register_New();\n\n\t\tvar = Variable_NewScalar(\n\t\t\t\"Scalar\",\n\t\t\tVariable_DataType_Double,\n\t\t\t&length,\n\t\t\t(void**)&array,\n\t\t\treg );\n\n\t\tvec = Variable_NewVector(\n\t\t\t\"Three\",\n\t\t\tVariable_DataType_Double,\n\t\t\t3,\n\t\t\t&length,\n\t\t\t(void**)&structArray,\n\t\t\treg,\n\t\t\t\"a\",\n\t\t\t\"b\",\n\t\t\t\"c\" );\n\n\t\tVariable_Register_BuildAll( reg );\n\n\t\tfor ( i = 0; i < length; ++i ) {\n\t\t\tVariable_SetValueDouble( var, i, 123.456 );\n\n\t\t\tVariable_SetValueAtDouble( vec, i, 0, 1.2 );\n\t\t\tVariable_SetValueAtDouble( vec, i, 1, 3.4 );\n\t\t\tVariable_SetValueAtDouble( vec, i, 2, 5.6 );\n\t\t}\n\n\t\tdumpStream = Journal_Register( VariableDumpStream_Type, \"scalar dump\" );\n\t\tVariableDumpStream_SetVariable( dumpStream, var, 1, 0, \"data/scalardump.dat\" );\n\n\t\tJournal_Printf( stream, \"Dumping scalar\\n\" );\n\t\tJournal_Dump( dumpStream, NULL );\n\n\t\tdumpStream = Journal_Register( VariableDumpStream_Type, \"vector dump\" );\n\t\tVariableDumpStream_SetVariable( dumpStream, vec, 1, 0, \"data/vectordump.dat\" );\n\n\t\tJournal_Printf( stream, \"Dumping vector\\n\" );\n\t\tJournal_Dump( dumpStream, NULL );\n\t}\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\t\n\n\t\n\treturn 0; \n\n}", "label": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\tStream*\t\tstream;\n\tStream*\t\tdumpStream;\n\n\t\n\t\n\n\tMPI_Init( &argc, &argv );\n\tMPI_Comm_size( MPI_COMM_WORLD, &procCount );\n\tMPI_Comm_rank( MPI_COMM_WORLD, &rank );\n\t\n\tBaseFoundation_Init( &argc, &argv );\n\tBaseIO_Init( &argc, &argv );\n\tBaseContainer_Init( &argc, &argv );\n\tBaseAutomation_Init( &argc, &argv );\n\n\tstream = Journal_Register( Info_Type, \"testVariable\" );\n\t\n\tif( argc >= 2 ) {\n\t\tprocToWatch = atoi( argv[1] );\n\t}\n\telse {\n\t\tprocToWatch = 0;\n\t}\n\tif( rank == procToWatch ) {\n\t\ttypedef double Triple[3];\n\n\t\tdouble* array;\n\t\tTriple* structArray;\n\t\tIndex length = 10;\n\t\t\n\t\tVariable* var;\n\t\tVariable* vec;\n\n\t\tVariable_Register* reg;\n\n\t\tint i;\n\n\t\tarray = Memory_Alloc_Array( double, length, \"test\" );\n\t\tstructArray = Memory_Alloc_Array( Triple, length, \"test\" );\n\n\t\treg = Variable_Register_New();\n\n\t\tvar = Variable_NewScalar(\n\t\t\t\"Scalar\",\n\t\t\tVariable_DataType_Double,\n\t\t\t&length,\n\t\t\t(void**)&array,\n\t\t\treg );\n\n\t\tvec = Variable_NewVector(\n\t\t\t\"Three\",\n\t\t\tVariable_DataType_Double,\n\t\t\t3,\n\t\t\t&length,\n\t\t\t(void**)&structArray,\n\t\t\treg,\n\t\t\t\"a\",\n\t\t\t\"b\",\n\t\t\t\"c\" );\n\n\t\tVariable_Register_BuildAll( reg );\n\n\t\tfor ( i = 0; i < length; ++i ) {\n\t\t\tVariable_SetValueDouble( var, i, 123.456 );\n\n\t\t\tVariable_SetValueAtDouble( vec, i, 0, 1.2 );\n\t\t\tVariable_SetValueAtDouble( vec, i, 1, 3.4 );\n\t\t\tVariable_SetValueAtDouble( vec, i, 2, 5.6 );\n\t\t}\n\n\t\tdumpStream = Journal_Register( VariableDumpStream_Type, \"scalar dump\" );\n\t\tVariableDumpStream_SetVariable( dumpStream, var, 1, 0, \"data/scalardump.dat\" );\n\n\t\tJournal_Printf( stream, \"Dumping scalar\\n\" );\n\t\tJournal_Dump( dumpStream, NULL );\n\n\t\tdumpStream = Journal_Register( VariableDumpStream_Type, \"vector dump\" );\n\t\tVariableDumpStream_SetVariable( dumpStream, vec, 1, 0, \"data/vectordump.dat\" );\n\n\t\tJournal_Printf( stream, \"Dumping vector\\n\" );\n\t\tJournal_Dump( dumpStream, NULL );\n\t}\n\t\n\tBaseAutomation_Finalise();\n\tBaseContainer_Finalise();\n\tBaseIO_Finalise();\n\tBaseFoundation_Finalise();\n\n\t\n\t\n\n\tMPI_Finalize();\n\t\n\treturn 0; \n\n}"}
{"program": "jianfeipan_1508", "code": "int main(int argc, char *argv[])\r\n{\r\n  MPI_Status status;\r\n  int i,j, cutsize, num, rank, size, tag,   nbslaves;\r\n  char inputstr[MAX_LENGTH+1], outstr[MAX_LENGTH+1];\r\n\r\n\r\n  \n\r\n\r\n \r\n  \n\r\n\r\n  tag = 201;\r\n\r\n  nbslaves = size -1;\r\n\r\n  if (rank == 0) {\r\n\r\n    if (strlen(argv[1]) > MAX_LENGTH * nbslaves) {\r\n      printf(\"The string to convert is too long, shrinking\\n\");\r\n      argv[1][MAX_LENGTH*nbslaves]='\\0';\r\n    }\r\n\r\n    \n\r\n     printf(\"number of arguments in the command line %d  \\n\", argc);\r\n\r\n     printf(\"I 'll convert  %s using %d slaves \\n\", argv [1], nbslaves);\r\n\r\n     cutsize = strlen(argv[1]) / nbslaves * sizeof(char);\r\n\r\n    \n\r\n    for( i=0 ; i < nbslaves ; i++ ) {\r\n      \n\r\n      \n\r\n      strncpy(outstr, argv[1]+(i*cutsize), cutsize+nbslaves);\r\n      if (i != (nbslaves-1)) \n\n\toutstr[cutsize]='\\0';\r\n\r\n    printf(\"Process sending %s to %d\\n\", outstr, i);\r\n\r\n  }\r\n    for( i=1 ; i < nbslaves+1 ; i++ ) {\r\n    printf(\"Process receiving %s from node %d \\n\", inputstr, i);\r\n    }\r\n    }\r\n\r\n  \r\n\r\nelse {\r\n\r\n \r\n    \n\r\n    for (i=0; inputstr [i] != '\\0' && (i < MAX_LENGTH); i++)\r\n        \n\r\n        outstr[i] = toupper(inputstr[i]);\r\n\r\n    outstr[i] = '\\0';\r\n}\r\n\r\n  fflush(stdout);\r\n  return 0;\r\n}", "label": "int main(int argc, char *argv[])\r\n{\r\n  MPI_Status status;\r\n  int i,j, cutsize, num, rank, size, tag,   nbslaves;\r\n  char inputstr[MAX_LENGTH+1], outstr[MAX_LENGTH+1];\r\n\r\n\r\n  \n\r\n\r\n  MPI_Init(&argc, &argv);\r\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\r\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\r\n \r\n  \n\r\n\r\n  tag = 201;\r\n\r\n  nbslaves = size -1;\r\n\r\n  if (rank == 0) {\r\n\r\n    if (strlen(argv[1]) > MAX_LENGTH * nbslaves) {\r\n      printf(\"The string to convert is too long, shrinking\\n\");\r\n      argv[1][MAX_LENGTH*nbslaves]='\\0';\r\n    }\r\n\r\n    \n\r\n     printf(\"number of arguments in the command line %d  \\n\", argc);\r\n\r\n     printf(\"I 'll convert  %s using %d slaves \\n\", argv [1], nbslaves);\r\n\r\n     cutsize = strlen(argv[1]) / nbslaves * sizeof(char);\r\n\r\n    \n\r\n    for( i=0 ; i < nbslaves ; i++ ) {\r\n      \n\r\n      \n\r\n      strncpy(outstr, argv[1]+(i*cutsize), cutsize+nbslaves);\r\n      if (i != (nbslaves-1)) \n\n\toutstr[cutsize]='\\0';\r\n\r\n    printf(\"Process sending %s to %d\\n\", outstr, i);\r\n\r\n    MPI_Send(outstr, strlen(outstr)+1, MPI_CHAR, i+1, tag, MPI_COMM_WORLD); \r\n  }\r\n    for( i=1 ; i < nbslaves+1 ; i++ ) {\r\n    MPI_Recv(&inputstr, MAX_LENGTH+1, MPI_CHAR, i, tag+1, MPI_COMM_WORLD, &status);\r\n    printf(\"Process receiving %s from node %d \\n\", inputstr, i);\r\n    }\r\n    }\r\n\r\n  \r\n\r\nelse {\r\n\r\n    MPI_Recv(&inputstr, MAX_LENGTH+1, MPI_CHAR, 0, tag, MPI_COMM_WORLD, &status);\r\n \r\n    \n\r\n    for (i=0; inputstr [i] != '\\0' && (i < MAX_LENGTH); i++)\r\n        \n\r\n        outstr[i] = toupper(inputstr[i]);\r\n\r\n    outstr[i] = '\\0';\r\n    MPI_Send(outstr, strlen(outstr)+1 , MPI_CHAR, 0, tag+1, MPI_COMM_WORLD); \r\n}\r\n\r\n  fflush(stdout);\r\n  MPI_Finalize();\r\n  return 0;\r\n}"}
{"program": "habanero-rice_1511", "code": "int main(int argc, char *argv[])\n{\n    str_len = STRING_LEN;\n    query_len = QUERY_LEN;\n    numstrings = DEFAULT_NUM_STRING;\n    numqueries = DEFAULT_NUM_QUERIES;\n    if (argc > 1)\n        get_options(argc, argv);\n    KMI_init();\n\n    \n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int myid, numprocs;\n    KMI_db_t db;\n    KMI_db_init(&db, KMI_DB_TYPE_ALPHABET_NT, comm);\n    KMI_table_t tb;\n    KMI_table_attribute_t tb_attr = {\n        .type = KMI_TABLE_TYPE_FIXED_LENGTH,\n        .rlen = str_len,        \n\n        .qlen = query_len\n    };\n    KMI_table_init(db, &tb, tb_attr);\n\n    double t0 =\n    \n\n    char *str;\n    str = (char *) malloc(str_len * sizeof(char));\n    srand((int) time(NULL) + myid);\n    int i;\n    for (i = 0; i < numstrings; i++) {\n        generate_random_string(str, str_len);\n        KMI_table_add_string(tb, str, str_len);\n    }\n    free(str);\n\n    double t1 =\n    KMI_table_commit(tb);\n    KMI_db_commit(db);\n    double t2 =\n\n    char *query;\n    query = (char *) malloc(KMI_BIN_STR_LEN(query_len) * sizeof(char) * numqueries);\n    char *buf;\n    buf = (char *)malloc(query_len * sizeof(char));\n    generate_random_queries(query, buf, numqueries);\n\n    double t3 =\n\n#ifdef DEBUG_PRINT\n    double debug_t1, debug_t2;\n    debug_t1 = t2;\n#endif\n    KMI_query_db_start(db);\n    \n    long long hit_count = query_loop(query, db, myid);\n    \n    KMI_query_db_finish(db);\n    double t4 =\n\n    free(query);\n\n    double time_add = t1 - t0;\n    double time_commit = t2 - t1;\n    double time_generate_query = t3 - t2;\n    double time_query = t4 - t3;\n\n    long long total_hit_count;\n\n    if (myid == 0) {\n        printf(\"Hit count: %lld\\n\"\n               \"Adding time cost (s): %f\\n\"\n               \"Commit time cost (s): %f\\n\"\n               \"Generate query time cost (s): %f\\n\"\n               \"Query time cost (s): %f\\n\"\n               \"Query throughput (global #queries/s): %.1f\\n\",\n               total_hit_count, time_add, time_commit, time_generate_query, time_query,\n               (double) numqueries * numprocs / time_query);\n    }\n\n    KMI_finalize();\n    return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n    MPI_Init(&argc, &argv);\n    str_len = STRING_LEN;\n    query_len = QUERY_LEN;\n    numstrings = DEFAULT_NUM_STRING;\n    numqueries = DEFAULT_NUM_QUERIES;\n    if (argc > 1)\n        get_options(argc, argv);\n    KMI_init();\n\n    \n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    int myid, numprocs;\n    MPI_Comm_size(comm, &numprocs);\n    MPI_Comm_rank(comm, &myid);\n    KMI_db_t db;\n    KMI_db_init(&db, KMI_DB_TYPE_ALPHABET_NT, comm);\n    KMI_table_t tb;\n    KMI_table_attribute_t tb_attr = {\n        .type = KMI_TABLE_TYPE_FIXED_LENGTH,\n        .rlen = str_len,        \n\n        .qlen = query_len\n    };\n    KMI_table_init(db, &tb, tb_attr);\n\n    double t0 = MPI_Wtime();\n    \n\n    char *str;\n    str = (char *) malloc(str_len * sizeof(char));\n    srand((int) time(NULL) + myid);\n    int i;\n    for (i = 0; i < numstrings; i++) {\n        generate_random_string(str, str_len);\n        KMI_table_add_string(tb, str, str_len);\n    }\n    free(str);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    double t1 = MPI_Wtime();\n    KMI_table_commit(tb);\n    KMI_db_commit(db);\n    MPI_Barrier(MPI_COMM_WORLD);\n    double t2 = MPI_Wtime();\n\n    char *query;\n    query = (char *) malloc(KMI_BIN_STR_LEN(query_len) * sizeof(char) * numqueries);\n    char *buf;\n    buf = (char *)malloc(query_len * sizeof(char));\n    generate_random_queries(query, buf, numqueries);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    double t3 = MPI_Wtime();\n\n#ifdef DEBUG_PRINT\n    double debug_t1, debug_t2;\n    debug_t1 = t2;\n#endif\n    KMI_query_db_start(db);\n    \n    long long hit_count = query_loop(query, db, myid);\n    \n    KMI_query_db_finish(db);\n    double t4 = MPI_Wtime();\n\n    free(query);\n\n    double time_add = t1 - t0;\n    double time_commit = t2 - t1;\n    double time_generate_query = t3 - t2;\n    double time_query = t4 - t3;\n\n    long long total_hit_count;\n    MPI_Reduce(&hit_count, &total_hit_count, 1, MPI_LONG_LONG_INT, MPI_SUM, 0, comm);\n\n    if (myid == 0) {\n        printf(\"Hit count: %lld\\n\"\n               \"Adding time cost (s): %f\\n\"\n               \"Commit time cost (s): %f\\n\"\n               \"Generate query time cost (s): %f\\n\"\n               \"Query time cost (s): %f\\n\"\n               \"Query throughput (global #queries/s): %.1f\\n\",\n               total_hit_count, time_add, time_commit, time_generate_query, time_query,\n               (double) numqueries * numprocs / time_query);\n    }\n\n    KMI_finalize();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "byu-vv-lab_1512", "code": "int main(int argc, char *argv[]) {\n  initialize();\n  write_frame();\n  printf(\"nx = %d\", nx);\n  for (time=1; time < nsteps; time++) {\n    exchange_ghost_cells();\n    update();\n    if (time%wstep==0)\n      write_frame();\n  }\n  free(u);\n  free(u_new);\n  if (rank == 0)\n    free(buf);\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  MPI_Init(&argc, &argv);\n  initialize();\n  write_frame();\n  printf(\"nx = %d\", nx);\n  for (time=1; time < nsteps; time++) {\n    exchange_ghost_cells();\n    update();\n    if (time%wstep==0)\n      write_frame();\n  }\n  MPI_Finalize();\n  free(u);\n  free(u_new);\n  if (rank == 0)\n    free(buf);\n  return 0;\n}"}
{"program": "CFDEMproject_1514", "code": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem;\n  UserData webdata;\n  long int SystemSize, local_N, mudq, mldq, mukeep, mlkeep;\n  realtype rtol, atol, t0, tout, tret;\n  N_Vector cc, cp, res, id;\n  int thispe, npes, maxl, iout, retval;\n\n  cc = cp = res = id = NULL;\n  webdata = NULL;\n  mem = NULL;\n\n  \n\n\n  comm = MPI_COMM_WORLD;\n\n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d not equal to NPEX*NPEY = %d\\n\", \n              npes, NPEX*NPEY);\n    return(1); \n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB*NUM_SPECIES;\n  SystemSize = NEQ;\n\n  \n\n\n  webdata = (UserData) malloc(sizeof *webdata);\n  webdata->rates = N_VNew_Parallel(comm, local_N, SystemSize);\n  webdata->acoef = newDenseMat(NUM_SPECIES, NUM_SPECIES);\n\n  InitUserData(webdata, thispe, npes, comm);\n  \n  \n\n  \n  cc  = N_VNew_Parallel(comm, local_N, SystemSize);\n\n  cp  = N_VNew_Parallel(comm, local_N, SystemSize);\n\n  res = N_VNew_Parallel(comm, local_N, SystemSize);\n\n  id  = N_VNew_Parallel(comm, local_N, SystemSize);\n  \n  SetInitialProfiles(cc, cp, id, res, webdata);\n  \n  N_VDestroy_Parallel(res);\n  \n  \n\n  \n  t0 = ZERO;\n  rtol = RTOL; \n  atol = ATOL;\n  \n  \n\n\n  mem = IDACreate();\n\n  retval = IDASetUserData(mem, webdata);\n\n  retval = IDASetId(mem, id);\n\n  retval = IDAInit(mem, resweb, t0, cc, cp);\n  \n  retval = IDASStolerances(mem, rtol, atol);\n\n  \n\n\n  maxl = 16;\n  retval = IDASpgmr(mem, maxl);\n\n  \n\n  \n  mudq = mldq = NSMXSUB;\n  mukeep = mlkeep = 2;\n  retval = IDABBDPrecInit(mem, local_N, mudq, mldq, mukeep, mlkeep, \n                          ZERO, reslocal, NULL);\n\n  \n\n  \n  tout = RCONST(0.001);\n  retval = IDACalcIC(mem, IDA_YA_YDP_INIT, tout);\n  \n  \n\n \n  if (thispe == 0) PrintHeader(SystemSize, maxl, \n                               mudq, mldq, mukeep, mlkeep,\n                               rtol, atol);\n  PrintOutput(mem, cc, t0, webdata, comm);\n\n  \n\n  \n  for (iout = 1; iout <= NOUT; iout++) {\n    \n    retval = IDASolve(mem, tout, &tret, cc, cp, IDA_NORMAL);\n    \n    PrintOutput(mem, cc, tret, webdata, comm);\n    \n    if (iout < 3) tout *= TMULT; \n    else          tout += TADD;\n\n  }\n  \n  \n\n  \n  if (thispe == 0)  PrintFinalStats(mem);\n\n  \n\n\n  N_VDestroy_Parallel(cc);\n  N_VDestroy_Parallel(cp);\n  N_VDestroy_Parallel(id);\n\n  IDAFree(&mem);\n\n  destroyMat(webdata->acoef);\n  N_VDestroy_Parallel(webdata->rates);\n  free(webdata);\n\n\n  return(0);\n}", "label": "int main(int argc, char *argv[])\n{\n  MPI_Comm comm;\n  void *mem;\n  UserData webdata;\n  long int SystemSize, local_N, mudq, mldq, mukeep, mlkeep;\n  realtype rtol, atol, t0, tout, tret;\n  N_Vector cc, cp, res, id;\n  int thispe, npes, maxl, iout, retval;\n\n  cc = cp = res = id = NULL;\n  webdata = NULL;\n  mem = NULL;\n\n  \n\n\n  MPI_Init(&argc, &argv);\n  comm = MPI_COMM_WORLD;\n  MPI_Comm_rank(comm, &thispe);\n  MPI_Comm_size(comm, &npes);\n\n  if (npes != NPEX*NPEY) {\n    if (thispe == 0)\n      fprintf(stderr, \n              \"\\nMPI_ERROR(0): npes = %d not equal to NPEX*NPEY = %d\\n\", \n              npes, NPEX*NPEY);\n    MPI_Finalize();\n    return(1); \n  }\n  \n  \n\n\n  local_N = MXSUB*MYSUB*NUM_SPECIES;\n  SystemSize = NEQ;\n\n  \n\n\n  webdata = (UserData) malloc(sizeof *webdata);\n  webdata->rates = N_VNew_Parallel(comm, local_N, SystemSize);\n  webdata->acoef = newDenseMat(NUM_SPECIES, NUM_SPECIES);\n\n  InitUserData(webdata, thispe, npes, comm);\n  \n  \n\n  \n  cc  = N_VNew_Parallel(comm, local_N, SystemSize);\n  if(check_flag((void *)cc, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  cp  = N_VNew_Parallel(comm, local_N, SystemSize);\n  if(check_flag((void *)cp, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  res = N_VNew_Parallel(comm, local_N, SystemSize);\n  if(check_flag((void *)res, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n\n  id  = N_VNew_Parallel(comm, local_N, SystemSize);\n  if(check_flag((void *)id, \"N_VNew_Parallel\", 0, thispe)) MPI_Abort(comm, 1);\n  \n  SetInitialProfiles(cc, cp, id, res, webdata);\n  \n  N_VDestroy_Parallel(res);\n  \n  \n\n  \n  t0 = ZERO;\n  rtol = RTOL; \n  atol = ATOL;\n  \n  \n\n\n  mem = IDACreate();\n  if(check_flag((void *)mem, \"IDACreate\", 0, thispe)) MPI_Abort(comm, 1);\n\n  retval = IDASetUserData(mem, webdata);\n  if(check_flag(&retval, \"IDASetUserData\", 1, thispe)) MPI_Abort(comm, 1);\n\n  retval = IDASetId(mem, id);\n  if(check_flag(&retval, \"IDASetId\", 1, thispe)) MPI_Abort(comm, 1);\n\n  retval = IDAInit(mem, resweb, t0, cc, cp);\n  if(check_flag(&retval, \"IDAInit\", 1, thispe)) MPI_Abort(comm, 1);\n  \n  retval = IDASStolerances(mem, rtol, atol);\n  if(check_flag(&retval, \"IDASStolerances\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n\n  maxl = 16;\n  retval = IDASpgmr(mem, maxl);\n  if(check_flag(&retval, \"IDASpgmr\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n  \n  mudq = mldq = NSMXSUB;\n  mukeep = mlkeep = 2;\n  retval = IDABBDPrecInit(mem, local_N, mudq, mldq, mukeep, mlkeep, \n                          ZERO, reslocal, NULL);\n  if(check_flag(&retval, \"IDABBDPrecInit\", 1, thispe)) MPI_Abort(comm, 1);\n\n  \n\n  \n  tout = RCONST(0.001);\n  retval = IDACalcIC(mem, IDA_YA_YDP_INIT, tout);\n  if(check_flag(&retval, \"IDACalcIC\", 1, thispe)) MPI_Abort(comm, 1);\n  \n  \n\n \n  if (thispe == 0) PrintHeader(SystemSize, maxl, \n                               mudq, mldq, mukeep, mlkeep,\n                               rtol, atol);\n  PrintOutput(mem, cc, t0, webdata, comm);\n\n  \n\n  \n  for (iout = 1; iout <= NOUT; iout++) {\n    \n    retval = IDASolve(mem, tout, &tret, cc, cp, IDA_NORMAL);\n    if(check_flag(&retval, \"IDASolve\", 1, thispe)) MPI_Abort(comm, 1);\n    \n    PrintOutput(mem, cc, tret, webdata, comm);\n    \n    if (iout < 3) tout *= TMULT; \n    else          tout += TADD;\n\n  }\n  \n  \n\n  \n  if (thispe == 0)  PrintFinalStats(mem);\n\n  \n\n\n  N_VDestroy_Parallel(cc);\n  N_VDestroy_Parallel(cp);\n  N_VDestroy_Parallel(id);\n\n  IDAFree(&mem);\n\n  destroyMat(webdata->acoef);\n  N_VDestroy_Parallel(webdata->rates);\n  free(webdata);\n\n  MPI_Finalize();\n\n  return(0);\n}"}
{"program": "Naarukarmic_1515", "code": "int main(int argc, char **argv)\n{\n\n  \n  int rank;\n  int size; \n\n  \n  int nb_line, id;\n  int i, j, h, w, k, l, levels ;\n    struct timeval tdeb, tfin;\n    \n  MPI_Status status;\n  \n  if(rank == 0){\n\n      read_image (image_in, IMAGE_IN, &h, &w, &levels); \n      gettimeofday(&tdeb, NULL);\n      int nb_line = (int) h / size; \n\n      int id;\n      \n\n      for (id = 1; id < size; id++) {\n      }\n      \n\n      for (id = 1; id < size; id++) {\n        int ligne;\n        sleep(2); \n\n        for (ligne = (id - 1) * nb_line; ligne < (id * nb_line) + 1; ligne++){\n        }\n      }\n      \n\n      for (id = 1; id < size; id++) {\n        int ligne;\n        for (ligne = (id - 1) * nb_line; ligne < id * nb_line + 1; ligne++){\n        }\n    }\n    gettimeofday(&tfin, NULL);\n    printf (\"computation time (microseconds): %ld\\n\",  (tfin.tv_sec - tdeb.tv_sec)*1000000 + (tfin.tv_usec - tdeb.tv_usec));\n\n    write_image(image_out, IMAGE_OUT, h, w, levels);\n  }  \n    \n  if (rank != 0){ \n\n    int ligne, i, j, k, l;\n    for(ligne = (rank - 1) * nb_line; ligne < rank * nb_line; ligne++){\n      \n\n    }\n    for (i = (rank - 1) * nb_line; i < rank * nb_line + 1 ; i++) { \n\n      for (j = 0; j < w; j++) {\n        if ( i==0 || i == h - 1 || j == 0 || j == w - 1){ \n\n          image_out[i][j] = image_in[i][j];\n        }\n        else{ \n\n          image_out[i][j] = -1 * image_in[i-1][j-1] + 1*image_in[i-1][j+1];\n          image_out[i][j] += -3 * image_in[i][j-1] + 3*image_in[i][j+1];\n          image_out[i][j] += -1 * image_in[i+1][j-1] + 1*image_in[i+1][j+1];\n          \n          if(image_out[i][j] < 0){ \n\n            image_out[i][j] = 0;\n          }\n          if(image_out[i][j] > 255){\n            image_out[i][j] = 255;\n          }\n        }\n      }\n    }\n    for(ligne = (rank - 1) * nb_line; ligne < rank * nb_line + 1; ligne++){ \n    }\n  }\n  \n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n  MPI_Init ( &argc, &argv);\n  \n  int rank;\n  int size; \n\n  \n  int nb_line, id;\n  int i, j, h, w, k, l, levels ;\n    struct timeval tdeb, tfin;\n    \n  MPI_Status status;\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n  \n  if(rank == 0){\n\n      read_image (image_in, IMAGE_IN, &h, &w, &levels); \n      gettimeofday(&tdeb, NULL);\n      int nb_line = (int) h / size; \n\n      int id;\n      \n\n      for (id = 1; id < size; id++) {\n        MPI_Send(&nb_line, 1, MPI_INT, id, 0, MPI_COMM_WORLD);\n        MPI_Send(&w, 1, MPI_INT, id, 0, MPI_COMM_WORLD);\n      }\n      \n\n      for (id = 1; id < size; id++) {\n        int ligne;\n        sleep(2); \n\n        for (ligne = (id - 1) * nb_line; ligne < (id * nb_line) + 1; ligne++){\n          MPI_Send(image_in[ligne], w, MPI_INT, id, 0, MPI_COMM_WORLD);\n        }\n      }\n      \n\n      for (id = 1; id < size; id++) {\n        int ligne;\n        for (ligne = (id - 1) * nb_line; ligne < id * nb_line + 1; ligne++){\n          MPI_Recv(image_out[ligne], w, MPI_INT, id, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    gettimeofday(&tfin, NULL);\n    printf (\"computation time (microseconds): %ld\\n\",  (tfin.tv_sec - tdeb.tv_sec)*1000000 + (tfin.tv_usec - tdeb.tv_usec));\n\n    write_image(image_out, IMAGE_OUT, h, w, levels);\n  }  \n    \n  if (rank != 0){ \n\n    MPI_Recv(&nb_line, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    MPI_Recv(&w, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    int ligne, i, j, k, l;\n    for(ligne = (rank - 1) * nb_line; ligne < rank * nb_line; ligne++){\n      \n\n      MPI_Recv(image_in[ligne], w, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);   \n    }\n    for (i = (rank - 1) * nb_line; i < rank * nb_line + 1 ; i++) { \n\n      for (j = 0; j < w; j++) {\n        if ( i==0 || i == h - 1 || j == 0 || j == w - 1){ \n\n          image_out[i][j] = image_in[i][j];\n        }\n        else{ \n\n          image_out[i][j] = -1 * image_in[i-1][j-1] + 1*image_in[i-1][j+1];\n          image_out[i][j] += -3 * image_in[i][j-1] + 3*image_in[i][j+1];\n          image_out[i][j] += -1 * image_in[i+1][j-1] + 1*image_in[i+1][j+1];\n          \n          if(image_out[i][j] < 0){ \n\n            image_out[i][j] = 0;\n          }\n          if(image_out[i][j] > 255){\n            image_out[i][j] = 255;\n          }\n        }\n      }\n    }\n    for(ligne = (rank - 1) * nb_line; ligne < rank * nb_line + 1; ligne++){ \n      MPI_Send(image_out[ligne], w, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n  }\n  \n  MPI_Finalize();\n  return 0;\n}"}
{"program": "rabauke_1518", "code": "int main(int argc, char *argv[]) {\n  int rank, size;\n  srand(time(NULL) * rank);\n\n  const size_t N = 100000000 / size;\n  double *v = malloc(N * sizeof(*v));\n  if (v == NULL)\n  fill_random((vector){v, N});\n  vector sorted = parallel_sort((vector){v, N});\n  free(sorted.data);\n  free(v);\n  return EXIT_SUCCESS;\n}", "label": "int main(int argc, char *argv[]) {\n  MPI_Init(&argc, &argv);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  srand(time(NULL) * rank);\n\n  const size_t N = 100000000 / size;\n  double *v = malloc(N * sizeof(*v));\n  if (v == NULL)\n    MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n  fill_random((vector){v, N});\n  vector sorted = parallel_sort((vector){v, N});\n  free(sorted.data);\n  free(v);\n  MPI_Finalize();\n  return EXIT_SUCCESS;\n}"}
{"program": "daidong_1519", "code": "int main(int argc, char **argv)\n{\n    struct test_case *tc;\n    int rc;\n    int rank;\n    int size;\n    int path_count = 0;\n    int opt;\n    int verbose = 0;\n    int i;\n    char *p;\n    char *path[4] = { \"/tmp\", NULL, NULL, NULL };\n\n    while((opt = getopt(argc, argv, \"b:p:v\")) != -1)\n    {\n        switch(opt)\n        {\n        case 'p':\n            path[path_count] = optarg;\n            for (p=optarg;((p!=NULL) && (*p!=0));p++)\n            {\n                if (*p == ':')\n                {\n                    *p = 0;\n                    path_count++;\n                    path[path_count] = p+1;\n                }\n            }\n            break;\n        case 'b':\n            BYTES_PER_RANK = atoi(optarg);\n            break;\n        case 'v':\n            verbose = 1;\n            break;\n        }\n    }\n    path_count++;\n\n    rc =\n\n    rc =\n\n    rc =\n\n    if ((verbose) && (rank == 0))\n    {\n        printf(\"bytes_per_rank: %d\\n\", BYTES_PER_RANK);\n        for (i=0; i < path_count; i++)\n        { printf(\"path[%d]: %s\\n\", i, path[i]); }\n    }\n\n    for (tc=&tests[0];tc->func!=NULL;tc++)\n    {\n        int trc = 0;\n        int all = 0;\n\n        trc = tc->func(path[rand()%path_count],size, rank);\n        rc =\n        if (rank == 0)\n        {\n            printf(\"%s: %s\\n\", tc->name, (all?\"passed\":\"failed\"));\n        }\n    }\n\n    rc =\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    struct test_case *tc;\n    int rc;\n    int rank;\n    int size;\n    int path_count = 0;\n    int opt;\n    int verbose = 0;\n    int i;\n    char *p;\n    char *path[4] = { \"/tmp\", NULL, NULL, NULL };\n\n    while((opt = getopt(argc, argv, \"b:p:v\")) != -1)\n    {\n        switch(opt)\n        {\n        case 'p':\n            path[path_count] = optarg;\n            for (p=optarg;((p!=NULL) && (*p!=0));p++)\n            {\n                if (*p == ':')\n                {\n                    *p = 0;\n                    path_count++;\n                    path[path_count] = p+1;\n                }\n            }\n            break;\n        case 'b':\n            BYTES_PER_RANK = atoi(optarg);\n            break;\n        case 'v':\n            verbose = 1;\n            break;\n        }\n    }\n    path_count++;\n\n    rc = MPI_Init(&argc, &argv);\n    MPI_CHECK(rc,\"MPI_Init\");\n\n    rc = MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_CHECK(rc,\"MPI_Comm_size\");\n\n    rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_CHECK(rc,\"MPI_Comm_rank\");\n\n    if ((verbose) && (rank == 0))\n    {\n        printf(\"bytes_per_rank: %d\\n\", BYTES_PER_RANK);\n        for (i=0; i < path_count; i++)\n        { printf(\"path[%d]: %s\\n\", i, path[i]); }\n    }\n\n    for (tc=&tests[0];tc->func!=NULL;tc++)\n    {\n        int trc = 0;\n        int all = 0;\n\n        trc = tc->func(path[rand()%path_count],size, rank);\n        rc = MPI_Reduce(&trc,&all,1,MPI_INT,\n                        MPI_LAND,0,MPI_COMM_WORLD);\n        if (rank == 0)\n        {\n            printf(\"%s: %s\\n\", tc->name, (all?\"passed\":\"failed\"));\n        }\n    }\n\n    rc = MPI_Finalize();\n    MPI_CHECK(rc,\"MPI_Finalize\");\n\n    return 0;\n}"}
{"program": "aocalderon_1520", "code": "int main (int argc, char *argv[]) {\n   unsigned int count; \n\n   double elapsed_time; \n\n   unsigned long  first; \n\n   unsigned int global_count; \n\n   unsigned long long high_value; \n\n   unsigned long long i;\n   int id; \n\n   unsigned long index; \n\n   unsigned long long low_value; \n\n   char *marked; \n\n   unsigned long long int  n; \n\n   int p; \n\n   unsigned long proc0_size; \n\n   unsigned long prime; \n\n   unsigned long long size; \n\n\n\n   \n\n   elapsed_time =\n   if (argc != 2) {\n      if (!id) printf (\"Command line: %s <m>\\n\", argv[0]);\n      exit (1);\n   }\n   char *e;\n   n = strtoull(argv[1], &e, 10);\n   \n\n   low_value = 2 + id*(n-1)/p;\n   high_value = 1 + (id+1)*(n-1)/p;\n   size = high_value - low_value + 1;\n   \n\n   proc0_size = (n-1)/p;\n   if ((2 + proc0_size) < (int) sqrt((double) n)) {\n      if (!id) printf (\"Too many processes\\n\");\n      exit (1);\n   }\n   \n\n   marked = (char *) malloc (size);\n   if (marked == NULL) {\n      printf (\"Cannot allocate enough memory\\n\");\n      exit (1);\n   }\n   for (i = 0; i < size; i++) marked[i] = 0;\n   if (!id) index = 0;\n   prime = 2;\n   do {\n      if (prime * prime > low_value)\n         first = prime * prime - low_value;\n      else {\n         if (!(low_value % prime)) first = 0;\n         else first = prime - (low_value % prime);\n      }\n      for (i = first; i < size; i += prime) marked[i] = 1;\n      if (!id) {\n         while (marked[++index]);\n         prime = index + 2;\n      }\n   } while (prime * prime <= n);\n   count = 0;\n   for (i = 0; i < size; i++)\n      if (!marked[i]) count++;\n   \n\n   elapsed_time +=\n   \n\n   if (!id) {\n      printf(\"S0, %llu, %d, %lu, %10.6f\\n\", n, p, global_count, elapsed_time);\n   }\n   return 0;\n}", "label": "int main (int argc, char *argv[]) {\n   unsigned int count; \n\n   double elapsed_time; \n\n   unsigned long  first; \n\n   unsigned int global_count; \n\n   unsigned long long high_value; \n\n   unsigned long long i;\n   int id; \n\n   unsigned long index; \n\n   unsigned long long low_value; \n\n   char *marked; \n\n   unsigned long long int  n; \n\n   int p; \n\n   unsigned long proc0_size; \n\n   unsigned long prime; \n\n   unsigned long long size; \n\n\n\n   MPI_Init (&argc, &argv);\n   \n\n   MPI_Comm_rank (MPI_COMM_WORLD, &id);\n   MPI_Comm_size (MPI_COMM_WORLD, &p);\n   MPI_Barrier(MPI_COMM_WORLD);\n   elapsed_time = -MPI_Wtime();\n   if (argc != 2) {\n      if (!id) printf (\"Command line: %s <m>\\n\", argv[0]);\n      MPI_Finalize();\n      exit (1);\n   }\n   char *e;\n   n = strtoull(argv[1], &e, 10);\n   \n\n   low_value = 2 + id*(n-1)/p;\n   high_value = 1 + (id+1)*(n-1)/p;\n   size = high_value - low_value + 1;\n   \n\n   proc0_size = (n-1)/p;\n   if ((2 + proc0_size) < (int) sqrt((double) n)) {\n      if (!id) printf (\"Too many processes\\n\");\n      MPI_Finalize();\n      exit (1);\n   }\n   \n\n   marked = (char *) malloc (size);\n   if (marked == NULL) {\n      printf (\"Cannot allocate enough memory\\n\");\n      MPI_Finalize();\n      exit (1);\n   }\n   for (i = 0; i < size; i++) marked[i] = 0;\n   if (!id) index = 0;\n   prime = 2;\n   do {\n      if (prime * prime > low_value)\n         first = prime * prime - low_value;\n      else {\n         if (!(low_value % prime)) first = 0;\n         else first = prime - (low_value % prime);\n      }\n      for (i = first; i < size; i += prime) marked[i] = 1;\n      if (!id) {\n         while (marked[++index]);\n         prime = index + 2;\n      }\n      if (p > 1) MPI_Bcast (&prime, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   } while (prime * prime <= n);\n   count = 0;\n   for (i = 0; i < size; i++)\n      if (!marked[i]) count++;\n   if (p > 1) MPI_Reduce (&count, &global_count, 1, MPI_INT, MPI_SUM,\n      0, MPI_COMM_WORLD);\n   \n\n   elapsed_time += MPI_Wtime();\n   \n\n   if (!id) {\n      printf(\"S0, %llu, %d, %lu, %10.6f\\n\", n, p, global_count, elapsed_time);\n   }\n   MPI_Finalize ();\n   return 0;\n}"}
{"program": "jiajuncao_1526", "code": "int main(int argc, char* argv[])\n{\n  int rank;\n  int size;\n  int i = 1;\n\n\n  if (rank == 0)\n    printf(\"*** Will print ten rows of dots.\\n\");  \n\n  printf(\"Hello, world, I am %d of %d\\n\", rank, size);\n\n  double count = 1e6;\n  for (i = 1; i < (int)count; i++)\n  { int buf;\n    MPI_Status status;\n\n    buf = i;\n    if (rank == 0) {\n      \n\n    }\n    \n\n    if (i != buf) {\n      fprintf(stderr, \"****** INCORRECT RESULT:  %d\\n\", i);\n      exit(1);\n    }\n    if (rank != 0) {\n      \n\n    }\n\n    if (rank == 0) {\n      if (i % (int)(count/100) == 0) {printf(\".\"); fflush(stdout);}\n      if (i % (int)(count/5) == 0) printf(\"\\n\");\n    }\n  }\n  \n  if( !rank )\n    printf(\"\\n\");\n    \n  return 0;\n}", "label": "int main(int argc, char* argv[])\n{\n  int rank;\n  int size;\n  int i = 1;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0)\n    printf(\"*** Will print ten rows of dots.\\n\");  \n\n  printf(\"Hello, world, I am %d of %d\\n\", rank, size);\n\n  double count = 1e6;\n  for (i = 1; i < (int)count; i++)\n  { int buf;\n    MPI_Status status;\n\n    buf = i;\n    if (rank == 0) {\n      \n\n      MPI_Send(&buf, 1, MPI_INT, (rank+1)%size, 0, MPI_COMM_WORLD);\n    }\n    \n\n    MPI_Recv(&buf, 1, MPI_INT, (rank-1+size)%size, 0, MPI_COMM_WORLD, &status);\n    if (i != buf) {\n      fprintf(stderr, \"****** INCORRECT RESULT:  %d\\n\", i);\n      exit(1);\n    }\n    if (rank != 0) {\n      \n\n      MPI_Send(&buf, 1, MPI_INT, (rank+1)%size, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n      if (i % (int)(count/100) == 0) {printf(\".\"); fflush(stdout);}\n      if (i % (int)(count/5) == 0) printf(\"\\n\");\n    }\n  }\n  \n  if( !rank )\n    printf(\"\\n\");\n    \n  MPI_Finalize();\n  return 0;\n}"}
{"program": "swift-lang_1529", "code": "int main(int argc, char **argv)\n{\n    int nprocs;\n    int myrank;\n    int stat;\n    int namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    stat =\n    if ( stat != 0 ) error (\"MPI_Comm_size returned an error code : %d\", stat);\n\n    stat =\n    if ( stat != 0 ) error (\"MPI_Comm_rank returned an error code : %d\", stat);\n\n\n    printf(\"Process %d on %s out of %d\\n\", myrank, processor_name, nprocs);\n    \n\n\n    if ( myrank == 0 ){\n        run_server(myrank, nprocs);\n    }else{\n        run_client(myrank, nprocs);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n    int nprocs;\n    int myrank;\n    int stat;\n    int namelen;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Init(&argc, &argv);\n    stat = MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    if ( stat != 0 ) error (\"MPI_Comm_size returned an error code : %d\", stat);\n\n    stat = MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    if ( stat != 0 ) error (\"MPI_Comm_rank returned an error code : %d\", stat);\n\n    MPI_Get_processor_name(processor_name, &namelen);\n\n    printf(\"Process %d on %s out of %d\\n\", myrank, processor_name, nprocs);\n    \n\n\n    if ( myrank == 0 ){\n        run_server(myrank, nprocs);\n    }else{\n        run_client(myrank, nprocs);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "syftalent_1532", "code": "int main(int argc, char ** argv) {\n  int rank, nproc, i;\n  double t_mpix_mtx, t_mcs_mtx;\n  MPI_Comm mtx_comm;\n  MCS_Mutex mcs_mtx;\n\n\n\n#ifdef USE_WIN_SHARED\n#else\n  mtx_comm = MPI_COMM_WORLD;\n#endif\n\n  MCS_Mutex_create(0, mtx_comm, &mcs_mtx);\n\n  t_mcs_mtx =\n\n  for (i = 0; i < NUM_ITER; i++) {\n    \n\n#ifdef USE_CONTIGUOUS_RANK\n    if (rank < nproc / 2) {\n#else\n    if (rank % 2) {\n#endif\n      int success = 0;\n      while (!success) {\n        MCS_Mutex_trylock(mcs_mtx, &success);\n      }\n    }\n    else {\n        MCS_Mutex_lock(mcs_mtx);\n    }\n    MCS_Mutex_unlock(mcs_mtx);\n  }\n\n  t_mcs_mtx = MPI_Wtime() - t_mcs_mtx;\n\n  MCS_Mutex_free(&mcs_mtx);\n\n  if (rank == 0) {\n      if (verbose) {\n          printf(\"Nproc %d, MCS Mtx = %f us\\n\", nproc, t_mcs_mtx/NUM_ITER*1.0e6);\n      }\n  }\n\n  if (mtx_comm != MPI_COMM_WORLD)\n\n  MTest_Finalize(0);\n\n  return 0;\n}", "label": "int main(int argc, char ** argv) {\n  int rank, nproc, i;\n  double t_mpix_mtx, t_mcs_mtx;\n  MPI_Comm mtx_comm;\n  MCS_Mutex mcs_mtx;\n\n  MPI_Init(&argc, &argv);\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n#ifdef USE_WIN_SHARED\n  MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank,\n                      MPI_INFO_NULL, &mtx_comm);\n#else\n  mtx_comm = MPI_COMM_WORLD;\n#endif\n\n  MCS_Mutex_create(0, mtx_comm, &mcs_mtx);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  t_mcs_mtx = MPI_Wtime();\n\n  for (i = 0; i < NUM_ITER; i++) {\n    \n\n#ifdef USE_CONTIGUOUS_RANK\n    if (rank < nproc / 2) {\n#else\n    if (rank % 2) {\n#endif\n      int success = 0;\n      while (!success) {\n        MCS_Mutex_trylock(mcs_mtx, &success);\n      }\n    }\n    else {\n        MCS_Mutex_lock(mcs_mtx);\n    }\n    MCS_Mutex_unlock(mcs_mtx);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  t_mcs_mtx = MPI_Wtime() - t_mcs_mtx;\n\n  MCS_Mutex_free(&mcs_mtx);\n\n  if (rank == 0) {\n      if (verbose) {\n          printf(\"Nproc %d, MCS Mtx = %f us\\n\", nproc, t_mcs_mtx/NUM_ITER*1.0e6);\n      }\n  }\n\n  if (mtx_comm != MPI_COMM_WORLD)\n      MPI_Comm_free(&mtx_comm);\n\n  MTest_Finalize(0);\n  MPI_Finalize();\n\n  return 0;\n}"}
{"program": "bjoern-leder_1534", "code": "int main(int argc,char *argv[])\r\n{\r\n   int nc,iend,status;\r\n   int nws,nwsd,nwv,nwvd;\r\n   double wt1,wt2,wtavg;\r\n   dfl_parms_t dfl;\r\n   \r\n\r\n   read_infile(argc,argv);\r\n   alloc_data();\r\n   check_files();\r\n   print_info();\r\n   dfl=dfl_parms();\r\n\r\n   geometry();\r\n   init_rng();\r\n\r\n   reweight_wsize(&nws,&nwsd,&nwv,&nwvd);\r\n   alloc_ws(nws);\r\n   alloc_wsd(nwsd+1);\r\n   alloc_wv(nwv);\r\n   alloc_wvd(nwvd);\r\n   \r\n   iend=0;   \r\n   wtavg=0.0;\r\n   \r\n   for (nc=first;(iend==0)&&(nc<=last);nc+=step)\r\n   {\r\n      \r\n      if (my_rank==0)\r\n         printf(\"Configuration no %d\\n\",nc);\r\n\r\n      if (noexp)\r\n      {\r\n         save_ranlux();\r\n         sprintf(cnfg_file,\"%s/%sn%d_%d\",loc_dir,nbase,nc,my_rank);\r\n         read_cnfg(cnfg_file);\r\n         restore_ranlux();\r\n      }\r\n      else\r\n      {\r\n         sprintf(cnfg_file,\"%s/%sn%d\",cnfg_dir,nbase,nc);\r\n\n\r\n         import_cnfg(cnfg_file);\r\n      }\r\n\r\n      if (dfl.Ns)\r\n      {\r\n         dfl_modes(&status);\r\n         error_root(status<0,1,\"main [ms5.c]\",\r\n                    \"Deflation subspace generation failed (status = %d)\",\r\n                    status);\r\n      }\r\n      \r\n      set_data(nc);\r\n      \r\n      if (my_rank==0)\r\n      {\r\n         fdat=fopen(dat_file,\"ab\");\r\n         error_root(fdat==NULL,1,\"main [ms5.c]\",\r\n                    \"Unable to open dat file\");\r\n         write_data();\r\n         fclose(fdat);\r\n      }\r\n\r\n      export_ranlux(nc,rng_file);\r\n      error_chk();\r\n   \r\n      wtavg+=(wt2-wt1);\r\n\r\n      if (my_rank==0)\r\n      {\r\n         printf(\"Configuration no %d fully processed in %.2e sec \",\r\n                nc,wt2-wt1);\r\n         printf(\"(average = %.2e sec)\\n\\n\",\r\n                wtavg/(double)((nc-first)/step+1));\r\n      }\r\n\r\n      check_endflag(&iend);      \r\n\r\n      if (my_rank==0)\r\n      {\r\n         fflush(flog);         \r\n         copy_file(log_file,log_save);\r\n         copy_file(dat_file,dat_save);\r\n         copy_file(rng_file,rng_save);\r\n      }\r\n   }\r\n      \r\n   if (my_rank==0)\r\n      fclose(flog);\r\n   \r\n   exit(0);\r\n}", "label": "int main(int argc,char *argv[])\r\n{\r\n   int nc,iend,status;\r\n   int nws,nwsd,nwv,nwvd;\r\n   double wt1,wt2,wtavg;\r\n   dfl_parms_t dfl;\r\n   \r\n   MPI_Init(&argc,&argv);\r\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\r\n\r\n   read_infile(argc,argv);\r\n   alloc_data();\r\n   check_files();\r\n   print_info();\r\n   dfl=dfl_parms();\r\n\r\n   geometry();\r\n   init_rng();\r\n\r\n   reweight_wsize(&nws,&nwsd,&nwv,&nwvd);\r\n   alloc_ws(nws);\r\n   alloc_wsd(nwsd+1);\r\n   alloc_wv(nwv);\r\n   alloc_wvd(nwvd);\r\n   \r\n   iend=0;   \r\n   wtavg=0.0;\r\n   \r\n   for (nc=first;(iend==0)&&(nc<=last);nc+=step)\r\n   {\r\n      MPI_Barrier(MPI_COMM_WORLD);\r\n      wt1=MPI_Wtime();\r\n      \r\n      if (my_rank==0)\r\n         printf(\"Configuration no %d\\n\",nc);\r\n\r\n      if (noexp)\r\n      {\r\n         save_ranlux();\r\n         sprintf(cnfg_file,\"%s/%sn%d_%d\",loc_dir,nbase,nc,my_rank);\r\n         read_cnfg(cnfg_file);\r\n         restore_ranlux();\r\n      }\r\n      else\r\n      {\r\n         sprintf(cnfg_file,\"%s/%sn%d\",cnfg_dir,nbase,nc);\r\n\n\r\n         import_cnfg(cnfg_file);\r\n      }\r\n\r\n      if (dfl.Ns)\r\n      {\r\n         dfl_modes(&status);\r\n         error_root(status<0,1,\"main [ms5.c]\",\r\n                    \"Deflation subspace generation failed (status = %d)\",\r\n                    status);\r\n      }\r\n      \r\n      set_data(nc);\r\n      \r\n      if (my_rank==0)\r\n      {\r\n         fdat=fopen(dat_file,\"ab\");\r\n         error_root(fdat==NULL,1,\"main [ms5.c]\",\r\n                    \"Unable to open dat file\");\r\n         write_data();\r\n         fclose(fdat);\r\n      }\r\n\r\n      export_ranlux(nc,rng_file);\r\n      error_chk();\r\n   \r\n      MPI_Barrier(MPI_COMM_WORLD);\r\n      wt2=MPI_Wtime();\r\n      wtavg+=(wt2-wt1);\r\n\r\n      if (my_rank==0)\r\n      {\r\n         printf(\"Configuration no %d fully processed in %.2e sec \",\r\n                nc,wt2-wt1);\r\n         printf(\"(average = %.2e sec)\\n\\n\",\r\n                wtavg/(double)((nc-first)/step+1));\r\n      }\r\n\r\n      check_endflag(&iend);      \r\n\r\n      if (my_rank==0)\r\n      {\r\n         fflush(flog);         \r\n         copy_file(log_file,log_save);\r\n         copy_file(dat_file,dat_save);\r\n         copy_file(rng_file,rng_save);\r\n      }\r\n   }\r\n      \r\n   if (my_rank==0)\r\n      fclose(flog);\r\n   \r\n   MPI_Finalize();    \r\n   exit(0);\r\n}"}
{"program": "miguelzf_1535", "code": "int main(int argc, char** argv)\n{\n\tint global_rank, global_nprocs, size;\n\tgraph *g;\n\ttime_t total_time = 0;\n\tchar procname[MPI_MAX_PROCESSOR_NAME], *fname = NULL;\n\tbyte *data = NULL;\n\n\n\tdprintf(\"Startup MPI: process %d of %d on %s\\n\", global_rank, global_nprocs, procname);\n\n\ttime_count_global(NULL, TRUE, global_rank);\n\tfname = process_args(argc, argv);\n\n\tif (global_rank == 0)\n\t{\n\t\tif ((g = parse_file(fname)) == NULL)\n\t\t\terror_exit(\"input file not found or invalid\");\n\t}\n\telse\n\t\tg = init_graph();\n\t\n\ttotal_time += time_count_global(\"Reading\", FALSE, global_rank);\n\n\t#pragma omp parallel\n\t#pragma omp single\n\tdprintf(\"[P %d] Using %d threads on %d cores\\n\", global_rank, omp_get_num_threads(), omp_get_num_procs());\n\n\t\n\n\t\n\n\t\n\tinit_mpi(g, global_rank, global_nprocs);\n\n\tg->rats->layers = find_layers(g->rats, g-> ats, &g->rats->num_layers);\n\tg-> ats->layers = find_layers(g-> ats, g->rats, &g->ats->num_layers);\n\tprint_layers(g->rats);\n\tprint_layers(g->ats);\n\n\n\n\n\n\ttotal_time += time_count_global(\"Creating Layers\", FALSE, global_rank);\n\n\tif (global_nprocs == 1)\n\t{\t\n\t\ttotal_time += time_count_global(\"Network Broadcasting\", FALSE, g->rank);\n\t\tcompute_times_layered(g-> ats, calc_at);\n\t\tcompute_times_layered(g->rats, calc_rat);\n\t}\n\t\n\telse if (FUNCTIONAL)\n\t{\n\t\tmpi_split_comm(g);\n\t\tmpi_broadcast_all(g);\n\n\t\ttotal_time += time_count_global(\"Network Broadcasting\", FALSE, g->rank);\n\t\tddprintf(\"[P %d] Main RAts: allnum %d, roots num %d\\n\", g->rank, g->rats->all_num, g->rats->roots_num);\n\t\tddprintf(\"[P %d] Main Ats: allnum %d, roots num %d\\n\", g->rank, g->ats->all_num, g->ats->roots_num);\n\t\ttime_count_local(NULL, TRUE, g->rank);\n\n\t\tif (g->rank % 2 == 0)\n\t\t\tcompute_times_layered(g->ats, calc_at);\n\t\telse\n\t\t\tcompute_times_layered(g->rats,calc_rat);\n\n\t\ttime_count_local(\"distributed computing\", FALSE, g->rank);\n\t\t\n\n\t\tfree(buff);\n\t\tif (g->rank < 2)\t\n\n\t\t\tmpi_receive_rats(g->rats, g->rank);\n\n\t\ttime_count_local(\"results' gathering\", FALSE, g->rank);\n\t}\n\t\n\telse\t\n\n\t{\t\n\n\t\tif (g->rank == 0)\n\t\t\tdata = mpi_create_graph_data(g->ats, g->rats, &size);\n\t\tmpi_broadcast_data(g, g->ats, data, size);\n\n\t\tif (g->rank == 0)\n\t\t\tdata = mpi_create_graph_data(g->rats, g->ats, &size);\n\t\tmpi_broadcast_data(g, g->rats, data, size);\n\t\t\n\t\ttotal_time += time_count_global(\"Network Broadcasting\", FALSE, g->rank);\n\n\t\tddprintf(\"[P %d] Main RAts: allnum %d, roots num %d\\n\", g->rank, g->rats->all_num, g->rats->roots_num);\n\t\tddprintf(\"[P %d] Main Aats: allnum %d, roots num %d\\n\", g->rank, g->ats->all_num, g->ats->roots_num);\n\t\t\n\t\tcompute_times_layered(g-> ats, calc_at);\n\t\tcompute_times_layered(g->rats, calc_rat);\n\t}\n\n\ttotal_time += time_count_global(\"Computing\", FALSE, g->rank);\n\n\t\n\n\tif (g->rank == 0)\n\t{\tstrcpy(fname+strlen(fname)-3, \".out\");\n\t\twrite_results(g, fname);\n\t}\n\n\ttotal_time += time_count_global(\"Writing\", FALSE, g->rank);\n\n\tif (g->rank == 0)\n\t\tprintf(\"Total time          %ld msecs\\n\", total_time);\n\n    return 0;\n}", "label": "int main(int argc, char** argv)\n{\n\tint global_rank, global_nprocs, size;\n\tgraph *g;\n\ttime_t total_time = 0;\n\tchar procname[MPI_MAX_PROCESSOR_NAME], *fname = NULL;\n\tbyte *data = NULL;\n\n\tMPI_Init (&argc, &argv);\n\tMPI_Comm_rank (MPI_COMM_WORLD, &global_rank);\n\tMPI_Comm_size (MPI_COMM_WORLD, &global_nprocs);\n\tMPI_Get_processor_name(procname, &size);\n\n\tdprintf(\"Startup MPI: process %d of %d on %s\\n\", global_rank, global_nprocs, procname);\n\n\ttime_count_global(NULL, TRUE, global_rank);\n\tfname = process_args(argc, argv);\n\n\tif (global_rank == 0)\n\t{\n\t\tif ((g = parse_file(fname)) == NULL)\n\t\t\terror_exit(\"input file not found or invalid\");\n\t}\n\telse\n\t\tg = init_graph();\n\t\n\ttotal_time += time_count_global(\"Reading\", FALSE, global_rank);\n\n\t#pragma omp parallel\n\t#pragma omp single\n\tdprintf(\"[P %d] Using %d threads on %d cores\\n\", global_rank, omp_get_num_threads(), omp_get_num_procs());\n\n\t\n\n\t\n\n\t\n\tinit_mpi(g, global_rank, global_nprocs);\n\n\tg->rats->layers = find_layers(g->rats, g-> ats, &g->rats->num_layers);\n\tg-> ats->layers = find_layers(g-> ats, g->rats, &g->ats->num_layers);\n\tprint_layers(g->rats);\n\tprint_layers(g->ats);\n\n\n\n\n\n\ttotal_time += time_count_global(\"Creating Layers\", FALSE, global_rank);\n\n\tif (global_nprocs == 1)\n\t{\t\n\t\ttotal_time += time_count_global(\"Network Broadcasting\", FALSE, g->rank);\n\t\tcompute_times_layered(g-> ats, calc_at);\n\t\tcompute_times_layered(g->rats, calc_rat);\n\t}\n\t\n\telse if (FUNCTIONAL)\n\t{\n\t\tmpi_split_comm(g);\n\t\tmpi_broadcast_all(g);\n\n\t\ttotal_time += time_count_global(\"Network Broadcasting\", FALSE, g->rank);\n\t\tddprintf(\"[P %d] Main RAts: allnum %d, roots num %d\\n\", g->rank, g->rats->all_num, g->rats->roots_num);\n\t\tddprintf(\"[P %d] Main Ats: allnum %d, roots num %d\\n\", g->rank, g->ats->all_num, g->ats->roots_num);\n\t\ttime_count_local(NULL, TRUE, g->rank);\n\n\t\tif (g->rank % 2 == 0)\n\t\t\tcompute_times_layered(g->ats, calc_at);\n\t\telse\n\t\t\tcompute_times_layered(g->rats,calc_rat);\n\n\t\ttime_count_local(\"distributed computing\", FALSE, g->rank);\n\t\t\n\n\t\tfree(buff);\n\t\tif (g->rank < 2)\t\n\n\t\t\tmpi_receive_rats(g->rats, g->rank);\n\n\t\ttime_count_local(\"results' gathering\", FALSE, g->rank);\n\t}\n\t\n\telse\t\n\n\t{\t\n\n\t\tif (g->rank == 0)\n\t\t\tdata = mpi_create_graph_data(g->ats, g->rats, &size);\n\t\tmpi_broadcast_data(g, g->ats, data, size);\n\n\t\tif (g->rank == 0)\n\t\t\tdata = mpi_create_graph_data(g->rats, g->ats, &size);\n\t\tmpi_broadcast_data(g, g->rats, data, size);\n\t\t\n\t\ttotal_time += time_count_global(\"Network Broadcasting\", FALSE, g->rank);\n\n\t\tddprintf(\"[P %d] Main RAts: allnum %d, roots num %d\\n\", g->rank, g->rats->all_num, g->rats->roots_num);\n\t\tddprintf(\"[P %d] Main Aats: allnum %d, roots num %d\\n\", g->rank, g->ats->all_num, g->ats->roots_num);\n\t\t\n\t\tcompute_times_layered(g-> ats, calc_at);\n\t\tcompute_times_layered(g->rats, calc_rat);\n\t}\n\n\ttotal_time += time_count_global(\"Computing\", FALSE, g->rank);\n\n\t\n\n\tif (g->rank == 0)\n\t{\tstrcpy(fname+strlen(fname)-3, \".out\");\n\t\twrite_results(g, fname);\n\t}\n\n\ttotal_time += time_count_global(\"Writing\", FALSE, g->rank);\n\n\tif (g->rank == 0)\n\t\tprintf(\"Total time          %ld msecs\\n\", total_time);\n\n\tMPI_Finalize();\n    return 0;\n}"}
{"program": "sergsagal1_1536", "code": "int main(int argc, char **argv)\n{\n    const char *ucm_path = UCS_PP_MAKE_STRING(UCM_LIB_DIR) \"/\" \"libucm.so\";\n    memtest_type_t *test = tests;\n    void *dl;\n    int ret;\n    int c;\n\n    while ((c = getopt(argc, argv, \"t:h\")) != -1) {\n        switch (c) {\n        case 't':\n            for (test = tests; test->name != NULL; ++test) {\n                if (!strcmp(test->name, optarg)){\n                    break;\n                }\n            }\n            if (test->name == NULL) {\n                fprintf(stderr, \"Wrong test name %s\\n\", optarg);\n                return -1;\n            }\n            break;\n        case 'h':\n        default:\n            usage();\n            return -1;\n        }\n    }\n\n    \n\n    dl = test->init(ucm_path);\n    if (dl == NULL) {\n        return -1;\n    }\n\n    printf(\"%s: initialized\\n\", test->name);\n\n\n    ret = test->run(dl);\n\n    printf(\"%s: %s\\n\", test->name, ret == 0 ? \"PASS\" : \"FAIL\");\n\n    return ret;\n}", "label": "int main(int argc, char **argv)\n{\n    const char *ucm_path = UCS_PP_MAKE_STRING(UCM_LIB_DIR) \"/\" \"libucm.so\";\n    memtest_type_t *test = tests;\n    void *dl;\n    int ret;\n    int c;\n\n    while ((c = getopt(argc, argv, \"t:h\")) != -1) {\n        switch (c) {\n        case 't':\n            for (test = tests; test->name != NULL; ++test) {\n                if (!strcmp(test->name, optarg)){\n                    break;\n                }\n            }\n            if (test->name == NULL) {\n                fprintf(stderr, \"Wrong test name %s\\n\", optarg);\n                return -1;\n            }\n            break;\n        case 'h':\n        default:\n            usage();\n            return -1;\n        }\n    }\n\n    \n\n    dl = test->init(ucm_path);\n    if (dl == NULL) {\n        return -1;\n    }\n\n    printf(\"%s: initialized\\n\", test->name);\n\n    MPI_Init(&argc, &argv);\n\n    ret = test->run(dl);\n\n    printf(\"%s: %s\\n\", test->name, ret == 0 ? \"PASS\" : \"FAIL\");\n\n    MPI_Finalize();\n    return ret;\n}"}
{"program": "reimashi_1537", "code": "void main(int argc, char* argv[]) {\n    int nDims = 2, dims[2] = {NFil, NCol};\n    int periods[2] = {0, 0};\n    MPI_Status status;\n\n\n\n    int myRank;\n    int numProcs;\n\n    \n\n    if (numProcs != NFil * NCol) {\n        if (myRank == 0) printf(\"El numero de procesos deberia ser %d\\n\", NFil * NCol);\n    } else {\n        \n\n        MPI_Comm comCartesiano;\n\n        \n\n        int numVec, vec[4];\n        numVec = getNumVec(comCartesiano);\n        getVec(vec, comCartesiano);\n\n        \n\n        printf(\"Soy el nodo %d y tengo %d vecinos: \", myRank, numVec);\n        for (int i = 0; i < 4; i++) {\n            if (vec[i] >= 0) {\n                printf(\"%d \", vec[i]);\n            }\n        }\n        printf(\"\\n\");\n\n        \n\n        float myTemp = (myRank == 0) ? HotTemp : ColdTemp;\n\n        float recvTemp, newTemp = 0, aux;\n        for (int contIter = 1; contIter <= NumIter; ++contIter) {\n            aux = 0;\n\n            for (int i = 0; i < 4; i++) {\n                \n\n                if (vec[i] >= 0) {\n                    \n\n\n                    \n\n                    aux = aux + recvTemp;\n                }\n            }\n\n            newTemp = (1 - Coeff) * myTemp + Coeff * (aux / numVec);\n\n            \n\n            if (myRank != 0) {\n                myTemp = newTemp;\n            }\n\n            \n\n            if (myRank == numProcs - 1)\n                if (contIter % 10 == 0)\n                    printf(\"Iter %d - soy el nodo %d y mi temp. es %f\\n\", contIter, myRank, myTemp);\n        }\n    }\n\n    exit(0);\n}", "label": "void main(int argc, char* argv[]) {\n    int nDims = 2, dims[2] = {NFil, NCol};\n    int periods[2] = {0, 0};\n    MPI_Status status;\n\n\n    MPI_Init(&argc, &argv);\n\n    int myRank;\n    int numProcs;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    \n\n    if (numProcs != NFil * NCol) {\n        if (myRank == 0) printf(\"El numero de procesos deberia ser %d\\n\", NFil * NCol);\n    } else {\n        \n\n        MPI_Comm comCartesiano;\n        MPI_Cart_create(MPI_COMM_WORLD, nDims, dims, periods, 0, &comCartesiano);\n\n        \n\n        int numVec, vec[4];\n        numVec = getNumVec(comCartesiano);\n        getVec(vec, comCartesiano);\n\n        \n\n        printf(\"Soy el nodo %d y tengo %d vecinos: \", myRank, numVec);\n        for (int i = 0; i < 4; i++) {\n            if (vec[i] >= 0) {\n                printf(\"%d \", vec[i]);\n            }\n        }\n        printf(\"\\n\");\n\n        \n\n        float myTemp = (myRank == 0) ? HotTemp : ColdTemp;\n\n        float recvTemp, newTemp = 0, aux;\n        for (int contIter = 1; contIter <= NumIter; ++contIter) {\n            aux = 0;\n\n            for (int i = 0; i < 4; i++) {\n                \n\n                if (vec[i] >= 0) {\n                    \n\n                    MPI_Send(&myTemp, 1, MPI_FLOAT, vec[i], 0, comCartesiano);\n\n                    \n\n                    MPI_Recv(&recvTemp, 1, MPI_FLOAT, vec[i], 0, comCartesiano, &status);\n                    aux = aux + recvTemp;\n                }\n            }\n\n            newTemp = (1 - Coeff) * myTemp + Coeff * (aux / numVec);\n\n            \n\n            if (myRank != 0) {\n                myTemp = newTemp;\n            }\n\n            \n\n            if (myRank == numProcs - 1)\n                if (contIter % 10 == 0)\n                    printf(\"Iter %d - soy el nodo %d y mi temp. es %f\\n\", contIter, myRank, myTemp);\n        }\n    }\n\n    MPI_Finalize();\n    exit(0);\n}"}
{"program": "dmalhotra_1538", "code": "int main(int argc, char** argv) {\n\n  const MPI_Comm comm = MPI_COMM_WORLD;\n  const int mult_order = 10, cheb_deg = 14, kdim0 = 3, kdim1 = 3;\n\n  \n\n  void* fmm = PVFMMCreateVolumeFMMD(mult_order, cheb_deg, PVFMMStokesVelocity, comm);\n\n  \n\n  test2(fmm, kdim0, kdim1, cheb_deg, comm);\n\n  PVFMMDestroyVolumeFMMD(&fmm);\n\n  return 0;\n}", "label": "int main(int argc, char** argv) {\n  MPI_Init(&argc, &argv);\n\n  const MPI_Comm comm = MPI_COMM_WORLD;\n  const int mult_order = 10, cheb_deg = 14, kdim0 = 3, kdim1 = 3;\n\n  \n\n  void* fmm = PVFMMCreateVolumeFMMD(mult_order, cheb_deg, PVFMMStokesVelocity, comm);\n\n  \n\n  test2(fmm, kdim0, kdim1, cheb_deg, comm);\n\n  PVFMMDestroyVolumeFMMD(&fmm);\n\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "ResearchComputing_1540", "code": "int\nmain(int argc, char **argv)\n{\n\tint ierr   = 0;\t\t\t\n\n\tint rank   = 0;\t\t\t\n\n\tint nprocs = 0;\t\t\t\n\n\tint i      = 0;\t\t\t\n\n\tint N      = 0;\t\t\t\n\n\tint *data  = NULL;\t\t\n\n\tint total  = 0;\t\t\t\n\n\n\t\n\n\tierr =\n\n\t\n\n\tierr =\n\tierr =\n\n\t\n\n\tif (rank == 0) {\n\t\tN = strtol(argv[1], NULL, 10);\n\t\tif (N <= 0) {\n\t\t\twarnx(\"Will not run with negative array size.\");\n\t\t}\n\t\tprintf(\"Running on %d processors.\\nWith %d elements.\\n\", nprocs,\n\t\t       N);\n\t}\n\n\t\n\n\tierr =\n\n\t\n\n\tierr = posix_memalign((void **)&data, ALIGNMENT, N * sizeof(int));\n\tif (ierr) {\n\t\twarnx(\"Unable to allocate: %lu\", N * sizeof(int));\n\t}\n\n\t\n\n\tfor (i = 0; i < N; ++i) {\n\t\tdata[i] = rank;\n\t}\n\n\t\n\n\tierr =\n\n\t\n\n\tfor (i = 0; i < N; ++i) {\n\t\ttotal += data[i];\n\t}\n\tprintf(\"[%02d] Total is: %d\\n\", rank, total);\n\n\t\n\n\tif (data) {\n\t\tfree(data);\n\t\tdata = NULL;\n\t}\n\n\t\n\n\tierr =\n\treturn(EXIT_SUCCESS);\n}", "label": "int\nmain(int argc, char **argv)\n{\n\tint ierr   = 0;\t\t\t\n\n\tint rank   = 0;\t\t\t\n\n\tint nprocs = 0;\t\t\t\n\n\tint i      = 0;\t\t\t\n\n\tint N      = 0;\t\t\t\n\n\tint *data  = NULL;\t\t\n\n\tint total  = 0;\t\t\t\n\n\n\t\n\n\tierr = MPI_Init(&argc, &argv);\n\n\t\n\n\tierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tierr = MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tif (rank == 0) {\n\t\tN = strtol(argv[1], NULL, 10);\n\t\tif (N <= 0) {\n\t\t\twarnx(\"Will not run with negative array size.\");\n\t\t\tMPI_Abort(MPI_COMM_WORLD, ierr);\n\t\t}\n\t\tprintf(\"Running on %d processors.\\nWith %d elements.\\n\", nprocs,\n\t\t       N);\n\t}\n\n\t\n\n\tierr = MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\t\n\n\tierr = posix_memalign((void **)&data, ALIGNMENT, N * sizeof(int));\n\tif (ierr) {\n\t\twarnx(\"Unable to allocate: %lu\", N * sizeof(int));\n\t\tMPI_Abort(MPI_COMM_WORLD, ierr);\n\t}\n\n\t\n\n\tfor (i = 0; i < N; ++i) {\n\t\tdata[i] = rank;\n\t}\n\n\t\n\n\tierr = MPI_Allreduce(MPI_IN_PLACE, &(data[0]), N, MPI_INT, MPI_SUM,\n\t\t\t     MPI_COMM_WORLD);\n\n\t\n\n\tfor (i = 0; i < N; ++i) {\n\t\ttotal += data[i];\n\t}\n\tprintf(\"[%02d] Total is: %d\\n\", rank, total);\n\n\t\n\n\tif (data) {\n\t\tfree(data);\n\t\tdata = NULL;\n\t}\n\n\t\n\n\tierr = MPI_Finalize();\n\treturn(EXIT_SUCCESS);\n}"}
{"program": "byu-vv-lab_1541", "code": "int main(int argc, char * argv[]) {\n  pthread_t threads[2];\n\n  for (int i=0; i<2; i++) {\n    pthread_create(&threads[i], NULL, Thread, (void *)(long)i);\n  }\n  for (int i=0; i<2; i++) {\n    pthread_join(threads[i], NULL);\n  }\n}", "label": "int main(int argc, char * argv[]) {\n  pthread_t threads[2];\n\n  MPI_Init(&argc, &argv);\n  for (int i=0; i<2; i++) {\n    pthread_create(&threads[i], NULL, Thread, (void *)(long)i);\n  }\n  for (int i=0; i<2; i++) {\n    pthread_join(threads[i], NULL);\n  }\n  MPI_Finalize();\n}"}
{"program": "bjoern-leder_1542", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,n,err1,err2,iw;\n   FILE *flog=NULL,*fdat=NULL;\n   \n   \n   if (my_rank==0)\n   {\n      flog=freopen(\"check1.log\",\"w\",stdout);      \n\n      printf(\"\\n\");\n      printf(\"Copying of .log and .dat files from process 0\\n\");\n      printf(\"---------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n   }\n\n   start_ranlux(0,1234); \n   ranlxs(r,NRAN);\n\n   if (my_rank==0)\n   {   \n      printf(\"Write 10 random numbers to check1.log (in asci format)\\n\");\n      printf(\"and %d numbers to check1.dat (in binary format)\\n\\n\",NRAN);\n\n      fdat=fopen(\"check1.dat\",\"wb\");\n      iw=fwrite(&r[0],sizeof(float),NRAN,fdat);\n      error_root(iw!=NRAN,1,\"main [check1.c]\",\"Incorrect write count\");\n      fclose(fdat);\n      \n      for (n=0;n<10;n++)\n         printf(\"r[%d] = %.6e\\n\",n,r[n]);\n\n      printf(\"\\n\");\n      printf(\"Copy the files to check1.log~ and check1.dat~ respectively.\\n\");\n      printf(\"The copying may then be verified using the diff utility\\n\\n\");\n      fclose(flog);\n\n      err1=copy_file(\"check1.log\",\"check1.log~\");\n      err2=copy_file(\"check1.dat\",\"check1.dat~\");\n\n      flog=freopen(\"check1.log\",\"a\",stdout);\n\n      if ((err1!=0)||(err2!=0))\n         printf(\"Copying failed: err1 = %d, err2 = %d\\n\",err1,err2);\n   }\n\n   error_chk();\n\n   if (my_rank==0)\n      fclose(flog);   \n   \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,n,err1,err2,iw;\n   FILE *flog=NULL,*fdat=NULL;\n   \n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n   \n   if (my_rank==0)\n   {\n      flog=freopen(\"check1.log\",\"w\",stdout);      \n\n      printf(\"\\n\");\n      printf(\"Copying of .log and .dat files from process 0\\n\");\n      printf(\"---------------------------------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n   }\n\n   start_ranlux(0,1234); \n   ranlxs(r,NRAN);\n\n   if (my_rank==0)\n   {   \n      printf(\"Write 10 random numbers to check1.log (in asci format)\\n\");\n      printf(\"and %d numbers to check1.dat (in binary format)\\n\\n\",NRAN);\n\n      fdat=fopen(\"check1.dat\",\"wb\");\n      iw=fwrite(&r[0],sizeof(float),NRAN,fdat);\n      error_root(iw!=NRAN,1,\"main [check1.c]\",\"Incorrect write count\");\n      fclose(fdat);\n      \n      for (n=0;n<10;n++)\n         printf(\"r[%d] = %.6e\\n\",n,r[n]);\n\n      printf(\"\\n\");\n      printf(\"Copy the files to check1.log~ and check1.dat~ respectively.\\n\");\n      printf(\"The copying may then be verified using the diff utility\\n\\n\");\n      fclose(flog);\n\n      err1=copy_file(\"check1.log\",\"check1.log~\");\n      err2=copy_file(\"check1.dat\",\"check1.dat~\");\n\n      flog=freopen(\"check1.log\",\"a\",stdout);\n\n      if ((err1!=0)||(err2!=0))\n         printf(\"Copying failed: err1 = %d, err2 = %d\\n\",err1,err2);\n   }\n\n   error_chk();\n\n   if (my_rank==0)\n      fclose(flog);   \n   \n   MPI_Finalize();  \n   exit(0);\n}"}
{"program": "pcaro90_1545", "code": "int main(int argc, char **argv)\n{\n\n    \n\n\n    \n\n    int nproc;\n\n    \n\n    int rank;\n\n    \n\n    int processor_name_len;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n\n    if (rank == 0) {\n        printf(\"Master process on processor %s. We have %d processes.\\n\", processor_name, nproc);\n    } else {\n        printf(\"Worker %2d on processor %s\\n\", rank, processor_name);\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n    \n\n    MPI_Init(&argc, &argv);\n\n    \n\n    int nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    \n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    \n\n    int processor_name_len;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    MPI_Get_processor_name(processor_name, &processor_name_len);\n\n    if (rank == 0) {\n        printf(\"Master process on processor %s. We have %d processes.\\n\", processor_name, nproc);\n    } else {\n        printf(\"Worker %2d on processor %s\\n\", rank, processor_name);\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "GuillaumeArruda_1547", "code": "int main(int argc, char **argv)\n{\n\tint numprocs, rank, namelen, i;\n\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n\tint *vx = NULL, *vy = NULL, *vz = NULL, *vxpart = NULL, *vypart = NULL,\n\t\t\t*vzpart = NULL, coeff[2];\n\tint exp = 0, act = 0;\n\tint *count = NULL;\n\tint *disp = NULL;\n\n\n\t\n\n\tif (rank == 0) {\n\t\tvx = (int *) malloc(sizeof(int) * DIM_GLOBAL);\n\t\tvy = (int *) malloc(sizeof(int) * DIM_GLOBAL);\n\t\tvz = (int *) malloc(sizeof(int) * DIM_GLOBAL);\n\n\t\tfor (i = 0; i < DIM_GLOBAL; i++) {\n\t\t\tvx[i] = i;\n\t\t\tvy[i] = i;\n\t\t\tvz[i] = 0;\n\t\t\texp += 2 * i + 3 * i;\n\t\t}\n\t\tcoeff[0] = 2;\n\t\tcoeff[1] = 3;\n\t}\n\n\t\n\n\tsendcounts_array(&count, numprocs, DIM_GLOBAL);\n\tdispls_array(&disp, count, numprocs);\n\n\t\n\n\tvxpart = (int *) malloc(sizeof(int) * count[rank]);\n\tvypart = (int *) malloc(sizeof(int) * count[rank]);\n\tvzpart = (int *) malloc(sizeof(int) * count[rank]);\n\n\t\n\n\n\t\n\n\n\t\n\n\tfor (i = 0; i < count[rank]; i++) {\n\t\tvzpart[i] = coeff[0] * vxpart[i] + coeff[1] * vypart[i];\n\t}\n\n\t\n\n\n\t\n\n\n\tif (rank == 0) {\n\t\tfor (i = 0; i < DIM_GLOBAL; i++) {\n\t\t\tact += vz[i];\n\t\t}\n\t\tprintf(\"exp=%d act=%d\\n\", exp, act);\n\t}\n\tif (rank == 0) {\n\t\tFREE(vx);\n\t\tFREE(vy);\n\t\tFREE(vz);\n\t}\n\tFREE(vxpart);\n\tFREE(vypart);\n\tFREE(vzpart);\n\tFREE(disp);\n\tFREE(count);\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\tint numprocs, rank, namelen, i;\n\tchar processor_name[MPI_MAX_PROCESSOR_NAME];\n\tint *vx = NULL, *vy = NULL, *vz = NULL, *vxpart = NULL, *vypart = NULL,\n\t\t\t*vzpart = NULL, coeff[2];\n\tint exp = 0, act = 0;\n\tint *count = NULL;\n\tint *disp = NULL;\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t\n\n\tif (rank == 0) {\n\t\tvx = (int *) malloc(sizeof(int) * DIM_GLOBAL);\n\t\tvy = (int *) malloc(sizeof(int) * DIM_GLOBAL);\n\t\tvz = (int *) malloc(sizeof(int) * DIM_GLOBAL);\n\n\t\tfor (i = 0; i < DIM_GLOBAL; i++) {\n\t\t\tvx[i] = i;\n\t\t\tvy[i] = i;\n\t\t\tvz[i] = 0;\n\t\t\texp += 2 * i + 3 * i;\n\t\t}\n\t\tcoeff[0] = 2;\n\t\tcoeff[1] = 3;\n\t}\n\n\t\n\n\tsendcounts_array(&count, numprocs, DIM_GLOBAL);\n\tdispls_array(&disp, count, numprocs);\n\n\t\n\n\tvxpart = (int *) malloc(sizeof(int) * count[rank]);\n\tvypart = (int *) malloc(sizeof(int) * count[rank]);\n\tvzpart = (int *) malloc(sizeof(int) * count[rank]);\n\n\t\n\n\tMPI_Scatterv(vx, count, disp, MPI_INTEGER, vxpart, count[rank],\n\t\t\tMPI_INTEGER, 0, MPI_COMM_WORLD);\n\tMPI_Scatterv(vy, count, disp, MPI_INTEGER, vypart, count[rank],\n\t\t\tMPI_INTEGER, 0, MPI_COMM_WORLD);\n\n\t\n\n\tMPI_Bcast(coeff, 2, MPI_INTEGER, 0, MPI_COMM_WORLD);\n\n\t\n\n\tfor (i = 0; i < count[rank]; i++) {\n\t\tvzpart[i] = coeff[0] * vxpart[i] + coeff[1] * vypart[i];\n\t}\n\n\t\n\n\tMPI_Gatherv(vzpart, count[rank], MPI_INTEGER, vz, count, disp, MPI_INTEGER,\n\t\t\t0, MPI_COMM_WORLD);\n\n\t\n\n\n\tif (rank == 0) {\n\t\tfor (i = 0; i < DIM_GLOBAL; i++) {\n\t\t\tact += vz[i];\n\t\t}\n\t\tprintf(\"exp=%d act=%d\\n\", exp, act);\n\t}\n\tif (rank == 0) {\n\t\tFREE(vx);\n\t\tFREE(vy);\n\t\tFREE(vz);\n\t}\n\tFREE(vxpart);\n\tFREE(vypart);\n\tFREE(vzpart);\n\tFREE(disp);\n\tFREE(count);\n\tMPI_Get_processor_name(processor_name, &namelen);\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "bmi-forum_1548", "code": "int main( int argc, char **argv ) {\n\tint status;\n\t\n\tstatus =\n\tif( status != MPI_SUCCESS ) {\n\t\tprintf( \"%s: MPI_Init failed! Exiting ...\\n\", argv[0] );\n\t\treturn status;\n\t}\n\n\tstatus = Py_Main( argc, argv );\n\n\n\treturn status;\n}", "label": "int main( int argc, char **argv ) {\n\tint status;\n\t\n\tstatus = MPI_Init( &argc, &argv );\n\tif( status != MPI_SUCCESS ) {\n\t\tprintf( \"%s: MPI_Init failed! Exiting ...\\n\", argv[0] );\n\t\treturn status;\n\t}\n\n\tstatus = Py_Main( argc, argv );\n\n\tMPI_Finalize();\n\n\treturn status;\n}"}
{"program": "pboueke_1550", "code": "int main(int argc, char** argv) {\n\n  int partner_rank;\n\n  int world_size;\n\n  int world_rank;\n\n  if (world_rank < world_size/2){\n    partner_rank = world_size/2 + world_rank;\n  }\n  else if (world_rank >= world_size/2) {\n    partner_rank = world_rank - world_size/2;\n  }\n\n  int out;\n  \n\n  if (world_rank < partner_rank) {\n  }\n  else {\n    printf(\"Hello from rank %d, reading from rank %d \\n\", world_rank, out);\n  }\n  if (world_rank > partner_rank) {\n  }\n  else {\n    printf(\"Hello from rank %d, reading from rank %d \\n\", world_rank, out);\n  }\n\n\n}", "label": "int main(int argc, char** argv) {\n  MPI_Init(NULL, NULL);\n\n  int partner_rank;\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  if (world_rank < world_size/2){\n    partner_rank = world_size/2 + world_rank;\n  }\n  else if (world_rank >= world_size/2) {\n    partner_rank = world_rank - world_size/2;\n  }\n\n  int out;\n  \n\n  if (world_rank < partner_rank) {\n    MPI_Send(&world_rank, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n  }\n  else {\n    MPI_Recv(&out, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    printf(\"Hello from rank %d, reading from rank %d \\n\", world_rank, out);\n  }\n  if (world_rank > partner_rank) {\n    MPI_Send(&world_rank, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);\n  }\n  else {\n    MPI_Recv(&out, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    printf(\"Hello from rank %d, reading from rank %d \\n\", world_rank, out);\n  }\n\n\n  MPI_Finalize();\n}"}
{"program": "time-river_1551", "code": "int main(int argc, char *argv[]){\n    \n    int world_rank;\n    int world_size;\n\n    int token;\n    \n\n    \n\n    if(world_rank != 0){\n        printf(\"Process %d received token %d from process %d\\n\", world_rank, token, world_rank-1);\n    }\n    else{\n        \n\n        token = -1;\n    }\n\n\n    \n\n    \n\n    if(world_rank == 0){\n        printf(\"Process %d received token %d from process %d\\n\", world_rank, token, world_size-1);\n    }\n\n    return 0;\n}", "label": "int main(int argc, char *argv[]){\n    MPI_Init(&argc, &argv);\n    \n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int token;\n    \n\n    \n\n    if(world_rank != 0){\n        MPI_Recv(&token, 1, MPI_INT, world_rank-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Process %d received token %d from process %d\\n\", world_rank, token, world_rank-1);\n    }\n    else{\n        \n\n        token = -1;\n    }\n\n    MPI_Send(&token, 1, MPI_INT, (world_rank+1)%world_size, 0, MPI_COMM_WORLD);\n\n    \n\n    \n\n    if(world_rank == 0){\n        MPI_Recv(&token, 1, MPI_INT, world_size-1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        printf(\"Process %d received token %d from process %d\\n\", world_rank, token, world_size-1);\n    }\n\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "mpip_1553", "code": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3], N[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  pfft_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n\n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r_3d(\n      n, in, out, comm_cart_3d, kinds_forw, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n \n  \n\n  plan_back = pfft_plan_r2r_3d(\n      n, out, in, comm_cart_3d, kinds_back, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_real_3d(n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in); pfft_free(out);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[3];\n  ptrdiff_t n[3], N[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err, *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_3d;\n  pfft_r2r_kind kinds_forw[3], kinds_back[3];\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2; np[2] = 2;\n\n  \n\n  kinds_forw[0] = PFFT_REDFT00; kinds_back[0] = PFFT_REDFT00;\n  kinds_forw[1] = PFFT_REDFT01; kinds_back[1] = PFFT_REDFT10;\n  kinds_forw[2] = PFFT_RODFT00; kinds_back[2] = PFFT_RODFT00;\n\n  \n\n  N[0] = 2*(n[0]-1);\n  N[1] = 2*n[1];\n  N[2] = 2*(n[2]+1); \n\n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh(3, MPI_COMM_WORLD, np, &comm_cart_3d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]*np[2]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_r2r_3d(n, comm_cart_3d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_real(alloc_local);\n  out = pfft_alloc_real(alloc_local);\n\n  \n\n  plan_forw = pfft_plan_r2r_3d(\n      n, in, out, comm_cart_3d, kinds_forw, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n \n  \n\n  plan_back = pfft_plan_r2r_3d(\n      n, out, in, comm_cart_3d, kinds_back, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_real_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_forw);\n\n  \n\n  pfft_clear_input_real_3d(n, local_ni, local_i_start,\n      in);\n\n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  for(ptrdiff_t l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (N[0]*N[1]*N[2]);\n\n  \n\n  err = pfft_check_output_real_3d(n, local_ni, local_i_start, in, comm_cart_3d);\n  pfft_printf(comm_cart_3d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_3d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_3d);\n  pfft_free(in); pfft_free(out);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "gentryx_1554", "code": "int main(int argc, char * argv[])\n{\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n\n    int rank, size;\n\n    if (size<1) {\n        printf(\"Use 1 or more processes. \\n\");\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    double * sbuf = NULL;\n    double * rbuf = NULL;\n\n    MPI_Aint bytes = n*sizeof(double);\n\n    for (MPI_Count i=0; i<n; i++) {\n        sbuf[i] = (double)rank+1.;\n    }\n    for (MPI_Count i=0; i<n; i++) {\n        rbuf[i] = 0.0;\n    }\n\n    \n\n\n    size_t errors = 0;\n    double val = (double)size*(size+1.)/2.;\n    errors = verify_doubles(rbuf, n, val);\n    if (errors) {\n        printf(\"There were %zu errors out of %zu elements!\\n\", errors, (size_t)n);\n        for (MPI_Count i=0; i<n; i++) {\n            printf(\"rbuf[%zu] = %lf (expected %lf - %s)\\n\",\n                    (size_t)i, rbuf[i], val, rbuf[i]==val ? \"RIGHT\" : \"WRONG\");\n        }\n        fflush(stdout);\n    }\n\n\n    \n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n\n    return 0;\n}", "label": "int main(int argc, char * argv[])\n{\n    const MPI_Count test_int_max = BigMPI_Get_max_int();\n\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size<1) {\n        printf(\"Use 1 or more processes. \\n\");\n        MPI_Finalize();\n        return 1;\n    }\n\n    int l = (argc > 1) ? atoi(argv[1]) : 2;\n    int m = (argc > 2) ? atoi(argv[2]) : 17777;\n    MPI_Count n = l * test_int_max + m;\n\n    double * sbuf = NULL;\n    double * rbuf = NULL;\n\n    MPI_Aint bytes = n*sizeof(double);\n    MPI_Alloc_mem(bytes, MPI_INFO_NULL, &sbuf);\n    MPI_Alloc_mem(bytes, MPI_INFO_NULL, &rbuf);\n\n    for (MPI_Count i=0; i<n; i++) {\n        sbuf[i] = (double)rank+1.;\n    }\n    for (MPI_Count i=0; i<n; i++) {\n        rbuf[i] = 0.0;\n    }\n\n    \n\n    MPIX_Allreduce_x(sbuf, rbuf, n, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    size_t errors = 0;\n    double val = (double)size*(size+1.)/2.;\n    errors = verify_doubles(rbuf, n, val);\n    if (errors) {\n        printf(\"There were %zu errors out of %zu elements!\\n\", errors, (size_t)n);\n        for (MPI_Count i=0; i<n; i++) {\n            printf(\"rbuf[%zu] = %lf (expected %lf - %s)\\n\",\n                    (size_t)i, rbuf[i], val, rbuf[i]==val ? \"RIGHT\" : \"WRONG\");\n        }\n        fflush(stdout);\n    }\n\n    MPI_Free_mem(sbuf);\n    MPI_Free_mem(rbuf);\n\n    \n\n    if (rank==0 && errors==0) {\n        printf(\"SUCCESS\\n\");\n    }\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "lapesd_1556", "code": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing JSON parser test...\\n\", stdout);\n  ctx_init ();\n\n  if (argc == 2)\n    {\n      int rerr = test_parse (argv[1]);\n      fprintf (stdout, \"[Task %d] Test parse:...%s!\\n\", ctx.task_id,\n\t       (rerr ? \"FAILED\" : \"SUCCESS\"));\n    }\n  else\n    fputs (\"Missign experiment definition file.\\n\", stderr);\n\n  fputs (\"Finalizing JSON parser test.\\n\", stdout);\n}", "label": "int\nmain (int argc, char **argv)\n{\n  fputs (\"Initializing JSON parser test...\\n\", stdout);\n  MPI_Init (&argc, &argv);\n  ctx_init ();\n\n  if (argc == 2)\n    {\n      int rerr = test_parse (argv[1]);\n      fprintf (stdout, \"[Task %d] Test parse:...%s!\\n\", ctx.task_id,\n\t       (rerr ? \"FAILED\" : \"SUCCESS\"));\n    }\n  else\n    fputs (\"Missign experiment definition file.\\n\", stderr);\n\n  MPI_Finalize ();\n  fputs (\"Finalizing JSON parser test.\\n\", stdout);\n}"}
{"program": "zaqwes8811_1557", "code": "int main (int argc, char *argv[])\n{\n    int rank, nprocs, ilen;\n    char processor[MPI_MAX_PROCESSOR_NAME];\n    double tstart = 0.0, tend = 0.0;\n\n    MPI_Status reqstat;\n    MPI_Request send_request;\n    MPI_Request recv_request;\n\n\n\n    if(nprocs != 2)\n    {\n        if(rank == 0) printf(\"This test requires exactly two processes\\n\");\n\n        exit(EXIT_FAILURE);\n    }\n\n    char *h_src, *h_rcv;\n\n    int other_proc = (rank == 1 ? 0 : 1);\n    int igpu = (rank == 1 ? 0 : 1);\n\n    int loop = LOOP_LARGE;\n\n    printf(\"node=%d(%s): my other _proc = %d and using GPU=%d loop %d\\n\", rank,\n           processor, other_proc, igpu, loop);\n\n    char *d_src, *d_rcv;\n    CHECK(cudaSetDevice(igpu));\n    CHECK(cudaMalloc((void **)&d_src, MYBUFSIZE));\n    CHECK(cudaMalloc((void **)&d_rcv, MYBUFSIZE));\n\n    for (int size = 1; size <= MAX_MSG_SIZE; size *= 2)\n    {\n\n        CHECK(cudaMemset(d_src, 'a', size));\n        CHECK(cudaMemset(d_rcv, 'b', size));\n\n        if(rank == 0)\n        {\n            tstart =\n\n            for(int i = 0; i < loop; i++)\n            {\n\n\n            }\n\n            tend =\n        }\n        else\n        {\n            for(int i = 0; i < loop; i++)\n            {\n\n            }\n        }\n\n\n        if(rank == 0)\n        {\n            double tmp = size / 1e6 * loop  * 2;\n            double t = (tend - tstart);\n\n            printf(\"%-*d%*.*f\\n\", 10, size, FIELD_WIDTH, FLOAT_PRECISION,\n                    tmp / t);\n            fflush(stdout);\n        }\n    }\n\n    CHECK(cudaSetDevice(igpu));\n    CHECK(cudaFree(d_src));\n    CHECK(cudaFree(d_rcv));\n\n\n    return EXIT_SUCCESS;\n}", "label": "int main (int argc, char *argv[])\n{\n    int rank, nprocs, ilen;\n    char processor[MPI_MAX_PROCESSOR_NAME];\n    double tstart = 0.0, tend = 0.0;\n\n    MPI_Status reqstat;\n    MPI_Request send_request;\n    MPI_Request recv_request;\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    MPI_Get_processor_name(processor, &ilen);\n\n    if(nprocs != 2)\n    {\n        if(rank == 0) printf(\"This test requires exactly two processes\\n\");\n\n        MPI_Finalize();\n        exit(EXIT_FAILURE);\n    }\n\n    char *h_src, *h_rcv;\n\n    int other_proc = (rank == 1 ? 0 : 1);\n    int igpu = (rank == 1 ? 0 : 1);\n\n    int loop = LOOP_LARGE;\n\n    printf(\"node=%d(%s): my other _proc = %d and using GPU=%d loop %d\\n\", rank,\n           processor, other_proc, igpu, loop);\n\n    char *d_src, *d_rcv;\n    CHECK(cudaSetDevice(igpu));\n    CHECK(cudaMalloc((void **)&d_src, MYBUFSIZE));\n    CHECK(cudaMalloc((void **)&d_rcv, MYBUFSIZE));\n\n    for (int size = 1; size <= MAX_MSG_SIZE; size *= 2)\n    {\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        CHECK(cudaMemset(d_src, 'a', size));\n        CHECK(cudaMemset(d_rcv, 'b', size));\n\n        if(rank == 0)\n        {\n            tstart = MPI_Wtime();\n\n            for(int i = 0; i < loop; i++)\n            {\n                MPI_Isend(d_src, size, MPI_CHAR, other_proc, 100,\n                        MPI_COMM_WORLD, &send_request);\n                MPI_Irecv(d_rcv, size, MPI_CHAR, other_proc, 10, MPI_COMM_WORLD,\n                        &recv_request);\n\n                MPI_Waitall(1, &recv_request, &reqstat);\n                MPI_Waitall(1, &send_request, &reqstat);\n\n            }\n\n            tend = MPI_Wtime();\n        }\n        else\n        {\n            for(int i = 0; i < loop; i++)\n            {\n                MPI_Isend(d_src, size, MPI_CHAR, other_proc, 10, MPI_COMM_WORLD,\n                        &send_request);\n                MPI_Irecv(d_rcv, size, MPI_CHAR, other_proc, 100,\n                        MPI_COMM_WORLD, &recv_request);\n\n                MPI_Waitall(1, &recv_request, &reqstat);\n                MPI_Waitall(1, &send_request, &reqstat);\n            }\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        if(rank == 0)\n        {\n            double tmp = size / 1e6 * loop  * 2;\n            double t = (tend - tstart);\n\n            printf(\"%-*d%*.*f\\n\", 10, size, FIELD_WIDTH, FLOAT_PRECISION,\n                    tmp / t);\n            fflush(stdout);\n        }\n    }\n\n    CHECK(cudaSetDevice(igpu));\n    CHECK(cudaFree(d_src));\n    CHECK(cudaFree(d_rcv));\n\n    MPI_Finalize();\n\n    return EXIT_SUCCESS;\n}"}
{"program": "radii_1559", "code": "int main(int argC, char *args[])\n\n\n{\n#ifdef MPI_MATSOLVE\n  int rank, size, i;\n#else\n  char       colName[64], depName[64], str[1024];\n  double     startTime, stopTime, wtFactor=DEFAULT_WT_FACTOR;\n  s32       *deps, origC, seed=DEFAULT_SEED;\n  struct stat fileInfo;\n  nfs_sparse_mat_t M;\n  int        i;\n  FILE      *fp, *ifp;\n#endif\n\n  strcpy(colName, DEFAULT_COLNAME);\n  strcpy(depName, DEFAULT_DEPNAME);\n  printf(START_MSG, GGNFS_VERSION);\n\n#ifdef MPI_MATSOLVE\n  msgLog(NULL, \"mpi-matsolve %s: %d of %d checking in.\\n\", GGNFS_VERSION, rank, size);\n  if (rank) {\n    \n\n    slave();\n    msgLog(NULL, \"mpi-matsolve: %d of %d terminating.\\n\", rank, size);\n    return 0;\n  } \n  master();\n  \n\n  for (i=1; i<numNodes; i++) {\n  }\n  msgLog(NULL, \"mpi-matsolve: %d of %d terminating.\\n\", rank, size);\n  return 0;\n}\n#else\n  if (stat(\"depinf\", &fileInfo)) {\n    printf(\"Could not stat depinf! Are you trying to run %s to soon?\\n\", args[0]);\n    return -1;\n  }\n\n  for (i=1; i<argC; i++) {\n    if (strcmp(args[i], \"-v\")==0) {\n      verbose++;\n    } else if (strcmp(args[i], \"-seed\")==0) {\n      if ((+i) < argC)\n        seed = atol(args[i]);\n    } else if (strcmp(args[i], \"-wt\")==0) {\n      if ((++i) < argC)\n        wtFactor = atof(args[i]);\n    } else if (strcmp(args[i], \"--help\")==0) {\n      printf(\"USAGE: %s %s\\n\", args[0], USAGE);\n      exit(0);\n    }\n  }\n  srand(seed);\n  seedBlockLanczos(seed);\n  startTime = sTime();\n  msgLog(\"\", \"GGNFS-%s : matsolve\", GGNFS_VERSION);\n\n\n\n  printf(\"Loading matrix into RAM...\\n\");\n  loadMat(&M, colName);\n  printf(\"Matrix loaded: it is %ld x %ld.\\n\", M.numRows, M.numCols);\n  if (M.numCols < (M.numRows + 64)) {\n    printf(\"More columns needed (current = %ld, min = %ld)\\n\",\n           M.numCols, M.numRows+64);\n    free(M.cEntry); free(M.cIndex);\n    exit(-1);\n  }\n  if (checkMat(&M)) {\n    printf(\"checkMat() returned some error! Terminating...\\n\");\n    exit(-1);\n  }\n\n  if (!(deps = (s32 *)malloc(M.numCols*sizeof(s32)))) {\n    printf(\"Could not allocate %ld bytes for the dependencies.\\n\", M.numCols*sizeof(s32));\n    free(M.cEntry); free(M.cIndex); return -1;\n  }\n  origC = M.numCols;\n  getDependencies(&M, wtFactor, deps);\n\n\n  if (!(ifp = fopen(\"depinf\", \"rb\"))) {\n    fprintf(stderr, \"Error opening depinf for read!\\n\");\n    exit(-1);\n  }\n  printf(\"Writing dependencies to file %s.\\n\", depName);\n  if (!(fp = fopen(depName, \"wb\"))) {\n    fprintf(stderr, \"Error opening %s for write!\\n\", depName);\n  } else {\n    \n\n    readBinField(str, 1024, ifp);\n    while (!(feof(ifp)) && strncmp(str, \"END_HEADER\",10)) {\n      writeBinField(fp, str);\n      readBinField(str,1024,ifp);\n    }\n    if (strncmp(str, \"END_HEADER\",10)) {\n      fprintf(stderr, \"Error: depinf is corrupt!\\n\");\n      fclose(ifp); fclose(fp); exit(-1);\n    }\n    writeBinField(fp, str);\n    fclose(ifp);\n    fwrite(deps, sizeof(s32), origC, fp);\n    fclose(fp);\n  }\n\n  stopTime = sTime();\n  printf(\"Total elapsed time: %1.2lf seconds.\\n\", stopTime-startTime);\n\n\n  free(M.cEntry); free(M.cIndex); free(deps);\n  return 0;\n}", "label": "int main(int argC, char *args[])\n\n\n{\n#ifdef MPI_MATSOLVE\n  int rank, size, i;\n#else\n  char       colName[64], depName[64], str[1024];\n  double     startTime, stopTime, wtFactor=DEFAULT_WT_FACTOR;\n  s32       *deps, origC, seed=DEFAULT_SEED;\n  struct stat fileInfo;\n  nfs_sparse_mat_t M;\n  int        i;\n  FILE      *fp, *ifp;\n#endif\n\n  strcpy(colName, DEFAULT_COLNAME);\n  strcpy(depName, DEFAULT_DEPNAME);\n  printf(START_MSG, GGNFS_VERSION);\n\n#ifdef MPI_MATSOLVE\n  MPI_Init(&argC, &args);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  msgLog(NULL, \"mpi-matsolve %s: %d of %d checking in.\\n\", GGNFS_VERSION, rank, size);\n  if (rank) {\n    \n\n    slave();\n    msgLog(NULL, \"mpi-matsolve: %d of %d terminating.\\n\", rank, size);\n    MPI_Finalize();\n    return 0;\n  } \n  master();\n  \n\n  for (i=1; i<numNodes; i++) {\n    MPI_Send(0, 0, MPI_INT, i, DIETAG, MPI_COMM_WORLD);\n  }\n  msgLog(NULL, \"mpi-matsolve: %d of %d terminating.\\n\", rank, size);\n  MPI_Finalize();\n  return 0;\n}\n#else\n  if (stat(\"depinf\", &fileInfo)) {\n    printf(\"Could not stat depinf! Are you trying to run %s to soon?\\n\", args[0]);\n    return -1;\n  }\n\n  for (i=1; i<argC; i++) {\n    if (strcmp(args[i], \"-v\")==0) {\n      verbose++;\n    } else if (strcmp(args[i], \"-seed\")==0) {\n      if ((+i) < argC)\n        seed = atol(args[i]);\n    } else if (strcmp(args[i], \"-wt\")==0) {\n      if ((++i) < argC)\n        wtFactor = atof(args[i]);\n    } else if (strcmp(args[i], \"--help\")==0) {\n      printf(\"USAGE: %s %s\\n\", args[0], USAGE);\n      exit(0);\n    }\n  }\n  srand(seed);\n  seedBlockLanczos(seed);\n  startTime = sTime();\n  msgLog(\"\", \"GGNFS-%s : matsolve\", GGNFS_VERSION);\n\n\n\n  printf(\"Loading matrix into RAM...\\n\");\n  loadMat(&M, colName);\n  printf(\"Matrix loaded: it is %ld x %ld.\\n\", M.numRows, M.numCols);\n  if (M.numCols < (M.numRows + 64)) {\n    printf(\"More columns needed (current = %ld, min = %ld)\\n\",\n           M.numCols, M.numRows+64);\n    free(M.cEntry); free(M.cIndex);\n    exit(-1);\n  }\n  if (checkMat(&M)) {\n    printf(\"checkMat() returned some error! Terminating...\\n\");\n    exit(-1);\n  }\n\n  if (!(deps = (s32 *)malloc(M.numCols*sizeof(s32)))) {\n    printf(\"Could not allocate %ld bytes for the dependencies.\\n\", M.numCols*sizeof(s32));\n    free(M.cEntry); free(M.cIndex); return -1;\n  }\n  origC = M.numCols;\n  getDependencies(&M, wtFactor, deps);\n\n\n  if (!(ifp = fopen(\"depinf\", \"rb\"))) {\n    fprintf(stderr, \"Error opening depinf for read!\\n\");\n    exit(-1);\n  }\n  printf(\"Writing dependencies to file %s.\\n\", depName);\n  if (!(fp = fopen(depName, \"wb\"))) {\n    fprintf(stderr, \"Error opening %s for write!\\n\", depName);\n  } else {\n    \n\n    readBinField(str, 1024, ifp);\n    while (!(feof(ifp)) && strncmp(str, \"END_HEADER\",10)) {\n      writeBinField(fp, str);\n      readBinField(str,1024,ifp);\n    }\n    if (strncmp(str, \"END_HEADER\",10)) {\n      fprintf(stderr, \"Error: depinf is corrupt!\\n\");\n      fclose(ifp); fclose(fp); exit(-1);\n    }\n    writeBinField(fp, str);\n    fclose(ifp);\n    fwrite(deps, sizeof(s32), origC, fp);\n    fclose(fp);\n  }\n\n  stopTime = sTime();\n  printf(\"Total elapsed time: %1.2lf seconds.\\n\", stopTime-startTime);\n\n\n  free(M.cEntry); free(M.cIndex); free(deps);\n  return 0;\n}"}
{"program": "rwanwork_1560", "code": "int main (int argc, char *argv[]) {\n  INFO *info;\n  bool result = false;\n\n  info = initialize ();\n\n#if HAVE_MPI\n\n#endif\n\n#if HAVE_OPENMP\n  info -> threads = omp_get_num_procs ();\n  omp_set_num_threads (info -> threads);\n#endif\n\n  \n\n  if ((!processOptions (argc, argv, info)) || (!checkSettings (info))) {\n    usage (argv[0]);\n  }\n  else {\n    result = run (info);\n  }\n\n#if HAVE_MPI\n#endif\n\n  uninitialize (info);\n\n  return (EXIT_SUCCESS);\n}", "label": "int main (int argc, char *argv[]) {\n  INFO *info;\n  bool result = false;\n\n  info = initialize ();\n\n#if HAVE_MPI\n  MPI_Init (&argc, &argv);\n\n  MPI_Comm_rank (MPI_COMM_WORLD, &(info -> world_id));\n  MPI_Comm_size (MPI_COMM_WORLD, &(info -> world_size));\n#endif\n\n#if HAVE_OPENMP\n  info -> threads = omp_get_num_procs ();\n  omp_set_num_threads (info -> threads);\n#endif\n\n  \n\n  if ((!processOptions (argc, argv, info)) || (!checkSettings (info))) {\n    usage (argv[0]);\n  }\n  else {\n    result = run (info);\n  }\n\n#if HAVE_MPI\n  MPI_Finalize ();\n#endif\n\n  uninitialize (info);\n\n  return (EXIT_SUCCESS);\n}"}
{"program": "mpip_1562", "code": "int main(int argc, char **argv){\n  int np[3], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M;\n  double x_max[3];\n  \n  \n\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=2;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2] = %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %d real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] = %td x %td processes (change with -pnfft_np * *)\\n\", np[0], np[1]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD);\n\n  \n\n  pnfft_cleanup();\n  return 0;\n}", "label": "int main(int argc, char **argv){\n  int np[3], m, window;\n  unsigned window_flag;\n  ptrdiff_t N[3], n[3], local_M;\n  double x_max[3];\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pnfft_init();\n  \n  \n\n  N[0] = N[1] = N[2] = 16;\n  n[0] = n[1] = n[2] = 0;\n  local_M = 0;\n  m = 6;\n  window = 4;\n  x_max[0] = x_max[1] = x_max[2] = 0.5;\n  np[0]=2; np[1]=2;\n  \n  \n\n  init_parameters(argc, argv, N, n, &local_M, &m, &window, x_max, np);\n\n  \n\n  local_M = (local_M==0) ? N[0]*N[1]*N[2]/(np[0]*np[1]) : local_M;\n  for(int t=0; t<3; t++)\n    n[t] = (n[t]==0) ? 2*N[t] : n[t];\n\n  switch(window){\n    case 0: window_flag = PNFFT_WINDOW_GAUSSIAN; break;\n    case 1: window_flag = PNFFT_WINDOW_BSPLINE; break;\n    case 2: window_flag = PNFFT_WINDOW_SINC_POWER; break;\n    case 3: window_flag = PNFFT_WINDOW_BESSEL_I0; break;\n    default: window_flag = PNFFT_WINDOW_KAISER_BESSEL;\n  }\n\n  pfft_printf(MPI_COMM_WORLD, \"******************************************************************************************************\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* Computation of parallel NFFT\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* for  N[0] x N[1] x N[2] = %td x %td x %td Fourier coefficients (change with -pnfft_N * * *)\\n\", N[0], N[1], N[2]);\n  pfft_printf(MPI_COMM_WORLD, \"* at   local_M = %td nodes per process (change with -pnfft_local_M *)\\n\", local_M);\n  pfft_printf(MPI_COMM_WORLD, \"* with n[0] x n[1] x n[2] = %td x %td x %td FFT grid size (change with -pnfft_n * * *),\\n\", n[0], n[1], n[2]);\n  pfft_printf(MPI_COMM_WORLD, \"*      m = %d real space cutoff (change with -pnfft_m *),\\n\", m);\n  pfft_printf(MPI_COMM_WORLD, \"*      window = %d window function \", window);\n  switch(window){\n    case 0: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_GAUSSIAN) \"); break;\n    case 1: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BSPLINE) \"); break;\n    case 2: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_SINC_POWER) \"); break;\n    case 3: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_BESSEL_I0) \"); break;\n    default: pfft_printf(MPI_COMM_WORLD, \"(PNFFT_WINDOW_KAISER_BESSEL) \"); break;\n  }\n  pfft_printf(MPI_COMM_WORLD, \"(change with -pnfft_window *),\\n\");\n  pfft_printf(MPI_COMM_WORLD, \"* on   np[0] x np[1] = %td x %td processes (change with -pnfft_np * *)\\n\", np[0], np[1]);\n  pfft_printf(MPI_COMM_WORLD, \"*******************************************************************************************************\\n\\n\");\n\n\n  \n\n  pnfft_perform_guru(N, n, local_M, m,   x_max, window_flag, np, MPI_COMM_WORLD);\n\n  \n\n  pnfft_cleanup();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "daidong_1565", "code": "int main(int argc, char **argv) \n{\n    int nprocs;\n    int mynod;\n    int i;\n    int ret;\n    int fd;\n    int iters;\n    double time1, time2, read1, read2, host1, host2;\n    char onechar;\n    char host[256];\n\n\n    \n\n    if(nprocs > 1)\n    {\n        if(mynod == 0)\n        {\n            fprintf(stderr, \"Error: this benchmark should be run with exactly one process.\\n\");\n        }\n        return(-1);\n    }\n\n    \n\n    if(argc != 3)\n    {\n        fprintf(stderr, \"Usage: %s <filename> <number of iterations>\\n\", argv[0]);\n        return(-1);\n    }\n\n    ret = sscanf(argv[2], \"%d\", &iters);\n    if(ret != 1)\n    {\n        fprintf(stderr, \"Usage: %s <filename> <number of iterations>\\n\", argv[0]);\n        return(-1);\n    }\n\n    \n\n    time1 =\n    for(i=0; i<iters; i++)\n    {\n        read1 =\n    }\n    time2 =\n\n    sleep(1);\n\n    fd = open(argv[1], O_RDONLY);\n    if(fd < 0)\n    {\n        perror(\"open\");\n        return(-1);\n    }\n\n    \n\n    read1 =\n    for(i=0; i<iters; i++)\n    {\n        ret = read(fd, &onechar, 1);\n        if(ret < 0)\n        {\n            perror(\"read\");\n            return(-1);\n        }\n    }\n    read2 =\n\n    close(fd);\n\n    sleep(1);\n \n    \n\n    host1 =\n    for(i=0; i<iters; i++)\n    {\n        ret = gethostname(host, 256);\n        if(ret < 0)\n        {\n            perror(\"gethostname\");\n            return(-1);\n        }\n    }\n    host2 =\n\n    \n\n    printf(\"#<op>\\t<iters>\\t<total (s)>\\t<per op (s)>\\n\");\n    printf(\"wtime\\t%d\\t%.9f\\t%.9f\\n\", iters, time2-time1, (time2-time1)/(double)iters);\n    printf(\"read\\t%d\\t%.9f\\t%.9f\\n\", iters, read2-read1, (read2-read1)/(double)iters);\n    printf(\"gethostname\\t%d\\t%.9f\\t%.9f\\n\", iters, host2-host1, (host2-host1)/(double)iters);\n\n    return(0);\n}", "label": "int main(int argc, char **argv) \n{\n    int nprocs;\n    int mynod;\n    int i;\n    int ret;\n    int fd;\n    int iters;\n    double time1, time2, read1, read2, host1, host2;\n    char onechar;\n    char host[256];\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mynod);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    \n\n    if(nprocs > 1)\n    {\n        if(mynod == 0)\n        {\n            fprintf(stderr, \"Error: this benchmark should be run with exactly one process.\\n\");\n        }\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    if(argc != 3)\n    {\n        fprintf(stderr, \"Usage: %s <filename> <number of iterations>\\n\", argv[0]);\n        MPI_Finalize();\n        return(-1);\n    }\n\n    ret = sscanf(argv[2], \"%d\", &iters);\n    if(ret != 1)\n    {\n        fprintf(stderr, \"Usage: %s <filename> <number of iterations>\\n\", argv[0]);\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    time1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        read1 = MPI_Wtime();\n    }\n    time2 = MPI_Wtime();\n\n    sleep(1);\n\n    fd = open(argv[1], O_RDONLY);\n    if(fd < 0)\n    {\n        perror(\"open\");\n        MPI_Finalize();\n        return(-1);\n    }\n\n    \n\n    read1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        ret = read(fd, &onechar, 1);\n        if(ret < 0)\n        {\n            perror(\"read\");\n            MPI_Finalize();\n            return(-1);\n        }\n    }\n    read2 = MPI_Wtime();\n\n    close(fd);\n\n    sleep(1);\n \n    \n\n    host1 = MPI_Wtime();\n    for(i=0; i<iters; i++)\n    {\n        ret = gethostname(host, 256);\n        if(ret < 0)\n        {\n            perror(\"gethostname\");\n            MPI_Finalize();\n            return(-1);\n        }\n    }\n    host2 = MPI_Wtime();\n\n    \n\n    printf(\"#<op>\\t<iters>\\t<total (s)>\\t<per op (s)>\\n\");\n    printf(\"wtime\\t%d\\t%.9f\\t%.9f\\n\", iters, time2-time1, (time2-time1)/(double)iters);\n    printf(\"read\\t%d\\t%.9f\\t%.9f\\n\", iters, read2-read1, (read2-read1)/(double)iters);\n    printf(\"gethostname\\t%d\\t%.9f\\t%.9f\\n\", iters, host2-host1, (host2-host1)/(double)iters);\n\n    MPI_Finalize();\n    return(0);\n}"}
{"program": "qingu_1566", "code": "int main( int argc, char **argv )\n{\n  int errs = 0;\n\n  printf( \"sequence number for communicators is %d\\n\", MPIR_All_communicators.sequence_number );\n  printf( \"head pointer is %p\\n\", MPIR_All_communicators.head );\n\n  if (MPIR_All_communicators.head == (void *)0) {\n    printf( \"ERROR: The communicator list field has a null head pointer\\n\" );\n    printf( \"Either the debugger support is not enabled (--enable-debuginfo)\\n or the necessary symbols, including the extern variable\\n MPIR_All_communicators, are not properly exported to the main program\\n\" );\n    errs++;\n  }\n\n  if (errs) {\n    printf( \" Found %d errors\\n\", errs );\n  }\n  else {\n    printf( \" No Errors\\n\" );\n  }\n  \n  return 0;\n}", "label": "int main( int argc, char **argv )\n{\n  int errs = 0;\n  MPI_Init( &argc, &argv );\n\n  printf( \"sequence number for communicators is %d\\n\", MPIR_All_communicators.sequence_number );\n  printf( \"head pointer is %p\\n\", MPIR_All_communicators.head );\n\n  if (MPIR_All_communicators.head == (void *)0) {\n    printf( \"ERROR: The communicator list field has a null head pointer\\n\" );\n    printf( \"Either the debugger support is not enabled (--enable-debuginfo)\\n or the necessary symbols, including the extern variable\\n MPIR_All_communicators, are not properly exported to the main program\\n\" );\n    errs++;\n  }\n\n  MPI_Finalize( );\n  if (errs) {\n    printf( \" Found %d errors\\n\", errs );\n  }\n  else {\n    printf( \" No Errors\\n\" );\n  }\n  \n  return 0;\n}"}
{"program": "william-dawson_1567", "code": "int main(int argc, char *argv[]) {\n  int my_rank;\n  int i;\n  char matrix_length_s[] = \"--matrix_length\";\n  char values_to_add_s[] = \"--values_to_add\";\n  int matrix_length;\n  int values_to_add;\n  int DA[SIZE_wrp];\n  int next_location;\n  double next_value;\n  double sum_value;\n\n\n  \n\n  for (i = 1; i < argc; i += 2) {\n    if (strcmp(argv[i], matrix_length_s) == 0) {\n      matrix_length = atoi(argv[i + 1]);\n    } else if (strcmp(argv[i], values_to_add_s) == 0) {\n      values_to_add = atoi(argv[i + 1]);\n    }\n  }\n\n  \n\n  ConstructDistributedArray_wrp(DA, &matrix_length);\n  next_value = 1.0;\n  for (i = 0; i < values_to_add; ++i) {\n    next_location = rand() % matrix_length + 1;\n    SetDistributedArrayAt_wrp(DA, &next_location, &next_value);\n  }\n\n  \n\n  sum_value = SumDistributedArray_wrp(DA);\n  if (my_rank == 0)\n    printf(\"Sum value %f\\n\", sum_value);\n\n  \n\n  DestructDistributedArray_wrp(DA);\n  return 0;\n}", "label": "int main(int argc, char *argv[]) {\n  int my_rank;\n  int i;\n  char matrix_length_s[] = \"--matrix_length\";\n  char values_to_add_s[] = \"--values_to_add\";\n  int matrix_length;\n  int values_to_add;\n  int DA[SIZE_wrp];\n  int next_location;\n  double next_value;\n  double sum_value;\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  \n\n  for (i = 1; i < argc; i += 2) {\n    if (strcmp(argv[i], matrix_length_s) == 0) {\n      matrix_length = atoi(argv[i + 1]);\n    } else if (strcmp(argv[i], values_to_add_s) == 0) {\n      values_to_add = atoi(argv[i + 1]);\n    }\n  }\n\n  \n\n  ConstructDistributedArray_wrp(DA, &matrix_length);\n  next_value = 1.0;\n  for (i = 0; i < values_to_add; ++i) {\n    next_location = rand() % matrix_length + 1;\n    SetDistributedArrayAt_wrp(DA, &next_location, &next_value);\n  }\n\n  \n\n  sum_value = SumDistributedArray_wrp(DA);\n  if (my_rank == 0)\n    printf(\"Sum value %f\\n\", sum_value);\n\n  \n\n  DestructDistributedArray_wrp(DA);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bjoern-leder_1568", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,i,ie;\n   double d;\n   complex_dble **wv,z;\n   dfl_grid_t dfl_grid;   \n   FILE *fin=NULL,*flog=NULL;   \n\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check4.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Check of the communication programs cpvd_int_bnd() \"\n             \"and cpvd_ext_bnd()\\n\");\n      printf(\"---------------------------------------------------\"\n             \"------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"bs\",\"%d %d %d %d\",&bs[0],&bs[1],&bs[2],&bs[3]);\n      fclose(fin);\n\n      printf(\"bs = %d %d %d %d\\n\\n\",bs[0],bs[1],bs[2],bs[3]);\n      fflush(flog);\n   }\n\n\n   start_ranlux(0,123456);   \n   geometry();\n   Ns=4;\n   set_dfl_parms(bs,Ns);\n\n   dfl_grid=dfl_geometry();\n   nb=dfl_grid.nb;\n   nbb=dfl_grid.nbb;\n   nbbe=dfl_grid.nbbe;\n   nbbo=dfl_grid.nbbo;\n   obbe=dfl_grid.obbe;\n   obbo=dfl_grid.obbo;\n   inn=dfl_grid.inn;\n   ipp=dfl_grid.ipp;\n   \n   alloc_wvd(4);\n   wv=reserve_wvd(4);\n\n   nv=Ns*nb;\n   nvec=Ns*(nb+nbb/2);\n   z.re=-1.0;\n   z.im=0.0;\n\n   for (i=0;i<2;i++)\n   {\n      random_vd(nvec,wv[i],1.0);\n      set_field(wv[i]);\n      assign_vd2vd(nv,wv[i],wv[i+1]);\n      cpvd_int_bnd(wv[i]);\n      mulc_vadd_dble(nv,wv[i+1],wv[i],z);\n      d=vnorm_square_dble(nv,1,wv[i+1]);\n\n      error_root(d!=0.0,1,\"main [check4.c]\",\n                 \"cpvd_int_bnd() modifies the input field on the local grid\");\n\n      ie=chk_ext_bnd(wv[i]);\n      error(ie==1,1,\"main [check4.c]\",\n            \"Boundary values are incorrectly mapped by cpvd_int_bnd()\");\n      error(ie==2,1,\"main [check3.c]\",\n            \"Boundary values are not set to zero where they should\");\n      \n      random_iv(nvec,wv[i]);\n      cpvd_int_bnd(wv[i]);\n      assign_vd2vd(nvec,wv[i],wv[i+1]);\n      cpvd_ext_bnd(wv[i]);\n      mulc_vadd_dble(nvec-nv,wv[i]+nv,wv[i+1]+nv,z);\n      d=vnorm_square_dble(nvec-nv,1,wv[i]+nv);\n\n      error_root(d!=0.0,1,\"main [check4.c]\",\n                 \"cpvd_ext_bnd() modifies the input field on the boundary\");\n\n      ie=chk_int_bnd(wv[i],wv[i+1]);\n      error(ie==1,1,\"main [check4.c]\",\n            \"Boundary values are incorrectly mapped by cpvd_ext_bnd()\");\n   }\n\n   error_chk();\n   \n   if (my_rank==0)\n   { \n      printf(\"No errors detected\\n\\n\");\n      fclose(flog);\n   }\n   \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,i,ie;\n   double d;\n   complex_dble **wv,z;\n   dfl_grid_t dfl_grid;   \n   FILE *fin=NULL,*flog=NULL;   \n\n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n\n   if (my_rank==0)\n   {\n      flog=freopen(\"check4.log\",\"w\",stdout);\n      fin=freopen(\"check1.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Check of the communication programs cpvd_int_bnd() \"\n             \"and cpvd_ext_bnd()\\n\");\n      printf(\"---------------------------------------------------\"\n             \"------------------\\n\\n\");\n\n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\",L0,L1,L2,L3);\n\n      read_line(\"bs\",\"%d %d %d %d\",&bs[0],&bs[1],&bs[2],&bs[3]);\n      fclose(fin);\n\n      printf(\"bs = %d %d %d %d\\n\\n\",bs[0],bs[1],bs[2],bs[3]);\n      fflush(flog);\n   }\n\n   MPI_Bcast(bs,4,MPI_INT,0,MPI_COMM_WORLD);\n\n   start_ranlux(0,123456);   \n   geometry();\n   Ns=4;\n   set_dfl_parms(bs,Ns);\n\n   dfl_grid=dfl_geometry();\n   nb=dfl_grid.nb;\n   nbb=dfl_grid.nbb;\n   nbbe=dfl_grid.nbbe;\n   nbbo=dfl_grid.nbbo;\n   obbe=dfl_grid.obbe;\n   obbo=dfl_grid.obbo;\n   inn=dfl_grid.inn;\n   ipp=dfl_grid.ipp;\n   \n   alloc_wvd(4);\n   wv=reserve_wvd(4);\n\n   nv=Ns*nb;\n   nvec=Ns*(nb+nbb/2);\n   z.re=-1.0;\n   z.im=0.0;\n\n   for (i=0;i<2;i++)\n   {\n      random_vd(nvec,wv[i],1.0);\n      set_field(wv[i]);\n      assign_vd2vd(nv,wv[i],wv[i+1]);\n      cpvd_int_bnd(wv[i]);\n      mulc_vadd_dble(nv,wv[i+1],wv[i],z);\n      d=vnorm_square_dble(nv,1,wv[i+1]);\n\n      error_root(d!=0.0,1,\"main [check4.c]\",\n                 \"cpvd_int_bnd() modifies the input field on the local grid\");\n\n      ie=chk_ext_bnd(wv[i]);\n      error(ie==1,1,\"main [check4.c]\",\n            \"Boundary values are incorrectly mapped by cpvd_int_bnd()\");\n      error(ie==2,1,\"main [check3.c]\",\n            \"Boundary values are not set to zero where they should\");\n      \n      random_iv(nvec,wv[i]);\n      cpvd_int_bnd(wv[i]);\n      assign_vd2vd(nvec,wv[i],wv[i+1]);\n      cpvd_ext_bnd(wv[i]);\n      mulc_vadd_dble(nvec-nv,wv[i]+nv,wv[i+1]+nv,z);\n      d=vnorm_square_dble(nvec-nv,1,wv[i]+nv);\n\n      error_root(d!=0.0,1,\"main [check4.c]\",\n                 \"cpvd_ext_bnd() modifies the input field on the boundary\");\n\n      ie=chk_int_bnd(wv[i],wv[i+1]);\n      error(ie==1,1,\"main [check4.c]\",\n            \"Boundary values are incorrectly mapped by cpvd_ext_bnd()\");\n   }\n\n   error_chk();\n   \n   if (my_rank==0)\n   { \n      printf(\"No errors detected\\n\\n\");\n      fclose(flog);\n   }\n   \n   MPI_Finalize();   \n   exit(0);\n}"}
{"program": "jmesmon_1571", "code": "int main(int argc, char **argv)\n{\n\n\n\tint rank;\n\tint nprocs;\n\n\n\tif (rank == 0) {\n\t\tmaster(nprocs);\n\t} else {\n\t\tslave(rank - 1, nprocs - 1);\n\t}\n\n\t\n\n\treturn 0;\n}", "label": "int main(int argc, char **argv)\n{\n\n\tMPI_Init(&argc, &argv);\n\n\tint rank;\n\tint nprocs;\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\tif (rank == 0) {\n\t\tmaster(nprocs);\n\t} else {\n\t\tslave(rank - 1, nprocs - 1);\n\t}\n\n\t\n\n\tMPI_Finalize();\n\treturn 0;\n}"}
{"program": "mkurnosov_1574", "code": "int main(int argc, char *argv[]) \n{\n    int commsize, rank;\n    double ttotal =\n    \n        \n    int px, py, rows, cols, ticks;\n    \n    \n\n    if (rank == 0) {\n        if (argc < 3) {\n            fprintf(stderr, \"Usage: %s <px> <py> [<rows> <cols> <ticks>]\\n\", argv[0]);\n        }\n        px = atoi(argv[1]);\n        py = atoi(argv[2]);\n        rows = (argc > 3) ? atoi(argv[3]) : 100;\n        cols = (argc > 4) ? atoi(argv[4]) : 100;\n        ticks = (argc > 5) ? atoi(argv[5]) : 10;\n        \n        if (px * py != commsize) {\n            fprintf(stderr, \"Invalid values of <px> and <py>: %d * %d != %d\\n\", px, py, commsize);\n        }\n\n        if (rows % py != 0 || cols % px != 0) {\n            fprintf(stderr, \"px and py must be devisors of rows and cols\\n\");\n        }\n        \n        if (rows < py) {\n            fprintf(stderr, \"Number of rows %d less then number of py processes %d\\n\", rows, py);\n        }\n        if (cols < px) {\n            fprintf(stderr, \"Number of cols %d less then number of px processes %d\\n\", cols, px);\n        }\n        \n        int args[5] = {px, py, rows, cols, ticks};\n    } else {\n        int args[5];\n        px = args[0]; \n        py = args[1];\n        rows = args[2];\n        cols = args[3];\n        ticks = args[4];\n    }    \n    \n    \n\n    int local_rows = rows / py;\n    int local_cols = cols / px;\n    cell_t *local_grid = xcalloc((local_rows + 2) * (local_cols + 2), sizeof(*local_grid));\n    cell_t *local_newgrid = xcalloc((local_rows + 2) * (local_cols + 2), sizeof(*local_newgrid));\n\n    \n\n    srand(rank);\n    for (int i = 1; i <= local_rows; i++) {\n        for (int j = 1; j <= local_cols; j++)\n            local_grid[IND(i, j)] = rand() % 10 > 0 ? 0 : 1;\n    }\n    \n    simulate_life(local_grid, local_newgrid, local_rows, local_cols, ticks, px, py);   \n    \n    free(local_newgrid);\n    free(local_grid);\n\n    ttotal +=\n    printf(\"Process %2d total time: %.6f\\n\", rank, ttotal);\n        \n    return 0;\n}", "label": "int main(int argc, char *argv[]) \n{\n    int commsize, rank;\n    MPI_Init(&argc, &argv);\n    double ttotal = -MPI_Wtime();\n    \n    MPI_Comm_size(MPI_COMM_WORLD, &commsize);   \n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        \n    int px, py, rows, cols, ticks;\n    \n    \n\n    if (rank == 0) {\n        if (argc < 3) {\n            fprintf(stderr, \"Usage: %s <px> <py> [<rows> <cols> <ticks>]\\n\", argv[0]);\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n        px = atoi(argv[1]);\n        py = atoi(argv[2]);\n        rows = (argc > 3) ? atoi(argv[3]) : 100;\n        cols = (argc > 4) ? atoi(argv[4]) : 100;\n        ticks = (argc > 5) ? atoi(argv[5]) : 10;\n        \n        if (px * py != commsize) {\n            fprintf(stderr, \"Invalid values of <px> and <py>: %d * %d != %d\\n\", px, py, commsize);\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n\n        if (rows % py != 0 || cols % px != 0) {\n            fprintf(stderr, \"px and py must be devisors of rows and cols\\n\");\n            MPI_Abort(MPI_COMM_WORLD, 1);\n        }\n        \n        if (rows < py) {\n            fprintf(stderr, \"Number of rows %d less then number of py processes %d\\n\", rows, py);\n            MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n        }\n        if (cols < px) {\n            fprintf(stderr, \"Number of cols %d less then number of px processes %d\\n\", cols, px);\n            MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n        }\n        \n        int args[5] = {px, py, rows, cols, ticks};\n        MPI_Bcast(&args, NELEMS(args), MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        int args[5];\n        MPI_Bcast(&args, NELEMS(args), MPI_INT, 0, MPI_COMM_WORLD);\n        px = args[0]; \n        py = args[1];\n        rows = args[2];\n        cols = args[3];\n        ticks = args[4];\n    }    \n    \n    \n\n    int local_rows = rows / py;\n    int local_cols = cols / px;\n    cell_t *local_grid = xcalloc((local_rows + 2) * (local_cols + 2), sizeof(*local_grid));\n    cell_t *local_newgrid = xcalloc((local_rows + 2) * (local_cols + 2), sizeof(*local_newgrid));\n\n    \n\n    srand(rank);\n    for (int i = 1; i <= local_rows; i++) {\n        for (int j = 1; j <= local_cols; j++)\n            local_grid[IND(i, j)] = rand() % 10 > 0 ? 0 : 1;\n    }\n    \n    simulate_life(local_grid, local_newgrid, local_rows, local_cols, ticks, px, py);   \n    \n    free(local_newgrid);\n    free(local_grid);\n\n    ttotal += MPI_Wtime();\n    printf(\"Process %2d total time: %.6f\\n\", rank, ttotal);\n        \n    MPI_Finalize();    \n    return 0;\n}"}
{"program": "mnakao_1575", "code": "int main(int argc, char *argv[]){\n  int i, me, target;\n  unsigned int size;\n  uint64_t laddr, raddr;\n  double t;\n\n  FJMPI_Rdma_init();\n  target = 1 - me;\n\n  init_buf(buf, me);\n\n  laddr = FJMPI_Rdma_reg_mem(0, buf, sizeof(char)*MAX_SIZE);\n  while((raddr = FJMPI_Rdma_get_remote_addr(target, 0)) == FJMPI_RDMA_ERROR);\n\n  if(me==0) print_items();\n\n  for(size=4;size<MAX_SIZE+1;size*=2){  \n\n\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n        t = wtime();\n\n      if(me == 0){\n\tFJMPI_Rdma_get(target, 0, raddr, laddr, size, FLAG_NIC);\n\twhile(buf[0] == '0' || buf[size-1] == '0'){\n          FJMPI_Rdma_poll_cq(SEND_NIC, &cq);\n        }\n        buf[0] = buf[size-1] = '0';\n      }\n      else{\n\tFJMPI_Rdma_get(target, 0, raddr, laddr, size, FLAG_NIC);\n\twhile(buf[0] == '1' || buf[size-1] == '1'){\n          FJMPI_Rdma_poll_cq(SEND_NIC, &cq);\n        }\n        buf[0] = buf[size-1] = '1';\n      }\n    }\n\n    t = wtime() - t;\n    if(me == 0)\n      print_results(size, t);\n  }\n\n  FJMPI_Rdma_finalize();\n  return 0;\n}", "label": "int main(int argc, char *argv[]){\n  int i, me, target;\n  unsigned int size;\n  uint64_t laddr, raddr;\n  double t;\n\n  MPI_Init(&argc, &argv);\n  FJMPI_Rdma_init();\n  MPI_Comm_rank(MPI_COMM_WORLD, &me);\n  target = 1 - me;\n\n  init_buf(buf, me);\n\n  laddr = FJMPI_Rdma_reg_mem(0, buf, sizeof(char)*MAX_SIZE);\n  while((raddr = FJMPI_Rdma_get_remote_addr(target, 0)) == FJMPI_RDMA_ERROR);\n\n  if(me==0) print_items();\n\n  for(size=4;size<MAX_SIZE+1;size*=2){  \n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    for(i=0;i<LOOP+WARMUP;i++){\n      if(WARMUP == i)\n        t = wtime();\n\n      if(me == 0){\n\tFJMPI_Rdma_get(target, 0, raddr, laddr, size, FLAG_NIC);\n\twhile(buf[0] == '0' || buf[size-1] == '0'){\n          FJMPI_Rdma_poll_cq(SEND_NIC, &cq);\n        }\n        buf[0] = buf[size-1] = '0';\n\tMPI_Barrier(MPI_COMM_WORLD);\n      }\n      else{\n\tMPI_Barrier(MPI_COMM_WORLD);\n\tFJMPI_Rdma_get(target, 0, raddr, laddr, size, FLAG_NIC);\n\twhile(buf[0] == '1' || buf[size-1] == '1'){\n          FJMPI_Rdma_poll_cq(SEND_NIC, &cq);\n        }\n        buf[0] = buf[size-1] = '1';\n      }\n      MPI_Barrier(MPI_COMM_WORLD);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    t = wtime() - t;\n    if(me == 0)\n      print_results(size, t);\n  }\n\n  FJMPI_Rdma_finalize();\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "bjoern-leder_1578", "code": "int main(int argc,char *argv[])\n{\n   int my_rank,ii,status;\n   wloop_parms_t wl;\n   double *wls,delta,eps,dmax;\n   double phi[2],phi_prime[2];\n   su3_dble *plnk;\n   FILE *log=NULL,*fin=NULL;   \n  \n  \n   if (my_rank==0)\n   {\n      log=freopen(\"check2.log\",\"w\",stdout);\n      fin=freopen(\"check2.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Gauge invariance check on the wloops routines\\n\");\n      printf(\"---------------------------------------------\\n\\n\");\n      \n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\\n\",L0,L1,L2,L3);\n      fflush(log);\n\n      bc=find_opt(argc,argv,\"-bc\");\n\n      if (bc!=0)\n         error_root(sscanf(argv[bc+1],\"%d\",&bc)!=1,1,\"main [check2.c]\",\n                    \"Syntax: check2 [-bc <type>]\");\n   }\n   \n\n   error_root((bc!=0)&&(bc!=3),1,\"main [check2.c]\",\n               \"Only open and periodic boundary conditions supported\");\n   read_wloop_parms();\n   if(my_rank==0)\n      fclose(fin);\n   wl=wloop_parms();\n\n   phi[0]=0.0;\n   phi[1]=0.0;\n   phi_prime[0]=0.0;\n   phi_prime[1]=0.0;\n   set_bc_parms(bc,1.0,1.0,1.0,1.0,phi,phi_prime);\n   print_bc_parms();\n\n   print_wloop_parms();\n\n   g=amalloc(NSPIN*sizeof(*g),4);\n   if (BNDRY!=0)\n      gbuf=amalloc((BNDRY/2)*sizeof(*gbuf),4);\n\n   error((g==NULL)||((BNDRY!=0)&&(gbuf==NULL)),1,\"main [check1.c]\",\n         \"Unable to allocate auxiliary arrays\");\n\n   start_ranlux(0,12345);\n   geometry();\n\n   random_ud();\n   plnk=alloc_lnk();\n   assign_ud2lnk(plnk);\n\n   hyp_links(wl.alpha_action[0],wl.alpha_action[1],wl.alpha_action[2],wl.proj,wl.proj_iter,0);\n  \n   wls=amalloc(wl.wls*sizeof(double),ALIGN);\n   error((wls==NULL),1,\"main [check2.c]\",\n          \"Unable to allocate auxiliary arrays\");\n      \n   message(\"Measure Wloops matrix\\n\");\n   \n   wloop_sum(0);\n      \n   for(ii=0; ii<wl.wls; ++ii)\n      wls[ii]=wl.wl_mat[ii];\n      \n   message(\"Perform gauge transformation\\n\");\n   \n   assign_lnk2ud(plnk);\n   random_g();\n   transform_ud();\n  \n   hyp_links(wl.alpha_action[0],wl.alpha_action[1],wl.alpha_action[2],wl.proj,wl.proj_iter,0);\n  \n   message(\"Measure Wloops matrix again\\n\");\n   \n   wloop_sum(0);\n      \n   status=-1;\n   eps=(double)(100)*sqrt((double)(wl.msl*wl.msl*wl.mwlr*wl.mwlt*3*NPROC)*(double)(VOLUME))*(double)(DBL_EPSILON);\n   dmax=(double)(DBL_EPSILON);\n   for (ii=0; ii<wl.wls; ++ii)\n   {\n      delta=fabs(1.0-wl.wl_mat[ii]/wls[ii]);\n      if (delta>=eps)\n      {\n         message(\"delta[%2d] = %.6e\\n\",ii,delta);\n         status+=2;\n      }\n      if (delta>dmax)\n         dmax=delta;\n   }\n   \n   if (status>0)\n      message(\"wloop routine is NOT gauge invariant!!\");\n   else   \n      message(\"wloops routine is gauge invariant (max. deviation: %.1e)!!\\n\\n\",dmax);\n\n   afree(wls);\n   afree(g);\n   afree(gbuf);\n   free_lnk(plnk,0);\n  \n   exit(0);\n}", "label": "int main(int argc,char *argv[])\n{\n   int my_rank,ii,status;\n   wloop_parms_t wl;\n   double *wls,delta,eps,dmax;\n   double phi[2],phi_prime[2];\n   su3_dble *plnk;\n   FILE *log=NULL,*fin=NULL;   \n  \n   MPI_Init(&argc,&argv);\n   MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n  \n   if (my_rank==0)\n   {\n      log=freopen(\"check2.log\",\"w\",stdout);\n      fin=freopen(\"check2.in\",\"r\",stdin);\n      \n      printf(\"\\n\");\n      printf(\"Gauge invariance check on the wloops routines\\n\");\n      printf(\"---------------------------------------------\\n\\n\");\n      \n      printf(\"%dx%dx%dx%d lattice, \",NPROC0*L0,NPROC1*L1,NPROC2*L2,NPROC3*L3);\n      printf(\"%dx%dx%dx%d process grid, \",NPROC0,NPROC1,NPROC2,NPROC3);\n      printf(\"%dx%dx%dx%d local lattice\\n\\n\\n\",L0,L1,L2,L3);\n      fflush(log);\n\n      bc=find_opt(argc,argv,\"-bc\");\n\n      if (bc!=0)\n         error_root(sscanf(argv[bc+1],\"%d\",&bc)!=1,1,\"main [check2.c]\",\n                    \"Syntax: check2 [-bc <type>]\");\n   }\n   \n   MPI_Bcast(&bc,1,MPI_INT,0,MPI_COMM_WORLD);\n\n   error_root((bc!=0)&&(bc!=3),1,\"main [check2.c]\",\n               \"Only open and periodic boundary conditions supported\");\n   read_wloop_parms();\n   if(my_rank==0)\n      fclose(fin);\n   wl=wloop_parms();\n\n   phi[0]=0.0;\n   phi[1]=0.0;\n   phi_prime[0]=0.0;\n   phi_prime[1]=0.0;\n   set_bc_parms(bc,1.0,1.0,1.0,1.0,phi,phi_prime);\n   print_bc_parms();\n\n   print_wloop_parms();\n\n   g=amalloc(NSPIN*sizeof(*g),4);\n   if (BNDRY!=0)\n      gbuf=amalloc((BNDRY/2)*sizeof(*gbuf),4);\n\n   error((g==NULL)||((BNDRY!=0)&&(gbuf==NULL)),1,\"main [check1.c]\",\n         \"Unable to allocate auxiliary arrays\");\n\n   start_ranlux(0,12345);\n   geometry();\n\n   random_ud();\n   plnk=alloc_lnk();\n   assign_ud2lnk(plnk);\n\n   hyp_links(wl.alpha_action[0],wl.alpha_action[1],wl.alpha_action[2],wl.proj,wl.proj_iter,0);\n  \n   wls=amalloc(wl.wls*sizeof(double),ALIGN);\n   error((wls==NULL),1,\"main [check2.c]\",\n          \"Unable to allocate auxiliary arrays\");\n      \n   message(\"Measure Wloops matrix\\n\");\n   \n   wloop_sum(0);\n      \n   for(ii=0; ii<wl.wls; ++ii)\n      wls[ii]=wl.wl_mat[ii];\n      \n   message(\"Perform gauge transformation\\n\");\n   \n   assign_lnk2ud(plnk);\n   random_g();\n   transform_ud();\n  \n   hyp_links(wl.alpha_action[0],wl.alpha_action[1],wl.alpha_action[2],wl.proj,wl.proj_iter,0);\n  \n   message(\"Measure Wloops matrix again\\n\");\n   \n   wloop_sum(0);\n      \n   status=-1;\n   eps=(double)(100)*sqrt((double)(wl.msl*wl.msl*wl.mwlr*wl.mwlt*3*NPROC)*(double)(VOLUME))*(double)(DBL_EPSILON);\n   dmax=(double)(DBL_EPSILON);\n   for (ii=0; ii<wl.wls; ++ii)\n   {\n      delta=fabs(1.0-wl.wl_mat[ii]/wls[ii]);\n      if (delta>=eps)\n      {\n         message(\"delta[%2d] = %.6e\\n\",ii,delta);\n         status+=2;\n      }\n      if (delta>dmax)\n         dmax=delta;\n   }\n   \n   if (status>0)\n      message(\"wloop routine is NOT gauge invariant!!\");\n   else   \n      message(\"wloops routine is gauge invariant (max. deviation: %.1e)!!\\n\\n\",dmax);\n\n   afree(wls);\n   afree(g);\n   afree(gbuf);\n   free_lnk(plnk,0);\n  \n   MPI_Finalize(); \n   exit(0);\n}"}
{"program": "MengbinZhu_1579", "code": "int main(int argc, char **argv)\n{\n    int mpi_size, mpi_rank;\t\t\t\t\n\n\n    \n\n    setbuf(stderr, NULL);\n    setbuf(stdout, NULL);\n\n\n    dim0 = ROW_FACTOR*mpi_size;\n    dim1 = COL_FACTOR*mpi_size;\n\n    if (MAINPROCESS){\n\tprintf(\"===================================\\n\");\n\tprintf(\"Shape Same Tests Start\\n\");\n        printf(\"\texpress_test = %d.\\n\", GetTestExpress());\n\tprintf(\"===================================\\n\");\n    }\n\n    \n\n    if (H5dont_atexit() < 0){\n\tprintf(\"%d: Failed to turn off atexit processing. Continue.\\n\", mpi_rank);\n    };\n    H5open();\n    h5_show_hostname();\n\n    \n\n    TestInit(argv[0], usage, parse_options);\n\n    \n\n#if 1\n    AddTest(\"sscontig1\", sscontig1, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig2\", sscontig2, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig3\", sscontig3, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sscontig4\", sscontig4, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, chunked datasets\", PARATESTFILE);\n#endif\n\n    \n\n    AddTest(\"sschecker1\", sschecker1, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker2\", sschecker2, NULL,\n\t\"Shape Same, checker hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker3\", sschecker3, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sschecker4\", sschecker4, NULL,\n\t\"Shape Same, checker hyperslab, col IO, chunked datasets\", PARATESTFILE);\n\n    \n\n    TestInfo(argv[0]);\n\n    \n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n\n    \n\n    TestParseCmdLine(argc, argv);\n\n    if (dxfer_coll_type == DXFER_INDEPENDENT_IO && MAINPROCESS){\n\tprintf(\"===================================\\n\"\n\t       \"   Using Independent I/O with file set view to replace collective I/O \\n\"\n\t       \"===================================\\n\");\n    }\n\n\n    \n\n    PerformTests();\n\n    \n\n\n    \n\n    if (MAINPROCESS && GetTestSummary())\n        TestSummary();\n\n    \n\n    h5_cleanup(FILENAME, fapl);\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    {\n        int temp;\n\tnerrors=temp;\n    }\n\n    if (MAINPROCESS){\t\t\n\n\tprintf(\"===================================\\n\");\n\tif (nerrors)\n\t    printf(\"***Shape Same tests detected %d errors***\\n\", nerrors);\n\telse\n\t    printf(\"Shape Same tests finished with no errors\\n\");\n\tprintf(\"===================================\\n\");\n    }\n    \n\n    H5close();\n\n    \n\n\n    \n\n    return(nerrors!=0);\n}", "label": "int main(int argc, char **argv)\n{\n    int mpi_size, mpi_rank;\t\t\t\t\n\n\n    \n\n    setbuf(stderr, NULL);\n    setbuf(stdout, NULL);\n\n    MPI_Init(&argc, &argv);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    dim0 = ROW_FACTOR*mpi_size;\n    dim1 = COL_FACTOR*mpi_size;\n\n    if (MAINPROCESS){\n\tprintf(\"===================================\\n\");\n\tprintf(\"Shape Same Tests Start\\n\");\n        printf(\"\texpress_test = %d.\\n\", GetTestExpress());\n\tprintf(\"===================================\\n\");\n    }\n\n    \n\n    if (H5dont_atexit() < 0){\n\tprintf(\"%d: Failed to turn off atexit processing. Continue.\\n\", mpi_rank);\n    };\n    H5open();\n    h5_show_hostname();\n\n    \n\n    TestInit(argv[0], usage, parse_options);\n\n    \n\n#if 1\n    AddTest(\"sscontig1\", sscontig1, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig2\", sscontig2, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sscontig3\", sscontig3, NULL,\n\t\"Shape Same, contigous hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sscontig4\", sscontig4, NULL,\n\t\"Shape Same, contigous hyperslab, col IO, chunked datasets\", PARATESTFILE);\n#endif\n\n    \n\n    AddTest(\"sschecker1\", sschecker1, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker2\", sschecker2, NULL,\n\t\"Shape Same, checker hyperslab, col IO, contig datasets\", PARATESTFILE);\n    AddTest(\"sschecker3\", sschecker3, NULL,\n\t\"Shape Same, checker hyperslab, ind IO, chunked datasets\", PARATESTFILE);\n    AddTest(\"sschecker4\", sschecker4, NULL,\n\t\"Shape Same, checker hyperslab, col IO, chunked datasets\", PARATESTFILE);\n\n    \n\n    TestInfo(argv[0]);\n\n    \n\n    fapl = H5Pcreate (H5P_FILE_ACCESS);\n    H5Pset_fapl_mpio(fapl, MPI_COMM_WORLD, MPI_INFO_NULL);\n\n    \n\n    TestParseCmdLine(argc, argv);\n\n    if (dxfer_coll_type == DXFER_INDEPENDENT_IO && MAINPROCESS){\n\tprintf(\"===================================\\n\"\n\t       \"   Using Independent I/O with file set view to replace collective I/O \\n\"\n\t       \"===================================\\n\");\n    }\n\n\n    \n\n    PerformTests();\n\n    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    \n\n    if (MAINPROCESS && GetTestSummary())\n        TestSummary();\n\n    \n\n    h5_cleanup(FILENAME, fapl);\n\n    nerrors += GetTestNumErrs();\n\n    \n\n    {\n        int temp;\n        MPI_Allreduce(&nerrors, &temp, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\tnerrors=temp;\n    }\n\n    if (MAINPROCESS){\t\t\n\n\tprintf(\"===================================\\n\");\n\tif (nerrors)\n\t    printf(\"***Shape Same tests detected %d errors***\\n\", nerrors);\n\telse\n\t    printf(\"Shape Same tests finished with no errors\\n\");\n\tprintf(\"===================================\\n\");\n    }\n    \n\n    H5close();\n\n    \n\n    MPI_Finalize();\n\n    \n\n    return(nerrors!=0);\n}"}
{"program": "cbries_1581", "code": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble A[DIMENSION] = {0};\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\n\t\n\n\n\t\n\n\n\t\n\n\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tA[i] = i + myrank;\n\t\tB[i] = -A[i];\n\t}\n\t\n\tprintf(\"Before the shift on processor %d A=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", A[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tprintf(\"Before the shift on processor %d B=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", B[i]);\n\t}\n\tprintf(\"\\n\");\n\n\ti = myrank - 1; \n\n\tj = myrank + 1; \n\n\t\n\tif(myrank == nprocs - 1) {\n\t\tj = 0;\n\t\ti = myrank - 1;\n\t}\n\n\tif(myrank == 0) {\n\t\tj = myrank + 1;\n\t\ti = nprocs - 1;\n\t}\n\t\n\n\tprintf(\"After the shift on processor %d A=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", A[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tprintf(\"After the shift on processor %d B=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", B[i]);\n\t}\n\tprintf(\"\\n\");\n\n\t\n\n\n\treturn 0;\n}\n", "label": "int main (int argc, char **argv)\n{\n\tint i, j;\n\tint myrank, nprocs;\n\tint namelen;\n\tchar name[MPI_MAX_PROCESSOR_NAME];\n\n\tdouble A[DIMENSION] = {0};\n\tdouble B[DIMENSION] = {0};\n\t\n\tMPI_Status status;\n\t\n\t\n \n\tMPI_Init(&argc, &argv);\n\n\t\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\t\n\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n\t\n\n\tMPI_Get_processor_name(name, &namelen);\n\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tA[i] = i + myrank;\n\t\tB[i] = -A[i];\n\t}\n\t\n\tprintf(\"Before the shift on processor %d A=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", A[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tprintf(\"Before the shift on processor %d B=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", B[i]);\n\t}\n\tprintf(\"\\n\");\n\n\ti = myrank - 1; \n\n\tj = myrank + 1; \n\n\t\n\tif(myrank == nprocs - 1) {\n\t\tj = 0;\n\t\ti = myrank - 1;\n\t}\n\n\tif(myrank == 0) {\n\t\tj = myrank + 1;\n\t\ti = nprocs - 1;\n\t}\n\t\n\tMPI_Sendrecv_replace(A, DIMENSION, MPI_DOUBLE, j, TAG, i, TAG, MPI_COMM_WORLD, &status);\n\tMPI_Sendrecv_replace(B, DIMENSION, MPI_DOUBLE, i, TAG, j, TAG, MPI_COMM_WORLD, &status);\n\n\tprintf(\"After the shift on processor %d A=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", A[i]);\n\t}\n\tprintf(\"\\n\");\n\n\tprintf(\"After the shift on processor %d B=\", myrank);\n\tfor(i=0; i<DIMENSION; i++) {\n\t\tprintf(\"%.2f    \", B[i]);\n\t}\n\tprintf(\"\\n\");\n\n\t\n\n\tMPI_Finalize();\n\n\treturn 0;\n}\n"}
{"program": "gnu3ra_1582", "code": "int main( int argc, char **argv )\n{\n    int              rank, size, i,j;\n    int              table[MAX_PROCESSES][MAX_PROCESSES];\n    int              errors=0;\n    int              participants;\n\n\n    \n\n    if ( size > MAX_PROCESSES ) participants = MAX_PROCESSES;\n    else              participants = size;\n\n    if (MAX_PROCESSES % participants) {\n\tfprintf( stderr, \"Number of processors must divide %d\\n\",\n\t\tMAX_PROCESSES );\n\t}\n    if ( (rank < participants) ) {\n\n      \n\n      int block_size = MAX_PROCESSES / participants;\n      int begin_row  = rank * block_size;\n      int end_row    = (rank+1) * block_size;\n      int send_count = block_size * MAX_PROCESSES;\n      int recv_count = send_count;\n\n      \n\n      for (i=begin_row; i<end_row ;i++)\n\tfor (j=0; j<MAX_PROCESSES; j++)\n\t  table[i][j] = rank + 10;\n\n      \n\n      \n\n      for (i=0; i<participants; i++) {\n        void *sendbuf = (i == rank ? MPI_IN_PLACE : &table[begin_row][0]);\n      }\n\n      \n\n      \n\n      \n\n      for (i=0; i<MAX_PROCESSES;i++) {\n\tif ( (table[i][0] - table[i][MAX_PROCESSES-1] !=0) ) \n\t  errors++;\n      }\n    } \n\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}", "label": "int main( int argc, char **argv )\n{\n    int              rank, size, i,j;\n    int              table[MAX_PROCESSES][MAX_PROCESSES];\n    int              errors=0;\n    int              participants;\n\n    MPI_Init( &argc, &argv );\n    MPI_Comm_rank( MPI_COMM_WORLD, &rank );\n    MPI_Comm_size( MPI_COMM_WORLD, &size );\n\n    \n\n    if ( size > MAX_PROCESSES ) participants = MAX_PROCESSES;\n    else              participants = size;\n\n    if (MAX_PROCESSES % participants) {\n\tfprintf( stderr, \"Number of processors must divide %d\\n\",\n\t\tMAX_PROCESSES );\n\tMPI_Abort( MPI_COMM_WORLD, 1 );\n\t}\n    if ( (rank < participants) ) {\n\n      \n\n      int block_size = MAX_PROCESSES / participants;\n      int begin_row  = rank * block_size;\n      int end_row    = (rank+1) * block_size;\n      int send_count = block_size * MAX_PROCESSES;\n      int recv_count = send_count;\n\n      \n\n      for (i=begin_row; i<end_row ;i++)\n\tfor (j=0; j<MAX_PROCESSES; j++)\n\t  table[i][j] = rank + 10;\n\n      \n\n      \n\n      for (i=0; i<participants; i++) {\n        void *sendbuf = (i == rank ? MPI_IN_PLACE : &table[begin_row][0]);\n\tMPI_Gather(sendbuf,              send_count, MPI_INT,\n\t\t   &table[0][0],         recv_count, MPI_INT, i, \n\t\t   MPI_COMM_WORLD );\n      }\n\n      \n\n      \n\n      \n\n      for (i=0; i<MAX_PROCESSES;i++) {\n\tif ( (table[i][0] - table[i][MAX_PROCESSES-1] !=0) ) \n\t  errors++;\n      }\n    } \n\n    MPI_Finalize();\n    if (errors)\n        printf( \"[%d] done with ERRORS(%d)!\\n\", rank, errors );\n    else {\n\tif (rank == 0) \n\t    printf(\" No Errors\\n\");\n    }\n    return errors;\n}"}
{"program": "nschloe_1584", "code": "int main(int argc, char *argv[])\n{\n  int rc, i, myRank, numProcs;\n  float ver;\n  struct Zoltan_Struct *zz;\n  int changes, numGidEntries, numLidEntries, numImport, numExport;\n  ZOLTAN_ID_PTR importGlobalGids, importLocalGids, exportGlobalGids, exportLocalGids; \n  int *importProcs, *importToPart, *exportProcs, *exportToPart;\n  int *parts;\n  FILE *fp;\n  MESH_DATA myMesh;\n\n  \n\n\n\n  rc = Zoltan_Initialize(argc, argv, &ver);\n\n  if (rc != ZOLTAN_OK){\n    printf(\"sorry...\\n\");\n    exit(0);\n  }\n\n  \n\n\n  fp = fopen(global_fname, \"r\");\n  if (!fp){\n    if (myRank == 0) fprintf(stderr,\"ERROR: Can not open %s\\n\",global_fname);\n    exit(1);\n  }\n  fclose(fp);\n\n  read_input_objects(myRank, numProcs, global_fname, &myMesh);\n\n  \n\n\n  zz = Zoltan_Create(MPI_COMM_WORLD);\n\n  \n\n\n  Zoltan_Set_Param(zz, \"DEBUG_LEVEL\", \"1\");\n  Zoltan_Set_Param(zz, \"LB_METHOD\", \"RCB\");\n  Zoltan_Set_Param(zz, \"NUM_GID_ENTRIES\", \"1\"); \n  Zoltan_Set_Param(zz, \"NUM_LID_ENTRIES\", \"1\");\n  Zoltan_Set_Param(zz, \"OBJ_WEIGHT_DIM\", \"0\");\n  Zoltan_Set_Param(zz, \"RETURN_LISTS\", \"ALL\");\n\n  \n\n\n  Zoltan_Set_Param(zz, \"RCB_OUTPUT_LEVEL\", \"0\");\n  Zoltan_Set_Param(zz, \"RCB_RECTILINEAR_BLOCKS\", \"1\"); \n  \n\n\n  \n\n\n  Zoltan_Set_Num_Obj_Fn(zz, get_number_of_objects, &myMesh);\n  Zoltan_Set_Obj_List_Fn(zz, get_object_list, &myMesh);\n  Zoltan_Set_Num_Geom_Fn(zz, get_num_geometry, &myMesh);\n  Zoltan_Set_Geom_Multi_Fn(zz, get_geometry_list, &myMesh);\n\n  \n\n  rc = Zoltan_LB_Partition(zz, \n\n        &changes,        \n \n        &numGidEntries,  \n\n        &numLidEntries,  \n\n        &numImport,      \n\n        &importGlobalGids,  \n\n        &importLocalGids,   \n\n        &importProcs,    \n\n        &importToPart,   \n\n        &numExport,      \n\n        &exportGlobalGids,  \n\n        &exportLocalGids,   \n\n        &exportProcs,    \n\n        &exportToPart);  \n\n\n  if (rc != ZOLTAN_OK){\n    printf(\"sorry...\\n\");\n    Zoltan_Destroy(&zz);\n    exit(0);\n  }\n\n  \n\n\n  parts = (int *)malloc(sizeof(int) * myMesh.numMyPoints);\n\n  for (i=0; i < myMesh.numMyPoints; i++){\n    parts[i] = myRank;\n  }\n\n  if (myRank== 0){\n    printf(\"\\nMesh partition assignments before calling Zoltan\\n\");\n  }\n\n  showSimpleMeshPartitions(myRank, myMesh.numMyPoints, myMesh.myGlobalIDs, parts);\n\n  for (i=0; i < numExport; i++){\n    parts[exportLocalGids[i]] = exportToPart[i];\n  }\n\n  if (myRank == 0){\n    printf(\"Mesh partition assignments after calling Zoltan\\n\");\n  }\n\n  showSimpleMeshPartitions(myRank, myMesh.numMyPoints, myMesh.myGlobalIDs, parts);\n\n  free(parts);\n\n  \n\n\n  Zoltan_LB_Free_Part(&importGlobalGids, &importLocalGids, \n                      &importProcs, &importToPart);\n  Zoltan_LB_Free_Part(&exportGlobalGids, &exportLocalGids, \n                      &exportProcs, &exportToPart);\n\n  Zoltan_Destroy(&zz);\n\n  \n\n\n\n  if (myMesh.numMyPoints > 0){\n    free(myMesh.myGlobalIDs);\n    free(myMesh.x);\n    free(myMesh.y);\n  }\n\n  return 0;\n}", "label": "int main(int argc, char *argv[])\n{\n  int rc, i, myRank, numProcs;\n  float ver;\n  struct Zoltan_Struct *zz;\n  int changes, numGidEntries, numLidEntries, numImport, numExport;\n  ZOLTAN_ID_PTR importGlobalGids, importLocalGids, exportGlobalGids, exportLocalGids; \n  int *importProcs, *importToPart, *exportProcs, *exportToPart;\n  int *parts;\n  FILE *fp;\n  MESH_DATA myMesh;\n\n  \n\n\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n  rc = Zoltan_Initialize(argc, argv, &ver);\n\n  if (rc != ZOLTAN_OK){\n    printf(\"sorry...\\n\");\n    MPI_Finalize();\n    exit(0);\n  }\n\n  \n\n\n  fp = fopen(global_fname, \"r\");\n  if (!fp){\n    if (myRank == 0) fprintf(stderr,\"ERROR: Can not open %s\\n\",global_fname);\n    MPI_Finalize();\n    exit(1);\n  }\n  fclose(fp);\n\n  read_input_objects(myRank, numProcs, global_fname, &myMesh);\n\n  \n\n\n  zz = Zoltan_Create(MPI_COMM_WORLD);\n\n  \n\n\n  Zoltan_Set_Param(zz, \"DEBUG_LEVEL\", \"1\");\n  Zoltan_Set_Param(zz, \"LB_METHOD\", \"RCB\");\n  Zoltan_Set_Param(zz, \"NUM_GID_ENTRIES\", \"1\"); \n  Zoltan_Set_Param(zz, \"NUM_LID_ENTRIES\", \"1\");\n  Zoltan_Set_Param(zz, \"OBJ_WEIGHT_DIM\", \"0\");\n  Zoltan_Set_Param(zz, \"RETURN_LISTS\", \"ALL\");\n\n  \n\n\n  Zoltan_Set_Param(zz, \"RCB_OUTPUT_LEVEL\", \"0\");\n  Zoltan_Set_Param(zz, \"RCB_RECTILINEAR_BLOCKS\", \"1\"); \n  \n\n\n  \n\n\n  Zoltan_Set_Num_Obj_Fn(zz, get_number_of_objects, &myMesh);\n  Zoltan_Set_Obj_List_Fn(zz, get_object_list, &myMesh);\n  Zoltan_Set_Num_Geom_Fn(zz, get_num_geometry, &myMesh);\n  Zoltan_Set_Geom_Multi_Fn(zz, get_geometry_list, &myMesh);\n\n  \n\n  rc = Zoltan_LB_Partition(zz, \n\n        &changes,        \n \n        &numGidEntries,  \n\n        &numLidEntries,  \n\n        &numImport,      \n\n        &importGlobalGids,  \n\n        &importLocalGids,   \n\n        &importProcs,    \n\n        &importToPart,   \n\n        &numExport,      \n\n        &exportGlobalGids,  \n\n        &exportLocalGids,   \n\n        &exportProcs,    \n\n        &exportToPart);  \n\n\n  if (rc != ZOLTAN_OK){\n    printf(\"sorry...\\n\");\n    MPI_Finalize();\n    Zoltan_Destroy(&zz);\n    exit(0);\n  }\n\n  \n\n\n  parts = (int *)malloc(sizeof(int) * myMesh.numMyPoints);\n\n  for (i=0; i < myMesh.numMyPoints; i++){\n    parts[i] = myRank;\n  }\n\n  if (myRank== 0){\n    printf(\"\\nMesh partition assignments before calling Zoltan\\n\");\n  }\n\n  showSimpleMeshPartitions(myRank, myMesh.numMyPoints, myMesh.myGlobalIDs, parts);\n\n  for (i=0; i < numExport; i++){\n    parts[exportLocalGids[i]] = exportToPart[i];\n  }\n\n  if (myRank == 0){\n    printf(\"Mesh partition assignments after calling Zoltan\\n\");\n  }\n\n  showSimpleMeshPartitions(myRank, myMesh.numMyPoints, myMesh.myGlobalIDs, parts);\n\n  free(parts);\n\n  \n\n\n  Zoltan_LB_Free_Part(&importGlobalGids, &importLocalGids, \n                      &importProcs, &importToPart);\n  Zoltan_LB_Free_Part(&exportGlobalGids, &exportLocalGids, \n                      &exportProcs, &exportToPart);\n\n  Zoltan_Destroy(&zz);\n\n  \n\n\n  MPI_Finalize();\n\n  if (myMesh.numMyPoints > 0){\n    free(myMesh.myGlobalIDs);\n    free(myMesh.x);\n    free(myMesh.y);\n  }\n\n  return 0;\n}"}
{"program": "gyaikhom_1585", "code": "int\nmain (int argc, char *argv[])\n{\n    bc_init(BC_ERR);\n       \n    if (bc_rank == 0) {\n        printf(\"Matric A:\\n\");\n        display_matrix(NROWS_A, NCOLS_A, (int *)matrix_a);        \n        printf(\"\\nMatric B:\\n\");\n        display_matrix(NROWS_B, NCOLS_B, (int *)matrix_b);\n    }\n    \n    multiply_matrices(NROWS_A, NCOLS_A, matrix_a, NROWS_B, NCOLS_B, matrix_b);\n    \n    bc_final();\n    return 0;\n}", "label": "int\nmain (int argc, char *argv[])\n{\n    MPI_Init(&argc, &argv);\n    bc_init(BC_ERR);\n       \n    if (bc_rank == 0) {\n        printf(\"Matric A:\\n\");\n        display_matrix(NROWS_A, NCOLS_A, (int *)matrix_a);        \n        printf(\"\\nMatric B:\\n\");\n        display_matrix(NROWS_B, NCOLS_B, (int *)matrix_b);\n    }\n    \n    multiply_matrices(NROWS_A, NCOLS_A, matrix_a, NROWS_B, NCOLS_B, matrix_b);\n    \n    bc_final();\n    MPI_Finalize();\n    return 0;\n}"}
{"program": "bmi-forum_1586", "code": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\t\n\tDictionary\t*dictionary;\n\tMeshTopology\t*pmt;\n\tMeshGeometry\t*mg, *pmg;\n\tMeshDecomp\t*pmd;\n\tMeshLayout\t*pml;\n\tCoord\t\tcoord;\n\tNode_Index\tsizeI, sizeJ, sizeK;\n\tIndex\t\ti;\n\t\n\t\n\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\n\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\t\n\tfor (sizeK = 1; sizeK < 4; sizeK++) {\n\t\tfor (sizeJ = 1; sizeJ < 4; sizeJ++) {\n\t\t\tfor (sizeI = 1; sizeI < 4; sizeI++) {\n\t\t\t\tDictionary_Set( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( sizeI ) );\n\t\t\t\tDictionary_Set( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( sizeJ ) );\n\t\t\t\tDictionary_Set( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( sizeK ) );\n\t\t\t\t\n\t\t\t\tpmt = (MeshTopology*)HexaMeshTopology_New(dictionary);\n\t\t\t\tpmg = (MeshGeometry*)HexaMeshGeometry_New(dictionary);\n\t\t\t\tpmd = (MeshDecomp*)RegularMeshDecomp_New(dictionary, MPI_COMM_WORLD, pmt);\n\t\t\t\tpml = (MeshLayout*)MeshLayout_New(pmt, pmg, pmd);\n\t\t\t\t\n\t\t\t\tmg = (MeshGeometry *)SingleCellMeshGeometry_New(pml);\n\t\t\t\t\n\t\t\t\tPrint(mg);\n\t\n\t\t\t\tfor (i = 0; i < mg->nodeCount; i++)\n\t\t\t\t{\n\t\t\t\t\tprintf(\"Node %u : \", i);\n\t\t\t\t\tmg->nodeAt(mg, i, coord);\n\t\t\t\t\tprintf(\"{%lf, %lf, %lf}\\n\", coord[0], coord[1], coord[2]);\n\t\t\t\t}\n\t\t\t\tprintf(\"\\n\");\n\t\t\t\t\n\t\t\t\tStg_Class_Delete(mg);\n\t\t\t\tStg_Class_Delete(pml);\n\t\t\t\tStg_Class_Delete(pmd);\n\t\t\t\tStg_Class_Delete(pmg);\n\t\t\t\tStg_Class_Delete(pmt);\n\t\t\t}\n\t\t}\n\t}\n\t\n\t\n\n\tStg_Class_Delete(dictionary);\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\n\treturn 0; \n\n}", "label": "int main( int argc, char *argv[] ) {\n\tint\t\trank;\n\tint\t\tprocCount;\n\tint\t\tprocToWatch;\n\t\n\tDictionary\t*dictionary;\n\tMeshTopology\t*pmt;\n\tMeshGeometry\t*mg, *pmg;\n\tMeshDecomp\t*pmd;\n\tMeshLayout\t*pml;\n\tCoord\t\tcoord;\n\tNode_Index\tsizeI, sizeJ, sizeK;\n\tIndex\t\ti;\n\t\n\t\n\n\tMPI_Init(&argc, &argv);\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tBase_Init( &argc, &argv );\n\t\n\tDiscretisationGeometry_Init( &argc, &argv );\n\tDiscretisationShape_Init( &argc, &argv );\n\tDiscretisationMesh_Init( &argc, &argv );\n\tMPI_Barrier( CommWorld ); \n\n\n\tprocToWatch = argc >= 2 ? atoi(argv[1]) : 0;\n\t\n\t\n\n\tdictionary = Dictionary_New();\n\tDictionary_Add( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\tDictionary_Add( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( 1 ) );\n\t\n\tfor (sizeK = 1; sizeK < 4; sizeK++) {\n\t\tfor (sizeJ = 1; sizeJ < 4; sizeJ++) {\n\t\t\tfor (sizeI = 1; sizeI < 4; sizeI++) {\n\t\t\t\tDictionary_Set( dictionary, \"meshSizeI\", Dictionary_Entry_Value_FromUnsignedInt( sizeI ) );\n\t\t\t\tDictionary_Set( dictionary, \"meshSizeJ\", Dictionary_Entry_Value_FromUnsignedInt( sizeJ ) );\n\t\t\t\tDictionary_Set( dictionary, \"meshSizeK\", Dictionary_Entry_Value_FromUnsignedInt( sizeK ) );\n\t\t\t\t\n\t\t\t\tpmt = (MeshTopology*)HexaMeshTopology_New(dictionary);\n\t\t\t\tpmg = (MeshGeometry*)HexaMeshGeometry_New(dictionary);\n\t\t\t\tpmd = (MeshDecomp*)RegularMeshDecomp_New(dictionary, MPI_COMM_WORLD, pmt);\n\t\t\t\tpml = (MeshLayout*)MeshLayout_New(pmt, pmg, pmd);\n\t\t\t\t\n\t\t\t\tmg = (MeshGeometry *)SingleCellMeshGeometry_New(pml);\n\t\t\t\t\n\t\t\t\tPrint(mg);\n\t\n\t\t\t\tfor (i = 0; i < mg->nodeCount; i++)\n\t\t\t\t{\n\t\t\t\t\tprintf(\"Node %u : \", i);\n\t\t\t\t\tmg->nodeAt(mg, i, coord);\n\t\t\t\t\tprintf(\"{%lf, %lf, %lf}\\n\", coord[0], coord[1], coord[2]);\n\t\t\t\t}\n\t\t\t\tprintf(\"\\n\");\n\t\t\t\t\n\t\t\t\tStg_Class_Delete(mg);\n\t\t\t\tStg_Class_Delete(pml);\n\t\t\t\tStg_Class_Delete(pmd);\n\t\t\t\tStg_Class_Delete(pmg);\n\t\t\t\tStg_Class_Delete(pmt);\n\t\t\t}\n\t\t}\n\t}\n\t\n\t\n\n\tStg_Class_Delete(dictionary);\n\t\n\tDiscretisationMesh_Finalise();\n\tDiscretisationShape_Finalise();\n\tDiscretisationGeometry_Finalise();\n\t\n\tBase_Finalise();\n\t\n\t\n\n\tMPI_Finalize();\n\n\treturn 0; \n\n}"}
{"program": "qingu_1587", "code": "int main(int argc, char **argv) {\n    int      i, j, rank, nproc;\n    int      shm_rank, shm_nproc;\n    MPI_Aint size;\n    int      errors = 0, all_errors = 0;\n    int     *base, *my_base;\n    int      disp_unit;\n    MPI_Win  shm_win;\n    MPI_Comm shm_comm;\n\n\n\n\n\n    \n\n\n    \n\n\n    if (verbose) printf(\"%d -- size = %d baseptr = %p my_baseptr = %p\\n\", shm_rank, \n                        (int) size, (void*) base, (void*) my_base);\n\n    assert(size == ELEM_PER_PROC * sizeof(int));\n    if (shm_rank == 0)\n        assert(base == my_base);\n    else\n        assert(base != my_base);\n\n\n    \n\n    for (i = 0; i < ELEM_PER_PROC; i++) {\n        my_base[i] = i;\n    }\n\n\n    \n\n    for (i = 0; i < shm_nproc; i++) {\n        for (j = 0; j < ELEM_PER_PROC; j++) {\n            if ( base[i*ELEM_PER_PROC + j] != j ) {\n                errors++;\n                printf(\"%d -- Got %d at rank %d index %d, expected %d\\n\", shm_rank,\n                       base[i*ELEM_PER_PROC + j], i, j, j);\n            }\n        }\n    }\n\n\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n\n    return 0;\n}", "label": "int main(int argc, char **argv) {\n    int      i, j, rank, nproc;\n    int      shm_rank, shm_nproc;\n    MPI_Aint size;\n    int      errors = 0, all_errors = 0;\n    int     *base, *my_base;\n    int      disp_unit;\n    MPI_Win  shm_win;\n    MPI_Comm shm_comm;\n\n    MPI_Init(&argc, &argv);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);\n\n    MPI_Comm_rank(shm_comm, &shm_rank);\n    MPI_Comm_size(shm_comm, &shm_nproc);\n\n    \n\n    MPI_Win_allocate_shared(sizeof(int)*ELEM_PER_PROC, sizeof(int), MPI_INFO_NULL, \n                             shm_comm, &my_base, &shm_win);\n\n    \n\n    MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base); \n\n    if (verbose) printf(\"%d -- size = %d baseptr = %p my_baseptr = %p\\n\", shm_rank, \n                        (int) size, (void*) base, (void*) my_base);\n\n    assert(size == ELEM_PER_PROC * sizeof(int));\n    if (shm_rank == 0)\n        assert(base == my_base);\n    else\n        assert(base != my_base);\n\n    MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);\n\n    \n\n    for (i = 0; i < ELEM_PER_PROC; i++) {\n        my_base[i] = i;\n    }\n\n    MPI_Win_sync(shm_win);\n    MPI_Barrier(shm_comm);\n    MPI_Win_sync(shm_win);\n\n    \n\n    for (i = 0; i < shm_nproc; i++) {\n        for (j = 0; j < ELEM_PER_PROC; j++) {\n            if ( base[i*ELEM_PER_PROC + j] != j ) {\n                errors++;\n                printf(\"%d -- Got %d at rank %d index %d, expected %d\\n\", shm_rank,\n                       base[i*ELEM_PER_PROC + j], i, j, j);\n            }\n        }\n    }\n\n    MPI_Win_unlock_all(shm_win);\n    MPI_Win_free(&shm_win);\n    MPI_Comm_free(&shm_comm);\n\n    MPI_Reduce(&errors, &all_errors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0 && all_errors == 0)\n        printf(\" No Errors\\n\");\n\n    MPI_Finalize();\n\n    return 0;\n}"}
{"program": "mpip_1588", "code": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = in;\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  ptrdiff_t l;\n  for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  pfft_free(in);\n  return 0;\n}", "label": "int main(int argc, char **argv)\n{\n  int np[2];\n  ptrdiff_t n[3];\n  ptrdiff_t alloc_local;\n  ptrdiff_t local_ni[3], local_i_start[3];\n  ptrdiff_t local_no[3], local_o_start[3];\n  double err;\n  pfft_complex *in, *out;\n  pfft_plan plan_forw=NULL, plan_back=NULL;\n  MPI_Comm comm_cart_2d;\n  \n  \n\n  n[0] = 29; n[1] = 27; n[2] = 31;\n  np[0] = 2; np[1] = 2;\n  \n  \n\n  MPI_Init(&argc, &argv);\n  pfft_init();\n\n  \n\n  if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1], &comm_cart_2d) ){\n    pfft_fprintf(MPI_COMM_WORLD, stderr, \"Error: This test file only works with %d processes.\\n\", np[0]*np[1]);\n    MPI_Finalize();\n    return 1;\n  }\n  \n  \n\n  alloc_local = pfft_local_size_dft_3d(n, comm_cart_2d, PFFT_TRANSPOSED_NONE,\n      local_ni, local_i_start, local_no, local_o_start);\n\n  \n\n  in  = pfft_alloc_complex(alloc_local);\n  out = in;\n\n  \n\n  plan_forw = pfft_plan_dft_3d(\n      n, in, out, comm_cart_2d, PFFT_FORWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n  \n  \n\n  plan_back = pfft_plan_dft_3d(\n      n, out, in, comm_cart_2d, PFFT_BACKWARD, PFFT_TRANSPOSED_NONE| PFFT_MEASURE| PFFT_DESTROY_INPUT);\n\n  \n\n  pfft_init_input_complex_3d(n, local_ni, local_i_start,\n      in);\n  \n  \n\n  pfft_execute(plan_forw);\n  \n  \n\n  pfft_execute(plan_back);\n  \n  \n\n  ptrdiff_t l;\n  for(l=0; l < local_ni[0] * local_ni[1] * local_ni[2]; l++)\n    in[l] /= (n[0]*n[1]*n[2]);\n\n  \n\n  err = pfft_check_output_complex_3d(n, local_ni, local_i_start, in, comm_cart_2d);\n  pfft_printf(comm_cart_2d, \"Error after one forward and backward trafo of size n=(%td, %td, %td):\\n\", n[0], n[1], n[2]); \n  pfft_printf(comm_cart_2d, \"maxerror = %6.2e;\\n\", err);\n  \n  \n\n  pfft_destroy_plan(plan_forw);\n  pfft_destroy_plan(plan_back);\n  MPI_Comm_free(&comm_cart_2d);\n  pfft_free(in);\n  MPI_Finalize();\n  return 0;\n}"}
{"program": "cc-hpc-itwm_1590", "code": "int main (int argc, char *argv[])\n{\n\n  \n  int nProc, iProc;\n\n\n  \n\n  const int NTHREADS = 6;\n\n  \n\n  const int NWAY     = 2;\n\n  \n\n  const int left  = LEFT(iProc, nProc);\n\n  \n\n  const int right = RIGHT(iProc, nProc);\n\n  \n\n  double* array = malloc (NWAY * (NTHREADS+2) * 2 * VLEN * sizeof (double));\n  ASSERT (array != 0);\n\n  \n\n  int buffer_id = 0;\n\n  \n\n  data_init (NTHREADS, iProc, buffer_id, array);\n  \n  omp_set_num_threads (NTHREADS);\n\n\n  double time = -now();\n\n#pragma omp parallel default (shared) firstprivate (buffer_id)\n  {\n    const int tid = omp_get_thread_num();\n\n    for (int k = 0; k < NITER; ++k)\n    {\n      for ( int i = 0; i < nProc * NTHREADS; ++i )\n      {\n\n\tconst int slice_id    = tid + 1;\n\tconst int left_halo   = 0;\n\tconst int right_halo  = NTHREADS+1;\n\n\tif (tid == 0)\n\t  {\n\n\t    MPI_Request send_req[2];\n\t    MPI_Request recv_req[2];\n\n\t    \n\n\n\t    \n\n\n\t    \n\n\n\t    \n\n\n\t    \n\n\t    \n\n\t    \n\n\n\t  }\n\n#pragma omp barrier\n\n\t\n\n\tdata_compute (NTHREADS, array, 1 - buffer_id, buffer_id, slice_id);\n\n#pragma omp barrier\n\n\t\n\n\tbuffer_id = 1 - buffer_id;\n\n      }\n    }\n  }\n  time += now();\n\n  data_verify (NTHREADS, iProc, ( NITER * nProc * NTHREADS ) % NWAY, array);\n\n  printf (\"# mpi %s nProc %d vlen %i niter %d nthreads %i nway %i time %g\\n\"\n         , argv[0], nProc, VLEN, NITER, NTHREADS, NWAY, time\n         );\n  \n\n  free (array);\n\n  return EXIT_SUCCESS;\n}", "label": "int main (int argc, char *argv[])\n{\n\n  MPI_Init (&argc, &argv);\n  \n  int nProc, iProc;\n\n  MPI_Comm_rank (MPI_COMM_WORLD, &iProc);\n  MPI_Comm_size (MPI_COMM_WORLD, &nProc);\n\n  \n\n  const int NTHREADS = 6;\n\n  \n\n  const int NWAY     = 2;\n\n  \n\n  const int left  = LEFT(iProc, nProc);\n\n  \n\n  const int right = RIGHT(iProc, nProc);\n\n  \n\n  double* array = malloc (NWAY * (NTHREADS+2) * 2 * VLEN * sizeof (double));\n  ASSERT (array != 0);\n\n  \n\n  int buffer_id = 0;\n\n  \n\n  data_init (NTHREADS, iProc, buffer_id, array);\n  \n  omp_set_num_threads (NTHREADS);\n\n  MPI_Barrier (MPI_COMM_WORLD);\n\n  double time = -now();\n\n#pragma omp parallel default (shared) firstprivate (buffer_id)\n  {\n    const int tid = omp_get_thread_num();\n\n    for (int k = 0; k < NITER; ++k)\n    {\n      for ( int i = 0; i < nProc * NTHREADS; ++i )\n      {\n\n\tconst int slice_id    = tid + 1;\n\tconst int left_halo   = 0;\n\tconst int right_halo  = NTHREADS+1;\n\n\tif (tid == 0)\n\t  {\n\n\t    MPI_Request send_req[2];\n\t    MPI_Request recv_req[2];\n\n\t    \n\n\t    MPI_Irecv ( &array_ELEM_right (buffer_id, left_halo, 0), VLEN, MPI_DOUBLE\n\t\t       , left, i, MPI_COMM_WORLD, &recv_req[0]);\n\n\t    \n\n\t    MPI_Irecv ( &array_ELEM_left (buffer_id, right_halo, 0), VLEN, MPI_DOUBLE\n\t\t       , right, i, MPI_COMM_WORLD, &recv_req[1]);\n\n\t    \n\n\t    MPI_Isend ( &array_ELEM_right (buffer_id, right_halo - 1, 0), VLEN, MPI_DOUBLE\n\t\t\t , right, i, MPI_COMM_WORLD, &send_req[0]);\n\n\t    \n\n\t    MPI_Isend ( &array_ELEM_left (buffer_id, left_halo + 1, 0), VLEN, MPI_DOUBLE\n\t\t\t , left, i, MPI_COMM_WORLD, &send_req[1]);\n\n\t    \n\n\t    MPI_Request_free(&send_req[0]);\n\t    \n\t    MPI_Request_free(&send_req[1]);\n\n\t    \n\n\t    MPI_Waitall (2, recv_req, MPI_STATUSES_IGNORE);\n\n\t  }\n\n#pragma omp barrier\n\n\t\n\n\tdata_compute (NTHREADS, array, 1 - buffer_id, buffer_id, slice_id);\n\n#pragma omp barrier\n\n\t\n\n\tbuffer_id = 1 - buffer_id;\n\n      }\n    }\n  }\n  time += now();\n\n  data_verify (NTHREADS, iProc, ( NITER * nProc * NTHREADS ) % NWAY, array);\n\n  printf (\"# mpi %s nProc %d vlen %i niter %d nthreads %i nway %i time %g\\n\"\n         , argv[0], nProc, VLEN, NITER, NTHREADS, NWAY, time\n         );\n  \n  MPI_Finalize();\n\n  free (array);\n\n  return EXIT_SUCCESS;\n}"}
{"program": "syftalent_1591", "code": "int\nmain(\n    int\t\t\t\t\targc,\n    char *\t\t\t\targv[])\n{\n    int\t\t\t\t\tnp;\n    int\t\t\t\t\tmy_rank;\n    int\t\t\t\t\tneighbor_rank;\n    MPI_Win\t\t\t\twin;\n    double\t\t\t\tmy_data;\n    double\t\t\t\tneighbor_data;\n    \n\n    neighbor_rank = (my_rank + 1) % np;\n    \n    if (np < 2)\n    {\n\tif ( my_rank == 0)\n\t{\n\t    printf(\"\\nERROR: fence_put_simple must be at least (2) \"\n\t\t   \"processes\\n\\n\");\n\t}\n\t\n\treturn 1;\n    }\n\n    \n\t\n    {\n\tneighbor_data = 42.0 * neighbor_rank;\n    }\n\t\n    if (my_data == 42.0 * my_rank)\n    {\n\tprintf(\"%d: data[%d]=%f\\n\", my_rank, my_rank, my_data);\n    }\n    else\n    {\n\tprintf(\"%d: ERROR - data[%d]=%f NOT %f\\n\",\n\t       my_rank, my_rank, my_data, my_rank * 42.0);\n    }\n\t\n\n    \n    return 0;\n}", "label": "int\nmain(\n    int\t\t\t\t\targc,\n    char *\t\t\t\targv[])\n{\n    int\t\t\t\t\tnp;\n    int\t\t\t\t\tmy_rank;\n    int\t\t\t\t\tneighbor_rank;\n    MPI_Win\t\t\t\twin;\n    double\t\t\t\tmy_data;\n    double\t\t\t\tneighbor_data;\n    \n    MPI_Init(NULL, NULL);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &np);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    neighbor_rank = (my_rank + 1) % np;\n    \n    if (np < 2)\n    {\n\tif ( my_rank == 0)\n\t{\n\t    printf(\"\\nERROR: fence_put_simple must be at least (2) \"\n\t\t   \"processes\\n\\n\");\n\t}\n\t\n\tMPI_Finalize();\n\treturn 1;\n    }\n\n    MPI_Win_create(&my_data, sizeof(double), sizeof(double), MPI_INFO_NULL,\n\t\t   MPI_COMM_WORLD, &win);\n    \n\t\n    MPI_Win_fence(MPI_MODE_NOSTORE | MPI_MODE_NOPRECEDE, win);\n    {\n\tneighbor_data = 42.0 * neighbor_rank;\n\tMPI_Put(&neighbor_data, 1, MPI_DOUBLE,\n\t\tneighbor_rank, 0, 1, MPI_DOUBLE, win);\n    }\n    MPI_Win_fence(MPI_MODE_NOSTORE | MPI_MODE_NOSUCCEED, win);\n\t\n    if (my_data == 42.0 * my_rank)\n    {\n\tprintf(\"%d: data[%d]=%f\\n\", my_rank, my_rank, my_data);\n    }\n    else\n    {\n\tprintf(\"%d: ERROR - data[%d]=%f NOT %f\\n\",\n\t       my_rank, my_rank, my_data, my_rank * 42.0);\n    }\n\t\n    MPI_Win_free(&win);\n\n    MPI_Finalize();\n    \n    return 0;\n}"}
